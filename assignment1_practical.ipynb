{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "mount_file_id": "1lAy_YT2kZxUVJvk7umNcXW3_Ded87R2i",
      "authorship_tag": "ABX9TyOeqxEEwILJczy2HVXTs3LD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitkrieg/dl-assignment-1/blob/main/assignment1_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1: DL Basics\n",
        "\n",
        "### Goal\n",
        "Implement [LeNet5](https://arxiv.org/pdf/1502.03167v3) and compare various regularization techniques on the network using the FashionMNSIT dataset."
      ],
      "metadata": {
        "id": "OQ56fD3aY8na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Imports"
      ],
      "metadata": {
        "id": "m912ceG9Z9X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhKGXWcCpynr",
        "outputId": "8212d56d-a809-406e-8622-2926d59da033"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.9)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmitkrieger\u001b[0m (\u001b[33mmitkrieger-cornell-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "7roLS_owoJPv",
        "outputId": "84ede7b1-b21c-436a-e432-85168cddd11d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmitkrieger\u001b[0m (\u001b[33mmitkrieger-cornell-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240910_041208-awqknwyv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/DL_Assignment_1/runs/awqknwyv' target=\"_blank\">major-darkness-5</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/DL_Assignment_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/DL_Assignment_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/DL_Assignment_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/DL_Assignment_1/runs/awqknwyv' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/DL_Assignment_1/runs/awqknwyv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mitkrieger-cornell-university/DL_Assignment_1/runs/awqknwyv?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7eae95bca680>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import gzip\n",
        "import typing as T\n",
        "import wandb\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "torch.manual_seed(123)\n",
        "\n",
        "wandb.init(project='DL_Assignment_1')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for GPU Access"
      ],
      "metadata": {
        "id": "Vi805ZxpaFyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"------ ACCELERATION INFO -----\")\n",
        "print('GPU Available:',torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('GPU Name:',torch.cuda.get_device_name(0))\n",
        "  print('GPU Count:',torch.cuda.device_count())\n",
        "  print('GPU Memory Allocated:',torch.cuda.memory_allocated(0))\n",
        "  print('GPU Memory Cached:',torch.cuda.memory_reserved(0))\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print('Using CPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwJwFQG4R8wy",
        "outputId": "c103cb24-ecbe-40ce-8ce5-58845cfda5dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ ACCELERATION INFO -----\n",
            "GPU Available: False\n",
            "Using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data using Pytorch"
      ],
      "metadata": {
        "id": "yWsWErVcSRIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PATH = '/content/drive/MyDrive/Fall 2024/Deep Learning/Assignment 1/data'\n"
      ],
      "metadata": {
        "id": "kGjJiU0lDHzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define FashionMNIST"
      ],
      "metadata": {
        "id": "8ji077ZjaQNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FasionMNISTDataset(Dataset):\n",
        "  def __init__(self, path: str, kind: str, transform=None, target_transform=None, device=None) -> None:\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.device = device\n",
        "    self.labels, self.images = self._load_data(path, kind)\n",
        "\n",
        "  def _load_data(self, path: str, kind: str) -> T.Tuple[np.ndarray, np.ndarray]:\n",
        "    with gzip.open(path + f'/{kind}-labels-idx1-ubyte.gz', 'rb') as lable_file:\n",
        "      lbls = np.frombuffer(lable_file.read(), dtype=np.int8, offset=8)\n",
        "      lbls = np.copy(lbls)\n",
        "    with gzip.open(path + f'/{kind}-images-idx3-ubyte.gz', 'rb') as lable_file:\n",
        "      imgs = np.frombuffer(lable_file.read(), dtype=np.uint8, offset=16).reshape(len(lbls), 1, 28, 28)\n",
        "      imgs = (np.copy(imgs) / 255).astype(np.float32)\n",
        "    return lbls, imgs\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return self.labels.size\n",
        "\n",
        "  def __getitem__(self, index: int) -> T.Tuple[torch.tensor, torch.tensor]:\n",
        "    label = torch.tensor(self.labels[index], dtype=torch.long)\n",
        "    img = torch.tensor(self.images[index])\n",
        "    if self.device:\n",
        "      img = img.to(self.device)\n",
        "      label = label.to(self.device)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "\n",
        "    return img, label\n",
        "\n",
        "def show_img(dataset: Dataset, index: int) -> None:\n",
        "  img, label = dataset[index]\n",
        "  labels_map = {\n",
        "            0: \"T-Shirt\",\n",
        "            1: \"Trouser\",\n",
        "            2: \"Pullover\",\n",
        "            3: \"Dress\",\n",
        "            4: \"Coat\",\n",
        "            5: \"Sandal\",\n",
        "            6: \"Shirt\",\n",
        "            7: \"Sneaker\",\n",
        "            8: \"Bag\",\n",
        "            9: \"Ankle Boot\",\n",
        "        }\n",
        "  plt.imshow(img.cpu().reshape(28,28), cmap='gray')\n",
        "  plt.title(labels_map[label.cpu().item()])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "1gxeQbXyRrqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Train, Validation and Test sets with loaders"
      ],
      "metadata": {
        "id": "hHx4kcRPabF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen = torch.Generator().manual_seed(123)\n",
        "\n",
        "train = FasionMNISTDataset(PATH, 'train', device=device)\n",
        "train, val = torch.utils.data.random_split(train, [0.8, 0.2], generator=gen)\n",
        "test = FasionMNISTDataset(PATH, 'test', device=device)\n",
        "\n",
        "batch = 128\n",
        "trainloader = DataLoader(train, batch, shuffle=True, generator=gen)\n",
        "valloader = DataLoader(val, batch, shuffle=True, generator=gen)\n",
        "testloader = DataLoader(test, batch, shuffle=True, generator=gen)\n",
        "\n",
        "dataloaders = {\n",
        "    'train': trainloader,\n",
        "    'val': valloader,\n",
        "    'test': testloader\n",
        "}"
      ],
      "metadata": {
        "id": "JXMp90QEcxwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example Image"
      ],
      "metadata": {
        "id": "mktyFGDEaxyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_img(train, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "SfV38Sn6PC89",
        "outputId": "12971f96-e85e-462c-a00a-c9768480fda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LeNet5"
      ],
      "metadata": {
        "id": "-u_NANyOa31H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Lenet5(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1)\n",
        "    self.max_pool1 = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "    self.max_pool2 = nn.MaxPool2d(2, 2)\n",
        "    self.fc1 = nn.Linear(16*4*4, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.max_pool1(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.max_pool2(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "net = Lenet5()\n",
        "net.to(device)\n",
        "\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "sgd = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9)"
      ],
      "metadata": {
        "id": "p0PflE0mV3Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Training Loop & train LeNet5"
      ],
      "metadata": {
        "id": "UI94H3E8a_Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(network, dataloader, loss_fn, optimizer, device, epoch, verbosity: int):\n",
        "  network.train()\n",
        "  batch_loss = 0\n",
        "  total_loss = 0\n",
        "  for i, data in enumerate(dataloader):\n",
        "    inputs, labels = data\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = network(inputs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    batch_loss += loss.item()\n",
        "    if i % verbosity == verbosity - 1:\n",
        "      print(f'Batch #{i + 1} Loss: {batch_loss / verbosity}')\n",
        "      batch_loss = 0\n",
        "\n",
        "  print(f'\\033[92mEpoch #{epoch + 1} Total Loss: {total_loss / len(dataloader)}\\033[0m')\n",
        "  total_loss = 0\n",
        "\n",
        "def eval_network(title, network, dataloader, loss_fn):\n",
        "  network.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data in dataloader:\n",
        "          images, labels = data\n",
        "          outputs = network(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "      wandb.log({\n",
        "          f'{title}-loss': loss / len(dataloader),\n",
        "          f'{title}-accuracy': correct / total\n",
        "      })\n",
        "\n",
        "  print(f'{title} accuracy: {correct}/{total} = {100 * correct / total : .4} % ||| loss {loss / len(dataloader)}')\n",
        "\n",
        "def train_network(network, dataloaders, loss_fn, optimizer, device, epochs: int, verbosity: int):\n",
        "  for epoch in range(epochs):\n",
        "    print(f'----------- Epoch #{epoch + 1} ------------')\n",
        "    train_epoch(network, dataloaders['train'], loss_fn, optimizer, device, epoch, verbosity)\n",
        "    eval_network('Train', network, dataloaders['train'], loss_fn)\n",
        "    eval_network('Validation', network, dataloaders['val'], loss_fn)\n",
        "    eval_network('Test', network, dataloaders['test'], loss_fn)\n",
        "    print('------------------------------------\\n')\n",
        "\n",
        "train_network(net, dataloaders, cross_entropy, sgd, device, 10, 100)\n",
        "\n",
        "# eval_network('Train', net, trainloader)\n",
        "# eval_network('Validation', net, valloader)\n",
        "# eval_network('Test', net, valloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp6BRgVybpiY",
        "outputId": "b1864a21-0a96-477e-9ddb-98de2258b03f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.1476374006271364\n",
            "Batch #200 Loss: 1.0365534168481827\n",
            "Batch #300 Loss: 0.7561711609363556\n",
            "\u001b[92mEpoch #1 Total Loss: 1.1873653337160746\u001b[0m\n",
            "Train accuracy: 35713/48000 =  74.4 %\n",
            "Validation accuracy: 8891/12000 =  74.09 %\n",
            "Test accuracy: 7365/10000 =  73.65 %\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6544040343165398\n",
            "Batch #200 Loss: 0.6067728072404861\n",
            "Batch #300 Loss: 0.5736526721715927\n",
            "\u001b[92mEpoch #2 Total Loss: 0.600652069568634\u001b[0m\n",
            "Train accuracy: 38415/48000 =  80.03 %\n",
            "Validation accuracy: 9648/12000 =  80.4 %\n",
            "Test accuracy: 7907/10000 =  79.07 %\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5370047271251679\n",
            "Batch #200 Loss: 0.504536963403225\n",
            "Batch #300 Loss: 0.4734526482224464\n",
            "\u001b[92mEpoch #3 Total Loss: 0.5033453831672668\u001b[0m\n",
            "Train accuracy: 39786/48000 =  82.89 %\n",
            "Validation accuracy: 10019/12000 =  83.49 %\n",
            "Test accuracy: 8201/10000 =  82.01 %\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.45909141838550566\n",
            "Batch #200 Loss: 0.461193188726902\n",
            "Batch #300 Loss: 0.43762499660253523\n",
            "\u001b[92mEpoch #4 Total Loss: 0.4480373821655909\u001b[0m\n",
            "Train accuracy: 40489/48000 =  84.35 %\n",
            "Validation accuracy: 10128/12000 =  84.4 %\n",
            "Test accuracy: 8322/10000 =  83.22 %\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.42525627970695495\n",
            "Batch #200 Loss: 0.4078578026592731\n",
            "Batch #300 Loss: 0.40099212020635605\n",
            "\u001b[92mEpoch #5 Total Loss: 0.412617116411527\u001b[0m\n",
            "Train accuracy: 41023/48000 =  85.46 %\n",
            "Validation accuracy: 10225/12000 =  85.21 %\n",
            "Test accuracy: 8416/10000 =  84.16 %\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.3839115159213543\n",
            "Batch #200 Loss: 0.3914364477992058\n",
            "Batch #300 Loss: 0.3787435281276703\n",
            "\u001b[92mEpoch #6 Total Loss: 0.38140582346916196\u001b[0m\n",
            "Train accuracy: 41787/48000 =  87.06 %\n",
            "Validation accuracy: 10344/12000 =  86.2 %\n",
            "Test accuracy: 8560/10000 =  85.6 %\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3736863347887993\n",
            "Batch #200 Loss: 0.3723404937982559\n",
            "Batch #300 Loss: 0.3471305187046528\n",
            "\u001b[92mEpoch #7 Total Loss: 0.3639599073330561\u001b[0m\n",
            "Train accuracy: 42055/48000 =  87.61 %\n",
            "Validation accuracy: 10426/12000 =  86.88 %\n",
            "Test accuracy: 8635/10000 =  86.35 %\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3538676989078522\n",
            "Batch #200 Loss: 0.3452677719295025\n",
            "Batch #300 Loss: 0.33944325119256974\n",
            "\u001b[92mEpoch #8 Total Loss: 0.3454165213902791\u001b[0m\n",
            "Train accuracy: 42266/48000 =  88.05 %\n",
            "Validation accuracy: 10451/12000 =  87.09 %\n",
            "Test accuracy: 8629/10000 =  86.29 %\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.33033532947301864\n",
            "Batch #200 Loss: 0.32683733612298965\n",
            "Batch #300 Loss: 0.3279358676075935\n",
            "\u001b[92mEpoch #9 Total Loss: 0.32752349650859836\u001b[0m\n",
            "Train accuracy: 42405/48000 =  88.34 %\n",
            "Validation accuracy: 10479/12000 =  87.33 %\n",
            "Test accuracy: 8665/10000 =  86.65 %\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.32239029705524447\n",
            "Batch #200 Loss: 0.319395594894886\n",
            "Batch #300 Loss: 0.3163384790718555\n",
            "\u001b[92mEpoch #10 Total Loss: 0.31984347355365755\u001b[0m\n",
            "Train accuracy: 42762/48000 =  89.09 %\n",
            "Validation accuracy: 10558/12000 =  87.98 %\n",
            "Test accuracy: 8723/10000 =  87.23 %\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variations on LeNet5"
      ],
      "metadata": {
        "id": "p4pr-yptbLJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Batch Normalization"
      ],
      "metadata": {
        "id": "QG5Q03J9bZV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Lenet5BN(Lenet5):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.BN1 = nn.BatchNorm2d(6)\n",
        "        self.BN2 = nn.BatchNorm2d(16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.BN1(x))\n",
        "        x = self.max_pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(self.BN2(x))\n",
        "        x = self.max_pool2(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net_bn = Lenet5BN()\n",
        "net_bn.to(device)\n",
        "\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "sgd = optim.SGD(net_bn.parameters(), lr=1e-2, momentum=0.9)\n",
        "\n",
        "train_network(net_bn, trainloader, cross_entropy, sgd, device, 5, 100)\n",
        "print('Train ', end=' ')\n",
        "eval_network(net_bn, trainloader)\n",
        "print('Val ', end=' ')\n",
        "eval_network(net_bn, valloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3rtOUZCKYWG",
        "outputId": "d7e23175-cc12-4be7-aef1-1a6ea6a0ee8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #1, Batch #100: 1.3038415053486825\n",
            "Epoch #1, Batch #200: 0.5852211400866508\n",
            "Epoch #1, Batch #300: 0.4887618997693062\n",
            "Epoch #2, Batch #100: 0.41760692834854124\n",
            "Epoch #2, Batch #200: 0.39652421697974205\n",
            "Epoch #2, Batch #300: 0.3763559329509735\n",
            "Epoch #3, Batch #100: 0.3489046224951744\n",
            "Epoch #3, Batch #200: 0.34393509075045586\n",
            "Epoch #3, Batch #300: 0.3283694978058338\n",
            "Epoch #4, Batch #100: 0.3054264672100544\n",
            "Epoch #4, Batch #200: 0.3263666643202305\n",
            "Epoch #4, Batch #300: 0.3100899827480316\n",
            "Epoch #5, Batch #100: 0.2936034318804741\n",
            "Epoch #5, Batch #200: 0.2958679489791393\n",
            "Epoch #5, Batch #300: 0.2838006091117859\n",
            "Done!\n",
            "Train  accuracy of the network: 43342/48000 =  90.3 %\n",
            "Val  accuracy of the network: 10674/12000 =  88.95 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Dropout"
      ],
      "metadata": {
        "id": "krfDLKqLbbyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Lenet5Dropout(Lenet5):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.max_pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.max_pool1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vN9DTB7ILfqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_drop = Lenet5Dropout()\n",
        "net_drop.to(device)\n",
        "\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "sgd = optim.SGD(net_drop.parameters(), lr=1e-2, momentum=0.9)\n",
        "\n",
        "train_network(net_drop, trainloader, cross_entropy, sgd, device, 5, 100)\n",
        "print('Train ', end=' ')\n",
        "eval_network(net_drop, trainloader)\n",
        "print('Val ', end=' ')\n",
        "eval_network(net_drop, valloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY_PPZpWMO4_",
        "outputId": "baf4a05d-fd60-432c-cff7-cee2800c8721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #1, Batch #100: 2.128076936006546\n",
            "Epoch #1, Batch #200: 1.1122429305315018\n",
            "Epoch #1, Batch #300: 0.9347625380754471\n",
            "Epoch #2, Batch #100: 0.7302924126386643\n",
            "Epoch #2, Batch #200: 0.7162318629026413\n",
            "Epoch #2, Batch #300: 0.6619235336780548\n",
            "Epoch #3, Batch #100: 0.6110357031226158\n",
            "Epoch #3, Batch #200: 0.5920484709739685\n",
            "Epoch #3, Batch #300: 0.5700480201840401\n",
            "Epoch #4, Batch #100: 0.5539535850286483\n",
            "Epoch #4, Batch #200: 0.5363448470830917\n",
            "Epoch #4, Batch #300: 0.5169841501116753\n",
            "Epoch #5, Batch #100: 0.4909411099553108\n",
            "Epoch #5, Batch #200: 0.4942573729157448\n",
            "Epoch #5, Batch #300: 0.48635844796895983\n",
            "Done!\n",
            "Train  accuracy of the network: 40412/48000 =  84.19 %\n",
            "Val  accuracy of the network: 10090/12000 =  84.08 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Weight Decay"
      ],
      "metadata": {
        "id": "LiiV2yXT4CFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_decay = Lenet5()\n",
        "net_decay.to(device)\n",
        "\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "sgd = optim.SGD(net_decay.parameters(), lr=1e-2, momentum=0.9, weight_decay=0.001)\n",
        "\n",
        "train_network(net_decay, trainloader, cross_entropy, sgd, device, 5, 100)\n",
        "print('Train ', end=' ')\n",
        "eval_network(net_decay, trainloader)\n",
        "print('Val ', end=' ')\n",
        "eval_network(net_decay, valloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHirXkQmNu2V",
        "outputId": "cd74126e-da53-4703-9710-93a3b01165ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #1, Batch #100: 2.2974420762062073\n",
            "Epoch #1, Batch #200: 1.8504662150144577\n",
            "Epoch #1, Batch #300: 0.849350825548172\n",
            "Epoch #2, Batch #100: 0.630090397298336\n",
            "Epoch #2, Batch #200: 0.5989323544502259\n",
            "Epoch #2, Batch #300: 0.56491309851408\n",
            "Epoch #3, Batch #100: 0.5215689361095428\n",
            "Epoch #3, Batch #200: 0.5109328374266624\n",
            "Epoch #3, Batch #300: 0.48035361170768737\n",
            "Epoch #4, Batch #100: 0.4602187183499336\n",
            "Epoch #4, Batch #200: 0.4552498507499695\n",
            "Epoch #4, Batch #300: 0.42894158005714417\n",
            "Epoch #5, Batch #100: 0.40615937143564224\n",
            "Epoch #5, Batch #200: 0.41247298181056974\n",
            "Epoch #5, Batch #300: 0.4040353463590145\n",
            "Done!\n",
            "Train  accuracy of the network: 41364/48000 =  86.17 %\n",
            "Val  accuracy of the network: 10291/12000 =  85.76 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ChEh4AqnjmkK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}