{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitkrieg/dl-assignment-1/blob/main/assignment1_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ56fD3aY8na"
      },
      "source": [
        "# Assignment 1: DL Basics\n",
        "\n",
        "### Goal\n",
        "Implement [LeNet5](https://arxiv.org/pdf/1502.03167v3) and compare various regularization techniques on the network using the FashionMNSIT dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m912ceG9Z9X-"
      },
      "source": [
        "## Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "7roLS_owoJPv",
        "outputId": "84ede7b1-b21c-436a-e432-85168cddd11d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmitkrieger\u001b[0m (\u001b[33mmitkrieger-cornell-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from time import time\n",
        "# from google.colab import drive\n",
        "import gzip\n",
        "import typing as T\n",
        "import wandb\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "torch.manual_seed(123)\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "# wandb.init(project='DL_Assignment_1')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi805ZxpaFyO"
      },
      "source": [
        "## Check for GPU Access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwJwFQG4R8wy",
        "outputId": "c103cb24-ecbe-40ce-8ce5-58845cfda5dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------ ACCELERATION INFO -----\n",
            "CUDA GPU Available: False\n",
            "MPS GPU Available: True\n",
            "Using CPU\n"
          ]
        }
      ],
      "source": [
        "print(\"------ ACCELERATION INFO -----\")\n",
        "print('CUDA GPU Available:',torch.cuda.is_available())\n",
        "print('MPS GPU Available:', torch.backends.mps.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('GPU Name:',torch.cuda.get_device_name(0))\n",
        "  print('GPU Count:',torch.cuda.device_count())\n",
        "  print('GPU Memory Allocated:',torch.cuda.memory_allocated(0))\n",
        "  print('GPU Memory Cached:',torch.cuda.memory_reserved(0))\n",
        "# elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "#   device = torch.device('mps')\n",
        "#   print('Pytorch GPU Build:',torch.backends.mps.is_built())\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print('Using CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWsWErVcSRIv"
      },
      "source": [
        "## Load Data using Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-11 22:09:38--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 3.5.135.60, 3.5.134.100, 52.219.171.224, ...\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|3.5.135.60|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26421880 (25M) [binary/octet-stream]\n",
            "Saving to: ‘./data/train-images-idx3-ubyte.gz’\n",
            "\n",
            "train-images-idx3-u 100%[===================>]  25.20M  5.06MB/s    in 6.8s    \n",
            "\n",
            "2024-09-11 22:09:46 (3.70 MB/s) - ‘./data/train-images-idx3-ubyte.gz’ saved [26421880/26421880]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz -P ./data\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz -P ./data\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz -P ./data\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz -P ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kGjJiU0lDHzb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# PATH = '/content/drive/MyDrive/Fall 2024/Deep Learning/Assignment 1/data'\n",
        "PATH = './data'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ji077ZjaQNK"
      },
      "source": [
        "### Define FashionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1gxeQbXyRrqN"
      },
      "outputs": [],
      "source": [
        "class FasionMNISTDataset(Dataset):\n",
        "  def __init__(self, path: str, kind: str, transform=None, target_transform=None, device=None) -> None:\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.device = device\n",
        "    self.labels, self.images = self._load_data(path, kind)\n",
        "\n",
        "  def _load_data(self, path: str, kind: str) -> T.Tuple[np.ndarray, np.ndarray]:\n",
        "    with gzip.open(path + f'/{kind}-labels-idx1-ubyte.gz', 'rb') as lable_file:\n",
        "      lbls = np.frombuffer(lable_file.read(), dtype=np.int8, offset=8)\n",
        "      lbls = np.copy(lbls)\n",
        "    with gzip.open(path + f'/{kind}-images-idx3-ubyte.gz', 'rb') as lable_file:\n",
        "      imgs = np.frombuffer(lable_file.read(), dtype=np.uint8, offset=16).reshape(len(lbls), 1, 28, 28)\n",
        "      imgs = (np.copy(imgs) / 255).astype(np.float32)\n",
        "    return lbls, imgs\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return self.labels.size\n",
        "\n",
        "  def __getitem__(self, index: int) -> T.Tuple[torch.tensor, torch.tensor]:\n",
        "    label = torch.tensor(self.labels[index], dtype=torch.long)\n",
        "    img = torch.tensor(self.images[index])\n",
        "    if self.device:\n",
        "      img = img.to(self.device)\n",
        "      label = label.to(self.device)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "\n",
        "    return img, label\n",
        "\n",
        "def show_img(dataset: Dataset, index: int) -> None:\n",
        "  img, label = dataset[index]\n",
        "  labels_map = {\n",
        "            0: \"T-Shirt\",\n",
        "            1: \"Trouser\",\n",
        "            2: \"Pullover\",\n",
        "            3: \"Dress\",\n",
        "            4: \"Coat\",\n",
        "            5: \"Sandal\",\n",
        "            6: \"Shirt\",\n",
        "            7: \"Sneaker\",\n",
        "            8: \"Bag\",\n",
        "            9: \"Ankle Boot\",\n",
        "        }\n",
        "  plt.imshow(img.cpu().reshape(28,28), cmap='gray')\n",
        "  plt.title(labels_map[label.cpu().item()])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHx4kcRPabF5"
      },
      "source": [
        "### Create Train, Validation and Test sets with loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JXMp90QEcxwP"
      },
      "outputs": [],
      "source": [
        "gen = torch.Generator().manual_seed(123)\n",
        "\n",
        "train = FasionMNISTDataset(PATH, 'train', device=device)\n",
        "train, val = torch.utils.data.random_split(train, [0.8, 0.2], generator=gen)\n",
        "test = FasionMNISTDataset(PATH, 'test', device=device)\n",
        "\n",
        "batch = 128\n",
        "trainloader = DataLoader(train, batch, shuffle=True, generator=gen)\n",
        "valloader = DataLoader(val, batch, shuffle=True, generator=gen)\n",
        "testloader = DataLoader(test, batch, shuffle=True, generator=gen)\n",
        "\n",
        "dataloaders = {\n",
        "    'train': trainloader,\n",
        "    'val': valloader,\n",
        "    'test': testloader\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mktyFGDEaxyp"
      },
      "source": [
        "#### Example Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "SfV38Sn6PC89",
        "outputId": "12971f96-e85e-462c-a00a-c9768480fda5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_img(train, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u_NANyOa31H"
      },
      "source": [
        "## LeNet5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p0PflE0mV3Q7"
      },
      "outputs": [],
      "source": [
        "class Lenet5(nn.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1)\n",
        "    self.max_pool1 = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "    self.max_pool2 = nn.MaxPool2d(2, 2)\n",
        "    self.fc1 = nn.Linear(16*4*4, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.max_pool1(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.max_pool2(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI94H3E8a_Ph"
      },
      "source": [
        "### Define Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train_epoch(network, dataloader, loss_fn, optimizer, device, epoch, verbosity: int):\n",
        "  \"\"\"Train one epoch of a Lenet5 network\"\"\"\n",
        "  network.train()\n",
        "  batch_loss = 0\n",
        "  total_loss = 0\n",
        "\n",
        "  # iterate over all batches\n",
        "  for i, data in enumerate(dataloader):\n",
        "    inputs, labels = data\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = network(inputs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    batch_loss += loss.item()\n",
        "    if i % verbosity == verbosity - 1:\n",
        "      print(f'Batch #{i + 1} Loss: {batch_loss / verbosity}')\n",
        "      batch_loss = 0\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "def eval_network(title, network, dataloader, loss_fn, epoch):\n",
        "  \"\"\"Evaluate model and log metrics to wandb\"\"\"\n",
        "  network.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data in dataloader:\n",
        "          images, labels = data\n",
        "          outputs = network(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "          loss += loss_fn(outputs, labels)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "      accuracy = correct / total\n",
        "      wandb.log({\n",
        "          f'{title}-loss': loss / len(dataloader),\n",
        "          f'{title}-accuracy': accuracy\n",
        "      }, step=epoch)\n",
        "  \n",
        "  print(f'\\033[92m{title} accuracy: {correct}/{total} = {100 * accuracy : .4} % ||| loss {loss / len(dataloader)}\\033[0m')\n",
        "  return accuracy\n",
        "\n",
        "def train_network(network, dataloaders, loss_fn, optimizer, device, epochs: int, verbosity: int):\n",
        "  \"\"\"Train network for given number of epochs using optimizer and loss_fn\"\"\"\n",
        "  train_accuracy, val_accuracy, test_accuracy = 0, 0, 0\n",
        "  for epoch in range(epochs):\n",
        "    print(f'----------- Epoch #{epoch + 1} ------------')\n",
        "    train_epoch(network, dataloaders['train'], loss_fn, optimizer, device, epoch, verbosity)\n",
        "    train_accuracy = eval_network('Train', network, dataloaders['train'], loss_fn, epoch)\n",
        "    val_accuracy = eval_network('Validation', network, dataloaders['val'], loss_fn, epoch)    \n",
        "    test_accuracy = eval_network('Test', network, dataloaders['test'], loss_fn, epoch)\n",
        "    print('------------------------------------\\n')\n",
        "  print('----------- Train Complete! ------------')\n",
        "  return {\n",
        "    \"train\":train_accuracy,\n",
        "    \"val\":val_accuracy,\n",
        "    \"test\":test_accuracy,\n",
        "  }\n",
        "\n",
        "\n",
        "def hyperparameter_tuning(network_cls, dataloaders, device, epochs: int, **kwargs):\n",
        "  \"\"\"Train multiple networks and print out hyperparameters & metrics for the highest performing model based on validation accuracy\"\"\"\n",
        "  ts = time()\n",
        "  keys = kwargs.keys()\n",
        "  best_model = None\n",
        "  for i, v in enumerate(itertools.product(*kwargs.values())):\n",
        "    hyperparams = dict(zip(keys, v))\n",
        "    network = network_cls(**hyperparams)\n",
        "    network.to(device)\n",
        "    name = network._get_name() + '_' + str(ts) + '_' + str(i)\n",
        "    run = wandb.init(\n",
        "      id=name,\n",
        "      config={\n",
        "        **hyperparams\n",
        "      }\n",
        "    )\n",
        "\n",
        "    print(f'XXXXXXXX Tuning Network {network._get_name()} XXXXXXXXX')\n",
        "    print('Hyperparameter Config:',hyperparams)\n",
        "\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "    sgd = optim.SGD(\n",
        "      network.parameters(), \n",
        "      lr=hyperparams['learning_rate'], \n",
        "      momentum=hyperparams['momentum'],\n",
        "      weight_decay=hyperparams.get('weight_decay',0)\n",
        "    )\n",
        "      \n",
        "    accuracies = train_network(network, dataloaders, cross_entropy, sgd, device, epochs, 100)\n",
        "    run.finish(quiet=True)\n",
        "    if best_model is None:\n",
        "      best_model = {'net':network, \"accuracy\":accuracies, \"name\":name}\n",
        "    elif best_model['accuracy']['val'] < accuracies['val']:\n",
        "      best_model = {'net':network, \"accuracy\":accuracies, \"name\":name}\n",
        "  print('\\033[93m!!!!!!! Hyper Param Tuning Finished!!!!!!!!!!!\\033[0m')\n",
        "  print(f'Best Model: {best_model['net']}\\nwandb name: {best_model['name']}\\n\\nHyperParams: {hyperparams}\\n\\nAccuracies: {best_model['accuracy']}')\n",
        "  return best_model\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Lenet5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp6BRgVybpiY",
        "outputId": "b1864a21-0a96-477e-9ddb-98de2258b03f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_162219-Lenet5_1726086139.930381_0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_0' target=\"_blank\">Lenet5_1726086139.930381_0</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.0236032807827\n",
            "Batch #200 Loss: 1.082595824599266\n",
            "Batch #300 Loss: 0.8314408749341965\n",
            "\u001b[92mTrain accuracy: 35359/48000 =  73.66 % ||| loss 0.678673267364502\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8811/12000 =  73.42 % ||| loss 0.6751672625541687\u001b[0m\n",
            "\u001b[92mTest accuracy: 7338/10000 =  73.38 % ||| loss 0.6999877691268921\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6678355169296265\n",
            "Batch #200 Loss: 0.6359583994746209\n",
            "Batch #300 Loss: 0.587376599907875\n",
            "\u001b[92mTrain accuracy: 38047/48000 =  79.26 % ||| loss 0.5283836126327515\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9546/12000 =  79.55 % ||| loss 0.5270049571990967\u001b[0m\n",
            "\u001b[92mTest accuracy: 7833/10000 =  78.33 % ||| loss 0.5533502101898193\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5336186403036117\n",
            "Batch #200 Loss: 0.5252243930101395\n",
            "Batch #300 Loss: 0.48057908356189727\n",
            "\u001b[92mTrain accuracy: 39913/48000 =  83.15 % ||| loss 0.45097073912620544\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10038/12000 =  83.65 % ||| loss 0.4572146534919739\u001b[0m\n",
            "\u001b[92mTest accuracy: 8257/10000 =  82.57 % ||| loss 0.4819338917732239\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.46604752868413923\n",
            "Batch #200 Loss: 0.4610351833701134\n",
            "Batch #300 Loss: 0.4417403429746628\n",
            "\u001b[92mTrain accuracy: 39947/48000 =  83.22 % ||| loss 0.4569363594055176\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9955/12000 =  82.96 % ||| loss 0.4658285081386566\u001b[0m\n",
            "\u001b[92mTest accuracy: 8202/10000 =  82.02 % ||| loss 0.4864170253276825\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4262390923500061\n",
            "Batch #200 Loss: 0.41328474164009094\n",
            "Batch #300 Loss: 0.4083079040050507\n",
            "\u001b[92mTrain accuracy: 41028/48000 =  85.47 % ||| loss 0.3936920166015625\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10234/12000 =  85.28 % ||| loss 0.40665510296821594\u001b[0m\n",
            "\u001b[92mTest accuracy: 8434/10000 =  84.34 % ||| loss 0.4276856482028961\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.3900501021742821\n",
            "Batch #200 Loss: 0.39696042776107787\n",
            "Batch #300 Loss: 0.38563161984086036\n",
            "\u001b[92mTrain accuracy: 41449/48000 =  86.35 % ||| loss 0.36992350220680237\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10268/12000 =  85.57 % ||| loss 0.3854683041572571\u001b[0m\n",
            "\u001b[92mTest accuracy: 8519/10000 =  85.19 % ||| loss 0.40751755237579346\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3768970514833927\n",
            "Batch #200 Loss: 0.36780044585466387\n",
            "Batch #300 Loss: 0.355398400425911\n",
            "\u001b[92mTrain accuracy: 41919/48000 =  87.33 % ||| loss 0.3437137007713318\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10405/12000 =  86.71 % ||| loss 0.36277392506599426\u001b[0m\n",
            "\u001b[92mTest accuracy: 8572/10000 =  85.72 % ||| loss 0.38592904806137085\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3494843317568302\n",
            "Batch #200 Loss: 0.3492188622057438\n",
            "Batch #300 Loss: 0.3492600978910923\n",
            "\u001b[92mTrain accuracy: 42304/48000 =  88.13 % ||| loss 0.32049450278282166\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10489/12000 =  87.41 % ||| loss 0.3443506956100464\u001b[0m\n",
            "\u001b[92mTest accuracy: 8663/10000 =  86.63 % ||| loss 0.3713708519935608\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.33728340551257135\n",
            "Batch #200 Loss: 0.33544810593128205\n",
            "Batch #300 Loss: 0.33962314009666444\n",
            "\u001b[92mTrain accuracy: 42047/48000 =  87.6 % ||| loss 0.3349352478981018\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10408/12000 =  86.73 % ||| loss 0.36197423934936523\u001b[0m\n",
            "\u001b[92mTest accuracy: 8585/10000 =  85.85 % ||| loss 0.38566043972969055\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3257807280123234\n",
            "Batch #200 Loss: 0.3224588257074356\n",
            "Batch #300 Loss: 0.3192489117383957\n",
            "\u001b[92mTrain accuracy: 42723/48000 =  89.01 % ||| loss 0.2975730001926422\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10550/12000 =  87.92 % ||| loss 0.32929542660713196\u001b[0m\n",
            "\u001b[92mTest accuracy: 8714/10000 =  87.14 % ||| loss 0.3510587811470032\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.30789743676781656\n",
            "Batch #200 Loss: 0.31433306068181993\n",
            "Batch #300 Loss: 0.31319813266396523\n",
            "\u001b[92mTrain accuracy: 42655/48000 =  88.86 % ||| loss 0.29732075333595276\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10506/12000 =  87.55 % ||| loss 0.3345699608325958\u001b[0m\n",
            "\u001b[92mTest accuracy: 8703/10000 =  87.03 % ||| loss 0.35940584540367126\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.2951235654950142\n",
            "Batch #200 Loss: 0.29690127074718475\n",
            "Batch #300 Loss: 0.30194244652986524\n",
            "\u001b[92mTrain accuracy: 43233/48000 =  90.07 % ||| loss 0.2742958068847656\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10651/12000 =  88.76 % ||| loss 0.31405821442604065\u001b[0m\n",
            "\u001b[92mTest accuracy: 8782/10000 =  87.82 % ||| loss 0.33693206310272217\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.29293678686022756\n",
            "Batch #200 Loss: 0.2916465689241886\n",
            "Batch #300 Loss: 0.3017915666103363\n",
            "\u001b[92mTrain accuracy: 43105/48000 =  89.8 % ||| loss 0.2742937505245209\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10589/12000 =  88.24 % ||| loss 0.31482821702957153\u001b[0m\n",
            "\u001b[92mTest accuracy: 8766/10000 =  87.66 % ||| loss 0.3379192054271698\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.28103969678282736\n",
            "Batch #200 Loss: 0.28265727281570435\n",
            "Batch #300 Loss: 0.29307284504175185\n",
            "\u001b[92mTrain accuracy: 43398/48000 =  90.41 % ||| loss 0.26182839274406433\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10619/12000 =  88.49 % ||| loss 0.312240868806839\u001b[0m\n",
            "\u001b[92mTest accuracy: 8822/10000 =  88.22 % ||| loss 0.33039048314094543\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.2823189423978329\n",
            "Batch #200 Loss: 0.2719204580783844\n",
            "Batch #300 Loss: 0.27529860615730284\n",
            "\u001b[92mTrain accuracy: 43281/48000 =  90.17 % ||| loss 0.26587384939193726\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10611/12000 =  88.42 % ||| loss 0.3124770224094391\u001b[0m\n",
            "\u001b[92mTest accuracy: 8774/10000 =  87.74 % ||| loss 0.33174610137939453\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.26272277802228927\n",
            "Batch #200 Loss: 0.27210382983088494\n",
            "Batch #300 Loss: 0.2713600185513496\n",
            "\u001b[92mTrain accuracy: 43364/48000 =  90.34 % ||| loss 0.25728368759155273\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10642/12000 =  88.68 % ||| loss 0.31288111209869385\u001b[0m\n",
            "\u001b[92mTest accuracy: 8790/10000 =  87.9 % ||| loss 0.3310709595680237\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.26207706294953825\n",
            "Batch #200 Loss: 0.26608760342001914\n",
            "Batch #300 Loss: 0.258732198625803\n",
            "\u001b[92mTrain accuracy: 43732/48000 =  91.11 % ||| loss 0.24264393746852875\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10656/12000 =  88.8 % ||| loss 0.3002973794937134\u001b[0m\n",
            "\u001b[92mTest accuracy: 8838/10000 =  88.38 % ||| loss 0.3190482556819916\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.258676992803812\n",
            "Batch #200 Loss: 0.25362797133624554\n",
            "Batch #300 Loss: 0.2519185897707939\n",
            "\u001b[92mTrain accuracy: 43838/48000 =  91.33 % ||| loss 0.23723824322223663\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10689/12000 =  89.08 % ||| loss 0.2959308624267578\u001b[0m\n",
            "\u001b[92mTest accuracy: 8865/10000 =  88.65 % ||| loss 0.31593894958496094\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.2558071246743202\n",
            "Batch #200 Loss: 0.2499357058852911\n",
            "Batch #300 Loss: 0.24764591462910177\n",
            "\u001b[92mTrain accuracy: 43693/48000 =  91.03 % ||| loss 0.23963165283203125\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10674/12000 =  88.95 % ||| loss 0.3053584396839142\u001b[0m\n",
            "\u001b[92mTest accuracy: 8851/10000 =  88.51 % ||| loss 0.3256593346595764\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.24951150447130202\n",
            "Batch #200 Loss: 0.24249834284186364\n",
            "Batch #300 Loss: 0.23206527069211005\n",
            "\u001b[92mTrain accuracy: 43890/48000 =  91.44 % ||| loss 0.2304423600435257\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10677/12000 =  88.98 % ||| loss 0.30865514278411865\u001b[0m\n",
            "\u001b[92mTest accuracy: 8838/10000 =  88.38 % ||| loss 0.327890008687973\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.23649809151887893\n",
            "Batch #200 Loss: 0.24070098519325256\n",
            "Batch #300 Loss: 0.23658302634954453\n",
            "\u001b[92mTrain accuracy: 43866/48000 =  91.39 % ||| loss 0.2308500111103058\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10649/12000 =  88.74 % ||| loss 0.3073367178440094\u001b[0m\n",
            "\u001b[92mTest accuracy: 8856/10000 =  88.56 % ||| loss 0.325056791305542\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.22689928010106086\n",
            "Batch #200 Loss: 0.22792710185050966\n",
            "Batch #300 Loss: 0.24031372882425786\n",
            "\u001b[92mTrain accuracy: 43196/48000 =  89.99 % ||| loss 0.2615443170070648\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10514/12000 =  87.62 % ||| loss 0.3372969329357147\u001b[0m\n",
            "\u001b[92mTest accuracy: 8739/10000 =  87.39 % ||| loss 0.3538402020931244\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.22813913017511367\n",
            "Batch #200 Loss: 0.22985574573278428\n",
            "Batch #300 Loss: 0.2280975078046322\n",
            "\u001b[92mTrain accuracy: 44209/48000 =  92.1 % ||| loss 0.21332517266273499\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10717/12000 =  89.31 % ||| loss 0.2979411482810974\u001b[0m\n",
            "\u001b[92mTest accuracy: 8884/10000 =  88.84 % ||| loss 0.3214251697063446\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.21697714164853096\n",
            "Batch #200 Loss: 0.2213521683216095\n",
            "Batch #300 Loss: 0.22884618692100048\n",
            "\u001b[92mTrain accuracy: 44139/48000 =  91.96 % ||| loss 0.21317875385284424\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10718/12000 =  89.32 % ||| loss 0.3060779571533203\u001b[0m\n",
            "\u001b[92mTest accuracy: 8883/10000 =  88.83 % ||| loss 0.32416871190071106\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.21426911734044551\n",
            "Batch #200 Loss: 0.2213870169967413\n",
            "Batch #300 Loss: 0.2150805735588074\n",
            "\u001b[92mTrain accuracy: 43908/48000 =  91.47 % ||| loss 0.2241395264863968\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10600/12000 =  88.33 % ||| loss 0.31194978952407837\u001b[0m\n",
            "\u001b[92mTest accuracy: 8847/10000 =  88.47 % ||| loss 0.32381364703178406\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726086139.930381_0</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_0</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_162409-Lenet5_1726086139.930381_1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_1' target=\"_blank\">Lenet5_1726086139.930381_1</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.372236957550049\n",
            "Batch #200 Loss: 1.44293381690979\n",
            "Batch #300 Loss: 0.9303632396459579\n",
            "\u001b[92mTrain accuracy: 33756/48000 =  70.33 % ||| loss 0.7710257768630981\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8450/12000 =  70.42 % ||| loss 0.7680749297142029\u001b[0m\n",
            "\u001b[92mTest accuracy: 6980/10000 =  69.8 % ||| loss 0.786946713924408\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7766092491149902\n",
            "Batch #200 Loss: 0.7097075185179711\n",
            "Batch #300 Loss: 0.6585011348128319\n",
            "\u001b[92mTrain accuracy: 35088/48000 =  73.1 % ||| loss 0.7028211355209351\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8776/12000 =  73.13 % ||| loss 0.7087828516960144\u001b[0m\n",
            "\u001b[92mTest accuracy: 7210/10000 =  72.1 % ||| loss 0.7435469031333923\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6134333011507987\n",
            "Batch #200 Loss: 0.6108840742707252\n",
            "Batch #300 Loss: 0.5950693905353546\n",
            "\u001b[92mTrain accuracy: 35824/48000 =  74.63 % ||| loss 0.6250595450401306\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8956/12000 =  74.63 % ||| loss 0.627271294593811\u001b[0m\n",
            "\u001b[92mTest accuracy: 7335/10000 =  73.35 % ||| loss 0.6598047018051147\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5750345441699029\n",
            "Batch #200 Loss: 0.598596730530262\n",
            "Batch #300 Loss: 0.5816196846961975\n",
            "\u001b[92mTrain accuracy: 37471/48000 =  78.06 % ||| loss 0.5686679482460022\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9386/12000 =  78.22 % ||| loss 0.569212794303894\u001b[0m\n",
            "\u001b[92mTest accuracy: 7654/10000 =  76.54 % ||| loss 0.6008034944534302\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5549809923768043\n",
            "Batch #200 Loss: 0.5633278366923332\n",
            "Batch #300 Loss: 0.5585631528496742\n",
            "\u001b[92mTrain accuracy: 38481/48000 =  80.17 % ||| loss 0.5240723490715027\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9581/12000 =  79.84 % ||| loss 0.5335977673530579\u001b[0m\n",
            "\u001b[92mTest accuracy: 7844/10000 =  78.44 % ||| loss 0.5647807717323303\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5561384499073029\n",
            "Batch #200 Loss: 0.5438793304562569\n",
            "Batch #300 Loss: 0.524244698882103\n",
            "\u001b[92mTrain accuracy: 38067/48000 =  79.31 % ||| loss 0.5347951650619507\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9518/12000 =  79.32 % ||| loss 0.550504207611084\u001b[0m\n",
            "\u001b[92mTest accuracy: 7754/10000 =  77.54 % ||| loss 0.5763421654701233\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5335251003503799\n",
            "Batch #200 Loss: 0.5404281294345856\n",
            "Batch #300 Loss: 0.5407413265109062\n",
            "\u001b[92mTrain accuracy: 38464/48000 =  80.13 % ||| loss 0.5182867646217346\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9588/12000 =  79.9 % ||| loss 0.533796489238739\u001b[0m\n",
            "\u001b[92mTest accuracy: 7878/10000 =  78.78 % ||| loss 0.5531856417655945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.5246077027916908\n",
            "Batch #200 Loss: 0.5365907782316208\n",
            "Batch #300 Loss: 0.5271703580021858\n",
            "\u001b[92mTrain accuracy: 38921/48000 =  81.09 % ||| loss 0.4934207499027252\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9681/12000 =  80.67 % ||| loss 0.5098323225975037\u001b[0m\n",
            "\u001b[92mTest accuracy: 7934/10000 =  79.34 % ||| loss 0.5384225249290466\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.5192611855268479\n",
            "Batch #200 Loss: 0.5025243854522705\n",
            "Batch #300 Loss: 0.5290078774094582\n",
            "\u001b[92mTrain accuracy: 38585/48000 =  80.39 % ||| loss 0.5158258080482483\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9572/12000 =  79.77 % ||| loss 0.5344957113265991\u001b[0m\n",
            "\u001b[92mTest accuracy: 7883/10000 =  78.83 % ||| loss 0.5588107109069824\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.5293222430348397\n",
            "Batch #200 Loss: 0.5018398377299309\n",
            "Batch #300 Loss: 0.514332365989685\n",
            "\u001b[92mTrain accuracy: 38480/48000 =  80.17 % ||| loss 0.5099818706512451\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9549/12000 =  79.57 % ||| loss 0.5275320410728455\u001b[0m\n",
            "\u001b[92mTest accuracy: 7867/10000 =  78.67 % ||| loss 0.5558211803436279\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.5075246998667717\n",
            "Batch #200 Loss: 0.5196706822514534\n",
            "Batch #300 Loss: 0.5052659526467324\n",
            "\u001b[92mTrain accuracy: 38881/48000 =  81.0 % ||| loss 0.49065595865249634\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9697/12000 =  80.81 % ||| loss 0.5077475905418396\u001b[0m\n",
            "\u001b[92mTest accuracy: 7962/10000 =  79.62 % ||| loss 0.5350884199142456\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5010913842916489\n",
            "Batch #200 Loss: 0.49642598271369937\n",
            "Batch #300 Loss: 0.5177223846316338\n",
            "\u001b[92mTrain accuracy: 38787/48000 =  80.81 % ||| loss 0.5106834173202515\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9688/12000 =  80.73 % ||| loss 0.5327622890472412\u001b[0m\n",
            "\u001b[92mTest accuracy: 7884/10000 =  78.84 % ||| loss 0.565143346786499\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5118742683529853\n",
            "Batch #200 Loss: 0.49435250997543334\n",
            "Batch #300 Loss: 0.5069214197993278\n",
            "\u001b[92mTrain accuracy: 38763/48000 =  80.76 % ||| loss 0.49593138694763184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9620/12000 =  80.17 % ||| loss 0.5154427289962769\u001b[0m\n",
            "\u001b[92mTest accuracy: 7923/10000 =  79.23 % ||| loss 0.5493893623352051\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.4790540888905525\n",
            "Batch #200 Loss: 0.4970319685339928\n",
            "Batch #300 Loss: 0.502795185148716\n",
            "\u001b[92mTrain accuracy: 38480/48000 =  80.17 % ||| loss 0.50682133436203\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9542/12000 =  79.52 % ||| loss 0.5340176820755005\u001b[0m\n",
            "\u001b[92mTest accuracy: 7817/10000 =  78.17 % ||| loss 0.5758333206176758\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5015497070550918\n",
            "Batch #200 Loss: 0.48988252133131027\n",
            "Batch #300 Loss: 0.4883702939748764\n",
            "\u001b[92mTrain accuracy: 39225/48000 =  81.72 % ||| loss 0.4751388430595398\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9735/12000 =  81.12 % ||| loss 0.498604953289032\u001b[0m\n",
            "\u001b[92mTest accuracy: 8016/10000 =  80.16 % ||| loss 0.5316986441612244\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.490105881690979\n",
            "Batch #200 Loss: 0.4937278345227242\n",
            "Batch #300 Loss: 0.4801194033026695\n",
            "\u001b[92mTrain accuracy: 37864/48000 =  78.88 % ||| loss 0.5159317851066589\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9411/12000 =  78.42 % ||| loss 0.5395498871803284\u001b[0m\n",
            "\u001b[92mTest accuracy: 7773/10000 =  77.73 % ||| loss 0.5627633929252625\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.4830008462071419\n",
            "Batch #200 Loss: 0.48933045744895937\n",
            "Batch #300 Loss: 0.49146005034446716\n",
            "\u001b[92mTrain accuracy: 38827/48000 =  80.89 % ||| loss 0.49699950218200684\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9635/12000 =  80.29 % ||| loss 0.5177439451217651\u001b[0m\n",
            "\u001b[92mTest accuracy: 7887/10000 =  78.87 % ||| loss 0.5462183356285095\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5007591366767883\n",
            "Batch #200 Loss: 0.4709903448820114\n",
            "Batch #300 Loss: 0.49552908778190613\n",
            "\u001b[92mTrain accuracy: 39164/48000 =  81.59 % ||| loss 0.47844910621643066\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9721/12000 =  81.01 % ||| loss 0.4967062473297119\u001b[0m\n",
            "\u001b[92mTest accuracy: 7947/10000 =  79.47 % ||| loss 0.5309451222419739\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.4841717958450317\n",
            "Batch #200 Loss: 0.4972355046868324\n",
            "Batch #300 Loss: 0.47354899227619174\n",
            "\u001b[92mTrain accuracy: 39247/48000 =  81.76 % ||| loss 0.47058427333831787\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9753/12000 =  81.27 % ||| loss 0.49728450179100037\u001b[0m\n",
            "\u001b[92mTest accuracy: 7969/10000 =  79.69 % ||| loss 0.5346736907958984\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.47656854152679445\n",
            "Batch #200 Loss: 0.4771608769893646\n",
            "Batch #300 Loss: 0.4818636903166771\n",
            "\u001b[92mTrain accuracy: 39303/48000 =  81.88 % ||| loss 0.46801742911338806\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9756/12000 =  81.3 % ||| loss 0.5009875893592834\u001b[0m\n",
            "\u001b[92mTest accuracy: 7967/10000 =  79.67 % ||| loss 0.5261250734329224\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.47915639519691466\n",
            "Batch #200 Loss: 0.46028132826089857\n",
            "Batch #300 Loss: 0.4819285029172897\n",
            "\u001b[92mTrain accuracy: 39182/48000 =  81.63 % ||| loss 0.47208133339881897\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9690/12000 =  80.75 % ||| loss 0.5043368935585022\u001b[0m\n",
            "\u001b[92mTest accuracy: 7993/10000 =  79.93 % ||| loss 0.5331049561500549\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.455993065237999\n",
            "Batch #200 Loss: 0.4823643928766251\n",
            "Batch #300 Loss: 0.4783064910769463\n",
            "\u001b[92mTrain accuracy: 38752/48000 =  80.73 % ||| loss 0.5021141767501831\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9595/12000 =  79.96 % ||| loss 0.5268897414207458\u001b[0m\n",
            "\u001b[92mTest accuracy: 7869/10000 =  78.69 % ||| loss 0.5732399821281433\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.4740844228863716\n",
            "Batch #200 Loss: 0.4749726301431656\n",
            "Batch #300 Loss: 0.4812523156404495\n",
            "\u001b[92mTrain accuracy: 39478/48000 =  82.25 % ||| loss 0.45800134539604187\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9816/12000 =  81.8 % ||| loss 0.4885496497154236\u001b[0m\n",
            "\u001b[92mTest accuracy: 8038/10000 =  80.38 % ||| loss 0.5168569684028625\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.45688045740127564\n",
            "Batch #200 Loss: 0.48171119928359984\n",
            "Batch #300 Loss: 0.4955128741264343\n",
            "\u001b[92mTrain accuracy: 39593/48000 =  82.49 % ||| loss 0.44489508867263794\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9799/12000 =  81.66 % ||| loss 0.47787678241729736\u001b[0m\n",
            "\u001b[92mTest accuracy: 8075/10000 =  80.75 % ||| loss 0.5141123533248901\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.46794798254966735\n",
            "Batch #200 Loss: 0.47151631891727447\n",
            "Batch #300 Loss: 0.4761553207039833\n",
            "\u001b[92mTrain accuracy: 38724/48000 =  80.67 % ||| loss 0.4915751814842224\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9619/12000 =  80.16 % ||| loss 0.522065281867981\u001b[0m\n",
            "\u001b[92mTest accuracy: 7902/10000 =  79.02 % ||| loss 0.5514393448829651\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726086139.930381_1</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_1</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_162558-Lenet5_1726086139.930381_2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_2' target=\"_blank\">Lenet5_1726086139.930381_2</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.8289011430740356\n",
            "Batch #200 Loss: 0.7458706274628639\n",
            "Batch #300 Loss: 0.5978384116291999\n",
            "\u001b[92mTrain accuracy: 38414/48000 =  80.03 % ||| loss 0.5146629810333252\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9601/12000 =  80.01 % ||| loss 0.5180776119232178\u001b[0m\n",
            "\u001b[92mTest accuracy: 7913/10000 =  79.13 % ||| loss 0.5339449048042297\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.4971723836660385\n",
            "Batch #200 Loss: 0.47286322206258774\n",
            "Batch #300 Loss: 0.45380102187395094\n",
            "\u001b[92mTrain accuracy: 41032/48000 =  85.48 % ||| loss 0.39086923003196716\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10211/12000 =  85.09 % ||| loss 0.39998525381088257\u001b[0m\n",
            "\u001b[92mTest accuracy: 8372/10000 =  83.72 % ||| loss 0.4256141185760498\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.40971171140670776\n",
            "Batch #200 Loss: 0.37996457308530807\n",
            "Batch #300 Loss: 0.38573729917407035\n",
            "\u001b[92mTrain accuracy: 41677/48000 =  86.83 % ||| loss 0.35766342282295227\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10343/12000 =  86.19 % ||| loss 0.3812748193740845\u001b[0m\n",
            "\u001b[92mTest accuracy: 8513/10000 =  85.13 % ||| loss 0.4059497117996216\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.3557493522763252\n",
            "Batch #200 Loss: 0.35547425001859667\n",
            "Batch #300 Loss: 0.36062741696834566\n",
            "\u001b[92mTrain accuracy: 42331/48000 =  88.19 % ||| loss 0.3173677921295166\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10486/12000 =  87.38 % ||| loss 0.3400740623474121\u001b[0m\n",
            "\u001b[92mTest accuracy: 8625/10000 =  86.25 % ||| loss 0.3763606548309326\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.34367079004645346\n",
            "Batch #200 Loss: 0.3402540764212608\n",
            "Batch #300 Loss: 0.33450266271829604\n",
            "\u001b[92mTrain accuracy: 42312/48000 =  88.15 % ||| loss 0.3152877688407898\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10473/12000 =  87.28 % ||| loss 0.342231422662735\u001b[0m\n",
            "\u001b[92mTest accuracy: 8644/10000 =  86.44 % ||| loss 0.3681814968585968\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.32591852635145185\n",
            "Batch #200 Loss: 0.32570762410759924\n",
            "Batch #300 Loss: 0.313779658973217\n",
            "\u001b[92mTrain accuracy: 42644/48000 =  88.84 % ||| loss 0.30714431405067444\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10539/12000 =  87.83 % ||| loss 0.3418499827384949\u001b[0m\n",
            "\u001b[92mTest accuracy: 8688/10000 =  86.88 % ||| loss 0.37141963839530945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.31155079454183576\n",
            "Batch #200 Loss: 0.3109177868068218\n",
            "Batch #300 Loss: 0.30149327352643013\n",
            "\u001b[92mTrain accuracy: 41651/48000 =  86.77 % ||| loss 0.3479263484477997\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10274/12000 =  85.62 % ||| loss 0.3777773976325989\u001b[0m\n",
            "\u001b[92mTest accuracy: 8482/10000 =  84.82 % ||| loss 0.4107210636138916\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.29775933653116227\n",
            "Batch #200 Loss: 0.28915744334459303\n",
            "Batch #300 Loss: 0.2980392724275589\n",
            "\u001b[92mTrain accuracy: 43051/48000 =  89.69 % ||| loss 0.27562499046325684\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10571/12000 =  88.09 % ||| loss 0.3241194188594818\u001b[0m\n",
            "\u001b[92mTest accuracy: 8727/10000 =  87.27 % ||| loss 0.34459736943244934\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.28396732717752454\n",
            "Batch #200 Loss: 0.28742479234933854\n",
            "Batch #300 Loss: 0.29178411900997164\n",
            "\u001b[92mTrain accuracy: 43140/48000 =  89.88 % ||| loss 0.2652413845062256\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10567/12000 =  88.06 % ||| loss 0.3165743947029114\u001b[0m\n",
            "\u001b[92mTest accuracy: 8755/10000 =  87.55 % ||| loss 0.34231826663017273\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.2690480034053326\n",
            "Batch #200 Loss: 0.2812679944932461\n",
            "Batch #300 Loss: 0.29089089199900625\n",
            "\u001b[92mTrain accuracy: 43452/48000 =  90.53 % ||| loss 0.25444456934928894\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10636/12000 =  88.63 % ||| loss 0.30888789892196655\u001b[0m\n",
            "\u001b[92mTest accuracy: 8784/10000 =  87.84 % ||| loss 0.3346365988254547\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.27714739337563515\n",
            "Batch #200 Loss: 0.2835516911745071\n",
            "Batch #300 Loss: 0.26032819613814356\n",
            "\u001b[92mTrain accuracy: 43349/48000 =  90.31 % ||| loss 0.2574937045574188\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10588/12000 =  88.23 % ||| loss 0.3184799253940582\u001b[0m\n",
            "\u001b[92mTest accuracy: 8757/10000 =  87.57 % ||| loss 0.3439384698867798\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.2654187887907028\n",
            "Batch #200 Loss: 0.26409869343042375\n",
            "Batch #300 Loss: 0.27213467843830585\n",
            "\u001b[92mTrain accuracy: 43627/48000 =  90.89 % ||| loss 0.24301396310329437\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10634/12000 =  88.62 % ||| loss 0.31189724802970886\u001b[0m\n",
            "\u001b[92mTest accuracy: 8816/10000 =  88.16 % ||| loss 0.3326312005519867\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.2559917344152927\n",
            "Batch #200 Loss: 0.2621600955724716\n",
            "Batch #300 Loss: 0.2579747925698757\n",
            "\u001b[92mTrain accuracy: 43802/48000 =  91.25 % ||| loss 0.23084335029125214\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10647/12000 =  88.72 % ||| loss 0.30314183235168457\u001b[0m\n",
            "\u001b[92mTest accuracy: 8849/10000 =  88.49 % ||| loss 0.32631924748420715\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.2514738927781582\n",
            "Batch #200 Loss: 0.25724797129631044\n",
            "Batch #300 Loss: 0.24984838843345641\n",
            "\u001b[92mTrain accuracy: 43711/48000 =  91.06 % ||| loss 0.23150908946990967\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10603/12000 =  88.36 % ||| loss 0.31222328543663025\u001b[0m\n",
            "\u001b[92mTest accuracy: 8812/10000 =  88.12 % ||| loss 0.34205541014671326\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.23397555492818356\n",
            "Batch #200 Loss: 0.2476368223130703\n",
            "Batch #300 Loss: 0.2516163417696953\n",
            "\u001b[92mTrain accuracy: 43726/48000 =  91.1 % ||| loss 0.2310483455657959\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10618/12000 =  88.48 % ||| loss 0.30631405115127563\u001b[0m\n",
            "\u001b[92mTest accuracy: 8771/10000 =  87.71 % ||| loss 0.33428066968917847\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.23793710961937906\n",
            "Batch #200 Loss: 0.24602824136614798\n",
            "Batch #300 Loss: 0.24930394686758517\n",
            "\u001b[92mTrain accuracy: 43828/48000 =  91.31 % ||| loss 0.23002217710018158\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10631/12000 =  88.59 % ||| loss 0.3212515711784363\u001b[0m\n",
            "\u001b[92mTest accuracy: 8806/10000 =  88.06 % ||| loss 0.33809012174606323\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.22401618339121343\n",
            "Batch #200 Loss: 0.24103141851723195\n",
            "Batch #300 Loss: 0.22710022538900376\n",
            "\u001b[92mTrain accuracy: 43266/48000 =  90.14 % ||| loss 0.2585754990577698\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10473/12000 =  87.28 % ||| loss 0.35619667172431946\u001b[0m\n",
            "\u001b[92mTest accuracy: 8677/10000 =  86.77 % ||| loss 0.3745727241039276\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.2195807108283043\n",
            "Batch #200 Loss: 0.23882607579231263\n",
            "Batch #300 Loss: 0.21989881031215192\n",
            "\u001b[92mTrain accuracy: 44162/48000 =  92.0 % ||| loss 0.21541859209537506\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10632/12000 =  88.6 % ||| loss 0.3181823194026947\u001b[0m\n",
            "\u001b[92mTest accuracy: 8821/10000 =  88.21 % ||| loss 0.33467304706573486\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.21650924071669578\n",
            "Batch #200 Loss: 0.22670095540583135\n",
            "Batch #300 Loss: 0.2363082665205002\n",
            "\u001b[92mTrain accuracy: 44153/48000 =  91.99 % ||| loss 0.2159365713596344\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10626/12000 =  88.55 % ||| loss 0.3445829749107361\u001b[0m\n",
            "\u001b[92mTest accuracy: 8814/10000 =  88.14 % ||| loss 0.3657587170600891\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.22160998478531838\n",
            "Batch #200 Loss: 0.228285221979022\n",
            "Batch #300 Loss: 0.21272869855165483\n",
            "\u001b[92mTrain accuracy: 43432/48000 =  90.48 % ||| loss 0.2544406056404114\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10483/12000 =  87.36 % ||| loss 0.3695448935031891\u001b[0m\n",
            "\u001b[92mTest accuracy: 8654/10000 =  86.54 % ||| loss 0.407256543636322\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.2102621704339981\n",
            "Batch #200 Loss: 0.219721240401268\n",
            "Batch #300 Loss: 0.22386609211564065\n",
            "\u001b[92mTrain accuracy: 44226/48000 =  92.14 % ||| loss 0.20733731985092163\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10636/12000 =  88.63 % ||| loss 0.34295085072517395\u001b[0m\n",
            "\u001b[92mTest accuracy: 8803/10000 =  88.03 % ||| loss 0.38297608494758606\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.20802396409213542\n",
            "Batch #200 Loss: 0.2130649520456791\n",
            "Batch #300 Loss: 0.20913098625838755\n",
            "\u001b[92mTrain accuracy: 44673/48000 =  93.07 % ||| loss 0.17950184643268585\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10709/12000 =  89.24 % ||| loss 0.31905874609947205\u001b[0m\n",
            "\u001b[92mTest accuracy: 8834/10000 =  88.34 % ||| loss 0.3517919182777405\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.1943615313619375\n",
            "Batch #200 Loss: 0.21053762659430503\n",
            "Batch #300 Loss: 0.2079076985269785\n",
            "\u001b[92mTrain accuracy: 44720/48000 =  93.17 % ||| loss 0.17985430359840393\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10654/12000 =  88.78 % ||| loss 0.32568037509918213\u001b[0m\n",
            "\u001b[92mTest accuracy: 8797/10000 =  87.97 % ||| loss 0.36454376578330994\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.18731454871594905\n",
            "Batch #200 Loss: 0.20098593190312386\n",
            "Batch #300 Loss: 0.20673102647066116\n",
            "\u001b[92mTrain accuracy: 44682/48000 =  93.09 % ||| loss 0.18038824200630188\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10650/12000 =  88.75 % ||| loss 0.3327805995941162\u001b[0m\n",
            "\u001b[92mTest accuracy: 8823/10000 =  88.23 % ||| loss 0.3572236895561218\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.18620953120291234\n",
            "Batch #200 Loss: 0.1895189293473959\n",
            "Batch #300 Loss: 0.20125182926654817\n",
            "\u001b[92mTrain accuracy: 44533/48000 =  92.78 % ||| loss 0.18752337992191315\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10608/12000 =  88.4 % ||| loss 0.3450639545917511\u001b[0m\n",
            "\u001b[92mTest accuracy: 8802/10000 =  88.02 % ||| loss 0.3743029832839966\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726086139.930381_2</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_2</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_162746-Lenet5_1726086139.930381_3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_3' target=\"_blank\">Lenet5_1726086139.930381_3</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3004798316955566\n",
            "Batch #200 Loss: 2.298731439113617\n",
            "Batch #300 Loss: 2.2958174967765808\n",
            "\u001b[92mTrain accuracy: 6002/48000 =  12.5 % ||| loss 2.2887251377105713\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1490/12000 =  12.42 % ||| loss 2.2890238761901855\u001b[0m\n",
            "\u001b[92mTest accuracy: 1211/10000 =  12.11 % ||| loss 2.2887988090515137\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.283385207653046\n",
            "Batch #200 Loss: 2.2627668452262877\n",
            "Batch #300 Loss: 2.1845869207382203\n",
            "\u001b[92mTrain accuracy: 22398/48000 =  46.66 % ||| loss 1.6309734582901\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5580/12000 =  46.5 % ||| loss 1.630801796913147\u001b[0m\n",
            "\u001b[92mTest accuracy: 4633/10000 =  46.33 % ||| loss 1.6331158876419067\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.3403572070598602\n",
            "Batch #200 Loss: 1.06612733066082\n",
            "Batch #300 Loss: 1.0087201648950577\n",
            "\u001b[92mTrain accuracy: 31477/48000 =  65.58 % ||| loss 0.9039889574050903\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7918/12000 =  65.98 % ||| loss 0.8939971923828125\u001b[0m\n",
            "\u001b[92mTest accuracy: 6487/10000 =  64.87 % ||| loss 0.921593189239502\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.9181961113214493\n",
            "Batch #200 Loss: 0.8899956852197647\n",
            "Batch #300 Loss: 0.8613488107919693\n",
            "\u001b[92mTrain accuracy: 33273/48000 =  69.32 % ||| loss 0.8143609166145325\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8383/12000 =  69.86 % ||| loss 0.8035643696784973\u001b[0m\n",
            "\u001b[92mTest accuracy: 6911/10000 =  69.11 % ||| loss 0.8297321200370789\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.8086050707101822\n",
            "Batch #200 Loss: 0.821742238998413\n",
            "Batch #300 Loss: 0.7659181159734726\n",
            "\u001b[92mTrain accuracy: 33923/48000 =  70.67 % ||| loss 0.7482390999794006\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8519/12000 =  70.99 % ||| loss 0.7366589307785034\u001b[0m\n",
            "\u001b[92mTest accuracy: 6966/10000 =  69.66 % ||| loss 0.7672034502029419\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.7503232777118682\n",
            "Batch #200 Loss: 0.7426560217142105\n",
            "Batch #300 Loss: 0.7323510330915451\n",
            "\u001b[92mTrain accuracy: 33928/48000 =  70.68 % ||| loss 0.7365331053733826\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8536/12000 =  71.13 % ||| loss 0.7287607192993164\u001b[0m\n",
            "\u001b[92mTest accuracy: 6977/10000 =  69.77 % ||| loss 0.7647774815559387\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.7074185365438461\n",
            "Batch #200 Loss: 0.6883903461694717\n",
            "Batch #300 Loss: 0.6958989211916924\n",
            "\u001b[92mTrain accuracy: 36261/48000 =  75.54 % ||| loss 0.6599818468093872\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9077/12000 =  75.64 % ||| loss 0.6530823111534119\u001b[0m\n",
            "\u001b[92mTest accuracy: 7488/10000 =  74.88 % ||| loss 0.6816493272781372\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.6667339470982552\n",
            "Batch #200 Loss: 0.668470915555954\n",
            "Batch #300 Loss: 0.6542320507764816\n",
            "\u001b[92mTrain accuracy: 36840/48000 =  76.75 % ||| loss 0.6225487589836121\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9246/12000 =  77.05 % ||| loss 0.6148644089698792\u001b[0m\n",
            "\u001b[92mTest accuracy: 7610/10000 =  76.1 % ||| loss 0.6429855823516846\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.6476376050710678\n",
            "Batch #200 Loss: 0.6288395872712136\n",
            "Batch #300 Loss: 0.6327238830924035\n",
            "\u001b[92mTrain accuracy: 37169/48000 =  77.44 % ||| loss 0.6102970242500305\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9347/12000 =  77.89 % ||| loss 0.6054109334945679\u001b[0m\n",
            "\u001b[92mTest accuracy: 7632/10000 =  76.32 % ||| loss 0.6376370787620544\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.6153007143735886\n",
            "Batch #200 Loss: 0.6159810855984688\n",
            "Batch #300 Loss: 0.5953377160429955\n",
            "\u001b[92mTrain accuracy: 37524/48000 =  78.17 % ||| loss 0.5881307721138\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9434/12000 =  78.62 % ||| loss 0.5847455263137817\u001b[0m\n",
            "\u001b[92mTest accuracy: 7713/10000 =  77.13 % ||| loss 0.6135844588279724\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.5890972390770912\n",
            "Batch #200 Loss: 0.5841956520080567\n",
            "Batch #300 Loss: 0.572077006995678\n",
            "\u001b[92mTrain accuracy: 37665/48000 =  78.47 % ||| loss 0.5741241574287415\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9403/12000 =  78.36 % ||| loss 0.5704500079154968\u001b[0m\n",
            "\u001b[92mTest accuracy: 7793/10000 =  77.93 % ||| loss 0.6037753820419312\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5765074801445007\n",
            "Batch #200 Loss: 0.568522185087204\n",
            "Batch #300 Loss: 0.5520430701971054\n",
            "\u001b[92mTrain accuracy: 38184/48000 =  79.55 % ||| loss 0.5562883019447327\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9566/12000 =  79.72 % ||| loss 0.5569199323654175\u001b[0m\n",
            "\u001b[92mTest accuracy: 7846/10000 =  78.46 % ||| loss 0.5887402296066284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.552929553091526\n",
            "Batch #200 Loss: 0.5487095901370048\n",
            "Batch #300 Loss: 0.5491806134581566\n",
            "\u001b[92mTrain accuracy: 38106/48000 =  79.39 % ||| loss 0.5427366495132446\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9550/12000 =  79.58 % ||| loss 0.5412043333053589\u001b[0m\n",
            "\u001b[92mTest accuracy: 7817/10000 =  78.17 % ||| loss 0.578393816947937\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.5162307465076447\n",
            "Batch #200 Loss: 0.546296863257885\n",
            "Batch #300 Loss: 0.5320670846104621\n",
            "\u001b[92mTrain accuracy: 38526/48000 =  80.26 % ||| loss 0.5213344097137451\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9621/12000 =  80.17 % ||| loss 0.5206208229064941\u001b[0m\n",
            "\u001b[92mTest accuracy: 7940/10000 =  79.4 % ||| loss 0.5490655303001404\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5172278121113777\n",
            "Batch #200 Loss: 0.5235006776452065\n",
            "Batch #300 Loss: 0.5157005864381791\n",
            "\u001b[92mTrain accuracy: 39136/48000 =  81.53 % ||| loss 0.49911096692085266\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9814/12000 =  81.78 % ||| loss 0.49993807077407837\u001b[0m\n",
            "\u001b[92mTest accuracy: 8040/10000 =  80.4 % ||| loss 0.5358023047447205\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.5074910560250282\n",
            "Batch #200 Loss: 0.50029766112566\n",
            "Batch #300 Loss: 0.5040457361936569\n",
            "\u001b[92mTrain accuracy: 39305/48000 =  81.89 % ||| loss 0.49174168705940247\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9876/12000 =  82.3 % ||| loss 0.49006661772727966\u001b[0m\n",
            "\u001b[92mTest accuracy: 8075/10000 =  80.75 % ||| loss 0.530819296836853\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.49979747623205184\n",
            "Batch #200 Loss: 0.492859870493412\n",
            "Batch #300 Loss: 0.4958606678247452\n",
            "\u001b[92mTrain accuracy: 39693/48000 =  82.69 % ||| loss 0.4796123504638672\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9928/12000 =  82.73 % ||| loss 0.4813411831855774\u001b[0m\n",
            "\u001b[92mTest accuracy: 8148/10000 =  81.48 % ||| loss 0.5167365074157715\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.4909879198670387\n",
            "Batch #200 Loss: 0.48382566213607786\n",
            "Batch #300 Loss: 0.49137859970331194\n",
            "\u001b[92mTrain accuracy: 39759/48000 =  82.83 % ||| loss 0.47249695658683777\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9938/12000 =  82.82 % ||| loss 0.4748111069202423\u001b[0m\n",
            "\u001b[92mTest accuracy: 8223/10000 =  82.23 % ||| loss 0.5045814514160156\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.4799023175239563\n",
            "Batch #200 Loss: 0.48025640577077866\n",
            "Batch #300 Loss: 0.4784733140468597\n",
            "\u001b[92mTrain accuracy: 39309/48000 =  81.89 % ||| loss 0.4831579923629761\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9831/12000 =  81.92 % ||| loss 0.48447316884994507\u001b[0m\n",
            "\u001b[92mTest accuracy: 8073/10000 =  80.73 % ||| loss 0.5172930955886841\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.4659343272447586\n",
            "Batch #200 Loss: 0.46014540880918503\n",
            "Batch #300 Loss: 0.4730514153838158\n",
            "\u001b[92mTrain accuracy: 39957/48000 =  83.24 % ||| loss 0.4641876518726349\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9951/12000 =  82.93 % ||| loss 0.4680636525154114\u001b[0m\n",
            "\u001b[92mTest accuracy: 8219/10000 =  82.19 % ||| loss 0.49577900767326355\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.46736912071704867\n",
            "Batch #200 Loss: 0.46548115223646164\n",
            "Batch #300 Loss: 0.44444619297981264\n",
            "\u001b[92mTrain accuracy: 40115/48000 =  83.57 % ||| loss 0.45038488507270813\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10018/12000 =  83.48 % ||| loss 0.4565759599208832\u001b[0m\n",
            "\u001b[92mTest accuracy: 8249/10000 =  82.49 % ||| loss 0.48554593324661255\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.45302159041166307\n",
            "Batch #200 Loss: 0.4556168672442436\n",
            "Batch #300 Loss: 0.4460603258013725\n",
            "\u001b[92mTrain accuracy: 40034/48000 =  83.4 % ||| loss 0.45010289549827576\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9992/12000 =  83.27 % ||| loss 0.45746085047721863\u001b[0m\n",
            "\u001b[92mTest accuracy: 8215/10000 =  82.15 % ||| loss 0.485554963350296\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.43908749520778656\n",
            "Batch #200 Loss: 0.43480776488780976\n",
            "Batch #300 Loss: 0.4525485110282898\n",
            "\u001b[92mTrain accuracy: 40463/48000 =  84.3 % ||| loss 0.43476545810699463\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10106/12000 =  84.22 % ||| loss 0.44287627935409546\u001b[0m\n",
            "\u001b[92mTest accuracy: 8322/10000 =  83.22 % ||| loss 0.47242650389671326\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.43889338850975035\n",
            "Batch #200 Loss: 0.44220031827688216\n",
            "Batch #300 Loss: 0.4351025214791298\n",
            "\u001b[92mTrain accuracy: 40649/48000 =  84.69 % ||| loss 0.42443859577178955\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10110/12000 =  84.25 % ||| loss 0.4332657754421234\u001b[0m\n",
            "\u001b[92mTest accuracy: 8376/10000 =  83.76 % ||| loss 0.4616447985172272\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.4286670345067978\n",
            "Batch #200 Loss: 0.4363866963982582\n",
            "Batch #300 Loss: 0.43363952815532686\n",
            "\u001b[92mTrain accuracy: 40246/48000 =  83.85 % ||| loss 0.4381040930747986\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10044/12000 =  83.7 % ||| loss 0.44730183482170105\u001b[0m\n",
            "\u001b[92mTest accuracy: 8273/10000 =  82.73 % ||| loss 0.4736238718032837\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726086139.930381_3</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_3</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_162935-Lenet5_1726086139.930381_4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_4' target=\"_blank\">Lenet5_1726086139.930381_4</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2647216057777406\n",
            "Batch #200 Loss: 1.1113633871078492\n",
            "Batch #300 Loss: 0.8174711203575135\n",
            "\u001b[92mTrain accuracy: 35496/48000 =  73.95 % ||| loss 0.7023919820785522\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8876/12000 =  73.97 % ||| loss 0.6963402032852173\u001b[0m\n",
            "\u001b[92mTest accuracy: 7346/10000 =  73.46 % ||| loss 0.7198643088340759\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6721812948584557\n",
            "Batch #200 Loss: 0.6212318435311317\n",
            "Batch #300 Loss: 0.5913525414466858\n",
            "\u001b[92mTrain accuracy: 38477/48000 =  80.16 % ||| loss 0.5213927626609802\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9638/12000 =  80.32 % ||| loss 0.5202727317810059\u001b[0m\n",
            "\u001b[92mTest accuracy: 7967/10000 =  79.67 % ||| loss 0.5518684983253479\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5416123470664025\n",
            "Batch #200 Loss: 0.5025940489768982\n",
            "Batch #300 Loss: 0.4940113976597786\n",
            "\u001b[92mTrain accuracy: 38729/48000 =  80.69 % ||| loss 0.5018914937973022\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9732/12000 =  81.1 % ||| loss 0.5007093548774719\u001b[0m\n",
            "\u001b[92mTest accuracy: 7969/10000 =  79.69 % ||| loss 0.5362968444824219\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.4697426727414131\n",
            "Batch #200 Loss: 0.4562531924247742\n",
            "Batch #300 Loss: 0.43806578099727633\n",
            "\u001b[92mTrain accuracy: 40584/48000 =  84.55 % ||| loss 0.4208023250102997\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10157/12000 =  84.64 % ||| loss 0.4281992018222809\u001b[0m\n",
            "\u001b[92mTest accuracy: 8380/10000 =  83.8 % ||| loss 0.4626622796058655\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.42653279364109037\n",
            "Batch #200 Loss: 0.41084809258580207\n",
            "Batch #300 Loss: 0.40618662908673286\n",
            "\u001b[92mTrain accuracy: 41337/48000 =  86.12 % ||| loss 0.381542444229126\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10305/12000 =  85.88 % ||| loss 0.39395982027053833\u001b[0m\n",
            "\u001b[92mTest accuracy: 8466/10000 =  84.66 % ||| loss 0.4171045422554016\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.3958035486936569\n",
            "Batch #200 Loss: 0.3872093157470226\n",
            "Batch #300 Loss: 0.37533842340111734\n",
            "\u001b[92mTrain accuracy: 41826/48000 =  87.14 % ||| loss 0.3550727069377899\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10384/12000 =  86.53 % ||| loss 0.3662172257900238\u001b[0m\n",
            "\u001b[92mTest accuracy: 8575/10000 =  85.75 % ||| loss 0.3933694362640381\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3647639186680317\n",
            "Batch #200 Loss: 0.36885954707860946\n",
            "Batch #300 Loss: 0.3459305949509144\n",
            "\u001b[92mTrain accuracy: 42001/48000 =  87.5 % ||| loss 0.33663976192474365\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10461/12000 =  87.17 % ||| loss 0.3545476496219635\u001b[0m\n",
            "\u001b[92mTest accuracy: 8623/10000 =  86.23 % ||| loss 0.3756539523601532\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3486397661268711\n",
            "Batch #200 Loss: 0.34822519704699517\n",
            "Batch #300 Loss: 0.34074308052659036\n",
            "\u001b[92mTrain accuracy: 41887/48000 =  87.26 % ||| loss 0.34775322675704956\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10368/12000 =  86.4 % ||| loss 0.3698442876338959\u001b[0m\n",
            "\u001b[92mTest accuracy: 8532/10000 =  85.32 % ||| loss 0.4041782021522522\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.32785648614168167\n",
            "Batch #200 Loss: 0.33300720393657685\n",
            "Batch #300 Loss: 0.3441021639108658\n",
            "\u001b[92mTrain accuracy: 42430/48000 =  88.4 % ||| loss 0.31738847494125366\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10514/12000 =  87.62 % ||| loss 0.34351786971092224\u001b[0m\n",
            "\u001b[92mTest accuracy: 8674/10000 =  86.74 % ||| loss 0.36575737595558167\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3086488336324692\n",
            "Batch #200 Loss: 0.3199490597844124\n",
            "Batch #300 Loss: 0.3255452750623226\n",
            "\u001b[92mTrain accuracy: 42222/48000 =  87.96 % ||| loss 0.3227009177207947\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10452/12000 =  87.1 % ||| loss 0.35685861110687256\u001b[0m\n",
            "\u001b[92mTest accuracy: 8652/10000 =  86.52 % ||| loss 0.37514013051986694\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.2978783230483532\n",
            "Batch #200 Loss: 0.3127672451734543\n",
            "Batch #300 Loss: 0.305836224257946\n",
            "\u001b[92mTrain accuracy: 42487/48000 =  88.51 % ||| loss 0.3073045313358307\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10515/12000 =  87.62 % ||| loss 0.3384888768196106\u001b[0m\n",
            "\u001b[92mTest accuracy: 8696/10000 =  86.96 % ||| loss 0.3594420552253723\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.29270726919174195\n",
            "Batch #200 Loss: 0.3000314688682556\n",
            "Batch #300 Loss: 0.3021180035173893\n",
            "\u001b[92mTrain accuracy: 43071/48000 =  89.73 % ||| loss 0.28086477518081665\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10626/12000 =  88.55 % ||| loss 0.3175666630268097\u001b[0m\n",
            "\u001b[92mTest accuracy: 8771/10000 =  87.71 % ||| loss 0.342079222202301\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.2939605747163296\n",
            "Batch #200 Loss: 0.2989205759763718\n",
            "Batch #300 Loss: 0.29165327951312064\n",
            "\u001b[92mTrain accuracy: 42936/48000 =  89.45 % ||| loss 0.28719261288642883\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10588/12000 =  88.23 % ||| loss 0.32637229561805725\u001b[0m\n",
            "\u001b[92mTest accuracy: 8730/10000 =  87.3 % ||| loss 0.3477227985858917\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.28661575555801394\n",
            "Batch #200 Loss: 0.27368976712226867\n",
            "Batch #300 Loss: 0.28638154894113543\n",
            "\u001b[92mTrain accuracy: 43223/48000 =  90.05 % ||| loss 0.273171991109848\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10608/12000 =  88.4 % ||| loss 0.3149513602256775\u001b[0m\n",
            "\u001b[92mTest accuracy: 8817/10000 =  88.17 % ||| loss 0.33339568972587585\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.27327836960554125\n",
            "Batch #200 Loss: 0.2821127927303314\n",
            "Batch #300 Loss: 0.27458224311470986\n",
            "\u001b[92mTrain accuracy: 43315/48000 =  90.24 % ||| loss 0.264403760433197\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10673/12000 =  88.94 % ||| loss 0.30922943353652954\u001b[0m\n",
            "\u001b[92mTest accuracy: 8818/10000 =  88.18 % ||| loss 0.32433775067329407\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.2705387355387211\n",
            "Batch #200 Loss: 0.26936594635248184\n",
            "Batch #300 Loss: 0.2763761313259602\n",
            "\u001b[92mTrain accuracy: 43172/48000 =  89.94 % ||| loss 0.2673625349998474\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10575/12000 =  88.12 % ||| loss 0.3171032965183258\u001b[0m\n",
            "\u001b[92mTest accuracy: 8776/10000 =  87.76 % ||| loss 0.3343490958213806\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.2647634944319725\n",
            "Batch #200 Loss: 0.2751105628907681\n",
            "Batch #300 Loss: 0.26987483449280264\n",
            "\u001b[92mTrain accuracy: 43483/48000 =  90.59 % ||| loss 0.2554682493209839\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10644/12000 =  88.7 % ||| loss 0.30673396587371826\u001b[0m\n",
            "\u001b[92mTest accuracy: 8817/10000 =  88.17 % ||| loss 0.3304649293422699\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.2568433843553066\n",
            "Batch #200 Loss: 0.2613592068850994\n",
            "Batch #300 Loss: 0.25783695980906485\n",
            "\u001b[92mTrain accuracy: 43315/48000 =  90.24 % ||| loss 0.2616032660007477\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10581/12000 =  88.17 % ||| loss 0.3192254900932312\u001b[0m\n",
            "\u001b[92mTest accuracy: 8775/10000 =  87.75 % ||| loss 0.33900660276412964\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.2524205879867077\n",
            "Batch #200 Loss: 0.2521834221482277\n",
            "Batch #300 Loss: 0.2504583656042814\n",
            "\u001b[92mTrain accuracy: 43886/48000 =  91.43 % ||| loss 0.23314251005649567\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10718/12000 =  89.32 % ||| loss 0.29130807518959045\u001b[0m\n",
            "\u001b[92mTest accuracy: 8909/10000 =  89.09 % ||| loss 0.3132486641407013\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.23397111639380455\n",
            "Batch #200 Loss: 0.2500714659690857\n",
            "Batch #300 Loss: 0.25612841479480264\n",
            "\u001b[92mTrain accuracy: 43950/48000 =  91.56 % ||| loss 0.22941739857196808\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10710/12000 =  89.25 % ||| loss 0.29263293743133545\u001b[0m\n",
            "\u001b[92mTest accuracy: 8889/10000 =  88.89 % ||| loss 0.31154048442840576\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.23469486512243748\n",
            "Batch #200 Loss: 0.24668628290295602\n",
            "Batch #300 Loss: 0.25281954601407053\n",
            "\u001b[92mTrain accuracy: 44097/48000 =  91.87 % ||| loss 0.21987192332744598\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10763/12000 =  89.69 % ||| loss 0.285332590341568\u001b[0m\n",
            "\u001b[92mTest accuracy: 8900/10000 =  89.0 % ||| loss 0.30843350291252136\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.2225877856463194\n",
            "Batch #200 Loss: 0.2394754458218813\n",
            "Batch #300 Loss: 0.2488995623588562\n",
            "\u001b[92mTrain accuracy: 44058/48000 =  91.79 % ||| loss 0.21763615310192108\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10706/12000 =  89.22 % ||| loss 0.29122495651245117\u001b[0m\n",
            "\u001b[92mTest accuracy: 8905/10000 =  89.05 % ||| loss 0.30591729283332825\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.23019387267529964\n",
            "Batch #200 Loss: 0.23005584344267846\n",
            "Batch #300 Loss: 0.2401965607702732\n",
            "\u001b[92mTrain accuracy: 44217/48000 =  92.12 % ||| loss 0.21332448720932007\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10780/12000 =  89.83 % ||| loss 0.28751280903816223\u001b[0m\n",
            "\u001b[92mTest accuracy: 8901/10000 =  89.01 % ||| loss 0.31053340435028076\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.22118725799024105\n",
            "Batch #200 Loss: 0.2320833993703127\n",
            "Batch #300 Loss: 0.2248514959216118\n",
            "\u001b[92mTrain accuracy: 44210/48000 =  92.1 % ||| loss 0.21110199391841888\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10761/12000 =  89.68 % ||| loss 0.2909785509109497\u001b[0m\n",
            "\u001b[92mTest accuracy: 8890/10000 =  88.9 % ||| loss 0.3127075433731079\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.2102212755382061\n",
            "Batch #200 Loss: 0.21894601985812187\n",
            "Batch #300 Loss: 0.22569212645292283\n",
            "\u001b[92mTrain accuracy: 44281/48000 =  92.25 % ||| loss 0.20897887647151947\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10747/12000 =  89.56 % ||| loss 0.29325464367866516\u001b[0m\n",
            "\u001b[92mTest accuracy: 8912/10000 =  89.12 % ||| loss 0.31210359930992126\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726086139.930381_4</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_4</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_163133-Lenet5_1726086139.930381_5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_5' target=\"_blank\">Lenet5_1726086139.930381_5</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302157437801361\n",
            "Batch #200 Loss: 2.2958237099647523\n",
            "Batch #300 Loss: 2.2803567123413084\n",
            "\u001b[92mTrain accuracy: 22502/48000 =  46.88 % ||| loss 1.9492415189743042\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5622/12000 =  46.85 % ||| loss 1.9476317167282104\u001b[0m\n",
            "\u001b[92mTest accuracy: 4682/10000 =  46.82 % ||| loss 1.9506611824035645\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 1.2308168429136277\n",
            "Batch #200 Loss: 0.874534747004509\n",
            "Batch #300 Loss: 0.7926766461133957\n",
            "\u001b[92mTrain accuracy: 34692/48000 =  72.28 % ||| loss 0.7441235184669495\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8737/12000 =  72.81 % ||| loss 0.72735196352005\u001b[0m\n",
            "\u001b[92mTest accuracy: 7223/10000 =  72.23 % ||| loss 0.7577136754989624\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6893545997142791\n",
            "Batch #200 Loss: 0.6657565227150917\n",
            "Batch #300 Loss: 0.6446489164233208\n",
            "\u001b[92mTrain accuracy: 37004/48000 =  77.09 % ||| loss 0.5918351411819458\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9307/12000 =  77.56 % ||| loss 0.584906280040741\u001b[0m\n",
            "\u001b[92mTest accuracy: 7627/10000 =  76.27 % ||| loss 0.6126964688301086\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.6121781355142594\n",
            "Batch #200 Loss: 0.5726757574081421\n",
            "Batch #300 Loss: 0.5702143383026123\n",
            "\u001b[92mTrain accuracy: 38119/48000 =  79.41 % ||| loss 0.5386345386505127\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9596/12000 =  79.97 % ||| loss 0.5355995893478394\u001b[0m\n",
            "\u001b[92mTest accuracy: 7853/10000 =  78.53 % ||| loss 0.5646294355392456\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5402382487058639\n",
            "Batch #200 Loss: 0.5274324107170105\n",
            "Batch #300 Loss: 0.5273090612888336\n",
            "\u001b[92mTrain accuracy: 38497/48000 =  80.2 % ||| loss 0.5217195749282837\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9639/12000 =  80.33 % ||| loss 0.5219237804412842\u001b[0m\n",
            "\u001b[92mTest accuracy: 7920/10000 =  79.2 % ||| loss 0.5527912378311157\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.49687876760959626\n",
            "Batch #200 Loss: 0.4981232592463493\n",
            "Batch #300 Loss: 0.48518639177083966\n",
            "\u001b[92mTrain accuracy: 39437/48000 =  82.16 % ||| loss 0.4671939015388489\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9815/12000 =  81.79 % ||| loss 0.47500911355018616\u001b[0m\n",
            "\u001b[92mTest accuracy: 8141/10000 =  81.41 % ||| loss 0.4992538094520569\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.4816667640209198\n",
            "Batch #200 Loss: 0.46757275879383087\n",
            "Batch #300 Loss: 0.44418235152959823\n",
            "\u001b[92mTrain accuracy: 40120/48000 =  83.58 % ||| loss 0.44422808289527893\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10034/12000 =  83.62 % ||| loss 0.4513111114501953\u001b[0m\n",
            "\u001b[92mTest accuracy: 8274/10000 =  82.74 % ||| loss 0.4751567244529724\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.4379628947377205\n",
            "Batch #200 Loss: 0.43847826182842253\n",
            "Batch #300 Loss: 0.4223031398653984\n",
            "\u001b[92mTrain accuracy: 40618/48000 =  84.62 % ||| loss 0.4168912470340729\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10113/12000 =  84.28 % ||| loss 0.4259093999862671\u001b[0m\n",
            "\u001b[92mTest accuracy: 8376/10000 =  83.76 % ||| loss 0.4506322741508484\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.4217738687992096\n",
            "Batch #200 Loss: 0.4097018733620644\n",
            "Batch #300 Loss: 0.40534322917461396\n",
            "\u001b[92mTrain accuracy: 41000/48000 =  85.42 % ||| loss 0.40057486295700073\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10226/12000 =  85.22 % ||| loss 0.4118594229221344\u001b[0m\n",
            "\u001b[92mTest accuracy: 8452/10000 =  84.52 % ||| loss 0.4360794425010681\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.40995027601718903\n",
            "Batch #200 Loss: 0.4002945823967457\n",
            "Batch #300 Loss: 0.3893736293911934\n",
            "\u001b[92mTrain accuracy: 41087/48000 =  85.6 % ||| loss 0.39720138907432556\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10229/12000 =  85.24 % ||| loss 0.4134446084499359\u001b[0m\n",
            "\u001b[92mTest accuracy: 8471/10000 =  84.71 % ||| loss 0.43111473321914673\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.3784260679781437\n",
            "Batch #200 Loss: 0.3801373191177845\n",
            "Batch #300 Loss: 0.39028206288814543\n",
            "\u001b[92mTrain accuracy: 41248/48000 =  85.93 % ||| loss 0.385263055562973\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10305/12000 =  85.88 % ||| loss 0.40421855449676514\u001b[0m\n",
            "\u001b[92mTest accuracy: 8520/10000 =  85.2 % ||| loss 0.4224601984024048\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3739033770561218\n",
            "Batch #200 Loss: 0.37945387497544286\n",
            "Batch #300 Loss: 0.3615385028719902\n",
            "\u001b[92mTrain accuracy: 41602/48000 =  86.67 % ||| loss 0.3625021278858185\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10322/12000 =  86.02 % ||| loss 0.37806233763694763\u001b[0m\n",
            "\u001b[92mTest accuracy: 8559/10000 =  85.59 % ||| loss 0.4019879102706909\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.3681191688776016\n",
            "Batch #200 Loss: 0.360924229323864\n",
            "Batch #300 Loss: 0.35790251776576043\n",
            "\u001b[92mTrain accuracy: 42067/48000 =  87.64 % ||| loss 0.3414326608181\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10444/12000 =  87.03 % ||| loss 0.3572317957878113\u001b[0m\n",
            "\u001b[92mTest accuracy: 8626/10000 =  86.26 % ||| loss 0.3824680745601654\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.36007762581110003\n",
            "Batch #200 Loss: 0.35448372811079026\n",
            "Batch #300 Loss: 0.34753458127379416\n",
            "\u001b[92mTrain accuracy: 42083/48000 =  87.67 % ||| loss 0.33631837368011475\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10426/12000 =  86.88 % ||| loss 0.35501208901405334\u001b[0m\n",
            "\u001b[92mTest accuracy: 8618/10000 =  86.18 % ||| loss 0.3780299723148346\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.34523691922426225\n",
            "Batch #200 Loss: 0.34226721584796904\n",
            "Batch #300 Loss: 0.343719070404768\n",
            "\u001b[92mTrain accuracy: 42163/48000 =  87.84 % ||| loss 0.33231985569000244\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10465/12000 =  87.21 % ||| loss 0.3520154058933258\u001b[0m\n",
            "\u001b[92mTest accuracy: 8632/10000 =  86.32 % ||| loss 0.37781015038490295\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.33718627035617826\n",
            "Batch #200 Loss: 0.34549716874957087\n",
            "Batch #300 Loss: 0.33502266585826873\n",
            "\u001b[92mTrain accuracy: 42627/48000 =  88.81 % ||| loss 0.3118189573287964\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10554/12000 =  87.95 % ||| loss 0.3348003327846527\u001b[0m\n",
            "\u001b[92mTest accuracy: 8698/10000 =  86.98 % ||| loss 0.36351218819618225\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3219035978615284\n",
            "Batch #200 Loss: 0.33525950849056246\n",
            "Batch #300 Loss: 0.33345088973641396\n",
            "\u001b[92mTrain accuracy: 42189/48000 =  87.89 % ||| loss 0.33038654923439026\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10455/12000 =  87.12 % ||| loss 0.35503333806991577\u001b[0m\n",
            "\u001b[92mTest accuracy: 8655/10000 =  86.55 % ||| loss 0.3756868541240692\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.32621267944574356\n",
            "Batch #200 Loss: 0.32195490553975104\n",
            "Batch #300 Loss: 0.3296770192682743\n",
            "\u001b[92mTrain accuracy: 42417/48000 =  88.37 % ||| loss 0.31591594219207764\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10523/12000 =  87.69 % ||| loss 0.3461066782474518\u001b[0m\n",
            "\u001b[92mTest accuracy: 8691/10000 =  86.91 % ||| loss 0.3672747015953064\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.3243849056959152\n",
            "Batch #200 Loss: 0.32154233381152153\n",
            "Batch #300 Loss: 0.3113009677827358\n",
            "\u001b[92mTrain accuracy: 42652/48000 =  88.86 % ||| loss 0.307606965303421\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10585/12000 =  88.21 % ||| loss 0.3391638696193695\u001b[0m\n",
            "\u001b[92mTest accuracy: 8713/10000 =  87.13 % ||| loss 0.3592280447483063\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.31659047335386276\n",
            "Batch #200 Loss: 0.31294605001807213\n",
            "Batch #300 Loss: 0.3165218509733677\n",
            "\u001b[92mTrain accuracy: 42472/48000 =  88.48 % ||| loss 0.3151623606681824\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10515/12000 =  87.62 % ||| loss 0.343971848487854\u001b[0m\n",
            "\u001b[92mTest accuracy: 8669/10000 =  86.69 % ||| loss 0.37163856625556946\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.30615127563476563\n",
            "Batch #200 Loss: 0.3024388074874878\n",
            "Batch #300 Loss: 0.3067124173045158\n",
            "\u001b[92mTrain accuracy: 42671/48000 =  88.9 % ||| loss 0.30404889583587646\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10539/12000 =  87.83 % ||| loss 0.3372030258178711\u001b[0m\n",
            "\u001b[92mTest accuracy: 8738/10000 =  87.38 % ||| loss 0.36336612701416016\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.29515073508024214\n",
            "Batch #200 Loss: 0.29815449759364127\n",
            "Batch #300 Loss: 0.3130842554569244\n",
            "\u001b[92mTrain accuracy: 42988/48000 =  89.56 % ||| loss 0.28658992052078247\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10611/12000 =  88.42 % ||| loss 0.3172885477542877\u001b[0m\n",
            "\u001b[92mTest accuracy: 8740/10000 =  87.4 % ||| loss 0.34763288497924805\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.2879245638847351\n",
            "Batch #200 Loss: 0.30116740450263024\n",
            "Batch #300 Loss: 0.3024767439067364\n",
            "\u001b[92mTrain accuracy: 42953/48000 =  89.49 % ||| loss 0.28548672795295715\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10606/12000 =  88.38 % ||| loss 0.3165648579597473\u001b[0m\n",
            "\u001b[92mTest accuracy: 8757/10000 =  87.57 % ||| loss 0.3453913927078247\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.29087107598781586\n",
            "Batch #200 Loss: 0.2995682325959206\n",
            "Batch #300 Loss: 0.29219874016940595\n",
            "\u001b[92mTrain accuracy: 43135/48000 =  89.86 % ||| loss 0.27816057205200195\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10611/12000 =  88.42 % ||| loss 0.31458210945129395\u001b[0m\n",
            "\u001b[92mTest accuracy: 8767/10000 =  87.67 % ||| loss 0.34245410561561584\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.2835524442791939\n",
            "Batch #200 Loss: 0.2908425813913345\n",
            "Batch #300 Loss: 0.28854168772697447\n",
            "\u001b[92mTrain accuracy: 42820/48000 =  89.21 % ||| loss 0.29154810309410095\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10554/12000 =  87.95 % ||| loss 0.32877638936042786\u001b[0m\n",
            "\u001b[92mTest accuracy: 8735/10000 =  87.35 % ||| loss 0.3533606231212616\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726086139.930381_5</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_5</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_163330-Lenet5_1726086139.930381_6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_6' target=\"_blank\">Lenet5_1726086139.930381_6</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.303585147857666\n",
            "Batch #200 Loss: 2.3025809073448182\n",
            "Batch #300 Loss: 2.3018727898597717\n",
            "\u001b[92mTrain accuracy: 5524/48000 =  11.51 % ||| loss 2.301271915435791\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1408/12000 =  11.73 % ||| loss 2.3016414642333984\u001b[0m\n",
            "\u001b[92mTest accuracy: 1154/10000 =  11.54 % ||| loss 2.3014867305755615\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3011486458778383\n",
            "Batch #200 Loss: 2.300493266582489\n",
            "Batch #300 Loss: 2.2998646783828733\n",
            "\u001b[92mTrain accuracy: 6972/48000 =  14.52 % ||| loss 2.2995645999908447\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1758/12000 =  14.65 % ||| loss 2.299929618835449\u001b[0m\n",
            "\u001b[92mTest accuracy: 1463/10000 =  14.63 % ||| loss 2.2996625900268555\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.2989095425605774\n",
            "Batch #200 Loss: 2.2992150592803955\n",
            "Batch #300 Loss: 2.2985727310180666\n",
            "\u001b[92mTrain accuracy: 8014/48000 =  16.7 % ||| loss 2.29784893989563\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2031/12000 =  16.93 % ||| loss 2.2981812953948975\u001b[0m\n",
            "\u001b[92mTest accuracy: 1662/10000 =  16.62 % ||| loss 2.297973871231079\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.297902190685272\n",
            "Batch #200 Loss: 2.297387726306915\n",
            "Batch #300 Loss: 2.2966246962547303\n",
            "\u001b[92mTrain accuracy: 8495/48000 =  17.7 % ||| loss 2.2960121631622314\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2159/12000 =  17.99 % ||| loss 2.296353816986084\u001b[0m\n",
            "\u001b[92mTest accuracy: 1741/10000 =  17.41 % ||| loss 2.295933485031128\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.296065056324005\n",
            "Batch #200 Loss: 2.2947304153442385\n",
            "Batch #300 Loss: 2.2951113033294677\n",
            "\u001b[92mTrain accuracy: 8718/48000 =  18.16 % ||| loss 2.293905019760132\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2225/12000 =  18.54 % ||| loss 2.294222593307495\u001b[0m\n",
            "\u001b[92mTest accuracy: 1794/10000 =  17.94 % ||| loss 2.2939436435699463\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.293048791885376\n",
            "Batch #200 Loss: 2.2930659222602845\n",
            "Batch #300 Loss: 2.2921367859840394\n",
            "\u001b[92mTrain accuracy: 8870/48000 =  18.48 % ||| loss 2.2913475036621094\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2252/12000 =  18.77 % ||| loss 2.291672945022583\u001b[0m\n",
            "\u001b[92mTest accuracy: 1830/10000 =  18.3 % ||| loss 2.2913284301757812\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.2914766383171083\n",
            "Batch #200 Loss: 2.2900385332107542\n",
            "Batch #300 Loss: 2.2890558362007143\n",
            "\u001b[92mTrain accuracy: 9688/48000 =  20.18 % ||| loss 2.288239002227783\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2465/12000 =  20.54 % ||| loss 2.288546562194824\u001b[0m\n",
            "\u001b[92mTest accuracy: 1997/10000 =  19.97 % ||| loss 2.2885212898254395\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.2877031898498537\n",
            "Batch #200 Loss: 2.287001793384552\n",
            "Batch #300 Loss: 2.2858122205734253\n",
            "\u001b[92mTrain accuracy: 12048/48000 =  25.1 % ||| loss 2.28427791595459\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3037/12000 =  25.31 % ||| loss 2.28458833694458\u001b[0m\n",
            "\u001b[92mTest accuracy: 2519/10000 =  25.19 % ||| loss 2.2842636108398438\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.283652527332306\n",
            "Batch #200 Loss: 2.2822688221931458\n",
            "Batch #300 Loss: 2.280728027820587\n",
            "\u001b[92mTrain accuracy: 14627/48000 =  30.47 % ||| loss 2.278841733932495\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3623/12000 =  30.19 % ||| loss 2.279154062271118\u001b[0m\n",
            "\u001b[92mTest accuracy: 3053/10000 =  30.53 % ||| loss 2.2789926528930664\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.2778698587417603\n",
            "Batch #200 Loss: 2.276154761314392\n",
            "Batch #300 Loss: 2.2743976664543153\n",
            "\u001b[92mTrain accuracy: 16867/48000 =  35.14 % ||| loss 2.2711267471313477\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4184/12000 =  34.87 % ||| loss 2.2714486122131348\u001b[0m\n",
            "\u001b[92mTest accuracy: 3511/10000 =  35.11 % ||| loss 2.2710604667663574\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.2695389342308045\n",
            "Batch #200 Loss: 2.267746844291687\n",
            "Batch #300 Loss: 2.2637557673454283\n",
            "\u001b[92mTrain accuracy: 18454/48000 =  38.45 % ||| loss 2.2595605850219727\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4589/12000 =  38.24 % ||| loss 2.2599117755889893\u001b[0m\n",
            "\u001b[92mTest accuracy: 3814/10000 =  38.14 % ||| loss 2.259411096572876\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.2573345351219176\n",
            "Batch #200 Loss: 2.2533821153640745\n",
            "Batch #300 Loss: 2.2480314493179323\n",
            "\u001b[92mTrain accuracy: 19942/48000 =  41.55 % ||| loss 2.240234613418579\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4979/12000 =  41.49 % ||| loss 2.2405924797058105\u001b[0m\n",
            "\u001b[92mTest accuracy: 4123/10000 =  41.23 % ||| loss 2.2401866912841797\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.2373407101631164\n",
            "Batch #200 Loss: 2.228783574104309\n",
            "Batch #300 Loss: 2.21860497713089\n",
            "\u001b[92mTrain accuracy: 21234/48000 =  44.24 % ||| loss 2.2040557861328125\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5280/12000 =  44.0 % ||| loss 2.2042853832244873\u001b[0m\n",
            "\u001b[92mTest accuracy: 4425/10000 =  44.25 % ||| loss 2.204245090484619\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.195640113353729\n",
            "Batch #200 Loss: 2.183151652812958\n",
            "Batch #300 Loss: 2.1646050453186034\n",
            "\u001b[92mTrain accuracy: 19675/48000 =  40.99 % ||| loss 2.134345293045044\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4847/12000 =  40.39 % ||| loss 2.1344780921936035\u001b[0m\n",
            "\u001b[92mTest accuracy: 4078/10000 =  40.78 % ||| loss 2.1342084407806396\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.120404477119446\n",
            "Batch #200 Loss: 2.0867428541183473\n",
            "Batch #300 Loss: 2.0500504577159884\n",
            "\u001b[92mTrain accuracy: 19271/48000 =  40.15 % ||| loss 1.9830461740493774\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4836/12000 =  40.3 % ||| loss 1.982956051826477\u001b[0m\n",
            "\u001b[92mTest accuracy: 3997/10000 =  39.97 % ||| loss 1.982932209968567\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 1.954772778749466\n",
            "Batch #200 Loss: 1.8837683832645415\n",
            "Batch #300 Loss: 1.8178178977966308\n",
            "\u001b[92mTrain accuracy: 21216/48000 =  44.2 % ||| loss 1.7165334224700928\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5381/12000 =  44.84 % ||| loss 1.7152067422866821\u001b[0m\n",
            "\u001b[92mTest accuracy: 4396/10000 =  43.96 % ||| loss 1.7171826362609863\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 1.6789756262302398\n",
            "Batch #200 Loss: 1.6014671206474305\n",
            "Batch #300 Loss: 1.5247731304168701\n",
            "\u001b[92mTrain accuracy: 26385/48000 =  54.97 % ||| loss 1.4442195892333984\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6614/12000 =  55.12 % ||| loss 1.4415782690048218\u001b[0m\n",
            "\u001b[92mTest accuracy: 5440/10000 =  54.4 % ||| loss 1.4449108839035034\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 1.4105964720249176\n",
            "Batch #200 Loss: 1.361451941728592\n",
            "Batch #300 Loss: 1.3009894609451294\n",
            "\u001b[92mTrain accuracy: 27846/48000 =  58.01 % ||| loss 1.2482097148895264\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6972/12000 =  58.1 % ||| loss 1.24408757686615\u001b[0m\n",
            "\u001b[92mTest accuracy: 5751/10000 =  57.51 % ||| loss 1.2534403800964355\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 1.2297065687179565\n",
            "Batch #200 Loss: 1.1943533432483673\n",
            "Batch #300 Loss: 1.1568147897720338\n",
            "\u001b[92mTrain accuracy: 28808/48000 =  60.02 % ||| loss 1.1318891048431396\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7215/12000 =  60.12 % ||| loss 1.125679850578308\u001b[0m\n",
            "\u001b[92mTest accuracy: 5966/10000 =  59.66 % ||| loss 1.1383938789367676\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 1.1294214951992034\n",
            "Batch #200 Loss: 1.0925635027885436\n",
            "Batch #300 Loss: 1.0824482548236847\n",
            "\u001b[92mTrain accuracy: 29727/48000 =  61.93 % ||| loss 1.0623624324798584\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7487/12000 =  62.39 % ||| loss 1.0541752576828003\u001b[0m\n",
            "\u001b[92mTest accuracy: 6147/10000 =  61.47 % ||| loss 1.0692089796066284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 1.042519696354866\n",
            "Batch #200 Loss: 1.0499531930685044\n",
            "Batch #300 Loss: 1.0313487285375595\n",
            "\u001b[92mTrain accuracy: 30054/48000 =  62.61 % ||| loss 1.015627145767212\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7574/12000 =  63.12 % ||| loss 1.0065947771072388\u001b[0m\n",
            "\u001b[92mTest accuracy: 6195/10000 =  61.95 % ||| loss 1.0269176959991455\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 1.0167097914218903\n",
            "Batch #200 Loss: 0.993200854063034\n",
            "Batch #300 Loss: 0.9894676566123962\n",
            "\u001b[92mTrain accuracy: 30560/48000 =  63.67 % ||| loss 0.9834895730018616\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7703/12000 =  64.19 % ||| loss 0.9746293425559998\u001b[0m\n",
            "\u001b[92mTest accuracy: 6328/10000 =  63.28 % ||| loss 0.9938574433326721\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.9724433052539826\n",
            "Batch #200 Loss: 0.9713692289590835\n",
            "Batch #300 Loss: 0.9670128434896469\n",
            "\u001b[92mTrain accuracy: 30747/48000 =  64.06 % ||| loss 0.9611342549324036\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7759/12000 =  64.66 % ||| loss 0.9524949789047241\u001b[0m\n",
            "\u001b[92mTest accuracy: 6353/10000 =  63.53 % ||| loss 0.9755836129188538\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.9548474967479705\n",
            "Batch #200 Loss: 0.9400916868448257\n",
            "Batch #300 Loss: 0.9498599988222122\n",
            "\u001b[92mTrain accuracy: 31239/48000 =  65.08 % ||| loss 0.9381389021873474\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7894/12000 =  65.78 % ||| loss 0.9280182719230652\u001b[0m\n",
            "\u001b[92mTest accuracy: 6441/10000 =  64.41 % ||| loss 0.9579623937606812\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.945551009774208\n",
            "Batch #200 Loss: 0.9181411218643188\n",
            "Batch #300 Loss: 0.9284214061498642\n",
            "\u001b[92mTrain accuracy: 31226/48000 =  65.05 % ||| loss 0.9218364357948303\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7807/12000 =  65.06 % ||| loss 0.9123519659042358\u001b[0m\n",
            "\u001b[92mTest accuracy: 6444/10000 =  64.44 % ||| loss 0.9420027136802673\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726086139.930381_6</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_6</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_163521-Lenet5_1726086139.930381_7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_7' target=\"_blank\">Lenet5_1726086139.930381_7</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302724525928497\n",
            "Batch #200 Loss: 2.300033738613129\n",
            "Batch #300 Loss: 2.297971234321594\n",
            "\u001b[92mTrain accuracy: 5282/48000 =  11.0 % ||| loss 2.292156457901001\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1290/12000 =  10.75 % ||| loss 2.2919869422912598\u001b[0m\n",
            "\u001b[92mTest accuracy: 1106/10000 =  11.06 % ||| loss 2.292065382003784\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.288941659927368\n",
            "Batch #200 Loss: 2.2780629539489747\n",
            "Batch #300 Loss: 2.2525090527534486\n",
            "\u001b[92mTrain accuracy: 9795/48000 =  20.41 % ||| loss 2.1421713829040527\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2445/12000 =  20.38 % ||| loss 2.1407570838928223\u001b[0m\n",
            "\u001b[92mTest accuracy: 2071/10000 =  20.71 % ||| loss 2.1427695751190186\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.9687795078754424\n",
            "Batch #200 Loss: 1.356781769990921\n",
            "Batch #300 Loss: 1.054727975130081\n",
            "\u001b[92mTrain accuracy: 31017/48000 =  64.62 % ||| loss 0.9512273669242859\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7813/12000 =  65.11 % ||| loss 0.941399335861206\u001b[0m\n",
            "\u001b[92mTest accuracy: 6420/10000 =  64.2 % ||| loss 0.9638310670852661\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.942530146241188\n",
            "Batch #200 Loss: 0.9027587360143662\n",
            "Batch #300 Loss: 0.8759881287813187\n",
            "\u001b[92mTrain accuracy: 32973/48000 =  68.69 % ||| loss 0.8401111364364624\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8246/12000 =  68.72 % ||| loss 0.8308099508285522\u001b[0m\n",
            "\u001b[92mTest accuracy: 6773/10000 =  67.73 % ||| loss 0.861822783946991\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.8315782463550567\n",
            "Batch #200 Loss: 0.8070786648988724\n",
            "Batch #300 Loss: 0.7815755540132523\n",
            "\u001b[92mTrain accuracy: 34451/48000 =  71.77 % ||| loss 0.770455539226532\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8644/12000 =  72.03 % ||| loss 0.7621490955352783\u001b[0m\n",
            "\u001b[92mTest accuracy: 7120/10000 =  71.2 % ||| loss 0.7874165177345276\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.7566966354846955\n",
            "Batch #200 Loss: 0.7495016747713089\n",
            "Batch #300 Loss: 0.7507551884651185\n",
            "\u001b[92mTrain accuracy: 35424/48000 =  73.8 % ||| loss 0.7104314565658569\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8871/12000 =  73.92 % ||| loss 0.7026447653770447\u001b[0m\n",
            "\u001b[92mTest accuracy: 7344/10000 =  73.44 % ||| loss 0.7325896620750427\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.7038167446851731\n",
            "Batch #200 Loss: 0.7167593657970428\n",
            "Batch #300 Loss: 0.7008334845304489\n",
            "\u001b[92mTrain accuracy: 36084/48000 =  75.17 % ||| loss 0.6707682609558105\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9054/12000 =  75.45 % ||| loss 0.66221684217453\u001b[0m\n",
            "\u001b[92mTest accuracy: 7454/10000 =  74.54 % ||| loss 0.6950280666351318\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.6800346153974534\n",
            "Batch #200 Loss: 0.674359102845192\n",
            "Batch #300 Loss: 0.663604279756546\n",
            "\u001b[92mTrain accuracy: 36468/48000 =  75.98 % ||| loss 0.6508097052574158\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9116/12000 =  75.97 % ||| loss 0.6438848376274109\u001b[0m\n",
            "\u001b[92mTest accuracy: 7498/10000 =  74.98 % ||| loss 0.6794844269752502\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.65811070561409\n",
            "Batch #200 Loss: 0.6413872757554054\n",
            "Batch #300 Loss: 0.6491544821858406\n",
            "\u001b[92mTrain accuracy: 36674/48000 =  76.4 % ||| loss 0.6223223209381104\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9223/12000 =  76.86 % ||| loss 0.614974319934845\u001b[0m\n",
            "\u001b[92mTest accuracy: 7565/10000 =  75.65 % ||| loss 0.6514606475830078\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.6319647955894471\n",
            "Batch #200 Loss: 0.6181796377897263\n",
            "Batch #300 Loss: 0.6149925312399864\n",
            "\u001b[92mTrain accuracy: 37124/48000 =  77.34 % ||| loss 0.6124968528747559\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9284/12000 =  77.37 % ||| loss 0.6078909635543823\u001b[0m\n",
            "\u001b[92mTest accuracy: 7665/10000 =  76.65 % ||| loss 0.6467745900154114\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6039303016662597\n",
            "Batch #200 Loss: 0.5988818526268005\n",
            "Batch #300 Loss: 0.5984697610139846\n",
            "\u001b[92mTrain accuracy: 37482/48000 =  78.09 % ||| loss 0.5866909027099609\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9409/12000 =  78.41 % ||| loss 0.5796025991439819\u001b[0m\n",
            "\u001b[92mTest accuracy: 7715/10000 =  77.15 % ||| loss 0.6173292994499207\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5868300724029542\n",
            "Batch #200 Loss: 0.5857903426885605\n",
            "Batch #300 Loss: 0.5800740653276444\n",
            "\u001b[92mTrain accuracy: 38389/48000 =  79.98 % ||| loss 0.5559415817260742\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9665/12000 =  80.54 % ||| loss 0.5535373091697693\u001b[0m\n",
            "\u001b[92mTest accuracy: 7864/10000 =  78.64 % ||| loss 0.5896570086479187\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5820403781533241\n",
            "Batch #200 Loss: 0.5616627290844918\n",
            "Batch #300 Loss: 0.545373754799366\n",
            "\u001b[92mTrain accuracy: 37941/48000 =  79.04 % ||| loss 0.5506992340087891\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9536/12000 =  79.47 % ||| loss 0.5483502149581909\u001b[0m\n",
            "\u001b[92mTest accuracy: 7765/10000 =  77.65 % ||| loss 0.5848644971847534\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.5469041332602501\n",
            "Batch #200 Loss: 0.5486713004112244\n",
            "Batch #300 Loss: 0.5392201098799706\n",
            "\u001b[92mTrain accuracy: 38677/48000 =  80.58 % ||| loss 0.5272812247276306\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9713/12000 =  80.94 % ||| loss 0.5279285907745361\u001b[0m\n",
            "\u001b[92mTest accuracy: 7950/10000 =  79.5 % ||| loss 0.5597284436225891\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5204610580205917\n",
            "Batch #200 Loss: 0.528242821097374\n",
            "Batch #300 Loss: 0.5298608937859535\n",
            "\u001b[92mTrain accuracy: 38834/48000 =  80.9 % ||| loss 0.525196373462677\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9672/12000 =  80.6 % ||| loss 0.5272635817527771\u001b[0m\n",
            "\u001b[92mTest accuracy: 7973/10000 =  79.73 % ||| loss 0.5551720857620239\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.52180085927248\n",
            "Batch #200 Loss: 0.5011003014445304\n",
            "Batch #300 Loss: 0.5209837758541107\n",
            "\u001b[92mTrain accuracy: 39420/48000 =  82.12 % ||| loss 0.5009781718254089\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9885/12000 =  82.38 % ||| loss 0.5014055371284485\u001b[0m\n",
            "\u001b[92mTest accuracy: 8078/10000 =  80.78 % ||| loss 0.5343567728996277\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5203913393616676\n",
            "Batch #200 Loss: 0.5021407353878021\n",
            "Batch #300 Loss: 0.48150349378585816\n",
            "\u001b[92mTrain accuracy: 39206/48000 =  81.68 % ||| loss 0.5027890801429749\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9769/12000 =  81.41 % ||| loss 0.5044587850570679\u001b[0m\n",
            "\u001b[92mTest accuracy: 8063/10000 =  80.63 % ||| loss 0.5398983359336853\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.4999662160873413\n",
            "Batch #200 Loss: 0.47989245504140854\n",
            "Batch #300 Loss: 0.49823050320148465\n",
            "\u001b[92mTrain accuracy: 39183/48000 =  81.63 % ||| loss 0.49579814076423645\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9784/12000 =  81.53 % ||| loss 0.4994945824146271\u001b[0m\n",
            "\u001b[92mTest accuracy: 8089/10000 =  80.89 % ||| loss 0.5281287431716919\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.4874801328778267\n",
            "Batch #200 Loss: 0.4790656042098999\n",
            "Batch #300 Loss: 0.4703328946232796\n",
            "\u001b[92mTrain accuracy: 39409/48000 =  82.1 % ||| loss 0.49180305004119873\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9826/12000 =  81.88 % ||| loss 0.4980560541152954\u001b[0m\n",
            "\u001b[92mTest accuracy: 8127/10000 =  81.27 % ||| loss 0.5289812684059143\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.47024801969528196\n",
            "Batch #200 Loss: 0.4763584366440773\n",
            "Batch #300 Loss: 0.4765686276555061\n",
            "\u001b[92mTrain accuracy: 39915/48000 =  83.16 % ||| loss 0.4670058488845825\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9967/12000 =  83.06 % ||| loss 0.4742482304573059\u001b[0m\n",
            "\u001b[92mTest accuracy: 8248/10000 =  82.48 % ||| loss 0.501400887966156\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.46842620611190794\n",
            "Batch #200 Loss: 0.4688309532403946\n",
            "Batch #300 Loss: 0.4623823137581348\n",
            "\u001b[92mTrain accuracy: 39966/48000 =  83.26 % ||| loss 0.46391305327415466\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9968/12000 =  83.07 % ||| loss 0.4709455966949463\u001b[0m\n",
            "\u001b[92mTest accuracy: 8238/10000 =  82.38 % ||| loss 0.502586841583252\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.4677521723508835\n",
            "Batch #200 Loss: 0.46773757427930834\n",
            "Batch #300 Loss: 0.45027316123247146\n",
            "\u001b[92mTrain accuracy: 40357/48000 =  84.08 % ||| loss 0.4429800510406494\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10099/12000 =  84.16 % ||| loss 0.44919678568840027\u001b[0m\n",
            "\u001b[92mTest accuracy: 8279/10000 =  82.79 % ||| loss 0.4755537211894989\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.4530151867866516\n",
            "Batch #200 Loss: 0.44582402348518374\n",
            "Batch #300 Loss: 0.4548600760102272\n",
            "\u001b[92mTrain accuracy: 40100/48000 =  83.54 % ||| loss 0.4522724747657776\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10002/12000 =  83.35 % ||| loss 0.458700031042099\u001b[0m\n",
            "\u001b[92mTest accuracy: 8252/10000 =  82.52 % ||| loss 0.4840976297855377\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.4435715273022652\n",
            "Batch #200 Loss: 0.44483681887388227\n",
            "Batch #300 Loss: 0.4538881716132164\n",
            "\u001b[92mTrain accuracy: 40421/48000 =  84.21 % ||| loss 0.4420313835144043\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10058/12000 =  83.82 % ||| loss 0.45071592926979065\u001b[0m\n",
            "\u001b[92mTest accuracy: 8319/10000 =  83.19 % ||| loss 0.4756339490413666\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.43443508982658385\n",
            "Batch #200 Loss: 0.44031220972537993\n",
            "Batch #300 Loss: 0.4366411367058754\n",
            "\u001b[92mTrain accuracy: 40276/48000 =  83.91 % ||| loss 0.44477903842926025\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10102/12000 =  84.18 % ||| loss 0.4518008828163147\u001b[0m\n",
            "\u001b[92mTest accuracy: 8280/10000 =  82.8 % ||| loss 0.48178666830062866\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726086139.930381_7</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_7</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_163710-Lenet5_1726086139.930381_8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_8' target=\"_blank\">Lenet5_1726086139.930381_8</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.305977053642273\n",
            "Batch #200 Loss: 2.3043637681007385\n",
            "Batch #300 Loss: 2.303945231437683\n",
            "\u001b[92mTrain accuracy: 4787/48000 =  9.973 % ||| loss 2.303093671798706\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1213/12000 =  10.11 % ||| loss 2.303616523742676\u001b[0m\n",
            "\u001b[92mTest accuracy: 1004/10000 =  10.04 % ||| loss 2.3035027980804443\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3025988864898683\n",
            "Batch #200 Loss: 2.3015040445327757\n",
            "Batch #300 Loss: 2.3017877030372618\n",
            "\u001b[92mTrain accuracy: 6566/48000 =  13.68 % ||| loss 2.299900770187378\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1674/12000 =  13.95 % ||| loss 2.3004066944122314\u001b[0m\n",
            "\u001b[92mTest accuracy: 1384/10000 =  13.84 % ||| loss 2.30014705657959\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.2988584542274477\n",
            "Batch #200 Loss: 2.2980621576309206\n",
            "Batch #300 Loss: 2.297765119075775\n",
            "\u001b[92mTrain accuracy: 11476/48000 =  23.91 % ||| loss 2.2958171367645264\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2879/12000 =  23.99 % ||| loss 2.2963056564331055\u001b[0m\n",
            "\u001b[92mTest accuracy: 2392/10000 =  23.92 % ||| loss 2.2958569526672363\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.2953617000579833\n",
            "Batch #200 Loss: 2.293246076107025\n",
            "Batch #300 Loss: 2.292414906024933\n",
            "\u001b[92mTrain accuracy: 12421/48000 =  25.88 % ||| loss 2.2899203300476074\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3100/12000 =  25.83 % ||| loss 2.2903733253479004\u001b[0m\n",
            "\u001b[92mTest accuracy: 2577/10000 =  25.77 % ||| loss 2.2901198863983154\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.289098219871521\n",
            "Batch #200 Loss: 2.2859233474731444\n",
            "Batch #300 Loss: 2.2833849954605103\n",
            "\u001b[92mTrain accuracy: 13931/48000 =  29.02 % ||| loss 2.2778666019439697\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3484/12000 =  29.03 % ||| loss 2.278334379196167\u001b[0m\n",
            "\u001b[92mTest accuracy: 2895/10000 =  28.95 % ||| loss 2.2780096530914307\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.2751264238357543\n",
            "Batch #200 Loss: 2.2688445901870726\n",
            "Batch #300 Loss: 2.2620227909088135\n",
            "\u001b[92mTrain accuracy: 15273/48000 =  31.82 % ||| loss 2.2454488277435303\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3801/12000 =  31.67 % ||| loss 2.2461297512054443\u001b[0m\n",
            "\u001b[92mTest accuracy: 3193/10000 =  31.93 % ||| loss 2.2457523345947266\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.234765202999115\n",
            "Batch #200 Loss: 2.2062687373161314\n",
            "Batch #300 Loss: 2.1562968635559083\n",
            "\u001b[92mTrain accuracy: 16800/48000 =  35.0 % ||| loss 2.0330848693847656\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4159/12000 =  34.66 % ||| loss 2.0353362560272217\u001b[0m\n",
            "\u001b[92mTest accuracy: 3481/10000 =  34.81 % ||| loss 2.032601833343506\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 1.9554823517799378\n",
            "Batch #200 Loss: 1.7680735981464386\n",
            "Batch #300 Loss: 1.5758598518371583\n",
            "\u001b[92mTrain accuracy: 25839/48000 =  53.83 % ||| loss 1.357571005821228\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6475/12000 =  53.96 % ||| loss 1.3560240268707275\u001b[0m\n",
            "\u001b[92mTest accuracy: 5380/10000 =  53.8 % ||| loss 1.3599164485931396\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 1.2764685666561126\n",
            "Batch #200 Loss: 1.1652139610052108\n",
            "Batch #300 Loss: 1.0868681740760804\n",
            "\u001b[92mTrain accuracy: 29630/48000 =  61.73 % ||| loss 1.0241758823394775\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7423/12000 =  61.86 % ||| loss 1.0189028978347778\u001b[0m\n",
            "\u001b[92mTest accuracy: 6109/10000 =  61.09 % ||| loss 1.0407212972640991\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 1.0092876172065735\n",
            "Batch #200 Loss: 0.9816256600618363\n",
            "Batch #300 Loss: 0.9651356589794159\n",
            "\u001b[92mTrain accuracy: 31060/48000 =  64.71 % ||| loss 0.937178373336792\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7821/12000 =  65.18 % ||| loss 0.9302971959114075\u001b[0m\n",
            "\u001b[92mTest accuracy: 6382/10000 =  63.82 % ||| loss 0.9535199999809265\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.9390171325206756\n",
            "Batch #200 Loss: 0.921190640926361\n",
            "Batch #300 Loss: 0.9111701983213425\n",
            "\u001b[92mTrain accuracy: 32025/48000 =  66.72 % ||| loss 0.8922228217124939\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8031/12000 =  66.92 % ||| loss 0.8835147023200989\u001b[0m\n",
            "\u001b[92mTest accuracy: 6562/10000 =  65.62 % ||| loss 0.9076539278030396\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.8958530128002167\n",
            "Batch #200 Loss: 0.8653629106283188\n",
            "Batch #300 Loss: 0.8602693402767181\n",
            "\u001b[92mTrain accuracy: 32500/48000 =  67.71 % ||| loss 0.8511254787445068\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8109/12000 =  67.58 % ||| loss 0.845191478729248\u001b[0m\n",
            "\u001b[92mTest accuracy: 6721/10000 =  67.21 % ||| loss 0.8661676645278931\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.8387392503023148\n",
            "Batch #200 Loss: 0.8306743907928467\n",
            "Batch #300 Loss: 0.821282354593277\n",
            "\u001b[92mTrain accuracy: 33501/48000 =  69.79 % ||| loss 0.8040127754211426\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8380/12000 =  69.83 % ||| loss 0.7978867888450623\u001b[0m\n",
            "\u001b[92mTest accuracy: 6898/10000 =  68.98 % ||| loss 0.8266973495483398\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.8006794148683548\n",
            "Batch #200 Loss: 0.811647732257843\n",
            "Batch #300 Loss: 0.7938833910226822\n",
            "\u001b[92mTrain accuracy: 34101/48000 =  71.04 % ||| loss 0.7711721062660217\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8535/12000 =  71.12 % ||| loss 0.7648725509643555\u001b[0m\n",
            "\u001b[92mTest accuracy: 6995/10000 =  69.95 % ||| loss 0.7903841137886047\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.7762893325090409\n",
            "Batch #200 Loss: 0.7766747564077378\n",
            "Batch #300 Loss: 0.7803778594732285\n",
            "\u001b[92mTrain accuracy: 34299/48000 =  71.46 % ||| loss 0.7569312453269958\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8614/12000 =  71.78 % ||| loss 0.749161958694458\u001b[0m\n",
            "\u001b[92mTest accuracy: 7072/10000 =  70.72 % ||| loss 0.778259813785553\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.7589164155721665\n",
            "Batch #200 Loss: 0.7537703365087509\n",
            "Batch #300 Loss: 0.742530682682991\n",
            "\u001b[92mTrain accuracy: 34677/48000 =  72.24 % ||| loss 0.7335955500602722\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8691/12000 =  72.42 % ||| loss 0.7274708151817322\u001b[0m\n",
            "\u001b[92mTest accuracy: 7148/10000 =  71.48 % ||| loss 0.7525728940963745\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.7404897451400757\n",
            "Batch #200 Loss: 0.7285354888439178\n",
            "Batch #300 Loss: 0.7312579929828644\n",
            "\u001b[92mTrain accuracy: 34505/48000 =  71.89 % ||| loss 0.719447672367096\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8666/12000 =  72.22 % ||| loss 0.7121222615242004\u001b[0m\n",
            "\u001b[92mTest accuracy: 7136/10000 =  71.36 % ||| loss 0.7407283782958984\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.7219063478708267\n",
            "Batch #200 Loss: 0.7081273913383483\n",
            "Batch #300 Loss: 0.71688412129879\n",
            "\u001b[92mTrain accuracy: 35066/48000 =  73.05 % ||| loss 0.7077247500419617\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8761/12000 =  73.01 % ||| loss 0.7036267518997192\u001b[0m\n",
            "\u001b[92mTest accuracy: 7226/10000 =  72.26 % ||| loss 0.7352584004402161\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.7077098071575165\n",
            "Batch #200 Loss: 0.7040161311626434\n",
            "Batch #300 Loss: 0.6931384015083313\n",
            "\u001b[92mTrain accuracy: 34820/48000 =  72.54 % ||| loss 0.6919442415237427\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8725/12000 =  72.71 % ||| loss 0.686323881149292\u001b[0m\n",
            "\u001b[92mTest accuracy: 7142/10000 =  71.42 % ||| loss 0.7136247754096985\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.6889479690790177\n",
            "Batch #200 Loss: 0.6943165108561515\n",
            "Batch #300 Loss: 0.6832005646824837\n",
            "\u001b[92mTrain accuracy: 35812/48000 =  74.61 % ||| loss 0.6790876388549805\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8976/12000 =  74.8 % ||| loss 0.674667239189148\u001b[0m\n",
            "\u001b[92mTest accuracy: 7412/10000 =  74.12 % ||| loss 0.6953970789909363\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.6751313459873199\n",
            "Batch #200 Loss: 0.6842047899961472\n",
            "Batch #300 Loss: 0.6733519697189331\n",
            "\u001b[92mTrain accuracy: 36116/48000 =  75.24 % ||| loss 0.6666739583015442\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9042/12000 =  75.35 % ||| loss 0.6632950901985168\u001b[0m\n",
            "\u001b[92mTest accuracy: 7444/10000 =  74.44 % ||| loss 0.6898089051246643\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.6721648904681206\n",
            "Batch #200 Loss: 0.6724898219108582\n",
            "Batch #300 Loss: 0.6596927037835121\n",
            "\u001b[92mTrain accuracy: 36267/48000 =  75.56 % ||| loss 0.6523597240447998\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9061/12000 =  75.51 % ||| loss 0.6498246788978577\u001b[0m\n",
            "\u001b[92mTest accuracy: 7448/10000 =  74.48 % ||| loss 0.6792107224464417\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.6515536734461784\n",
            "Batch #200 Loss: 0.6642630416154861\n",
            "Batch #300 Loss: 0.6510388484597206\n",
            "\u001b[92mTrain accuracy: 35545/48000 =  74.05 % ||| loss 0.6596822738647461\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8857/12000 =  73.81 % ||| loss 0.6564430594444275\u001b[0m\n",
            "\u001b[92mTest accuracy: 7296/10000 =  72.96 % ||| loss 0.6866617798805237\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.656551017165184\n",
            "Batch #200 Loss: 0.6662587341666222\n",
            "Batch #300 Loss: 0.6339018392562866\n",
            "\u001b[92mTrain accuracy: 36423/48000 =  75.88 % ||| loss 0.6520568132400513\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9084/12000 =  75.7 % ||| loss 0.6458989977836609\u001b[0m\n",
            "\u001b[92mTest accuracy: 7510/10000 =  75.1 % ||| loss 0.6759456992149353\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.6369116845726966\n",
            "Batch #200 Loss: 0.6370271876454353\n",
            "Batch #300 Loss: 0.6425982910394669\n",
            "\u001b[92mTrain accuracy: 36712/48000 =  76.48 % ||| loss 0.6250124573707581\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9180/12000 =  76.5 % ||| loss 0.6205022931098938\u001b[0m\n",
            "\u001b[92mTest accuracy: 7570/10000 =  75.7 % ||| loss 0.6503114700317383\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726086139.930381_8</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726086139.930381_8</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!!!!! Hyper Param Tuning Finished!!!!!!!!!!!\n",
            "Best Model: Lenet5(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "HyperParams: {'learning_rate': 0.001, 'momentum': 0.7}\n",
            "\n",
            "Accuracies: {'train': 0.9225208333333333, 'val': 0.8955833333333333, 'test': 0.8912}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "param_grid = {\n",
        "  'learning_rate':[0.1, 0.01,0.001],\n",
        "  'momentum':[0, 0.9, 0.7]\n",
        "}\n",
        "\n",
        "best_lenet = hyperparameter_tuning(Lenet5, dataloaders, device, 25, **param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4pr-yptbLJT"
      },
      "source": [
        "### Variations on LeNet5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG5Q03J9bZV_"
      },
      "source": [
        "#### Using Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3rtOUZCKYWG",
        "outputId": "d7e23175-cc12-4be7-aef1-1a6ea6a0ee8c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_165303-Lenet5BN_1726087983.826163_0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_0' target=\"_blank\">Lenet5BN_1726087983.826163_0</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.0918693089485167\n",
            "Batch #200 Loss: 0.5731843969225884\n",
            "Batch #300 Loss: 0.4882547771930695\n",
            "\u001b[92mTrain accuracy: 40554/48000 =  84.49 % ||| loss 0.4308229088783264\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10138/12000 =  84.48 % ||| loss 0.4348497986793518\u001b[0m\n",
            "\u001b[92mTest accuracy: 8345/10000 =  83.45 % ||| loss 0.45388683676719666\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.428620548248291\n",
            "Batch #200 Loss: 0.39510360807180406\n",
            "Batch #300 Loss: 0.38443693801760676\n",
            "\u001b[92mTrain accuracy: 41434/48000 =  86.32 % ||| loss 0.37690454721450806\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10326/12000 =  86.05 % ||| loss 0.38144558668136597\u001b[0m\n",
            "\u001b[92mTest accuracy: 8508/10000 =  85.08 % ||| loss 0.4072236120700836\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.3621974065899849\n",
            "Batch #200 Loss: 0.3631885758042335\n",
            "Batch #300 Loss: 0.33911539494991305\n",
            "\u001b[92mTrain accuracy: 42521/48000 =  88.59 % ||| loss 0.3144378066062927\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10522/12000 =  87.68 % ||| loss 0.33632054924964905\u001b[0m\n",
            "\u001b[92mTest accuracy: 8675/10000 =  86.75 % ||| loss 0.3585347831249237\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.3332081826031208\n",
            "Batch #200 Loss: 0.3123899810016155\n",
            "Batch #300 Loss: 0.3111198352277279\n",
            "\u001b[92mTrain accuracy: 40900/48000 =  85.21 % ||| loss 0.3790431618690491\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10110/12000 =  84.25 % ||| loss 0.40451309084892273\u001b[0m\n",
            "\u001b[92mTest accuracy: 8356/10000 =  83.56 % ||| loss 0.4355696141719818\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.29950790643692016\n",
            "Batch #200 Loss: 0.2969981175661087\n",
            "Batch #300 Loss: 0.29897389814257624\n",
            "\u001b[92mTrain accuracy: 43010/48000 =  89.6 % ||| loss 0.28851988911628723\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10567/12000 =  88.06 % ||| loss 0.32297199964523315\u001b[0m\n",
            "\u001b[92mTest accuracy: 8769/10000 =  87.69 % ||| loss 0.3476756513118744\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.28058108016848565\n",
            "Batch #200 Loss: 0.2879175753891468\n",
            "Batch #300 Loss: 0.28500992104411127\n",
            "\u001b[92mTrain accuracy: 42806/48000 =  89.18 % ||| loss 0.28870952129364014\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10571/12000 =  88.09 % ||| loss 0.32804936170578003\u001b[0m\n",
            "\u001b[92mTest accuracy: 8727/10000 =  87.27 % ||| loss 0.350812166929245\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.2634263035655022\n",
            "Batch #200 Loss: 0.26592081725597383\n",
            "Batch #300 Loss: 0.27318241134285925\n",
            "\u001b[92mTrain accuracy: 43254/48000 =  90.11 % ||| loss 0.26931893825531006\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10627/12000 =  88.56 % ||| loss 0.31273630261421204\u001b[0m\n",
            "\u001b[92mTest accuracy: 8782/10000 =  87.82 % ||| loss 0.33684998750686646\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.2559102841466665\n",
            "Batch #200 Loss: 0.25886473059654236\n",
            "Batch #300 Loss: 0.2539342071115971\n",
            "\u001b[92mTrain accuracy: 43843/48000 =  91.34 % ||| loss 0.23545096814632416\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10742/12000 =  89.52 % ||| loss 0.2853541970252991\u001b[0m\n",
            "\u001b[92mTest accuracy: 8893/10000 =  88.93 % ||| loss 0.30870506167411804\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.24530269227921964\n",
            "Batch #200 Loss: 0.24099390029907228\n",
            "Batch #300 Loss: 0.2557243686914444\n",
            "\u001b[92mTrain accuracy: 43131/48000 =  89.86 % ||| loss 0.26643896102905273\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10599/12000 =  88.33 % ||| loss 0.316474050283432\u001b[0m\n",
            "\u001b[92mTest accuracy: 8757/10000 =  87.57 % ||| loss 0.34347209334373474\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.23423061951994895\n",
            "Batch #200 Loss: 0.24320449247956277\n",
            "Batch #300 Loss: 0.2421656633913517\n",
            "\u001b[92mTrain accuracy: 43822/48000 =  91.3 % ||| loss 0.2333434522151947\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10770/12000 =  89.75 % ||| loss 0.2882337272167206\u001b[0m\n",
            "\u001b[92mTest accuracy: 8894/10000 =  88.94 % ||| loss 0.32238760590553284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.23045538052916525\n",
            "Batch #200 Loss: 0.23781660832464696\n",
            "Batch #300 Loss: 0.23246332004666329\n",
            "\u001b[92mTrain accuracy: 44126/48000 =  91.93 % ||| loss 0.21837589144706726\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10770/12000 =  89.75 % ||| loss 0.28116533160209656\u001b[0m\n",
            "\u001b[92mTest accuracy: 8895/10000 =  88.95 % ||| loss 0.3110639750957489\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.2281396494805813\n",
            "Batch #200 Loss: 0.2226653293520212\n",
            "Batch #300 Loss: 0.21786055594682693\n",
            "\u001b[92mTrain accuracy: 44078/48000 =  91.83 % ||| loss 0.21553464233875275\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10731/12000 =  89.42 % ||| loss 0.2853737771511078\u001b[0m\n",
            "\u001b[92mTest accuracy: 8866/10000 =  88.66 % ||| loss 0.3103782832622528\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.21602731876075268\n",
            "Batch #200 Loss: 0.2092885734885931\n",
            "Batch #300 Loss: 0.22111000321805477\n",
            "\u001b[92mTrain accuracy: 44551/48000 =  92.81 % ||| loss 0.19544433057308197\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10823/12000 =  90.19 % ||| loss 0.2675904333591461\u001b[0m\n",
            "\u001b[92mTest accuracy: 8977/10000 =  89.77 % ||| loss 0.29376375675201416\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.20674404822289943\n",
            "Batch #200 Loss: 0.21764676384627818\n",
            "Batch #300 Loss: 0.20547349743545054\n",
            "\u001b[92mTrain accuracy: 43862/48000 =  91.38 % ||| loss 0.22360116243362427\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10617/12000 =  88.48 % ||| loss 0.30837956070899963\u001b[0m\n",
            "\u001b[92mTest accuracy: 8801/10000 =  88.01 % ||| loss 0.33078619837760925\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.20846296295523645\n",
            "Batch #200 Loss: 0.1983951110392809\n",
            "Batch #300 Loss: 0.20449341759085654\n",
            "\u001b[92mTrain accuracy: 44570/48000 =  92.85 % ||| loss 0.19288763403892517\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10790/12000 =  89.92 % ||| loss 0.27914130687713623\u001b[0m\n",
            "\u001b[92mTest accuracy: 8908/10000 =  89.08 % ||| loss 0.307917058467865\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.1896348310261965\n",
            "Batch #200 Loss: 0.20243610933423042\n",
            "Batch #300 Loss: 0.20003403708338738\n",
            "\u001b[92mTrain accuracy: 44306/48000 =  92.3 % ||| loss 0.2024819403886795\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10736/12000 =  89.47 % ||| loss 0.2908321022987366\u001b[0m\n",
            "\u001b[92mTest accuracy: 8855/10000 =  88.55 % ||| loss 0.3208386301994324\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.18949212856590747\n",
            "Batch #200 Loss: 0.18760220497846603\n",
            "Batch #300 Loss: 0.20739700943231582\n",
            "\u001b[92mTrain accuracy: 44386/48000 =  92.47 % ||| loss 0.19916535913944244\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10735/12000 =  89.46 % ||| loss 0.2978023290634155\u001b[0m\n",
            "\u001b[92mTest accuracy: 8850/10000 =  88.5 % ||| loss 0.3377581834793091\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.173801199644804\n",
            "Batch #200 Loss: 0.17809856474399566\n",
            "Batch #300 Loss: 0.20022346168756486\n",
            "\u001b[92mTrain accuracy: 44597/48000 =  92.91 % ||| loss 0.1913221776485443\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10791/12000 =  89.92 % ||| loss 0.2898406386375427\u001b[0m\n",
            "\u001b[92mTest accuracy: 8876/10000 =  88.76 % ||| loss 0.32320278882980347\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.18030115336179733\n",
            "Batch #200 Loss: 0.18031371176242827\n",
            "Batch #300 Loss: 0.17943597868084907\n",
            "\u001b[92mTrain accuracy: 44807/48000 =  93.35 % ||| loss 0.177714541554451\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10772/12000 =  89.77 % ||| loss 0.28867045044898987\u001b[0m\n",
            "\u001b[92mTest accuracy: 8911/10000 =  89.11 % ||| loss 0.3262263834476471\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.17551787964999677\n",
            "Batch #200 Loss: 0.17213762134313584\n",
            "Batch #300 Loss: 0.1825383672863245\n",
            "\u001b[92mTrain accuracy: 45000/48000 =  93.75 % ||| loss 0.1698482781648636\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10823/12000 =  90.19 % ||| loss 0.28118419647216797\u001b[0m\n",
            "\u001b[92mTest accuracy: 8928/10000 =  89.28 % ||| loss 0.3192569315433502\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.17126224674284457\n",
            "Batch #200 Loss: 0.16944243162870407\n",
            "Batch #300 Loss: 0.1728860008716583\n",
            "\u001b[92mTrain accuracy: 45478/48000 =  94.75 % ||| loss 0.14866755902767181\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10880/12000 =  90.67 % ||| loss 0.26257428526878357\u001b[0m\n",
            "\u001b[92mTest accuracy: 9005/10000 =  90.05 % ||| loss 0.3003372550010681\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.1589232812821865\n",
            "Batch #200 Loss: 0.17081011403352023\n",
            "Batch #300 Loss: 0.1770483858883381\n",
            "\u001b[92mTrain accuracy: 44904/48000 =  93.55 % ||| loss 0.16682393848896027\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10792/12000 =  89.93 % ||| loss 0.2815508246421814\u001b[0m\n",
            "\u001b[92mTest accuracy: 8878/10000 =  88.78 % ||| loss 0.3163357675075531\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.16359445348381996\n",
            "Batch #200 Loss: 0.16409089505672456\n",
            "Batch #300 Loss: 0.15943544693291187\n",
            "\u001b[92mTrain accuracy: 45343/48000 =  94.46 % ||| loss 0.15035727620124817\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10850/12000 =  90.42 % ||| loss 0.27776917815208435\u001b[0m\n",
            "\u001b[92mTest accuracy: 8960/10000 =  89.6 % ||| loss 0.3197893500328064\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.15497247852385043\n",
            "Batch #200 Loss: 0.1558570636063814\n",
            "Batch #300 Loss: 0.16171222738921642\n",
            "\u001b[92mTrain accuracy: 45625/48000 =  95.05 % ||| loss 0.13548478484153748\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10839/12000 =  90.33 % ||| loss 0.27871155738830566\u001b[0m\n",
            "\u001b[92mTest accuracy: 8948/10000 =  89.48 % ||| loss 0.3198487162590027\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.14570035107433796\n",
            "Batch #200 Loss: 0.14880623087286948\n",
            "Batch #300 Loss: 0.15777338579297065\n",
            "\u001b[92mTrain accuracy: 45730/48000 =  95.27 % ||| loss 0.13367213308811188\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10881/12000 =  90.67 % ||| loss 0.27803757786750793\u001b[0m\n",
            "\u001b[92mTest accuracy: 9002/10000 =  90.02 % ||| loss 0.3187621235847473\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726087983.826163_0</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_0</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_165501-Lenet5BN_1726087983.826163_1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_1' target=\"_blank\">Lenet5BN_1726087983.826163_1</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 0.9935321161150932\n",
            "Batch #200 Loss: 0.5134154757857323\n",
            "Batch #300 Loss: 0.46340573489665987\n",
            "\u001b[92mTrain accuracy: 41022/48000 =  85.46 % ||| loss 0.3874186873435974\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10209/12000 =  85.08 % ||| loss 0.3985355496406555\u001b[0m\n",
            "\u001b[92mTest accuracy: 8462/10000 =  84.62 % ||| loss 0.40989941358566284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.38847166270017625\n",
            "Batch #200 Loss: 0.37721108555793764\n",
            "Batch #300 Loss: 0.3552295814454556\n",
            "\u001b[92mTrain accuracy: 42001/48000 =  87.5 % ||| loss 0.33719897270202637\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10414/12000 =  86.78 % ||| loss 0.3544819951057434\u001b[0m\n",
            "\u001b[92mTest accuracy: 8645/10000 =  86.45 % ||| loss 0.3721124231815338\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.3348969057202339\n",
            "Batch #200 Loss: 0.34726156026124955\n",
            "Batch #300 Loss: 0.3310461601614952\n",
            "\u001b[92mTrain accuracy: 42502/48000 =  88.55 % ||| loss 0.3085443675518036\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10480/12000 =  87.33 % ||| loss 0.33809754252433777\u001b[0m\n",
            "\u001b[92mTest accuracy: 8679/10000 =  86.79 % ||| loss 0.34737682342529297\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.30694290034472943\n",
            "Batch #200 Loss: 0.32166289746761323\n",
            "Batch #300 Loss: 0.31735376209020616\n",
            "\u001b[92mTrain accuracy: 42894/48000 =  89.36 % ||| loss 0.28530794382095337\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10558/12000 =  87.98 % ||| loss 0.3206307291984558\u001b[0m\n",
            "\u001b[92mTest accuracy: 8745/10000 =  87.45 % ||| loss 0.3323940634727478\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.30071246027946474\n",
            "Batch #200 Loss: 0.2935087567567825\n",
            "Batch #300 Loss: 0.2975715771317482\n",
            "\u001b[92mTrain accuracy: 42759/48000 =  89.08 % ||| loss 0.28256845474243164\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10505/12000 =  87.54 % ||| loss 0.32158350944519043\u001b[0m\n",
            "\u001b[92mTest accuracy: 8714/10000 =  87.14 % ||| loss 0.3422256112098694\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.2750890228152275\n",
            "Batch #200 Loss: 0.27634032160043714\n",
            "Batch #300 Loss: 0.28281169563531877\n",
            "\u001b[92mTrain accuracy: 43637/48000 =  90.91 % ||| loss 0.24330991506576538\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10701/12000 =  89.18 % ||| loss 0.28992384672164917\u001b[0m\n",
            "\u001b[92mTest accuracy: 8908/10000 =  89.08 % ||| loss 0.3101453483104706\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.2605536277592182\n",
            "Batch #200 Loss: 0.26039537116885186\n",
            "Batch #300 Loss: 0.27087566912174227\n",
            "\u001b[92mTrain accuracy: 43359/48000 =  90.33 % ||| loss 0.2565939724445343\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10577/12000 =  88.14 % ||| loss 0.3151074945926666\u001b[0m\n",
            "\u001b[92mTest accuracy: 8812/10000 =  88.12 % ||| loss 0.33395159244537354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.252997278124094\n",
            "Batch #200 Loss: 0.2654472900927067\n",
            "Batch #300 Loss: 0.2643200385570526\n",
            "\u001b[92mTrain accuracy: 43293/48000 =  90.19 % ||| loss 0.25966018438339233\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10615/12000 =  88.46 % ||| loss 0.3138185143470764\u001b[0m\n",
            "\u001b[92mTest accuracy: 8788/10000 =  87.88 % ||| loss 0.34228280186653137\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.2342746177315712\n",
            "Batch #200 Loss: 0.2414163152128458\n",
            "Batch #300 Loss: 0.2513485226035118\n",
            "\u001b[92mTrain accuracy: 44062/48000 =  91.8 % ||| loss 0.21479006111621857\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10734/12000 =  89.45 % ||| loss 0.2826833426952362\u001b[0m\n",
            "\u001b[92mTest accuracy: 8906/10000 =  89.06 % ||| loss 0.30005717277526855\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.24488180354237557\n",
            "Batch #200 Loss: 0.23513295732438563\n",
            "Batch #300 Loss: 0.24260303154587745\n",
            "\u001b[92mTrain accuracy: 44189/48000 =  92.06 % ||| loss 0.21902884542942047\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10730/12000 =  89.42 % ||| loss 0.2918000817298889\u001b[0m\n",
            "\u001b[92mTest accuracy: 8908/10000 =  89.08 % ||| loss 0.3143405020236969\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.22777710758149625\n",
            "Batch #200 Loss: 0.23642904080450536\n",
            "Batch #300 Loss: 0.22255203045904637\n",
            "\u001b[92mTrain accuracy: 43553/48000 =  90.74 % ||| loss 0.24751979112625122\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10647/12000 =  88.72 % ||| loss 0.31948578357696533\u001b[0m\n",
            "\u001b[92mTest accuracy: 8814/10000 =  88.14 % ||| loss 0.3417416512966156\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.2152864220738411\n",
            "Batch #200 Loss: 0.2198481720685959\n",
            "Batch #300 Loss: 0.22510652542114257\n",
            "\u001b[92mTrain accuracy: 44094/48000 =  91.86 % ||| loss 0.22062547504901886\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10678/12000 =  88.98 % ||| loss 0.30786922574043274\u001b[0m\n",
            "\u001b[92mTest accuracy: 8877/10000 =  88.77 % ||| loss 0.32876262068748474\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.20913172796368598\n",
            "Batch #200 Loss: 0.20402151085436343\n",
            "Batch #300 Loss: 0.23261895000934601\n",
            "\u001b[92mTrain accuracy: 44265/48000 =  92.22 % ||| loss 0.20356109738349915\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10689/12000 =  89.08 % ||| loss 0.3012222349643707\u001b[0m\n",
            "\u001b[92mTest accuracy: 8892/10000 =  88.92 % ||| loss 0.3296668231487274\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.19972728960216046\n",
            "Batch #200 Loss: 0.2072228880226612\n",
            "Batch #300 Loss: 0.2206915933638811\n",
            "\u001b[92mTrain accuracy: 44489/48000 =  92.69 % ||| loss 0.2001170814037323\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10756/12000 =  89.63 % ||| loss 0.307974249124527\u001b[0m\n",
            "\u001b[92mTest accuracy: 8908/10000 =  89.08 % ||| loss 0.33436912298202515\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.18707787841558457\n",
            "Batch #200 Loss: 0.19634784251451493\n",
            "Batch #300 Loss: 0.2112779612839222\n",
            "\u001b[92mTrain accuracy: 44363/48000 =  92.42 % ||| loss 0.20384767651557922\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10682/12000 =  89.02 % ||| loss 0.310848206281662\u001b[0m\n",
            "\u001b[92mTest accuracy: 8907/10000 =  89.07 % ||| loss 0.3256974220275879\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.19381948783993722\n",
            "Batch #200 Loss: 0.20040196768939494\n",
            "Batch #300 Loss: 0.20106978945434092\n",
            "\u001b[92mTrain accuracy: 43383/48000 =  90.38 % ||| loss 0.2680889070034027\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10463/12000 =  87.19 % ||| loss 0.39446109533309937\u001b[0m\n",
            "\u001b[92mTest accuracy: 8722/10000 =  87.22 % ||| loss 0.40748244524002075\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.18966907441616057\n",
            "Batch #200 Loss: 0.19856781154870987\n",
            "Batch #300 Loss: 0.20378814361989497\n",
            "\u001b[92mTrain accuracy: 44947/48000 =  93.64 % ||| loss 0.1676405668258667\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10819/12000 =  90.16 % ||| loss 0.28804510831832886\u001b[0m\n",
            "\u001b[92mTest accuracy: 8982/10000 =  89.82 % ||| loss 0.3114162087440491\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.17209832035005093\n",
            "Batch #200 Loss: 0.18865330040454864\n",
            "Batch #300 Loss: 0.19551000475883484\n",
            "\u001b[92mTrain accuracy: 44345/48000 =  92.39 % ||| loss 0.19646787643432617\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10680/12000 =  89.0 % ||| loss 0.3048039376735687\u001b[0m\n",
            "\u001b[92mTest accuracy: 8836/10000 =  88.36 % ||| loss 0.33217278122901917\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.18258397750556468\n",
            "Batch #200 Loss: 0.17895147249102591\n",
            "Batch #300 Loss: 0.18633144475519658\n",
            "\u001b[92mTrain accuracy: 45011/48000 =  93.77 % ||| loss 0.16432249546051025\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10774/12000 =  89.78 % ||| loss 0.3211081027984619\u001b[0m\n",
            "\u001b[92mTest accuracy: 8935/10000 =  89.35 % ||| loss 0.35036370158195496\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.1745084407925606\n",
            "Batch #200 Loss: 0.18507081836462022\n",
            "Batch #300 Loss: 0.1821714360266924\n",
            "\u001b[92mTrain accuracy: 45087/48000 =  93.93 % ||| loss 0.15842020511627197\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10764/12000 =  89.7 % ||| loss 0.31320720911026\u001b[0m\n",
            "\u001b[92mTest accuracy: 8990/10000 =  89.9 % ||| loss 0.3322318196296692\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.16183451380580663\n",
            "Batch #200 Loss: 0.17237312469631433\n",
            "Batch #300 Loss: 0.176826111972332\n",
            "\u001b[92mTrain accuracy: 44811/48000 =  93.36 % ||| loss 0.17785431444644928\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10714/12000 =  89.28 % ||| loss 0.32941943407058716\u001b[0m\n",
            "\u001b[92mTest accuracy: 8877/10000 =  88.77 % ||| loss 0.358632355928421\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.16548618093132972\n",
            "Batch #200 Loss: 0.17032178901135922\n",
            "Batch #300 Loss: 0.1684073331579566\n",
            "\u001b[92mTrain accuracy: 44917/48000 =  93.58 % ||| loss 0.17222560942173004\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10739/12000 =  89.49 % ||| loss 0.3412834703922272\u001b[0m\n",
            "\u001b[92mTest accuracy: 8910/10000 =  89.1 % ||| loss 0.379960834980011\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.1668228217214346\n",
            "Batch #200 Loss: 0.16402397587895393\n",
            "Batch #300 Loss: 0.1701854679733515\n",
            "\u001b[92mTrain accuracy: 45137/48000 =  94.04 % ||| loss 0.16004233062267303\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10716/12000 =  89.3 % ||| loss 0.3309206962585449\u001b[0m\n",
            "\u001b[92mTest accuracy: 8918/10000 =  89.18 % ||| loss 0.3486047685146332\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.16890178568661213\n",
            "Batch #200 Loss: 0.1586156001314521\n",
            "Batch #300 Loss: 0.16767598688602448\n",
            "\u001b[92mTrain accuracy: 44769/48000 =  93.27 % ||| loss 0.17726127803325653\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10671/12000 =  88.92 % ||| loss 0.3597157597541809\u001b[0m\n",
            "\u001b[92mTest accuracy: 8844/10000 =  88.44 % ||| loss 0.3948710560798645\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.1463486699014902\n",
            "Batch #200 Loss: 0.15375897750258447\n",
            "Batch #300 Loss: 0.16462741956114768\n",
            "\u001b[92mTrain accuracy: 45645/48000 =  95.09 % ||| loss 0.13027434051036835\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10784/12000 =  89.87 % ||| loss 0.33198434114456177\u001b[0m\n",
            "\u001b[92mTest accuracy: 8946/10000 =  89.46 % ||| loss 0.36805427074432373\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726087983.826163_1</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_1</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_165700-Lenet5BN_1726087983.826163_2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_2' target=\"_blank\">Lenet5BN_1726087983.826163_2</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 0.8562760981917381\n",
            "Batch #200 Loss: 0.48960247814655306\n",
            "Batch #300 Loss: 0.43837496876716614\n",
            "\u001b[92mTrain accuracy: 41177/48000 =  85.79 % ||| loss 0.39201444387435913\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10246/12000 =  85.38 % ||| loss 0.39783498644828796\u001b[0m\n",
            "\u001b[92mTest accuracy: 8474/10000 =  84.74 % ||| loss 0.4265286326408386\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.3823950935900211\n",
            "Batch #200 Loss: 0.3470021843910217\n",
            "Batch #300 Loss: 0.36385163575410845\n",
            "\u001b[92mTrain accuracy: 42408/48000 =  88.35 % ||| loss 0.31451764702796936\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10527/12000 =  87.72 % ||| loss 0.33332669734954834\u001b[0m\n",
            "\u001b[92mTest accuracy: 8745/10000 =  87.45 % ||| loss 0.349338561296463\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.3245084962248802\n",
            "Batch #200 Loss: 0.3297124974429607\n",
            "Batch #300 Loss: 0.32020771846175194\n",
            "\u001b[92mTrain accuracy: 42910/48000 =  89.4 % ||| loss 0.2862786650657654\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10583/12000 =  88.19 % ||| loss 0.3120753765106201\u001b[0m\n",
            "\u001b[92mTest accuracy: 8784/10000 =  87.84 % ||| loss 0.33011889457702637\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.28770397379994395\n",
            "Batch #200 Loss: 0.29821719497442245\n",
            "Batch #300 Loss: 0.28973151102662087\n",
            "\u001b[92mTrain accuracy: 42645/48000 =  88.84 % ||| loss 0.2920176088809967\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10508/12000 =  87.57 % ||| loss 0.32685422897338867\u001b[0m\n",
            "\u001b[92mTest accuracy: 8688/10000 =  86.88 % ||| loss 0.3416205644607544\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.27408089250326156\n",
            "Batch #200 Loss: 0.2765706263482571\n",
            "Batch #300 Loss: 0.2618885138630867\n",
            "\u001b[92mTrain accuracy: 43212/48000 =  90.03 % ||| loss 0.26498106122016907\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10608/12000 =  88.4 % ||| loss 0.3063782751560211\u001b[0m\n",
            "\u001b[92mTest accuracy: 8792/10000 =  87.92 % ||| loss 0.3238794803619385\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.2558331474661827\n",
            "Batch #200 Loss: 0.26602359503507617\n",
            "Batch #300 Loss: 0.26171560503542424\n",
            "\u001b[92mTrain accuracy: 43103/48000 =  89.8 % ||| loss 0.2740667462348938\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10574/12000 =  88.12 % ||| loss 0.321480929851532\u001b[0m\n",
            "\u001b[92mTest accuracy: 8758/10000 =  87.58 % ||| loss 0.34170880913734436\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.24332950249314308\n",
            "Batch #200 Loss: 0.25084481701254846\n",
            "Batch #300 Loss: 0.25177488118410113\n",
            "\u001b[92mTrain accuracy: 43635/48000 =  90.91 % ||| loss 0.24503113329410553\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10667/12000 =  88.89 % ||| loss 0.309854120016098\u001b[0m\n",
            "\u001b[92mTest accuracy: 8854/10000 =  88.54 % ||| loss 0.3245510160923004\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.22896540887653827\n",
            "Batch #200 Loss: 0.23137719698250295\n",
            "Batch #300 Loss: 0.2502987000346184\n",
            "\u001b[92mTrain accuracy: 43828/48000 =  91.31 % ||| loss 0.2292216569185257\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10695/12000 =  89.12 % ||| loss 0.29536712169647217\u001b[0m\n",
            "\u001b[92mTest accuracy: 8868/10000 =  88.68 % ||| loss 0.30776920914649963\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.2156049655377865\n",
            "Batch #200 Loss: 0.22918056607246398\n",
            "Batch #300 Loss: 0.2251847968250513\n",
            "\u001b[92mTrain accuracy: 44023/48000 =  91.71 % ||| loss 0.22357216477394104\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10656/12000 =  88.8 % ||| loss 0.29519081115722656\u001b[0m\n",
            "\u001b[92mTest accuracy: 8866/10000 =  88.66 % ||| loss 0.31968507170677185\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.20836711138486863\n",
            "Batch #200 Loss: 0.21493315629661083\n",
            "Batch #300 Loss: 0.2220131829380989\n",
            "\u001b[92mTrain accuracy: 44408/48000 =  92.52 % ||| loss 0.20049268007278442\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10796/12000 =  89.97 % ||| loss 0.27835002541542053\u001b[0m\n",
            "\u001b[92mTest accuracy: 8966/10000 =  89.66 % ||| loss 0.29184767603874207\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.1991066784411669\n",
            "Batch #200 Loss: 0.20822831273078918\n",
            "Batch #300 Loss: 0.2130798999220133\n",
            "\u001b[92mTrain accuracy: 43804/48000 =  91.26 % ||| loss 0.22704806923866272\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10596/12000 =  88.3 % ||| loss 0.3094983696937561\u001b[0m\n",
            "\u001b[92mTest accuracy: 8798/10000 =  87.98 % ||| loss 0.32951515913009644\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.194865003824234\n",
            "Batch #200 Loss: 0.2045459846407175\n",
            "Batch #300 Loss: 0.19850194752216338\n",
            "\u001b[92mTrain accuracy: 43626/48000 =  90.89 % ||| loss 0.2366270273923874\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10603/12000 =  88.36 % ||| loss 0.32474285364151\u001b[0m\n",
            "\u001b[92mTest accuracy: 8735/10000 =  87.35 % ||| loss 0.35140934586524963\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.18221950061619283\n",
            "Batch #200 Loss: 0.19469712749123574\n",
            "Batch #300 Loss: 0.18618138693273067\n",
            "\u001b[92mTrain accuracy: 44759/48000 =  93.25 % ||| loss 0.17657198011875153\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10757/12000 =  89.64 % ||| loss 0.28769776225090027\u001b[0m\n",
            "\u001b[92mTest accuracy: 8938/10000 =  89.38 % ||| loss 0.31103166937828064\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.176010747179389\n",
            "Batch #200 Loss: 0.18874286212027072\n",
            "Batch #300 Loss: 0.19156793132424355\n",
            "\u001b[92mTrain accuracy: 44971/48000 =  93.69 % ||| loss 0.16849124431610107\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10747/12000 =  89.56 % ||| loss 0.29200345277786255\u001b[0m\n",
            "\u001b[92mTest accuracy: 8938/10000 =  89.38 % ||| loss 0.31250816583633423\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.16224014960229396\n",
            "Batch #200 Loss: 0.17858027353882788\n",
            "Batch #300 Loss: 0.18297988530248405\n",
            "\u001b[92mTrain accuracy: 44892/48000 =  93.53 % ||| loss 0.17003309726715088\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10746/12000 =  89.55 % ||| loss 0.29834237694740295\u001b[0m\n",
            "\u001b[92mTest accuracy: 8912/10000 =  89.12 % ||| loss 0.3192552328109741\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.16561860881745816\n",
            "Batch #200 Loss: 0.1701229205727577\n",
            "Batch #300 Loss: 0.168844383507967\n",
            "\u001b[92mTrain accuracy: 45463/48000 =  94.71 % ||| loss 0.14337727427482605\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10815/12000 =  90.12 % ||| loss 0.2958391010761261\u001b[0m\n",
            "\u001b[92mTest accuracy: 8980/10000 =  89.8 % ||| loss 0.32146650552749634\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.15135152705013752\n",
            "Batch #200 Loss: 0.17269795440137387\n",
            "Batch #300 Loss: 0.16603400312364103\n",
            "\u001b[92mTrain accuracy: 45282/48000 =  94.34 % ||| loss 0.14924292266368866\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10784/12000 =  89.87 % ||| loss 0.29802003502845764\u001b[0m\n",
            "\u001b[92mTest accuracy: 8928/10000 =  89.28 % ||| loss 0.31585487723350525\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.14789993457496167\n",
            "Batch #200 Loss: 0.16162201814353466\n",
            "Batch #300 Loss: 0.15990560553967953\n",
            "\u001b[92mTrain accuracy: 44649/48000 =  93.02 % ||| loss 0.18008661270141602\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10664/12000 =  88.87 % ||| loss 0.3266902565956116\u001b[0m\n",
            "\u001b[92mTest accuracy: 8833/10000 =  88.33 % ||| loss 0.35760369896888733\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.14005917545408011\n",
            "Batch #200 Loss: 0.15455245580524207\n",
            "Batch #300 Loss: 0.15101193625479936\n",
            "\u001b[92mTrain accuracy: 44974/48000 =  93.7 % ||| loss 0.16519980132579803\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10654/12000 =  88.78 % ||| loss 0.3455757796764374\u001b[0m\n",
            "\u001b[92mTest accuracy: 8884/10000 =  88.84 % ||| loss 0.3652098476886749\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.13240393292158842\n",
            "Batch #200 Loss: 0.1448989487439394\n",
            "Batch #300 Loss: 0.1506930723041296\n",
            "\u001b[92mTrain accuracy: 44688/48000 =  93.1 % ||| loss 0.17844179272651672\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10585/12000 =  88.21 % ||| loss 0.3673640787601471\u001b[0m\n",
            "\u001b[92mTest accuracy: 8794/10000 =  87.94 % ||| loss 0.38954707980155945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.1386349979043007\n",
            "Batch #200 Loss: 0.14419655587524174\n",
            "Batch #300 Loss: 0.13398313615471125\n",
            "\u001b[92mTrain accuracy: 45423/48000 =  94.63 % ||| loss 0.14024768769741058\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10736/12000 =  89.47 % ||| loss 0.3426160514354706\u001b[0m\n",
            "\u001b[92mTest accuracy: 8923/10000 =  89.23 % ||| loss 0.3626345694065094\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.1275013403967023\n",
            "Batch #200 Loss: 0.13427599418908356\n",
            "Batch #300 Loss: 0.14389945860952139\n",
            "\u001b[92mTrain accuracy: 45662/48000 =  95.13 % ||| loss 0.129499614238739\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10730/12000 =  89.42 % ||| loss 0.3347455561161041\u001b[0m\n",
            "\u001b[92mTest accuracy: 8875/10000 =  88.75 % ||| loss 0.3618233799934387\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.12000686444342136\n",
            "Batch #200 Loss: 0.11756348144263029\n",
            "Batch #300 Loss: 0.1291238685324788\n",
            "\u001b[92mTrain accuracy: 45833/48000 =  95.49 % ||| loss 0.11764451116323471\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10772/12000 =  89.77 % ||| loss 0.3319069445133209\u001b[0m\n",
            "\u001b[92mTest accuracy: 8914/10000 =  89.14 % ||| loss 0.3532713055610657\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.11742521245032549\n",
            "Batch #200 Loss: 0.11800302248448133\n",
            "Batch #300 Loss: 0.1251941052824259\n",
            "\u001b[92mTrain accuracy: 45996/48000 =  95.83 % ||| loss 0.11090811342000961\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10751/12000 =  89.59 % ||| loss 0.34951815009117126\u001b[0m\n",
            "\u001b[92mTest accuracy: 8934/10000 =  89.34 % ||| loss 0.3680912256240845\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.11174857418984174\n",
            "Batch #200 Loss: 0.11936709012836218\n",
            "Batch #300 Loss: 0.11861510686576367\n",
            "\u001b[92mTrain accuracy: 45679/48000 =  95.16 % ||| loss 0.12244880199432373\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10689/12000 =  89.08 % ||| loss 0.3884110450744629\u001b[0m\n",
            "\u001b[92mTest accuracy: 8887/10000 =  88.87 % ||| loss 0.4138786792755127\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726087983.826163_2</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_2</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_165858-Lenet5BN_1726087983.826163_3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_3' target=\"_blank\">Lenet5BN_1726087983.826163_3</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.178512649536133\n",
            "Batch #200 Loss: 1.6742618155479432\n",
            "Batch #300 Loss: 1.1257507419586181\n",
            "\u001b[92mTrain accuracy: 33938/48000 =  70.7 % ||| loss 0.8377549052238464\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8494/12000 =  70.78 % ||| loss 0.8317321538925171\u001b[0m\n",
            "\u001b[92mTest accuracy: 7006/10000 =  70.06 % ||| loss 0.8460841178894043\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7902303844690323\n",
            "Batch #200 Loss: 0.7069436472654342\n",
            "Batch #300 Loss: 0.661941009759903\n",
            "\u001b[92mTrain accuracy: 36908/48000 =  76.89 % ||| loss 0.6139974594116211\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9275/12000 =  77.29 % ||| loss 0.6110281348228455\u001b[0m\n",
            "\u001b[92mTest accuracy: 7652/10000 =  76.52 % ||| loss 0.6254181861877441\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6125414207577705\n",
            "Batch #200 Loss: 0.5672504484653473\n",
            "Batch #300 Loss: 0.557720513343811\n",
            "\u001b[92mTrain accuracy: 38497/48000 =  80.2 % ||| loss 0.5316218137741089\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9637/12000 =  80.31 % ||| loss 0.5332173109054565\u001b[0m\n",
            "\u001b[92mTest accuracy: 7948/10000 =  79.48 % ||| loss 0.5516289472579956\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5160821944475174\n",
            "Batch #200 Loss: 0.4997155711054802\n",
            "Batch #300 Loss: 0.48992649048566816\n",
            "\u001b[92mTrain accuracy: 39932/48000 =  83.19 % ||| loss 0.4684704542160034\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9973/12000 =  83.11 % ||| loss 0.47318077087402344\u001b[0m\n",
            "\u001b[92mTest accuracy: 8243/10000 =  82.43 % ||| loss 0.488047331571579\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.46056561678647995\n",
            "Batch #200 Loss: 0.4653319200873375\n",
            "Batch #300 Loss: 0.44129936695098876\n",
            "\u001b[92mTrain accuracy: 40346/48000 =  84.05 % ||| loss 0.4428594708442688\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10107/12000 =  84.23 % ||| loss 0.45048052072525024\u001b[0m\n",
            "\u001b[92mTest accuracy: 8304/10000 =  83.04 % ||| loss 0.4689103066921234\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.43349687576293944\n",
            "Batch #200 Loss: 0.4113964551687241\n",
            "Batch #300 Loss: 0.4303623646497726\n",
            "\u001b[92mTrain accuracy: 40940/48000 =  85.29 % ||| loss 0.41096699237823486\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10172/12000 =  84.77 % ||| loss 0.42263391613960266\u001b[0m\n",
            "\u001b[92mTest accuracy: 8436/10000 =  84.36 % ||| loss 0.4334249496459961\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.40826224982738496\n",
            "Batch #200 Loss: 0.4032980582118034\n",
            "Batch #300 Loss: 0.39364062875509265\n",
            "\u001b[92mTrain accuracy: 41307/48000 =  86.06 % ||| loss 0.3907544016838074\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10291/12000 =  85.76 % ||| loss 0.4003680646419525\u001b[0m\n",
            "\u001b[92mTest accuracy: 8472/10000 =  84.72 % ||| loss 0.4159809947013855\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3863471119105816\n",
            "Batch #200 Loss: 0.38299485757946966\n",
            "Batch #300 Loss: 0.3851537375152111\n",
            "\u001b[92mTrain accuracy: 41695/48000 =  86.86 % ||| loss 0.3654950261116028\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10378/12000 =  86.48 % ||| loss 0.3801354765892029\u001b[0m\n",
            "\u001b[92mTest accuracy: 8605/10000 =  86.05 % ||| loss 0.38956189155578613\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3861760410666466\n",
            "Batch #200 Loss: 0.35860224068164825\n",
            "Batch #300 Loss: 0.36064180597662926\n",
            "\u001b[92mTrain accuracy: 41922/48000 =  87.34 % ||| loss 0.35558265447616577\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10410/12000 =  86.75 % ||| loss 0.3733843266963959\u001b[0m\n",
            "\u001b[92mTest accuracy: 8623/10000 =  86.23 % ||| loss 0.3840906023979187\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3614794693887234\n",
            "Batch #200 Loss: 0.3510770386457443\n",
            "Batch #300 Loss: 0.3648498007655144\n",
            "\u001b[92mTrain accuracy: 41822/48000 =  87.13 % ||| loss 0.3554610311985016\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10404/12000 =  86.7 % ||| loss 0.3731381893157959\u001b[0m\n",
            "\u001b[92mTest accuracy: 8605/10000 =  86.05 % ||| loss 0.3850421607494354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.3595658466219902\n",
            "Batch #200 Loss: 0.3526019462943077\n",
            "Batch #300 Loss: 0.33526560679078105\n",
            "\u001b[92mTrain accuracy: 42057/48000 =  87.62 % ||| loss 0.3447222411632538\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10407/12000 =  86.72 % ||| loss 0.3630756139755249\u001b[0m\n",
            "\u001b[92mTest accuracy: 8646/10000 =  86.46 % ||| loss 0.37681815028190613\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.32881858825683596\n",
            "Batch #200 Loss: 0.3399685265123844\n",
            "Batch #300 Loss: 0.34155906438827516\n",
            "\u001b[92mTrain accuracy: 41880/48000 =  87.25 % ||| loss 0.3477347791194916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10406/12000 =  86.72 % ||| loss 0.3646271824836731\u001b[0m\n",
            "\u001b[92mTest accuracy: 8563/10000 =  85.63 % ||| loss 0.3814696669578552\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.33426262080669406\n",
            "Batch #200 Loss: 0.3351270881295204\n",
            "Batch #300 Loss: 0.33108599796891214\n",
            "\u001b[92mTrain accuracy: 42491/48000 =  88.52 % ||| loss 0.32498493790626526\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10527/12000 =  87.72 % ||| loss 0.34709063172340393\u001b[0m\n",
            "\u001b[92mTest accuracy: 8713/10000 =  87.13 % ||| loss 0.3620263636112213\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.33403880342841147\n",
            "Batch #200 Loss: 0.315547486692667\n",
            "Batch #300 Loss: 0.33024910151958464\n",
            "\u001b[92mTrain accuracy: 42596/48000 =  88.74 % ||| loss 0.31678372621536255\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10549/12000 =  87.91 % ||| loss 0.3378410041332245\u001b[0m\n",
            "\u001b[92mTest accuracy: 8727/10000 =  87.27 % ||| loss 0.35702723264694214\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3160287117958069\n",
            "Batch #200 Loss: 0.3235986490547657\n",
            "Batch #300 Loss: 0.31343991339206695\n",
            "\u001b[92mTrain accuracy: 42757/48000 =  89.08 % ||| loss 0.30676087737083435\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10600/12000 =  88.33 % ||| loss 0.33008384704589844\u001b[0m\n",
            "\u001b[92mTest accuracy: 8739/10000 =  87.39 % ||| loss 0.34668803215026855\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.31847711056470873\n",
            "Batch #200 Loss: 0.3117102186381817\n",
            "Batch #300 Loss: 0.3285358773171902\n",
            "\u001b[92mTrain accuracy: 42907/48000 =  89.39 % ||| loss 0.2985983192920685\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10623/12000 =  88.52 % ||| loss 0.32525163888931274\u001b[0m\n",
            "\u001b[92mTest accuracy: 8748/10000 =  87.48 % ||| loss 0.34407755732536316\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.31361099481582644\n",
            "Batch #200 Loss: 0.3065237782895565\n",
            "Batch #300 Loss: 0.3143758627772331\n",
            "\u001b[92mTrain accuracy: 42677/48000 =  88.91 % ||| loss 0.306227445602417\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10560/12000 =  88.0 % ||| loss 0.3345756232738495\u001b[0m\n",
            "\u001b[92mTest accuracy: 8687/10000 =  86.87 % ||| loss 0.35188931226730347\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3040870800614357\n",
            "Batch #200 Loss: 0.30093645319342616\n",
            "Batch #300 Loss: 0.3045543646812439\n",
            "\u001b[92mTrain accuracy: 42775/48000 =  89.11 % ||| loss 0.30026811361312866\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10578/12000 =  88.15 % ||| loss 0.33098146319389343\u001b[0m\n",
            "\u001b[92mTest accuracy: 8739/10000 =  87.39 % ||| loss 0.3472721576690674\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.2984516158699989\n",
            "Batch #200 Loss: 0.29772529244422913\n",
            "Batch #300 Loss: 0.3062497629225254\n",
            "\u001b[92mTrain accuracy: 42569/48000 =  88.69 % ||| loss 0.3049488663673401\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10551/12000 =  87.92 % ||| loss 0.3341754674911499\u001b[0m\n",
            "\u001b[92mTest accuracy: 8679/10000 =  86.79 % ||| loss 0.35019171237945557\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.2910453724861145\n",
            "Batch #200 Loss: 0.3020239970088005\n",
            "Batch #300 Loss: 0.28958541274070737\n",
            "\u001b[92mTrain accuracy: 42994/48000 =  89.57 % ||| loss 0.28993967175483704\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10630/12000 =  88.58 % ||| loss 0.32116439938545227\u001b[0m\n",
            "\u001b[92mTest accuracy: 8772/10000 =  87.72 % ||| loss 0.3373451828956604\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.29232181996107104\n",
            "Batch #200 Loss: 0.3037527932226658\n",
            "Batch #300 Loss: 0.29024064764380453\n",
            "\u001b[92mTrain accuracy: 43133/48000 =  89.86 % ||| loss 0.2818870544433594\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10612/12000 =  88.43 % ||| loss 0.3164452016353607\u001b[0m\n",
            "\u001b[92mTest accuracy: 8781/10000 =  87.81 % ||| loss 0.33508020639419556\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.2984657609462738\n",
            "Batch #200 Loss: 0.2871154859662056\n",
            "Batch #300 Loss: 0.29119748577475546\n",
            "\u001b[92mTrain accuracy: 43048/48000 =  89.68 % ||| loss 0.28533169627189636\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10615/12000 =  88.46 % ||| loss 0.3198501169681549\u001b[0m\n",
            "\u001b[92mTest accuracy: 8789/10000 =  87.89 % ||| loss 0.3370440602302551\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.2837337689101696\n",
            "Batch #200 Loss: 0.2844277659058571\n",
            "Batch #300 Loss: 0.28972642600536347\n",
            "\u001b[92mTrain accuracy: 43143/48000 =  89.88 % ||| loss 0.27809786796569824\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10643/12000 =  88.69 % ||| loss 0.3131099045276642\u001b[0m\n",
            "\u001b[92mTest accuracy: 8789/10000 =  87.89 % ||| loss 0.3311493396759033\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.28338960945606234\n",
            "Batch #200 Loss: 0.28042454659938815\n",
            "Batch #300 Loss: 0.284892235994339\n",
            "\u001b[92mTrain accuracy: 43202/48000 =  90.0 % ||| loss 0.2758031487464905\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10667/12000 =  88.89 % ||| loss 0.3131459355354309\u001b[0m\n",
            "\u001b[92mTest accuracy: 8808/10000 =  88.08 % ||| loss 0.33734700083732605\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.2614979410171509\n",
            "Batch #200 Loss: 0.2887324747443199\n",
            "Batch #300 Loss: 0.2835386447608471\n",
            "\u001b[92mTrain accuracy: 43310/48000 =  90.23 % ||| loss 0.2693014442920685\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10676/12000 =  88.97 % ||| loss 0.30693933367729187\u001b[0m\n",
            "\u001b[92mTest accuracy: 8791/10000 =  87.91 % ||| loss 0.33087125420570374\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726087983.826163_3</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_3</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_170055-Lenet5BN_1726087983.826163_4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_4' target=\"_blank\">Lenet5BN_1726087983.826163_4</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.30896819293499\n",
            "Batch #200 Loss: 0.5895030754804611\n",
            "Batch #300 Loss: 0.4996562898159027\n",
            "\u001b[92mTrain accuracy: 40278/48000 =  83.91 % ||| loss 0.44084054231643677\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10002/12000 =  83.35 % ||| loss 0.44378170371055603\u001b[0m\n",
            "\u001b[92mTest accuracy: 8326/10000 =  83.26 % ||| loss 0.4650436043739319\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.4197103378176689\n",
            "Batch #200 Loss: 0.40751411974430085\n",
            "Batch #300 Loss: 0.3823903886973858\n",
            "\u001b[92mTrain accuracy: 40532/48000 =  84.44 % ||| loss 0.4289611876010895\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10071/12000 =  83.93 % ||| loss 0.44394201040267944\u001b[0m\n",
            "\u001b[92mTest accuracy: 8349/10000 =  83.49 % ||| loss 0.4665185511112213\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.3719278936088085\n",
            "Batch #200 Loss: 0.34727256596088407\n",
            "Batch #300 Loss: 0.35193838223814966\n",
            "\u001b[92mTrain accuracy: 42191/48000 =  87.9 % ||| loss 0.33626464009284973\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10473/12000 =  87.28 % ||| loss 0.3515236973762512\u001b[0m\n",
            "\u001b[92mTest accuracy: 8629/10000 =  86.29 % ||| loss 0.3791084289550781\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.3400001130998135\n",
            "Batch #200 Loss: 0.3308278480172157\n",
            "Batch #300 Loss: 0.3208075426518917\n",
            "\u001b[92mTrain accuracy: 42276/48000 =  88.08 % ||| loss 0.32136210799217224\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10500/12000 =  87.5 % ||| loss 0.3418739438056946\u001b[0m\n",
            "\u001b[92mTest accuracy: 8668/10000 =  86.68 % ||| loss 0.36018815636634827\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.30396478161215784\n",
            "Batch #200 Loss: 0.305932028144598\n",
            "Batch #300 Loss: 0.2978005921840668\n",
            "\u001b[92mTrain accuracy: 42766/48000 =  89.1 % ||| loss 0.2987114191055298\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10536/12000 =  87.8 % ||| loss 0.32638007402420044\u001b[0m\n",
            "\u001b[92mTest accuracy: 8738/10000 =  87.38 % ||| loss 0.34503844380378723\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.2941471184790134\n",
            "Batch #200 Loss: 0.3066760416328907\n",
            "Batch #300 Loss: 0.28489119723439216\n",
            "\u001b[92mTrain accuracy: 42083/48000 =  87.67 % ||| loss 0.32685691118240356\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10454/12000 =  87.12 % ||| loss 0.3547217547893524\u001b[0m\n",
            "\u001b[92mTest accuracy: 8588/10000 =  85.88 % ||| loss 0.37625738978385925\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.26449804127216336\n",
            "Batch #200 Loss: 0.2847806942462921\n",
            "Batch #300 Loss: 0.2662756860256195\n",
            "\u001b[92mTrain accuracy: 43265/48000 =  90.14 % ||| loss 0.2660658359527588\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10625/12000 =  88.54 % ||| loss 0.30674830079078674\u001b[0m\n",
            "\u001b[92mTest accuracy: 8761/10000 =  87.61 % ||| loss 0.33071622252464294\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.2588819402456284\n",
            "Batch #200 Loss: 0.26698142409324643\n",
            "Batch #300 Loss: 0.26468496695160865\n",
            "\u001b[92mTrain accuracy: 43879/48000 =  91.41 % ||| loss 0.23772525787353516\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10767/12000 =  89.72 % ||| loss 0.28033024072647095\u001b[0m\n",
            "\u001b[92mTest accuracy: 8934/10000 =  89.34 % ||| loss 0.2985093295574188\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.25805265352129936\n",
            "Batch #200 Loss: 0.24411755487322806\n",
            "Batch #300 Loss: 0.2609222702682018\n",
            "\u001b[92mTrain accuracy: 41844/48000 =  87.17 % ||| loss 0.32872071862220764\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10324/12000 =  86.03 % ||| loss 0.3718811571598053\u001b[0m\n",
            "\u001b[92mTest accuracy: 8477/10000 =  84.77 % ||| loss 0.4014570116996765\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.2479418107867241\n",
            "Batch #200 Loss: 0.24445964090526104\n",
            "Batch #300 Loss: 0.24803364023566246\n",
            "\u001b[92mTrain accuracy: 42959/48000 =  89.5 % ||| loss 0.27431413531303406\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10565/12000 =  88.04 % ||| loss 0.3183518648147583\u001b[0m\n",
            "\u001b[92mTest accuracy: 8676/10000 =  86.76 % ||| loss 0.3391650319099426\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.23977268978953362\n",
            "Batch #200 Loss: 0.23540926173329355\n",
            "Batch #300 Loss: 0.24299285024404527\n",
            "\u001b[92mTrain accuracy: 44201/48000 =  92.09 % ||| loss 0.21500183641910553\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10791/12000 =  89.92 % ||| loss 0.27454155683517456\u001b[0m\n",
            "\u001b[92mTest accuracy: 8906/10000 =  89.06 % ||| loss 0.2998480498790741\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.22987551346421242\n",
            "Batch #200 Loss: 0.23007069766521454\n",
            "Batch #300 Loss: 0.23371066123247147\n",
            "\u001b[92mTrain accuracy: 44351/48000 =  92.4 % ||| loss 0.2102031707763672\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10801/12000 =  90.01 % ||| loss 0.2715073227882385\u001b[0m\n",
            "\u001b[92mTest accuracy: 8965/10000 =  89.65 % ||| loss 0.2949642241001129\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.22250246189534664\n",
            "Batch #200 Loss: 0.22522019051015377\n",
            "Batch #300 Loss: 0.21921508319675923\n",
            "\u001b[92mTrain accuracy: 44118/48000 =  91.91 % ||| loss 0.22102077305316925\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10703/12000 =  89.19 % ||| loss 0.29270797967910767\u001b[0m\n",
            "\u001b[92mTest accuracy: 8878/10000 =  88.78 % ||| loss 0.31496983766555786\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.21476200319826602\n",
            "Batch #200 Loss: 0.21861928425729274\n",
            "Batch #300 Loss: 0.2125558040291071\n",
            "\u001b[92mTrain accuracy: 44495/48000 =  92.7 % ||| loss 0.19882340729236603\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10807/12000 =  90.06 % ||| loss 0.2731015086174011\u001b[0m\n",
            "\u001b[92mTest accuracy: 8922/10000 =  89.22 % ||| loss 0.29688796401023865\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.20630635224282742\n",
            "Batch #200 Loss: 0.20004116281867027\n",
            "Batch #300 Loss: 0.21491656303405762\n",
            "\u001b[92mTrain accuracy: 44500/48000 =  92.71 % ||| loss 0.19469256699085236\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10767/12000 =  89.72 % ||| loss 0.275881826877594\u001b[0m\n",
            "\u001b[92mTest accuracy: 8911/10000 =  89.11 % ||| loss 0.29582881927490234\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.20425226338207722\n",
            "Batch #200 Loss: 0.21118570543825627\n",
            "Batch #300 Loss: 0.2050357396900654\n",
            "\u001b[92mTrain accuracy: 44297/48000 =  92.29 % ||| loss 0.2064051479101181\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10715/12000 =  89.29 % ||| loss 0.3003733456134796\u001b[0m\n",
            "\u001b[92mTest accuracy: 8874/10000 =  88.74 % ||| loss 0.3278256058692932\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.18952355481684208\n",
            "Batch #200 Loss: 0.1994884594529867\n",
            "Batch #300 Loss: 0.20491749055683614\n",
            "\u001b[92mTrain accuracy: 44293/48000 =  92.28 % ||| loss 0.20360471308231354\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10704/12000 =  89.2 % ||| loss 0.3054695427417755\u001b[0m\n",
            "\u001b[92mTest accuracy: 8871/10000 =  88.71 % ||| loss 0.3286781907081604\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.19216322131454944\n",
            "Batch #200 Loss: 0.19219213858246803\n",
            "Batch #300 Loss: 0.1905763364583254\n",
            "\u001b[92mTrain accuracy: 43903/48000 =  91.46 % ||| loss 0.22372066974639893\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10616/12000 =  88.47 % ||| loss 0.33464527130126953\u001b[0m\n",
            "\u001b[92mTest accuracy: 8797/10000 =  87.97 % ||| loss 0.35830971598625183\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.1800732784718275\n",
            "Batch #200 Loss: 0.1825494433939457\n",
            "Batch #300 Loss: 0.19259209595620633\n",
            "\u001b[92mTrain accuracy: 44942/48000 =  93.63 % ||| loss 0.1734236180782318\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10803/12000 =  90.03 % ||| loss 0.2849338948726654\u001b[0m\n",
            "\u001b[92mTest accuracy: 8916/10000 =  89.16 % ||| loss 0.3094959557056427\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.1753782720863819\n",
            "Batch #200 Loss: 0.18261581271886826\n",
            "Batch #300 Loss: 0.1849031326174736\n",
            "\u001b[92mTrain accuracy: 44034/48000 =  91.74 % ||| loss 0.2176002860069275\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10586/12000 =  88.22 % ||| loss 0.3410816192626953\u001b[0m\n",
            "\u001b[92mTest accuracy: 8781/10000 =  87.81 % ||| loss 0.358767569065094\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.16291086710989475\n",
            "Batch #200 Loss: 0.17609576418995856\n",
            "Batch #300 Loss: 0.17665758036077023\n",
            "\u001b[92mTrain accuracy: 44731/48000 =  93.19 % ||| loss 0.18499399721622467\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10729/12000 =  89.41 % ||| loss 0.29860106110572815\u001b[0m\n",
            "\u001b[92mTest accuracy: 8846/10000 =  88.46 % ||| loss 0.319265216588974\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.15988808140158653\n",
            "Batch #200 Loss: 0.17700260370969773\n",
            "Batch #300 Loss: 0.16994865790009497\n",
            "\u001b[92mTrain accuracy: 44946/48000 =  93.64 % ||| loss 0.16696292161941528\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10786/12000 =  89.88 % ||| loss 0.2928714454174042\u001b[0m\n",
            "\u001b[92mTest accuracy: 8893/10000 =  88.93 % ||| loss 0.3234074115753174\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.15929122544825078\n",
            "Batch #200 Loss: 0.161538370475173\n",
            "Batch #300 Loss: 0.16743444710969924\n",
            "\u001b[92mTrain accuracy: 44922/48000 =  93.59 % ||| loss 0.1739039570093155\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10744/12000 =  89.53 % ||| loss 0.30708450078964233\u001b[0m\n",
            "\u001b[92mTest accuracy: 8866/10000 =  88.66 % ||| loss 0.33381304144859314\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.15904485017061235\n",
            "Batch #200 Loss: 0.155825424939394\n",
            "Batch #300 Loss: 0.16688490740954876\n",
            "\u001b[92mTrain accuracy: 45013/48000 =  93.78 % ||| loss 0.16504044830799103\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10724/12000 =  89.37 % ||| loss 0.3193366527557373\u001b[0m\n",
            "\u001b[92mTest accuracy: 8893/10000 =  88.93 % ||| loss 0.3433612287044525\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.15429296031594275\n",
            "Batch #200 Loss: 0.1528839363157749\n",
            "Batch #300 Loss: 0.16172048583626747\n",
            "\u001b[92mTrain accuracy: 44823/48000 =  93.38 % ||| loss 0.1703500896692276\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10669/12000 =  88.91 % ||| loss 0.3254997432231903\u001b[0m\n",
            "\u001b[92mTest accuracy: 8885/10000 =  88.85 % ||| loss 0.344987154006958\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726087983.826163_4</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_4</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_170257-Lenet5BN_1726087983.826163_5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_5' target=\"_blank\">Lenet5BN_1726087983.826163_5</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.796340789794922\n",
            "Batch #200 Loss: 0.8401311552524566\n",
            "Batch #300 Loss: 0.6155732488632202\n",
            "\u001b[92mTrain accuracy: 38417/48000 =  80.04 % ||| loss 0.527275025844574\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9600/12000 =  80.0 % ||| loss 0.5297437906265259\u001b[0m\n",
            "\u001b[92mTest accuracy: 7912/10000 =  79.12 % ||| loss 0.5499461889266968\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.5073026782274246\n",
            "Batch #200 Loss: 0.46626132547855376\n",
            "Batch #300 Loss: 0.45285633862018587\n",
            "\u001b[92mTrain accuracy: 40895/48000 =  85.2 % ||| loss 0.41250696778297424\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10193/12000 =  84.94 % ||| loss 0.4225658178329468\u001b[0m\n",
            "\u001b[92mTest accuracy: 8412/10000 =  84.12 % ||| loss 0.43962860107421875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.4092708374559879\n",
            "Batch #200 Loss: 0.4076686878502369\n",
            "Batch #300 Loss: 0.40252885222435\n",
            "\u001b[92mTrain accuracy: 41343/48000 =  86.13 % ||| loss 0.38242343068122864\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10257/12000 =  85.47 % ||| loss 0.39630475640296936\u001b[0m\n",
            "\u001b[92mTest accuracy: 8474/10000 =  84.74 % ||| loss 0.4130389392375946\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.3775747033953667\n",
            "Batch #200 Loss: 0.3674343420565128\n",
            "Batch #300 Loss: 0.3800771901011467\n",
            "\u001b[92mTrain accuracy: 41794/48000 =  87.07 % ||| loss 0.3574790060520172\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10369/12000 =  86.41 % ||| loss 0.37434297800064087\u001b[0m\n",
            "\u001b[92mTest accuracy: 8596/10000 =  85.96 % ||| loss 0.3937115967273712\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.36040559858083726\n",
            "Batch #200 Loss: 0.33865263894200326\n",
            "Batch #300 Loss: 0.3469631230831146\n",
            "\u001b[92mTrain accuracy: 41477/48000 =  86.41 % ||| loss 0.3697403073310852\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10310/12000 =  85.92 % ||| loss 0.39165958762168884\u001b[0m\n",
            "\u001b[92mTest accuracy: 8490/10000 =  84.9 % ||| loss 0.41463765501976013\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.33083880990743636\n",
            "Batch #200 Loss: 0.3331500354409218\n",
            "Batch #300 Loss: 0.33382827147841454\n",
            "\u001b[92mTrain accuracy: 42479/48000 =  88.5 % ||| loss 0.3197596073150635\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10508/12000 =  87.57 % ||| loss 0.3458525538444519\u001b[0m\n",
            "\u001b[92mTest accuracy: 8717/10000 =  87.17 % ||| loss 0.36128202080726624\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3255611483752727\n",
            "Batch #200 Loss: 0.32150516375899313\n",
            "Batch #300 Loss: 0.31936216458678246\n",
            "\u001b[92mTrain accuracy: 42631/48000 =  88.81 % ||| loss 0.30963587760925293\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10548/12000 =  87.9 % ||| loss 0.3335990309715271\u001b[0m\n",
            "\u001b[92mTest accuracy: 8734/10000 =  87.34 % ||| loss 0.35239556431770325\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3101415294408798\n",
            "Batch #200 Loss: 0.30792055785655975\n",
            "Batch #300 Loss: 0.3072301983833313\n",
            "\u001b[92mTrain accuracy: 43099/48000 =  89.79 % ||| loss 0.2837810814380646\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10655/12000 =  88.79 % ||| loss 0.3140484392642975\u001b[0m\n",
            "\u001b[92mTest accuracy: 8779/10000 =  87.79 % ||| loss 0.332926481962204\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.29920280262827875\n",
            "Batch #200 Loss: 0.30432484298944473\n",
            "Batch #300 Loss: 0.2966253951191902\n",
            "\u001b[92mTrain accuracy: 42097/48000 =  87.7 % ||| loss 0.3301297426223755\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10364/12000 =  86.37 % ||| loss 0.3667322099208832\u001b[0m\n",
            "\u001b[92mTest accuracy: 8578/10000 =  85.78 % ||| loss 0.38868096470832825\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.28521163925528525\n",
            "Batch #200 Loss: 0.29745293587446214\n",
            "Batch #300 Loss: 0.2951767288148403\n",
            "\u001b[92mTrain accuracy: 43261/48000 =  90.13 % ||| loss 0.2715320289134979\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10665/12000 =  88.88 % ||| loss 0.3100989758968353\u001b[0m\n",
            "\u001b[92mTest accuracy: 8802/10000 =  88.02 % ||| loss 0.33217158913612366\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.28442091420292853\n",
            "Batch #200 Loss: 0.290172827988863\n",
            "Batch #300 Loss: 0.27293484717607497\n",
            "\u001b[92mTrain accuracy: 43443/48000 =  90.51 % ||| loss 0.26347658038139343\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10714/12000 =  89.28 % ||| loss 0.30167850852012634\u001b[0m\n",
            "\u001b[92mTest accuracy: 8826/10000 =  88.26 % ||| loss 0.32208091020584106\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.27414053186774257\n",
            "Batch #200 Loss: 0.2719019336998463\n",
            "Batch #300 Loss: 0.28342478156089784\n",
            "\u001b[92mTrain accuracy: 43363/48000 =  90.34 % ||| loss 0.2687310576438904\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10620/12000 =  88.5 % ||| loss 0.3142119348049164\u001b[0m\n",
            "\u001b[92mTest accuracy: 8799/10000 =  87.99 % ||| loss 0.3320232927799225\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.2724000786244869\n",
            "Batch #200 Loss: 0.27238852724432944\n",
            "Batch #300 Loss: 0.27010409101843835\n",
            "\u001b[92mTrain accuracy: 43301/48000 =  90.21 % ||| loss 0.26872628927230835\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10666/12000 =  88.88 % ||| loss 0.31303951144218445\u001b[0m\n",
            "\u001b[92mTest accuracy: 8788/10000 =  87.88 % ||| loss 0.33719292283058167\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.2686623013019562\n",
            "Batch #200 Loss: 0.272407401651144\n",
            "Batch #300 Loss: 0.2660357043147087\n",
            "\u001b[92mTrain accuracy: 43641/48000 =  90.92 % ||| loss 0.25097787380218506\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10739/12000 =  89.49 % ||| loss 0.2947230935096741\u001b[0m\n",
            "\u001b[92mTest accuracy: 8851/10000 =  88.51 % ||| loss 0.3202158510684967\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.2611718111485243\n",
            "Batch #200 Loss: 0.26083967834711075\n",
            "Batch #300 Loss: 0.2576248548924923\n",
            "\u001b[92mTrain accuracy: 43288/48000 =  90.18 % ||| loss 0.2660793960094452\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10642/12000 =  88.68 % ||| loss 0.31399890780448914\u001b[0m\n",
            "\u001b[92mTest accuracy: 8785/10000 =  87.85 % ||| loss 0.33857807517051697\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.2529803301393986\n",
            "Batch #200 Loss: 0.2637270550429821\n",
            "Batch #300 Loss: 0.26367227986454966\n",
            "\u001b[92mTrain accuracy: 43432/48000 =  90.48 % ||| loss 0.2565319836139679\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10615/12000 =  88.46 % ||| loss 0.31503838300704956\u001b[0m\n",
            "\u001b[92mTest accuracy: 8801/10000 =  88.01 % ||| loss 0.3318406641483307\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.24753162145614624\n",
            "Batch #200 Loss: 0.2514689940959215\n",
            "Batch #300 Loss: 0.2621113830804825\n",
            "\u001b[92mTrain accuracy: 43927/48000 =  91.51 % ||| loss 0.2362019419670105\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10746/12000 =  89.55 % ||| loss 0.29303884506225586\u001b[0m\n",
            "\u001b[92mTest accuracy: 8871/10000 =  88.71 % ||| loss 0.3142085075378418\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.23946991369128226\n",
            "Batch #200 Loss: 0.26390493467450143\n",
            "Batch #300 Loss: 0.24548227593302727\n",
            "\u001b[92mTrain accuracy: 43761/48000 =  91.17 % ||| loss 0.24253366887569427\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10708/12000 =  89.23 % ||| loss 0.30350393056869507\u001b[0m\n",
            "\u001b[92mTest accuracy: 8859/10000 =  88.59 % ||| loss 0.3235628604888916\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.24443256601691246\n",
            "Batch #200 Loss: 0.2395165729522705\n",
            "Batch #300 Loss: 0.24731777459383011\n",
            "\u001b[92mTrain accuracy: 43928/48000 =  91.52 % ||| loss 0.23192614316940308\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10726/12000 =  89.38 % ||| loss 0.2968182861804962\u001b[0m\n",
            "\u001b[92mTest accuracy: 8884/10000 =  88.84 % ||| loss 0.3152964413166046\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.23514915257692337\n",
            "Batch #200 Loss: 0.23388320609927177\n",
            "Batch #300 Loss: 0.24102332405745983\n",
            "\u001b[92mTrain accuracy: 43727/48000 =  91.1 % ||| loss 0.24032431840896606\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10676/12000 =  88.97 % ||| loss 0.3057178854942322\u001b[0m\n",
            "\u001b[92mTest accuracy: 8816/10000 =  88.16 % ||| loss 0.3252760171890259\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.23565703392028808\n",
            "Batch #200 Loss: 0.22592314921319484\n",
            "Batch #300 Loss: 0.2441951374709606\n",
            "\u001b[92mTrain accuracy: 43355/48000 =  90.32 % ||| loss 0.2539920508861542\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10580/12000 =  88.17 % ||| loss 0.3246995210647583\u001b[0m\n",
            "\u001b[92mTest accuracy: 8762/10000 =  87.62 % ||| loss 0.34009242057800293\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.2264617957174778\n",
            "Batch #200 Loss: 0.2361089912056923\n",
            "Batch #300 Loss: 0.22962333500385285\n",
            "\u001b[92mTrain accuracy: 43862/48000 =  91.38 % ||| loss 0.2361084371805191\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10684/12000 =  89.03 % ||| loss 0.3125210404396057\u001b[0m\n",
            "\u001b[92mTest accuracy: 8834/10000 =  88.34 % ||| loss 0.33762890100479126\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.21913212567567825\n",
            "Batch #200 Loss: 0.2261865485459566\n",
            "Batch #300 Loss: 0.23834398090839387\n",
            "\u001b[92mTrain accuracy: 44019/48000 =  91.71 % ||| loss 0.22505860030651093\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10657/12000 =  88.81 % ||| loss 0.30834516882896423\u001b[0m\n",
            "\u001b[92mTest accuracy: 8814/10000 =  88.14 % ||| loss 0.32484960556030273\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.21753973498940468\n",
            "Batch #200 Loss: 0.22975274704396725\n",
            "Batch #300 Loss: 0.2243804756551981\n",
            "\u001b[92mTrain accuracy: 44276/48000 =  92.24 % ||| loss 0.21175813674926758\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10729/12000 =  89.41 % ||| loss 0.29411712288856506\u001b[0m\n",
            "\u001b[92mTest accuracy: 8890/10000 =  88.9 % ||| loss 0.3082031011581421\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.2224098565429449\n",
            "Batch #200 Loss: 0.21516344264149667\n",
            "Batch #300 Loss: 0.21718495279550554\n",
            "\u001b[92mTrain accuracy: 44309/48000 =  92.31 % ||| loss 0.21130645275115967\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10708/12000 =  89.23 % ||| loss 0.29933831095695496\u001b[0m\n",
            "\u001b[92mTest accuracy: 8902/10000 =  89.02 % ||| loss 0.31301578879356384\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726087983.826163_5</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_5</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_170454-Lenet5BN_1726087983.826163_6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_6' target=\"_blank\">Lenet5BN_1726087983.826163_6</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2856762337684633\n",
            "Batch #200 Loss: 2.2634411692619323\n",
            "Batch #300 Loss: 2.2426475882530212\n",
            "\u001b[92mTrain accuracy: 17902/48000 =  37.3 % ||| loss 2.2114834785461426\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4466/12000 =  37.22 % ||| loss 2.212064266204834\u001b[0m\n",
            "\u001b[92mTest accuracy: 3734/10000 =  37.34 % ||| loss 2.2115838527679443\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.19739305973053\n",
            "Batch #200 Loss: 2.1695296812057494\n",
            "Batch #300 Loss: 2.132632248401642\n",
            "\u001b[92mTrain accuracy: 20042/48000 =  41.75 % ||| loss 2.084836483001709\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4962/12000 =  41.35 % ||| loss 2.086228132247925\u001b[0m\n",
            "\u001b[92mTest accuracy: 4183/10000 =  41.83 % ||| loss 2.087036609649658\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.060846166610718\n",
            "Batch #200 Loss: 2.0173727893829345\n",
            "Batch #300 Loss: 1.9713837957382203\n",
            "\u001b[92mTrain accuracy: 21742/48000 =  45.3 % ||| loss 1.9008233547210693\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5342/12000 =  44.52 % ||| loss 1.9025145769119263\u001b[0m\n",
            "\u001b[92mTest accuracy: 4522/10000 =  45.22 % ||| loss 1.9033162593841553\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.8729853105545045\n",
            "Batch #200 Loss: 1.8200443589687347\n",
            "Batch #300 Loss: 1.7717236113548278\n",
            "\u001b[92mTrain accuracy: 25154/48000 =  52.4 % ||| loss 1.693144679069519\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6249/12000 =  52.08 % ||| loss 1.694169521331787\u001b[0m\n",
            "\u001b[92mTest accuracy: 5202/10000 =  52.02 % ||| loss 1.6967511177062988\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 1.6632692897319794\n",
            "Batch #200 Loss: 1.6005529975891113\n",
            "Batch #300 Loss: 1.5458602821826934\n",
            "\u001b[92mTrain accuracy: 29026/48000 =  60.47 % ||| loss 1.4707696437835693\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7250/12000 =  60.42 % ||| loss 1.469893217086792\u001b[0m\n",
            "\u001b[92mTest accuracy: 6039/10000 =  60.39 % ||| loss 1.4745948314666748\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 1.437558720111847\n",
            "Batch #200 Loss: 1.3827603232860566\n",
            "Batch #300 Loss: 1.3379408180713654\n",
            "\u001b[92mTrain accuracy: 31582/48000 =  65.8 % ||| loss 1.2695494890213013\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7939/12000 =  66.16 % ||| loss 1.266953468322754\u001b[0m\n",
            "\u001b[92mTest accuracy: 6582/10000 =  65.82 % ||| loss 1.2784446477890015\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 1.2452250134944916\n",
            "Batch #200 Loss: 1.2066024625301361\n",
            "Batch #300 Loss: 1.1677051770687104\n",
            "\u001b[92mTrain accuracy: 32726/48000 =  68.18 % ||| loss 1.1124807596206665\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8222/12000 =  68.52 % ||| loss 1.1083626747131348\u001b[0m\n",
            "\u001b[92mTest accuracy: 6798/10000 =  67.98 % ||| loss 1.1186389923095703\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 1.0932916980981826\n",
            "Batch #200 Loss: 1.0664466267824173\n",
            "Batch #300 Loss: 1.029344197511673\n",
            "\u001b[92mTrain accuracy: 33674/48000 =  70.15 % ||| loss 0.9946258664131165\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8470/12000 =  70.58 % ||| loss 0.9891875982284546\u001b[0m\n",
            "\u001b[92mTest accuracy: 6953/10000 =  69.53 % ||| loss 1.0044336318969727\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.9794185745716095\n",
            "Batch #200 Loss: 0.9615475314855576\n",
            "Batch #300 Loss: 0.934897836446762\n",
            "\u001b[92mTrain accuracy: 34202/48000 =  71.25 % ||| loss 0.9102234840393066\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8607/12000 =  71.73 % ||| loss 0.9044132828712463\u001b[0m\n",
            "\u001b[92mTest accuracy: 7124/10000 =  71.24 % ||| loss 0.9210049510002136\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.8979040414094925\n",
            "Batch #200 Loss: 0.8885869228839874\n",
            "Batch #300 Loss: 0.8669937700033188\n",
            "\u001b[92mTrain accuracy: 34584/48000 =  72.05 % ||| loss 0.8509004712104797\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8704/12000 =  72.53 % ||| loss 0.8444900512695312\u001b[0m\n",
            "\u001b[92mTest accuracy: 7172/10000 =  71.72 % ||| loss 0.8609567284584045\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.8491824072599411\n",
            "Batch #200 Loss: 0.8286312752962113\n",
            "Batch #300 Loss: 0.8099812126159668\n",
            "\u001b[92mTrain accuracy: 34949/48000 =  72.81 % ||| loss 0.8006555438041687\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8771/12000 =  73.09 % ||| loss 0.794217050075531\u001b[0m\n",
            "\u001b[92mTest accuracy: 7243/10000 =  72.43 % ||| loss 0.8124420642852783\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.7862694758176804\n",
            "Batch #200 Loss: 0.7981356483697891\n",
            "Batch #300 Loss: 0.7792606610059738\n",
            "\u001b[92mTrain accuracy: 35304/48000 =  73.55 % ||| loss 0.762940526008606\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8852/12000 =  73.77 % ||| loss 0.7563440203666687\u001b[0m\n",
            "\u001b[92mTest accuracy: 7293/10000 =  72.93 % ||| loss 0.7742698192596436\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.7546673756837845\n",
            "Batch #200 Loss: 0.7506579238176346\n",
            "Batch #300 Loss: 0.7529580748081207\n",
            "\u001b[92mTrain accuracy: 35573/48000 =  74.11 % ||| loss 0.7338021397590637\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8920/12000 =  74.33 % ||| loss 0.7274325489997864\u001b[0m\n",
            "\u001b[92mTest accuracy: 7344/10000 =  73.44 % ||| loss 0.7510274052619934\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.7283897483348847\n",
            "Batch #200 Loss: 0.7331779819726943\n",
            "Batch #300 Loss: 0.7150196009874343\n",
            "\u001b[92mTrain accuracy: 35884/48000 =  74.76 % ||| loss 0.708503246307373\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8991/12000 =  74.92 % ||| loss 0.7032580971717834\u001b[0m\n",
            "\u001b[92mTest accuracy: 7397/10000 =  73.97 % ||| loss 0.7226341366767883\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.7021691519021988\n",
            "Batch #200 Loss: 0.7131976890563965\n",
            "Batch #300 Loss: 0.686164430975914\n",
            "\u001b[92mTrain accuracy: 36112/48000 =  75.23 % ||| loss 0.6878998875617981\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9058/12000 =  75.48 % ||| loss 0.6826591491699219\u001b[0m\n",
            "\u001b[92mTest accuracy: 7447/10000 =  74.47 % ||| loss 0.7021762132644653\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.6751208394765854\n",
            "Batch #200 Loss: 0.6827279096841812\n",
            "Batch #300 Loss: 0.6717151123285293\n",
            "\u001b[92mTrain accuracy: 36356/48000 =  75.74 % ||| loss 0.6670849919319153\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9106/12000 =  75.88 % ||| loss 0.6620103120803833\u001b[0m\n",
            "\u001b[92mTest accuracy: 7502/10000 =  75.02 % ||| loss 0.6843827962875366\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.6634312319755554\n",
            "Batch #200 Loss: 0.6631327533721924\n",
            "Batch #300 Loss: 0.6590271335840225\n",
            "\u001b[92mTrain accuracy: 36573/48000 =  76.19 % ||| loss 0.6485397815704346\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9190/12000 =  76.58 % ||| loss 0.6440643668174744\u001b[0m\n",
            "\u001b[92mTest accuracy: 7527/10000 =  75.27 % ||| loss 0.6644672155380249\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.6398225250840187\n",
            "Batch #200 Loss: 0.6452617561817169\n",
            "Batch #300 Loss: 0.6393126785755158\n",
            "\u001b[92mTrain accuracy: 36795/48000 =  76.66 % ||| loss 0.6326017379760742\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9245/12000 =  77.04 % ||| loss 0.6286598443984985\u001b[0m\n",
            "\u001b[92mTest accuracy: 7574/10000 =  75.74 % ||| loss 0.648392379283905\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.6396009728312493\n",
            "Batch #200 Loss: 0.6293280935287475\n",
            "Batch #300 Loss: 0.6202868306636811\n",
            "\u001b[92mTrain accuracy: 37009/48000 =  77.1 % ||| loss 0.6191567778587341\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9299/12000 =  77.49 % ||| loss 0.6155526041984558\u001b[0m\n",
            "\u001b[92mTest accuracy: 7607/10000 =  76.07 % ||| loss 0.6407887935638428\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.6125723865628242\n",
            "Batch #200 Loss: 0.6092734956741332\n",
            "Batch #300 Loss: 0.6223134452104568\n",
            "\u001b[92mTrain accuracy: 37238/48000 =  77.58 % ||| loss 0.6061629056930542\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9349/12000 =  77.91 % ||| loss 0.6028506755828857\u001b[0m\n",
            "\u001b[92mTest accuracy: 7669/10000 =  76.69 % ||| loss 0.6236583590507507\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.61280831605196\n",
            "Batch #200 Loss: 0.6014628207683563\n",
            "Batch #300 Loss: 0.5974809700250625\n",
            "\u001b[92mTrain accuracy: 37466/48000 =  78.05 % ||| loss 0.5940301418304443\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9401/12000 =  78.34 % ||| loss 0.5912045240402222\u001b[0m\n",
            "\u001b[92mTest accuracy: 7694/10000 =  76.94 % ||| loss 0.611111581325531\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.6027193972468377\n",
            "Batch #200 Loss: 0.5793988904356957\n",
            "Batch #300 Loss: 0.587371426820755\n",
            "\u001b[92mTrain accuracy: 37592/48000 =  78.32 % ||| loss 0.5835909247398376\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9446/12000 =  78.72 % ||| loss 0.5810794830322266\u001b[0m\n",
            "\u001b[92mTest accuracy: 7746/10000 =  77.46 % ||| loss 0.6001005172729492\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5860188865661621\n",
            "Batch #200 Loss: 0.5772319200634957\n",
            "Batch #300 Loss: 0.5809198844432831\n",
            "\u001b[92mTrain accuracy: 37782/48000 =  78.71 % ||| loss 0.5723589658737183\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9486/12000 =  79.05 % ||| loss 0.5705099105834961\u001b[0m\n",
            "\u001b[92mTest accuracy: 7769/10000 =  77.69 % ||| loss 0.591566264629364\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5725275745987892\n",
            "Batch #200 Loss: 0.5670391473174096\n",
            "Batch #300 Loss: 0.5700765186548233\n",
            "\u001b[92mTrain accuracy: 38002/48000 =  79.17 % ||| loss 0.5640025734901428\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9544/12000 =  79.53 % ||| loss 0.5621386766433716\u001b[0m\n",
            "\u001b[92mTest accuracy: 7801/10000 =  78.01 % ||| loss 0.5823909640312195\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5662521719932556\n",
            "Batch #200 Loss: 0.5565529331564903\n",
            "Batch #300 Loss: 0.5632101052999496\n",
            "\u001b[92mTrain accuracy: 38196/48000 =  79.57 % ||| loss 0.5545269250869751\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9581/12000 =  79.84 % ||| loss 0.5530901551246643\u001b[0m\n",
            "\u001b[92mTest accuracy: 7835/10000 =  78.35 % ||| loss 0.5736685395240784\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726087983.826163_6</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_6</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_170648-Lenet5BN_1726087983.826163_7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_7' target=\"_blank\">Lenet5BN_1726087983.826163_7</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.222204394340515\n",
            "Batch #200 Loss: 1.8380243360996247\n",
            "Batch #300 Loss: 1.3198363387584686\n",
            "\u001b[92mTrain accuracy: 34001/48000 =  70.84 % ||| loss 0.9408679008483887\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8543/12000 =  71.19 % ||| loss 0.9314034581184387\u001b[0m\n",
            "\u001b[92mTest accuracy: 7023/10000 =  70.23 % ||| loss 0.95041823387146\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.8804208594560623\n",
            "Batch #200 Loss: 0.7651836723089218\n",
            "Batch #300 Loss: 0.6939000529050827\n",
            "\u001b[92mTrain accuracy: 36773/48000 =  76.61 % ||| loss 0.6390272378921509\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9264/12000 =  77.2 % ||| loss 0.6321969628334045\u001b[0m\n",
            "\u001b[92mTest accuracy: 7618/10000 =  76.18 % ||| loss 0.6482226848602295\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6246786054968834\n",
            "Batch #200 Loss: 0.5924420538544655\n",
            "Batch #300 Loss: 0.5598356321454048\n",
            "\u001b[92mTrain accuracy: 38308/48000 =  79.81 % ||| loss 0.5333200097084045\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9618/12000 =  80.15 % ||| loss 0.5318799614906311\u001b[0m\n",
            "\u001b[92mTest accuracy: 7921/10000 =  79.21 % ||| loss 0.5468896627426147\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5200188755989075\n",
            "Batch #200 Loss: 0.5142739409208298\n",
            "Batch #300 Loss: 0.501857434809208\n",
            "\u001b[92mTrain accuracy: 39639/48000 =  82.58 % ||| loss 0.47248005867004395\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9935/12000 =  82.79 % ||| loss 0.4738319516181946\u001b[0m\n",
            "\u001b[92mTest accuracy: 8165/10000 =  81.65 % ||| loss 0.48787277936935425\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4643717297911644\n",
            "Batch #200 Loss: 0.463316076695919\n",
            "Batch #300 Loss: 0.4569004347920418\n",
            "\u001b[92mTrain accuracy: 40336/48000 =  84.03 % ||| loss 0.4394819736480713\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10072/12000 =  83.93 % ||| loss 0.44482162594795227\u001b[0m\n",
            "\u001b[92mTest accuracy: 8290/10000 =  82.9 % ||| loss 0.4613828957080841\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.42286530643701553\n",
            "Batch #200 Loss: 0.4366046544909477\n",
            "Batch #300 Loss: 0.422945910692215\n",
            "\u001b[92mTrain accuracy: 40971/48000 =  85.36 % ||| loss 0.40747013688087463\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10236/12000 =  85.3 % ||| loss 0.4141360819339752\u001b[0m\n",
            "\u001b[92mTest accuracy: 8429/10000 =  84.29 % ||| loss 0.432817667722702\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.40480952098965645\n",
            "Batch #200 Loss: 0.39255947843194006\n",
            "Batch #300 Loss: 0.40351934522390365\n",
            "\u001b[92mTrain accuracy: 41197/48000 =  85.83 % ||| loss 0.392290323972702\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10263/12000 =  85.52 % ||| loss 0.4019106328487396\u001b[0m\n",
            "\u001b[92mTest accuracy: 8516/10000 =  85.16 % ||| loss 0.4146289527416229\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3897625109553337\n",
            "Batch #200 Loss: 0.39291394472122193\n",
            "Batch #300 Loss: 0.376182501912117\n",
            "\u001b[92mTrain accuracy: 41580/48000 =  86.62 % ||| loss 0.3690055310726166\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10343/12000 =  86.19 % ||| loss 0.37953925132751465\u001b[0m\n",
            "\u001b[92mTest accuracy: 8565/10000 =  85.65 % ||| loss 0.396183580160141\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.36185993269085887\n",
            "Batch #200 Loss: 0.3775524088740349\n",
            "Batch #300 Loss: 0.3731077109277248\n",
            "\u001b[92mTrain accuracy: 41340/48000 =  86.12 % ||| loss 0.3761596977710724\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10311/12000 =  85.92 % ||| loss 0.38840168714523315\u001b[0m\n",
            "\u001b[92mTest accuracy: 8504/10000 =  85.04 % ||| loss 0.40454426407814026\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.346548306196928\n",
            "Batch #200 Loss: 0.3574277013540268\n",
            "Batch #300 Loss: 0.35423196360468867\n",
            "\u001b[92mTrain accuracy: 42083/48000 =  87.67 % ||| loss 0.3426712155342102\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10441/12000 =  87.01 % ||| loss 0.35920342803001404\u001b[0m\n",
            "\u001b[92mTest accuracy: 8662/10000 =  86.62 % ||| loss 0.3725610673427582\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.3534963259100914\n",
            "Batch #200 Loss: 0.3306552417576313\n",
            "Batch #300 Loss: 0.3501070712506771\n",
            "\u001b[92mTrain accuracy: 41800/48000 =  87.08 % ||| loss 0.35012826323509216\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10414/12000 =  86.78 % ||| loss 0.3664151132106781\u001b[0m\n",
            "\u001b[92mTest accuracy: 8595/10000 =  85.95 % ||| loss 0.38564884662628174\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3362782932817936\n",
            "Batch #200 Loss: 0.33300192475318907\n",
            "Batch #300 Loss: 0.3462944902479649\n",
            "\u001b[92mTrain accuracy: 42463/48000 =  88.46 % ||| loss 0.3215250074863434\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10526/12000 =  87.72 % ||| loss 0.34085994958877563\u001b[0m\n",
            "\u001b[92mTest accuracy: 8719/10000 =  87.19 % ||| loss 0.35920217633247375\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.33605686366558074\n",
            "Batch #200 Loss: 0.32880956009030343\n",
            "Batch #300 Loss: 0.32057700634002684\n",
            "\u001b[92mTrain accuracy: 42103/48000 =  87.71 % ||| loss 0.3383222818374634\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10400/12000 =  86.67 % ||| loss 0.36434635519981384\u001b[0m\n",
            "\u001b[92mTest accuracy: 8624/10000 =  86.24 % ||| loss 0.382437139749527\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.33457924142479895\n",
            "Batch #200 Loss: 0.31665226578712463\n",
            "Batch #300 Loss: 0.31174238651990893\n",
            "\u001b[92mTrain accuracy: 42470/48000 =  88.48 % ||| loss 0.3185957968235016\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10511/12000 =  87.59 % ||| loss 0.34125107526779175\u001b[0m\n",
            "\u001b[92mTest accuracy: 8731/10000 =  87.31 % ||| loss 0.3596441447734833\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3167334072291851\n",
            "Batch #200 Loss: 0.3098137700557709\n",
            "Batch #300 Loss: 0.30985929772257803\n",
            "\u001b[92mTrain accuracy: 42634/48000 =  88.82 % ||| loss 0.31008729338645935\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10506/12000 =  87.55 % ||| loss 0.338111013174057\u001b[0m\n",
            "\u001b[92mTest accuracy: 8746/10000 =  87.46 % ||| loss 0.3530783951282501\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3089596362411976\n",
            "Batch #200 Loss: 0.3031461411714554\n",
            "Batch #300 Loss: 0.312544668763876\n",
            "\u001b[92mTrain accuracy: 42428/48000 =  88.39 % ||| loss 0.3143835663795471\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10505/12000 =  87.54 % ||| loss 0.34249529242515564\u001b[0m\n",
            "\u001b[92mTest accuracy: 8735/10000 =  87.35 % ||| loss 0.35710635781288147\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3107803554832935\n",
            "Batch #200 Loss: 0.2958334478735924\n",
            "Batch #300 Loss: 0.30200970619916917\n",
            "\u001b[92mTrain accuracy: 42785/48000 =  89.14 % ||| loss 0.30162733793258667\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10554/12000 =  87.95 % ||| loss 0.33119648694992065\u001b[0m\n",
            "\u001b[92mTest accuracy: 8762/10000 =  87.62 % ||| loss 0.3477843403816223\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3057129366695881\n",
            "Batch #200 Loss: 0.29063483744859697\n",
            "Batch #300 Loss: 0.29985435083508494\n",
            "\u001b[92mTrain accuracy: 42726/48000 =  89.01 % ||| loss 0.30144450068473816\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10521/12000 =  87.67 % ||| loss 0.3323077857494354\u001b[0m\n",
            "\u001b[92mTest accuracy: 8733/10000 =  87.33 % ||| loss 0.3560108542442322\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.2838497431576252\n",
            "Batch #200 Loss: 0.2943485075235367\n",
            "Batch #300 Loss: 0.30077060997486116\n",
            "\u001b[92mTrain accuracy: 42530/48000 =  88.6 % ||| loss 0.3105323910713196\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10491/12000 =  87.42 % ||| loss 0.3432493507862091\u001b[0m\n",
            "\u001b[92mTest accuracy: 8689/10000 =  86.89 % ||| loss 0.36453816294670105\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.28778027549386026\n",
            "Batch #200 Loss: 0.285086627304554\n",
            "Batch #300 Loss: 0.2921831403672695\n",
            "\u001b[92mTrain accuracy: 42973/48000 =  89.53 % ||| loss 0.28833088278770447\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10598/12000 =  88.32 % ||| loss 0.32300320267677307\u001b[0m\n",
            "\u001b[92mTest accuracy: 8771/10000 =  87.71 % ||| loss 0.3402174115180969\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.27399664238095284\n",
            "Batch #200 Loss: 0.27649901926517484\n",
            "Batch #300 Loss: 0.2996752290427685\n",
            "\u001b[92mTrain accuracy: 43229/48000 =  90.06 % ||| loss 0.27419179677963257\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10663/12000 =  88.86 % ||| loss 0.3094449043273926\u001b[0m\n",
            "\u001b[92mTest accuracy: 8825/10000 =  88.25 % ||| loss 0.32884106040000916\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.2799192550778389\n",
            "Batch #200 Loss: 0.2773305593430996\n",
            "Batch #300 Loss: 0.2913478881120682\n",
            "\u001b[92mTrain accuracy: 43096/48000 =  89.78 % ||| loss 0.27660077810287476\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10621/12000 =  88.51 % ||| loss 0.3124948740005493\u001b[0m\n",
            "\u001b[92mTest accuracy: 8800/10000 =  88.0 % ||| loss 0.33646896481513977\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.2771809774637222\n",
            "Batch #200 Loss: 0.2768952488899231\n",
            "Batch #300 Loss: 0.27535308942198755\n",
            "\u001b[92mTrain accuracy: 43285/48000 =  90.18 % ||| loss 0.271004855632782\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10660/12000 =  88.83 % ||| loss 0.31047406792640686\u001b[0m\n",
            "\u001b[92mTest accuracy: 8839/10000 =  88.39 % ||| loss 0.32876458764076233\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.2774678856134415\n",
            "Batch #200 Loss: 0.27460135638713834\n",
            "Batch #300 Loss: 0.27624046206474306\n",
            "\u001b[92mTrain accuracy: 43324/48000 =  90.26 % ||| loss 0.2732468843460083\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10639/12000 =  88.66 % ||| loss 0.3142191171646118\u001b[0m\n",
            "\u001b[92mTest accuracy: 8831/10000 =  88.31 % ||| loss 0.33861181139945984\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.2640812119096518\n",
            "Batch #200 Loss: 0.2707488860189915\n",
            "Batch #300 Loss: 0.28036482125520706\n",
            "\u001b[92mTrain accuracy: 43153/48000 =  89.9 % ||| loss 0.2776555120944977\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10591/12000 =  88.26 % ||| loss 0.3228676915168762\u001b[0m\n",
            "\u001b[92mTest accuracy: 8804/10000 =  88.04 % ||| loss 0.33941060304641724\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726087983.826163_7</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_7</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_170849-Lenet5BN_1726087983.826163_8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_8' target=\"_blank\">Lenet5BN_1726087983.826163_8</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2615723252296447\n",
            "Batch #200 Loss: 2.171421525478363\n",
            "Batch #300 Loss: 2.0518499886989594\n",
            "\u001b[92mTrain accuracy: 22505/48000 =  46.89 % ||| loss 1.8355486392974854\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5580/12000 =  46.5 % ||| loss 1.8366438150405884\u001b[0m\n",
            "\u001b[92mTest accuracy: 4692/10000 =  46.92 % ||| loss 1.8385438919067383\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 1.7264230811595918\n",
            "Batch #200 Loss: 1.5149186956882477\n",
            "Batch #300 Loss: 1.3231341862678527\n",
            "\u001b[92mTrain accuracy: 32676/48000 =  68.08 % ||| loss 1.1481695175170898\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8213/12000 =  68.44 % ||| loss 1.1448118686676025\u001b[0m\n",
            "\u001b[92mTest accuracy: 6805/10000 =  68.05 % ||| loss 1.1514519453048706\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.0863224810361862\n",
            "Batch #200 Loss: 1.0026645022630691\n",
            "Batch #300 Loss: 0.9285617059469223\n",
            "\u001b[92mTrain accuracy: 35150/48000 =  73.23 % ||| loss 0.8488022685050964\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8802/12000 =  73.35 % ||| loss 0.8432938456535339\u001b[0m\n",
            "\u001b[92mTest accuracy: 7296/10000 =  72.96 % ||| loss 0.853866696357727\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.8290653520822525\n",
            "Batch #200 Loss: 0.7867563778162002\n",
            "Batch #300 Loss: 0.7479676610231399\n",
            "\u001b[92mTrain accuracy: 36320/48000 =  75.67 % ||| loss 0.713178813457489\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9102/12000 =  75.85 % ||| loss 0.7077741622924805\u001b[0m\n",
            "\u001b[92mTest accuracy: 7518/10000 =  75.18 % ||| loss 0.7243540287017822\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.6949826294183731\n",
            "Batch #200 Loss: 0.684869281053543\n",
            "Batch #300 Loss: 0.654294510781765\n",
            "\u001b[92mTrain accuracy: 37078/48000 =  77.25 % ||| loss 0.6373488903045654\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9278/12000 =  77.32 % ||| loss 0.6343392133712769\u001b[0m\n",
            "\u001b[92mTest accuracy: 7670/10000 =  76.7 % ||| loss 0.6482041478157043\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.6320140427350998\n",
            "Batch #200 Loss: 0.6240084010362625\n",
            "Batch #300 Loss: 0.5912188160419464\n",
            "\u001b[92mTrain accuracy: 37673/48000 =  78.49 % ||| loss 0.584703266620636\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9434/12000 =  78.62 % ||| loss 0.5826472043991089\u001b[0m\n",
            "\u001b[92mTest accuracy: 7765/10000 =  77.65 % ||| loss 0.5990145802497864\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5914473384618759\n",
            "Batch #200 Loss: 0.569304156601429\n",
            "Batch #300 Loss: 0.5592463165521622\n",
            "\u001b[92mTrain accuracy: 38236/48000 =  79.66 % ||| loss 0.5488418936729431\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9565/12000 =  79.71 % ||| loss 0.5477601289749146\u001b[0m\n",
            "\u001b[92mTest accuracy: 7867/10000 =  78.67 % ||| loss 0.5628262758255005\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.5460974064469337\n",
            "Batch #200 Loss: 0.5504974293708801\n",
            "Batch #300 Loss: 0.5271586000919342\n",
            "\u001b[92mTrain accuracy: 38691/48000 =  80.61 % ||| loss 0.5226006507873535\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9687/12000 =  80.73 % ||| loss 0.5231781005859375\u001b[0m\n",
            "\u001b[92mTest accuracy: 7973/10000 =  79.73 % ||| loss 0.539566159248352\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.519975398182869\n",
            "Batch #200 Loss: 0.5106886515021324\n",
            "Batch #300 Loss: 0.5131566870212555\n",
            "\u001b[92mTrain accuracy: 39120/48000 =  81.5 % ||| loss 0.4978600740432739\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9793/12000 =  81.61 % ||| loss 0.49994513392448425\u001b[0m\n",
            "\u001b[92mTest accuracy: 8056/10000 =  80.56 % ||| loss 0.5154402256011963\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.4986838802695274\n",
            "Batch #200 Loss: 0.4979289403557777\n",
            "Batch #300 Loss: 0.48230290293693545\n",
            "\u001b[92mTrain accuracy: 39528/48000 =  82.35 % ||| loss 0.48037371039390564\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9885/12000 =  82.38 % ||| loss 0.4828311502933502\u001b[0m\n",
            "\u001b[92mTest accuracy: 8132/10000 =  81.32 % ||| loss 0.49867698550224304\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.46927573323249816\n",
            "Batch #200 Loss: 0.4844609367847443\n",
            "Batch #300 Loss: 0.4702151000499725\n",
            "\u001b[92mTrain accuracy: 39846/48000 =  83.01 % ||| loss 0.463688462972641\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9958/12000 =  82.98 % ||| loss 0.4669395685195923\u001b[0m\n",
            "\u001b[92mTest accuracy: 8198/10000 =  81.98 % ||| loss 0.4840279519557953\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.47192991495132447\n",
            "Batch #200 Loss: 0.45624432027339934\n",
            "Batch #300 Loss: 0.4507132661342621\n",
            "\u001b[92mTrain accuracy: 40132/48000 =  83.61 % ||| loss 0.4489300847053528\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10025/12000 =  83.54 % ||| loss 0.4537559151649475\u001b[0m\n",
            "\u001b[92mTest accuracy: 8240/10000 =  82.4 % ||| loss 0.46899980306625366\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.4487896755337715\n",
            "Batch #200 Loss: 0.4498898786306381\n",
            "Batch #300 Loss: 0.44039475202560424\n",
            "\u001b[92mTrain accuracy: 40473/48000 =  84.32 % ||| loss 0.43557119369506836\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10107/12000 =  84.23 % ||| loss 0.44017159938812256\u001b[0m\n",
            "\u001b[92mTest accuracy: 8312/10000 =  83.12 % ||| loss 0.46255818009376526\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.4343141993880272\n",
            "Batch #200 Loss: 0.43233766704797744\n",
            "Batch #300 Loss: 0.4322316387295723\n",
            "\u001b[92mTrain accuracy: 40637/48000 =  84.66 % ||| loss 0.4214642345905304\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10136/12000 =  84.47 % ||| loss 0.42693060636520386\u001b[0m\n",
            "\u001b[92mTest accuracy: 8355/10000 =  83.55 % ||| loss 0.44584858417510986\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.4168229275941849\n",
            "Batch #200 Loss: 0.42699738562107087\n",
            "Batch #300 Loss: 0.4187408256530762\n",
            "\u001b[92mTrain accuracy: 40855/48000 =  85.11 % ||| loss 0.4124755561351776\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10187/12000 =  84.89 % ||| loss 0.4186517894268036\u001b[0m\n",
            "\u001b[92mTest accuracy: 8387/10000 =  83.87 % ||| loss 0.43312686681747437\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.4059537193179131\n",
            "Batch #200 Loss: 0.41107356995344163\n",
            "Batch #300 Loss: 0.41046553194522856\n",
            "\u001b[92mTrain accuracy: 41011/48000 =  85.44 % ||| loss 0.4033712148666382\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10210/12000 =  85.08 % ||| loss 0.4117233455181122\u001b[0m\n",
            "\u001b[92mTest accuracy: 8430/10000 =  84.3 % ||| loss 0.43389207124710083\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.4001180148124695\n",
            "Batch #200 Loss: 0.40690884351730344\n",
            "Batch #300 Loss: 0.4016514414548874\n",
            "\u001b[92mTrain accuracy: 41157/48000 =  85.74 % ||| loss 0.394477903842926\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10238/12000 =  85.32 % ||| loss 0.40280646085739136\u001b[0m\n",
            "\u001b[92mTest accuracy: 8466/10000 =  84.66 % ||| loss 0.42095521092414856\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3982269024848938\n",
            "Batch #200 Loss: 0.39878976792097093\n",
            "Batch #300 Loss: 0.3876731187105179\n",
            "\u001b[92mTrain accuracy: 41300/48000 =  86.04 % ||| loss 0.38801082968711853\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10302/12000 =  85.85 % ||| loss 0.3968409299850464\u001b[0m\n",
            "\u001b[92mTest accuracy: 8442/10000 =  84.42 % ||| loss 0.4136054217815399\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.39160130769014356\n",
            "Batch #200 Loss: 0.39480974674224856\n",
            "Batch #300 Loss: 0.38348174184560774\n",
            "\u001b[92mTrain accuracy: 41426/48000 =  86.3 % ||| loss 0.38069140911102295\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10313/12000 =  85.94 % ||| loss 0.39031723141670227\u001b[0m\n",
            "\u001b[92mTest accuracy: 8502/10000 =  85.02 % ||| loss 0.4089886546134949\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.3881101557612419\n",
            "Batch #200 Loss: 0.3858208152651787\n",
            "Batch #300 Loss: 0.3733427566289902\n",
            "\u001b[92mTrain accuracy: 41543/48000 =  86.55 % ||| loss 0.3749660551548004\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10335/12000 =  86.12 % ||| loss 0.385256290435791\u001b[0m\n",
            "\u001b[92mTest accuracy: 8514/10000 =  85.14 % ||| loss 0.4089810848236084\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.3676303490996361\n",
            "Batch #200 Loss: 0.38250752836465834\n",
            "Batch #300 Loss: 0.3765919479727745\n",
            "\u001b[92mTrain accuracy: 41497/48000 =  86.45 % ||| loss 0.3744610548019409\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10292/12000 =  85.77 % ||| loss 0.38690054416656494\u001b[0m\n",
            "\u001b[92mTest accuracy: 8520/10000 =  85.2 % ||| loss 0.401552677154541\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.37996061086654664\n",
            "Batch #200 Loss: 0.36699342235922816\n",
            "Batch #300 Loss: 0.36924723073840143\n",
            "\u001b[92mTrain accuracy: 41670/48000 =  86.81 % ||| loss 0.36603203415870667\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10354/12000 =  86.28 % ||| loss 0.3782351613044739\u001b[0m\n",
            "\u001b[92mTest accuracy: 8541/10000 =  85.41 % ||| loss 0.3951924443244934\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.37161208361387255\n",
            "Batch #200 Loss: 0.3628127984702587\n",
            "Batch #300 Loss: 0.3538533586263657\n",
            "\u001b[92mTrain accuracy: 41789/48000 =  87.06 % ||| loss 0.3619396984577179\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10359/12000 =  86.33 % ||| loss 0.37628811597824097\u001b[0m\n",
            "\u001b[92mTest accuracy: 8555/10000 =  85.55 % ||| loss 0.3935788869857788\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.3524073818325996\n",
            "Batch #200 Loss: 0.37032647252082823\n",
            "Batch #300 Loss: 0.3613084498047829\n",
            "\u001b[92mTrain accuracy: 41695/48000 =  86.86 % ||| loss 0.36432984471321106\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10368/12000 =  86.4 % ||| loss 0.3784431219100952\u001b[0m\n",
            "\u001b[92mTest accuracy: 8523/10000 =  85.23 % ||| loss 0.4019014835357666\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3582064898312092\n",
            "Batch #200 Loss: 0.35274784326553343\n",
            "Batch #300 Loss: 0.3577365089952946\n",
            "\u001b[92mTrain accuracy: 41956/48000 =  87.41 % ||| loss 0.35066214203834534\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10388/12000 =  86.57 % ||| loss 0.3668116331100464\u001b[0m\n",
            "\u001b[92mTest accuracy: 8602/10000 =  86.02 % ||| loss 0.3900820314884186\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726087983.826163_8</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726087983.826163_8</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!!!!! Hyper Param Tuning Finished!!!!!!!!!!!\n",
            "Best Model: Lenet5BN(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            "  (BN1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (BN2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "\n",
            "HyperParams: {'learning_rate': 0.001, 'momentum': 0.7}\n",
            "\n",
            "Accuracies: {'train': 0.9527083333333334, 'val': 0.90675, 'test': 0.9002}\n"
          ]
        }
      ],
      "source": [
        "class Lenet5BN(Lenet5):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.BN1 = nn.BatchNorm2d(6)\n",
        "        self.BN2 = nn.BatchNorm2d(16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.BN1(x))\n",
        "        x = self.max_pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(self.BN2(x))\n",
        "        x = self.max_pool2(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "  'learning_rate':[0.1, 0.01,0.001],\n",
        "  'momentum':[0, 0.9, 0.7]\n",
        "}\n",
        "\n",
        "best_batchnorm = hyperparameter_tuning(Lenet5BN, dataloaders, device, 25, **param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krfDLKqLbbyu"
      },
      "source": [
        "#### Using Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vN9DTB7ILfqk"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_172118-Lenet5Dropout_1726089678.067411_0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_0' target=\"_blank\">Lenet5Dropout_1726089678.067411_0</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2967526865005494\n",
            "Batch #200 Loss: 1.6814701330661774\n",
            "Batch #300 Loss: 1.0656963312625884\n",
            "\u001b[92mTrain accuracy: 33596/48000 =  69.99 % ||| loss 0.7606455087661743\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8359/12000 =  69.66 % ||| loss 0.7571238875389099\u001b[0m\n",
            "\u001b[92mTest accuracy: 6902/10000 =  69.02 % ||| loss 0.7858148217201233\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7927420252561569\n",
            "Batch #200 Loss: 0.7378638511896134\n",
            "Batch #300 Loss: 0.6641693717241287\n",
            "\u001b[92mTrain accuracy: 37419/48000 =  77.96 % ||| loss 0.5708009600639343\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9354/12000 =  77.95 % ||| loss 0.5666521191596985\u001b[0m\n",
            "\u001b[92mTest accuracy: 7704/10000 =  77.04 % ||| loss 0.5867547392845154\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6134250581264495\n",
            "Batch #200 Loss: 0.6039151301980019\n",
            "Batch #300 Loss: 0.5735316291451454\n",
            "\u001b[92mTrain accuracy: 39209/48000 =  81.69 % ||| loss 0.48887303471565247\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9837/12000 =  81.97 % ||| loss 0.48731011152267456\u001b[0m\n",
            "\u001b[92mTest accuracy: 8078/10000 =  80.78 % ||| loss 0.5143238306045532\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5335833445191384\n",
            "Batch #200 Loss: 0.5235680794715881\n",
            "Batch #300 Loss: 0.5088811752200126\n",
            "\u001b[92mTrain accuracy: 40076/48000 =  83.49 % ||| loss 0.44507884979248047\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9978/12000 =  83.15 % ||| loss 0.45329439640045166\u001b[0m\n",
            "\u001b[92mTest accuracy: 8276/10000 =  82.76 % ||| loss 0.473736971616745\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.47914580851793287\n",
            "Batch #200 Loss: 0.46595590710639956\n",
            "Batch #300 Loss: 0.467822989821434\n",
            "\u001b[92mTrain accuracy: 40882/48000 =  85.17 % ||| loss 0.39809542894363403\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10172/12000 =  84.77 % ||| loss 0.4081297516822815\u001b[0m\n",
            "\u001b[92mTest accuracy: 8404/10000 =  84.04 % ||| loss 0.42695486545562744\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.4489441564679146\n",
            "Batch #200 Loss: 0.43387916922569275\n",
            "Batch #300 Loss: 0.4348165786266327\n",
            "\u001b[92mTrain accuracy: 41153/48000 =  85.74 % ||| loss 0.3873641788959503\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10234/12000 =  85.28 % ||| loss 0.3957892060279846\u001b[0m\n",
            "\u001b[92mTest accuracy: 8444/10000 =  84.44 % ||| loss 0.4166313707828522\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.4170263832807541\n",
            "Batch #200 Loss: 0.41015575736761095\n",
            "Batch #300 Loss: 0.41568650275468827\n",
            "\u001b[92mTrain accuracy: 41367/48000 =  86.18 % ||| loss 0.3679792582988739\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10315/12000 =  85.96 % ||| loss 0.3849755525588989\u001b[0m\n",
            "\u001b[92mTest accuracy: 8509/10000 =  85.09 % ||| loss 0.40029609203338623\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.39326052471995354\n",
            "Batch #200 Loss: 0.39949849620461464\n",
            "Batch #300 Loss: 0.3874633364379406\n",
            "\u001b[92mTrain accuracy: 40773/48000 =  84.94 % ||| loss 0.38757890462875366\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10136/12000 =  84.47 % ||| loss 0.4004271924495697\u001b[0m\n",
            "\u001b[92mTest accuracy: 8407/10000 =  84.07 % ||| loss 0.41994819045066833\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.38494590163230896\n",
            "Batch #200 Loss: 0.38382271498441695\n",
            "Batch #300 Loss: 0.3855718791484833\n",
            "\u001b[92mTrain accuracy: 41633/48000 =  86.74 % ||| loss 0.35999563336372375\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10340/12000 =  86.17 % ||| loss 0.3783339262008667\u001b[0m\n",
            "\u001b[92mTest accuracy: 8509/10000 =  85.09 % ||| loss 0.4021247327327728\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.36819884583353996\n",
            "Batch #200 Loss: 0.3724266928434372\n",
            "Batch #300 Loss: 0.36900627225637433\n",
            "\u001b[92mTrain accuracy: 42174/48000 =  87.86 % ||| loss 0.3233375549316406\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10442/12000 =  87.02 % ||| loss 0.3456423282623291\u001b[0m\n",
            "\u001b[92mTest accuracy: 8640/10000 =  86.4 % ||| loss 0.36854660511016846\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.35466561242938044\n",
            "Batch #200 Loss: 0.35758278086781503\n",
            "Batch #300 Loss: 0.35393598169088364\n",
            "\u001b[92mTrain accuracy: 42160/48000 =  87.83 % ||| loss 0.3219883143901825\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10461/12000 =  87.17 % ||| loss 0.3484964966773987\u001b[0m\n",
            "\u001b[92mTest accuracy: 8659/10000 =  86.59 % ||| loss 0.37511003017425537\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.35342197626829147\n",
            "Batch #200 Loss: 0.34462788969278335\n",
            "Batch #300 Loss: 0.3480476213991642\n",
            "\u001b[92mTrain accuracy: 42586/48000 =  88.72 % ||| loss 0.303975373506546\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10512/12000 =  87.6 % ||| loss 0.32869404554367065\u001b[0m\n",
            "\u001b[92mTest accuracy: 8706/10000 =  87.06 % ||| loss 0.35266271233558655\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.3381053428351879\n",
            "Batch #200 Loss: 0.33741089686751363\n",
            "Batch #300 Loss: 0.34943340316414834\n",
            "\u001b[92mTrain accuracy: 42589/48000 =  88.73 % ||| loss 0.29904812574386597\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10520/12000 =  87.67 % ||| loss 0.3296666145324707\u001b[0m\n",
            "\u001b[92mTest accuracy: 8678/10000 =  86.78 % ||| loss 0.3525402843952179\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3314794187247753\n",
            "Batch #200 Loss: 0.3209012591838837\n",
            "Batch #300 Loss: 0.3465013761818409\n",
            "\u001b[92mTrain accuracy: 42853/48000 =  89.28 % ||| loss 0.2916850745677948\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10555/12000 =  87.96 % ||| loss 0.31921669840812683\u001b[0m\n",
            "\u001b[92mTest accuracy: 8711/10000 =  87.11 % ||| loss 0.3452223241329193\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3266973997652531\n",
            "Batch #200 Loss: 0.32410155549645425\n",
            "Batch #300 Loss: 0.3317385056614876\n",
            "\u001b[92mTrain accuracy: 42964/48000 =  89.51 % ||| loss 0.28260403871536255\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10603/12000 =  88.36 % ||| loss 0.31651872396469116\u001b[0m\n",
            "\u001b[92mTest accuracy: 8749/10000 =  87.49 % ||| loss 0.3370246887207031\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.32286144226789476\n",
            "Batch #200 Loss: 0.32296702921390535\n",
            "Batch #300 Loss: 0.3147677092254162\n",
            "\u001b[92mTrain accuracy: 43097/48000 =  89.79 % ||| loss 0.2748538553714752\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10597/12000 =  88.31 % ||| loss 0.3098568320274353\u001b[0m\n",
            "\u001b[92mTest accuracy: 8772/10000 =  87.72 % ||| loss 0.33228740096092224\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3163068449497223\n",
            "Batch #200 Loss: 0.317867319136858\n",
            "Batch #300 Loss: 0.31277904495596887\n",
            "\u001b[92mTrain accuracy: 43162/48000 =  89.92 % ||| loss 0.2707439661026001\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10647/12000 =  88.72 % ||| loss 0.3069903552532196\u001b[0m\n",
            "\u001b[92mTest accuracy: 8780/10000 =  87.8 % ||| loss 0.3277263045310974\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3006139622628689\n",
            "Batch #200 Loss: 0.30775405108928683\n",
            "Batch #300 Loss: 0.3022741523385048\n",
            "\u001b[92mTrain accuracy: 43068/48000 =  89.72 % ||| loss 0.2748655378818512\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10532/12000 =  87.77 % ||| loss 0.3206998407840729\u001b[0m\n",
            "\u001b[92mTest accuracy: 8756/10000 =  87.56 % ||| loss 0.3412396013736725\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.3018042919039726\n",
            "Batch #200 Loss: 0.3095207692682743\n",
            "Batch #300 Loss: 0.30573228433728217\n",
            "\u001b[92mTrain accuracy: 43313/48000 =  90.24 % ||| loss 0.2619940936565399\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10677/12000 =  88.98 % ||| loss 0.3019380569458008\u001b[0m\n",
            "\u001b[92mTest accuracy: 8801/10000 =  88.01 % ||| loss 0.3235730826854706\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.29316575318574906\n",
            "Batch #200 Loss: 0.3082902543246746\n",
            "Batch #300 Loss: 0.3033550579845905\n",
            "\u001b[92mTrain accuracy: 43372/48000 =  90.36 % ||| loss 0.25685417652130127\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10637/12000 =  88.64 % ||| loss 0.29840078949928284\u001b[0m\n",
            "\u001b[92mTest accuracy: 8795/10000 =  87.95 % ||| loss 0.32641351222991943\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.29342278361320495\n",
            "Batch #200 Loss: 0.30396720945835115\n",
            "Batch #300 Loss: 0.2986575447022915\n",
            "\u001b[92mTrain accuracy: 43564/48000 =  90.76 % ||| loss 0.24692633748054504\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10682/12000 =  89.02 % ||| loss 0.298001766204834\u001b[0m\n",
            "\u001b[92mTest accuracy: 8843/10000 =  88.43 % ||| loss 0.3180848956108093\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.2879680949449539\n",
            "Batch #200 Loss: 0.2877152554690838\n",
            "Batch #300 Loss: 0.2972369314730167\n",
            "\u001b[92mTrain accuracy: 43588/48000 =  90.81 % ||| loss 0.24872249364852905\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10658/12000 =  88.82 % ||| loss 0.2959241271018982\u001b[0m\n",
            "\u001b[92mTest accuracy: 8811/10000 =  88.11 % ||| loss 0.3140019476413727\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.27439351454377175\n",
            "Batch #200 Loss: 0.2934154629707336\n",
            "Batch #300 Loss: 0.2834104850888252\n",
            "\u001b[92mTrain accuracy: 43449/48000 =  90.52 % ||| loss 0.25199559330940247\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10649/12000 =  88.74 % ||| loss 0.2961152195930481\u001b[0m\n",
            "\u001b[92mTest accuracy: 8795/10000 =  87.95 % ||| loss 0.3231861889362335\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.28513205610215664\n",
            "Batch #200 Loss: 0.27057510510087013\n",
            "Batch #300 Loss: 0.2796443854272366\n",
            "\u001b[92mTrain accuracy: 43795/48000 =  91.24 % ||| loss 0.23522284626960754\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10704/12000 =  89.2 % ||| loss 0.2887512445449829\u001b[0m\n",
            "\u001b[92mTest accuracy: 8879/10000 =  88.79 % ||| loss 0.30725109577178955\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.27552332609891894\n",
            "Batch #200 Loss: 0.28070113316178325\n",
            "Batch #300 Loss: 0.2764395493268967\n",
            "\u001b[92mTrain accuracy: 43706/48000 =  91.05 % ||| loss 0.23786918818950653\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10684/12000 =  89.03 % ||| loss 0.2902185618877411\u001b[0m\n",
            "\u001b[92mTest accuracy: 8867/10000 =  88.67 % ||| loss 0.307505339384079\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_0</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_0</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_172307-Lenet5Dropout_1726089678.067411_1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_1' target=\"_blank\">Lenet5Dropout_1726089678.067411_1</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.1879753601551055\n",
            "Batch #200 Loss: 1.3221731489896775\n",
            "Batch #300 Loss: 1.0395991057157516\n",
            "\u001b[92mTrain accuracy: 34462/48000 =  71.8 % ||| loss 0.7476757168769836\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8599/12000 =  71.66 % ||| loss 0.7422230243682861\u001b[0m\n",
            "\u001b[92mTest accuracy: 7111/10000 =  71.11 % ||| loss 0.7632440328598022\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7998095852136612\n",
            "Batch #200 Loss: 0.7632695233821869\n",
            "Batch #300 Loss: 0.699059984087944\n",
            "\u001b[92mTrain accuracy: 36299/48000 =  75.62 % ||| loss 0.6148649454116821\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9031/12000 =  75.26 % ||| loss 0.6171642541885376\u001b[0m\n",
            "\u001b[92mTest accuracy: 7519/10000 =  75.19 % ||| loss 0.6364995837211609\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6414203995466232\n",
            "Batch #200 Loss: 0.6058100196719169\n",
            "Batch #300 Loss: 0.6035892015695572\n",
            "\u001b[92mTrain accuracy: 39062/48000 =  81.38 % ||| loss 0.5097445249557495\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9757/12000 =  81.31 % ||| loss 0.5119685530662537\u001b[0m\n",
            "\u001b[92mTest accuracy: 8055/10000 =  80.55 % ||| loss 0.5368021130561829\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5747810590267182\n",
            "Batch #200 Loss: 0.5469949623942375\n",
            "Batch #300 Loss: 0.5475975546240807\n",
            "\u001b[92mTrain accuracy: 39931/48000 =  83.19 % ||| loss 0.45982739329338074\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9993/12000 =  83.28 % ||| loss 0.46065810322761536\u001b[0m\n",
            "\u001b[92mTest accuracy: 8226/10000 =  82.26 % ||| loss 0.4841747581958771\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5246356156468391\n",
            "Batch #200 Loss: 0.5106212767958641\n",
            "Batch #300 Loss: 0.5049562576413155\n",
            "\u001b[92mTrain accuracy: 40436/48000 =  84.24 % ||| loss 0.42544206976890564\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10095/12000 =  84.12 % ||| loss 0.42979609966278076\u001b[0m\n",
            "\u001b[92mTest accuracy: 8352/10000 =  83.52 % ||| loss 0.45067307353019714\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.4740674877166748\n",
            "Batch #200 Loss: 0.49532965242862703\n",
            "Batch #300 Loss: 0.47637778013944626\n",
            "\u001b[92mTrain accuracy: 41085/48000 =  85.59 % ||| loss 0.39659661054611206\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10230/12000 =  85.25 % ||| loss 0.40201321244239807\u001b[0m\n",
            "\u001b[92mTest accuracy: 8454/10000 =  84.54 % ||| loss 0.4246179461479187\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.48078282326459887\n",
            "Batch #200 Loss: 0.45966847985982895\n",
            "Batch #300 Loss: 0.4531539267301559\n",
            "\u001b[92mTrain accuracy: 41206/48000 =  85.85 % ||| loss 0.38327527046203613\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10276/12000 =  85.63 % ||| loss 0.3945959806442261\u001b[0m\n",
            "\u001b[92mTest accuracy: 8463/10000 =  84.63 % ||| loss 0.41638773679733276\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.4226564899086952\n",
            "Batch #200 Loss: 0.4446870225667954\n",
            "Batch #300 Loss: 0.43846284568309785\n",
            "\u001b[92mTrain accuracy: 41584/48000 =  86.63 % ||| loss 0.3620765805244446\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10323/12000 =  86.02 % ||| loss 0.3731743395328522\u001b[0m\n",
            "\u001b[92mTest accuracy: 8571/10000 =  85.71 % ||| loss 0.3926299512386322\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.41402562975883483\n",
            "Batch #200 Loss: 0.42863198041915895\n",
            "Batch #300 Loss: 0.4182220259308815\n",
            "\u001b[92mTrain accuracy: 41954/48000 =  87.4 % ||| loss 0.3469551205635071\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10411/12000 =  86.76 % ||| loss 0.3572623133659363\u001b[0m\n",
            "\u001b[92mTest accuracy: 8615/10000 =  86.15 % ||| loss 0.381306916475296\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.401772201359272\n",
            "Batch #200 Loss: 0.41045565500855447\n",
            "Batch #300 Loss: 0.4010292762517929\n",
            "\u001b[92mTrain accuracy: 42054/48000 =  87.61 % ||| loss 0.33533725142478943\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10458/12000 =  87.15 % ||| loss 0.3498765528202057\u001b[0m\n",
            "\u001b[92mTest accuracy: 8646/10000 =  86.46 % ||| loss 0.3713754415512085\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.4002381774783135\n",
            "Batch #200 Loss: 0.3867921929061413\n",
            "Batch #300 Loss: 0.39400038167834284\n",
            "\u001b[92mTrain accuracy: 42288/48000 =  88.1 % ||| loss 0.3246572017669678\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10504/12000 =  87.53 % ||| loss 0.3373412489891052\u001b[0m\n",
            "\u001b[92mTest accuracy: 8691/10000 =  86.91 % ||| loss 0.36110883951187134\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3699201235175133\n",
            "Batch #200 Loss: 0.37099012479186055\n",
            "Batch #300 Loss: 0.38548971965909007\n",
            "\u001b[92mTrain accuracy: 42422/48000 =  88.38 % ||| loss 0.31452858448028564\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10496/12000 =  87.47 % ||| loss 0.33162641525268555\u001b[0m\n",
            "\u001b[92mTest accuracy: 8698/10000 =  86.98 % ||| loss 0.35209324955940247\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.37102531641721725\n",
            "Batch #200 Loss: 0.36986365750432015\n",
            "Batch #300 Loss: 0.37445272341370583\n",
            "\u001b[92mTrain accuracy: 42388/48000 =  88.31 % ||| loss 0.3144834041595459\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10530/12000 =  87.75 % ||| loss 0.3314937353134155\u001b[0m\n",
            "\u001b[92mTest accuracy: 8676/10000 =  86.76 % ||| loss 0.3566499650478363\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3514809215068817\n",
            "Batch #200 Loss: 0.3630604915320873\n",
            "Batch #300 Loss: 0.37306600689888003\n",
            "\u001b[92mTrain accuracy: 42530/48000 =  88.6 % ||| loss 0.3068723678588867\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10514/12000 =  87.62 % ||| loss 0.3267544209957123\u001b[0m\n",
            "\u001b[92mTest accuracy: 8700/10000 =  87.0 % ||| loss 0.35250505805015564\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3458536346256733\n",
            "Batch #200 Loss: 0.3570528221130371\n",
            "Batch #300 Loss: 0.35350424870848657\n",
            "\u001b[92mTrain accuracy: 42626/48000 =  88.8 % ||| loss 0.30049827694892883\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10546/12000 =  87.88 % ||| loss 0.3235878348350525\u001b[0m\n",
            "\u001b[92mTest accuracy: 8720/10000 =  87.2 % ||| loss 0.3465503752231598\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.35141010969877245\n",
            "Batch #200 Loss: 0.34557175561785697\n",
            "Batch #300 Loss: 0.347234642803669\n",
            "\u001b[92mTrain accuracy: 42709/48000 =  88.98 % ||| loss 0.2953380048274994\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10566/12000 =  88.05 % ||| loss 0.3200324773788452\u001b[0m\n",
            "\u001b[92mTest accuracy: 8743/10000 =  87.43 % ||| loss 0.33865535259246826\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3330174623429775\n",
            "Batch #200 Loss: 0.3469908367097378\n",
            "Batch #300 Loss: 0.33693439200520514\n",
            "\u001b[92mTrain accuracy: 43115/48000 =  89.82 % ||| loss 0.2760322690010071\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10663/12000 =  88.86 % ||| loss 0.30345821380615234\u001b[0m\n",
            "\u001b[92mTest accuracy: 8818/10000 =  88.18 % ||| loss 0.32261335849761963\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.33341762110590933\n",
            "Batch #200 Loss: 0.32891843423247336\n",
            "Batch #300 Loss: 0.3280575551092625\n",
            "\u001b[92mTrain accuracy: 42996/48000 =  89.58 % ||| loss 0.2746502161026001\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10600/12000 =  88.33 % ||| loss 0.30582454800605774\u001b[0m\n",
            "\u001b[92mTest accuracy: 8806/10000 =  88.06 % ||| loss 0.33110368251800537\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.3255250886082649\n",
            "Batch #200 Loss: 0.32737788930535316\n",
            "Batch #300 Loss: 0.325658026188612\n",
            "\u001b[92mTrain accuracy: 43154/48000 =  89.9 % ||| loss 0.2711971402168274\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10651/12000 =  88.76 % ||| loss 0.3032369613647461\u001b[0m\n",
            "\u001b[92mTest accuracy: 8829/10000 =  88.29 % ||| loss 0.32333821058273315\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.3179664115607739\n",
            "Batch #200 Loss: 0.32084205642342567\n",
            "Batch #300 Loss: 0.3256920471787453\n",
            "\u001b[92mTrain accuracy: 42998/48000 =  89.58 % ||| loss 0.2742951512336731\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10587/12000 =  88.22 % ||| loss 0.31187668442726135\u001b[0m\n",
            "\u001b[92mTest accuracy: 8802/10000 =  88.02 % ||| loss 0.33326244354248047\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.316898729801178\n",
            "Batch #200 Loss: 0.31451822102069854\n",
            "Batch #300 Loss: 0.3123770046234131\n",
            "\u001b[92mTrain accuracy: 43364/48000 =  90.34 % ||| loss 0.25799307227134705\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10651/12000 =  88.76 % ||| loss 0.29343119263648987\u001b[0m\n",
            "\u001b[92mTest accuracy: 8838/10000 =  88.38 % ||| loss 0.31820976734161377\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.3225226719677448\n",
            "Batch #200 Loss: 0.30979172825813295\n",
            "Batch #300 Loss: 0.3116723623871803\n",
            "\u001b[92mTrain accuracy: 43385/48000 =  90.39 % ||| loss 0.2535807192325592\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10689/12000 =  89.08 % ||| loss 0.2926504909992218\u001b[0m\n",
            "\u001b[92mTest accuracy: 8863/10000 =  88.63 % ||| loss 0.3171483874320984\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.30540196985006335\n",
            "Batch #200 Loss: 0.31396825075149537\n",
            "Batch #300 Loss: 0.30960900351405146\n",
            "\u001b[92mTrain accuracy: 43428/48000 =  90.48 % ||| loss 0.2554420530796051\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10697/12000 =  89.14 % ||| loss 0.2920767068862915\u001b[0m\n",
            "\u001b[92mTest accuracy: 8852/10000 =  88.52 % ||| loss 0.31015658378601074\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.29706919968128204\n",
            "Batch #200 Loss: 0.30875722542405126\n",
            "Batch #300 Loss: 0.3181094533205032\n",
            "\u001b[92mTrain accuracy: 43575/48000 =  90.78 % ||| loss 0.2470887303352356\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10681/12000 =  89.01 % ||| loss 0.2956956624984741\u001b[0m\n",
            "\u001b[92mTest accuracy: 8884/10000 =  88.84 % ||| loss 0.31421661376953125\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3011344286799431\n",
            "Batch #200 Loss: 0.30739838868379593\n",
            "Batch #300 Loss: 0.30107697010040285\n",
            "\u001b[92mTrain accuracy: 43456/48000 =  90.53 % ||| loss 0.2486175149679184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10673/12000 =  88.94 % ||| loss 0.2951282262802124\u001b[0m\n",
            "\u001b[92mTest accuracy: 8833/10000 =  88.33 % ||| loss 0.3116267919540405\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_1</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_1</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_172456-Lenet5Dropout_1726089678.067411_2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_2' target=\"_blank\">Lenet5Dropout_1726089678.067411_2</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2540293633937836\n",
            "Batch #200 Loss: 1.5027018976211548\n",
            "Batch #300 Loss: 1.1650890809297563\n",
            "\u001b[92mTrain accuracy: 32477/48000 =  67.66 % ||| loss 0.8160099983215332\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8144/12000 =  67.87 % ||| loss 0.8034597635269165\u001b[0m\n",
            "\u001b[92mTest accuracy: 6712/10000 =  67.12 % ||| loss 0.8245924115180969\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.8885250771045685\n",
            "Batch #200 Loss: 0.831328312754631\n",
            "Batch #300 Loss: 0.7726940184831619\n",
            "\u001b[92mTrain accuracy: 36072/48000 =  75.15 % ||| loss 0.6241834759712219\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9064/12000 =  75.53 % ||| loss 0.6146882772445679\u001b[0m\n",
            "\u001b[92mTest accuracy: 7436/10000 =  74.36 % ||| loss 0.640768826007843\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.700551832318306\n",
            "Batch #200 Loss: 0.6824092841148377\n",
            "Batch #300 Loss: 0.6709414252638817\n",
            "\u001b[92mTrain accuracy: 37492/48000 =  78.11 % ||| loss 0.5600219964981079\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9385/12000 =  78.21 % ||| loss 0.5492469072341919\u001b[0m\n",
            "\u001b[92mTest accuracy: 7751/10000 =  77.51 % ||| loss 0.5788860321044922\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.6262951728701591\n",
            "Batch #200 Loss: 0.627352941930294\n",
            "Batch #300 Loss: 0.6012525206804276\n",
            "\u001b[92mTrain accuracy: 37695/48000 =  78.53 % ||| loss 0.5200657844543457\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9393/12000 =  78.27 % ||| loss 0.5217729806900024\u001b[0m\n",
            "\u001b[92mTest accuracy: 7775/10000 =  77.75 % ||| loss 0.5457051992416382\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5736563500761985\n",
            "Batch #200 Loss: 0.5778693568706512\n",
            "Batch #300 Loss: 0.5702280110120773\n",
            "\u001b[92mTrain accuracy: 38428/48000 =  80.06 % ||| loss 0.48906636238098145\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9584/12000 =  79.87 % ||| loss 0.4924951195716858\u001b[0m\n",
            "\u001b[92mTest accuracy: 7959/10000 =  79.59 % ||| loss 0.5105839967727661\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5598354050517083\n",
            "Batch #200 Loss: 0.5625867372751236\n",
            "Batch #300 Loss: 0.5464120012521744\n",
            "\u001b[92mTrain accuracy: 39636/48000 =  82.58 % ||| loss 0.4523419141769409\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9931/12000 =  82.76 % ||| loss 0.45499932765960693\u001b[0m\n",
            "\u001b[92mTest accuracy: 8197/10000 =  81.97 % ||| loss 0.47821176052093506\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5318465510010719\n",
            "Batch #200 Loss: 0.5231091704964638\n",
            "Batch #300 Loss: 0.5311895316839218\n",
            "\u001b[92mTrain accuracy: 40423/48000 =  84.21 % ||| loss 0.42988643050193787\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10146/12000 =  84.55 % ||| loss 0.43308019638061523\u001b[0m\n",
            "\u001b[92mTest accuracy: 8318/10000 =  83.18 % ||| loss 0.4542175829410553\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.5136475613713265\n",
            "Batch #200 Loss: 0.498784804046154\n",
            "Batch #300 Loss: 0.5023322832584382\n",
            "\u001b[92mTrain accuracy: 40464/48000 =  84.3 % ||| loss 0.42193838953971863\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10141/12000 =  84.51 % ||| loss 0.42487287521362305\u001b[0m\n",
            "\u001b[92mTest accuracy: 8323/10000 =  83.23 % ||| loss 0.44785645604133606\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.4988225740194321\n",
            "Batch #200 Loss: 0.48521302282810214\n",
            "Batch #300 Loss: 0.4911029615998268\n",
            "\u001b[92mTrain accuracy: 40902/48000 =  85.21 % ||| loss 0.3930489420890808\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10225/12000 =  85.21 % ||| loss 0.4017292857170105\u001b[0m\n",
            "\u001b[92mTest accuracy: 8443/10000 =  84.43 % ||| loss 0.4206205904483795\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.4793536043167114\n",
            "Batch #200 Loss: 0.45788280576467516\n",
            "Batch #300 Loss: 0.4742016741633415\n",
            "\u001b[92mTrain accuracy: 40974/48000 =  85.36 % ||| loss 0.3834669589996338\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10232/12000 =  85.27 % ||| loss 0.3949582576751709\u001b[0m\n",
            "\u001b[92mTest accuracy: 8460/10000 =  84.6 % ||| loss 0.41303789615631104\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.4618681114912033\n",
            "Batch #200 Loss: 0.4567157980799675\n",
            "Batch #300 Loss: 0.45340404748916624\n",
            "\u001b[92mTrain accuracy: 41432/48000 =  86.32 % ||| loss 0.36509668827056885\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10358/12000 =  86.32 % ||| loss 0.37490618228912354\u001b[0m\n",
            "\u001b[92mTest accuracy: 8540/10000 =  85.4 % ||| loss 0.39672037959098816\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.4410155788064003\n",
            "Batch #200 Loss: 0.4496346291899681\n",
            "Batch #300 Loss: 0.44801566064357756\n",
            "\u001b[92mTrain accuracy: 41530/48000 =  86.52 % ||| loss 0.35184726119041443\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10379/12000 =  86.49 % ||| loss 0.3664945363998413\u001b[0m\n",
            "\u001b[92mTest accuracy: 8553/10000 =  85.53 % ||| loss 0.3864840269088745\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.43664385795593263\n",
            "Batch #200 Loss: 0.43044572353363036\n",
            "Batch #300 Loss: 0.43792187750339506\n",
            "\u001b[92mTrain accuracy: 41653/48000 =  86.78 % ||| loss 0.34327784180641174\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10353/12000 =  86.28 % ||| loss 0.3568642735481262\u001b[0m\n",
            "\u001b[92mTest accuracy: 8564/10000 =  85.64 % ||| loss 0.3815872073173523\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.4328606900572777\n",
            "Batch #200 Loss: 0.4219163727760315\n",
            "Batch #300 Loss: 0.4177772358059883\n",
            "\u001b[92mTrain accuracy: 41824/48000 =  87.13 % ||| loss 0.34330683946609497\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10415/12000 =  86.79 % ||| loss 0.3592053949832916\u001b[0m\n",
            "\u001b[92mTest accuracy: 8626/10000 =  86.26 % ||| loss 0.3802417814731598\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.4071378096938133\n",
            "Batch #200 Loss: 0.4149217754602432\n",
            "Batch #300 Loss: 0.41010348334908486\n",
            "\u001b[92mTrain accuracy: 41848/48000 =  87.18 % ||| loss 0.32993370294570923\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10442/12000 =  87.02 % ||| loss 0.3471783995628357\u001b[0m\n",
            "\u001b[92mTest accuracy: 8578/10000 =  85.78 % ||| loss 0.37724921107292175\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.4099964982271194\n",
            "Batch #200 Loss: 0.3954916480183601\n",
            "Batch #300 Loss: 0.407122403383255\n",
            "\u001b[92mTrain accuracy: 42178/48000 =  87.87 % ||| loss 0.3185407519340515\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10485/12000 =  87.38 % ||| loss 0.3397534489631653\u001b[0m\n",
            "\u001b[92mTest accuracy: 8635/10000 =  86.35 % ||| loss 0.36316704750061035\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.39502507507801055\n",
            "Batch #200 Loss: 0.39386908009648325\n",
            "Batch #300 Loss: 0.4128672960400581\n",
            "\u001b[92mTrain accuracy: 42493/48000 =  88.53 % ||| loss 0.30904486775398254\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10515/12000 =  87.62 % ||| loss 0.3327155113220215\u001b[0m\n",
            "\u001b[92mTest accuracy: 8676/10000 =  86.76 % ||| loss 0.3531893491744995\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.38389738246798516\n",
            "Batch #200 Loss: 0.4044725731015205\n",
            "Batch #300 Loss: 0.393710859566927\n",
            "\u001b[92mTrain accuracy: 42154/48000 =  87.82 % ||| loss 0.3219318985939026\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10428/12000 =  86.9 % ||| loss 0.34467896819114685\u001b[0m\n",
            "\u001b[92mTest accuracy: 8597/10000 =  85.97 % ||| loss 0.37047702074050903\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.39296468064188955\n",
            "Batch #200 Loss: 0.38577241465449336\n",
            "Batch #300 Loss: 0.376487744897604\n",
            "\u001b[92mTrain accuracy: 42495/48000 =  88.53 % ||| loss 0.3009934425354004\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10550/12000 =  87.92 % ||| loss 0.3239656686782837\u001b[0m\n",
            "\u001b[92mTest accuracy: 8736/10000 =  87.36 % ||| loss 0.3488743305206299\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.3814611706137657\n",
            "Batch #200 Loss: 0.37652492597699166\n",
            "Batch #300 Loss: 0.3751641836762428\n",
            "\u001b[92mTrain accuracy: 42567/48000 =  88.68 % ||| loss 0.2999457120895386\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10544/12000 =  87.87 % ||| loss 0.32594233751296997\u001b[0m\n",
            "\u001b[92mTest accuracy: 8709/10000 =  87.09 % ||| loss 0.34454694390296936\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.3659990593791008\n",
            "Batch #200 Loss: 0.3864394375681877\n",
            "Batch #300 Loss: 0.3672189195454121\n",
            "\u001b[92mTrain accuracy: 42733/48000 =  89.03 % ||| loss 0.29230278730392456\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10559/12000 =  87.99 % ||| loss 0.32036757469177246\u001b[0m\n",
            "\u001b[92mTest accuracy: 8733/10000 =  87.33 % ||| loss 0.34198248386383057\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.37532894030213354\n",
            "Batch #200 Loss: 0.3614374692738056\n",
            "Batch #300 Loss: 0.3735552754998207\n",
            "\u001b[92mTrain accuracy: 42648/48000 =  88.85 % ||| loss 0.2938600182533264\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10579/12000 =  88.16 % ||| loss 0.32309579849243164\u001b[0m\n",
            "\u001b[92mTest accuracy: 8742/10000 =  87.42 % ||| loss 0.35081884264945984\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.3651124006509781\n",
            "Batch #200 Loss: 0.364002361446619\n",
            "Batch #300 Loss: 0.3671342276036739\n",
            "\u001b[92mTrain accuracy: 42768/48000 =  89.1 % ||| loss 0.288963258266449\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10560/12000 =  88.0 % ||| loss 0.32086437940597534\u001b[0m\n",
            "\u001b[92mTest accuracy: 8738/10000 =  87.38 % ||| loss 0.3423382341861725\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.36122458666563034\n",
            "Batch #200 Loss: 0.3635419189929962\n",
            "Batch #300 Loss: 0.3669872465729713\n",
            "\u001b[92mTrain accuracy: 42833/48000 =  89.24 % ||| loss 0.28215575218200684\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10602/12000 =  88.35 % ||| loss 0.3144280016422272\u001b[0m\n",
            "\u001b[92mTest accuracy: 8760/10000 =  87.6 % ||| loss 0.3351092040538788\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3524618202447891\n",
            "Batch #200 Loss: 0.3598544061183929\n",
            "Batch #300 Loss: 0.3499977168440819\n",
            "\u001b[92mTrain accuracy: 42993/48000 =  89.57 % ||| loss 0.278424471616745\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10604/12000 =  88.37 % ||| loss 0.3131007254123688\u001b[0m\n",
            "\u001b[92mTest accuracy: 8791/10000 =  87.91 % ||| loss 0.33805549144744873\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_2</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_2</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_172735-Lenet5Dropout_1726089678.067411_3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_3' target=\"_blank\">Lenet5Dropout_1726089678.067411_3</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.4171263074874878\n",
            "Batch #200 Loss: 0.6999848559498787\n",
            "Batch #300 Loss: 0.6068363869190216\n",
            "\u001b[92mTrain accuracy: 38884/48000 =  81.01 % ||| loss 0.5040724873542786\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9750/12000 =  81.25 % ||| loss 0.49646463990211487\u001b[0m\n",
            "\u001b[92mTest accuracy: 8039/10000 =  80.39 % ||| loss 0.5267516374588013\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.5058696463704109\n",
            "Batch #200 Loss: 0.47221211552619935\n",
            "Batch #300 Loss: 0.48788372963666915\n",
            "\u001b[92mTrain accuracy: 40786/48000 =  84.97 % ||| loss 0.39612239599227905\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10204/12000 =  85.03 % ||| loss 0.3982788920402527\u001b[0m\n",
            "\u001b[92mTest accuracy: 8441/10000 =  84.41 % ||| loss 0.42216649651527405\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.46104108184576037\n",
            "Batch #200 Loss: 0.4313023230433464\n",
            "Batch #300 Loss: 0.42925194054842\n",
            "\u001b[92mTrain accuracy: 41085/48000 =  85.59 % ||| loss 0.3885977268218994\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10211/12000 =  85.09 % ||| loss 0.4070039987564087\u001b[0m\n",
            "\u001b[92mTest accuracy: 8431/10000 =  84.31 % ||| loss 0.4289664626121521\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.4106995436549187\n",
            "Batch #200 Loss: 0.42326695919036866\n",
            "Batch #300 Loss: 0.40233190819621084\n",
            "\u001b[92mTrain accuracy: 41556/48000 =  86.58 % ||| loss 0.35386425256729126\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10320/12000 =  86.0 % ||| loss 0.37638506293296814\u001b[0m\n",
            "\u001b[92mTest accuracy: 8561/10000 =  85.61 % ||| loss 0.38803231716156006\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4056082797050476\n",
            "Batch #200 Loss: 0.4007217618823051\n",
            "Batch #300 Loss: 0.3999130707979202\n",
            "\u001b[92mTrain accuracy: 41520/48000 =  86.5 % ||| loss 0.3490094840526581\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10320/12000 =  86.0 % ||| loss 0.3703879415988922\u001b[0m\n",
            "\u001b[92mTest accuracy: 8498/10000 =  84.98 % ||| loss 0.38946208357810974\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.38931832611560824\n",
            "Batch #200 Loss: 0.37645299449563024\n",
            "Batch #300 Loss: 0.3872318649291992\n",
            "\u001b[92mTrain accuracy: 42241/48000 =  88.0 % ||| loss 0.3110993802547455\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10453/12000 =  87.11 % ||| loss 0.33576399087905884\u001b[0m\n",
            "\u001b[92mTest accuracy: 8676/10000 =  86.76 % ||| loss 0.3556554615497589\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3634809271991253\n",
            "Batch #200 Loss: 0.3824455079436302\n",
            "Batch #300 Loss: 0.37798475414514543\n",
            "\u001b[92mTrain accuracy: 41858/48000 =  87.2 % ||| loss 0.3317168056964874\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10355/12000 =  86.29 % ||| loss 0.357328861951828\u001b[0m\n",
            "\u001b[92mTest accuracy: 8573/10000 =  85.73 % ||| loss 0.38499581813812256\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.36654768511652946\n",
            "Batch #200 Loss: 0.3829744279384613\n",
            "Batch #300 Loss: 0.3559387159347534\n",
            "\u001b[92mTrain accuracy: 41951/48000 =  87.4 % ||| loss 0.32342109084129333\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10410/12000 =  86.75 % ||| loss 0.34642985463142395\u001b[0m\n",
            "\u001b[92mTest accuracy: 8578/10000 =  85.78 % ||| loss 0.3773505985736847\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3674193674325943\n",
            "Batch #200 Loss: 0.3410352636873722\n",
            "Batch #300 Loss: 0.35736759841442106\n",
            "\u001b[92mTrain accuracy: 42143/48000 =  87.8 % ||| loss 0.3119109869003296\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10341/12000 =  86.17 % ||| loss 0.3551042377948761\u001b[0m\n",
            "\u001b[92mTest accuracy: 8574/10000 =  85.74 % ||| loss 0.3689652979373932\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3487822908163071\n",
            "Batch #200 Loss: 0.34919747680425645\n",
            "Batch #300 Loss: 0.3593836157023907\n",
            "\u001b[92mTrain accuracy: 42612/48000 =  88.78 % ||| loss 0.29446953535079956\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10457/12000 =  87.14 % ||| loss 0.3366953134536743\u001b[0m\n",
            "\u001b[92mTest accuracy: 8694/10000 =  86.94 % ||| loss 0.35895198583602905\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.33123737305402756\n",
            "Batch #200 Loss: 0.3619502033293247\n",
            "Batch #300 Loss: 0.3437721534073353\n",
            "\u001b[92mTrain accuracy: 42041/48000 =  87.59 % ||| loss 0.3126959204673767\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10335/12000 =  86.12 % ||| loss 0.35844817757606506\u001b[0m\n",
            "\u001b[92mTest accuracy: 8536/10000 =  85.36 % ||| loss 0.37882623076438904\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3600346884131432\n",
            "Batch #200 Loss: 0.3251307237148285\n",
            "Batch #300 Loss: 0.3555993463099003\n",
            "\u001b[92mTrain accuracy: 42043/48000 =  87.59 % ||| loss 0.3203316926956177\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10410/12000 =  86.75 % ||| loss 0.354178249835968\u001b[0m\n",
            "\u001b[92mTest accuracy: 8633/10000 =  86.33 % ||| loss 0.3713045120239258\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.34774798318743705\n",
            "Batch #200 Loss: 0.3342542825639248\n",
            "Batch #300 Loss: 0.3501530659198761\n",
            "\u001b[92mTrain accuracy: 42848/48000 =  89.27 % ||| loss 0.2754482328891754\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10548/12000 =  87.9 % ||| loss 0.32103821635246277\u001b[0m\n",
            "\u001b[92mTest accuracy: 8735/10000 =  87.35 % ||| loss 0.34215348958969116\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3273140488564968\n",
            "Batch #200 Loss: 0.3384375639259815\n",
            "Batch #300 Loss: 0.3323157325387001\n",
            "\u001b[92mTrain accuracy: 42362/48000 =  88.25 % ||| loss 0.3284771144390106\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10425/12000 =  86.88 % ||| loss 0.37691643834114075\u001b[0m\n",
            "\u001b[92mTest accuracy: 8670/10000 =  86.7 % ||| loss 0.3954232335090637\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.31997468441724775\n",
            "Batch #200 Loss: 0.34159830927848817\n",
            "Batch #300 Loss: 0.3383365382254124\n",
            "\u001b[92mTrain accuracy: 42865/48000 =  89.3 % ||| loss 0.27894893288612366\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10533/12000 =  87.78 % ||| loss 0.3403271436691284\u001b[0m\n",
            "\u001b[92mTest accuracy: 8714/10000 =  87.14 % ||| loss 0.3638133406639099\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3164161120355129\n",
            "Batch #200 Loss: 0.3343177103996277\n",
            "Batch #300 Loss: 0.3486166772246361\n",
            "\u001b[92mTrain accuracy: 43079/48000 =  89.75 % ||| loss 0.26555997133255005\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10541/12000 =  87.84 % ||| loss 0.33140987157821655\u001b[0m\n",
            "\u001b[92mTest accuracy: 8777/10000 =  87.77 % ||| loss 0.34807658195495605\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3236491806805134\n",
            "Batch #200 Loss: 0.3224602533876896\n",
            "Batch #300 Loss: 0.3452638518810272\n",
            "\u001b[92mTrain accuracy: 42465/48000 =  88.47 % ||| loss 0.3026459813117981\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10414/12000 =  86.78 % ||| loss 0.35884979367256165\u001b[0m\n",
            "\u001b[92mTest accuracy: 8603/10000 =  86.03 % ||| loss 0.38315728306770325\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.31819399118423464\n",
            "Batch #200 Loss: 0.3225307312607765\n",
            "Batch #300 Loss: 0.32633852228522303\n",
            "\u001b[92mTrain accuracy: 43072/48000 =  89.73 % ||| loss 0.2717406451702118\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10544/12000 =  87.87 % ||| loss 0.3379925787448883\u001b[0m\n",
            "\u001b[92mTest accuracy: 8734/10000 =  87.34 % ||| loss 0.36003777384757996\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.32828767970204353\n",
            "Batch #200 Loss: 0.3180464978516102\n",
            "Batch #300 Loss: 0.3313243456184864\n",
            "\u001b[92mTrain accuracy: 43001/48000 =  89.59 % ||| loss 0.2663779854774475\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10523/12000 =  87.69 % ||| loss 0.33624550700187683\u001b[0m\n",
            "\u001b[92mTest accuracy: 8737/10000 =  87.37 % ||| loss 0.35755470395088196\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.32007300153374674\n",
            "Batch #200 Loss: 0.3452412022650242\n",
            "Batch #300 Loss: 0.32857272267341614\n",
            "\u001b[92mTrain accuracy: 42820/48000 =  89.21 % ||| loss 0.2839621305465698\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10477/12000 =  87.31 % ||| loss 0.34539714455604553\u001b[0m\n",
            "\u001b[92mTest accuracy: 8736/10000 =  87.36 % ||| loss 0.3584387004375458\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.32419980376958846\n",
            "Batch #200 Loss: 0.32946868672966956\n",
            "Batch #300 Loss: 0.31961457908153534\n",
            "\u001b[92mTrain accuracy: 42901/48000 =  89.38 % ||| loss 0.2771746516227722\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10448/12000 =  87.07 % ||| loss 0.342077374458313\u001b[0m\n",
            "\u001b[92mTest accuracy: 8629/10000 =  86.29 % ||| loss 0.3742219805717468\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.3207478484511375\n",
            "Batch #200 Loss: 0.3215153242647648\n",
            "Batch #300 Loss: 0.3236661690473557\n",
            "\u001b[92mTrain accuracy: 43146/48000 =  89.89 % ||| loss 0.25803518295288086\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10484/12000 =  87.37 % ||| loss 0.339011549949646\u001b[0m\n",
            "\u001b[92mTest accuracy: 8699/10000 =  86.99 % ||| loss 0.3572343587875366\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.30958917886018755\n",
            "Batch #200 Loss: 0.319469223767519\n",
            "Batch #300 Loss: 0.3265997423231602\n",
            "\u001b[92mTrain accuracy: 42973/48000 =  89.53 % ||| loss 0.2780936062335968\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10522/12000 =  87.68 % ||| loss 0.3414710462093353\u001b[0m\n",
            "\u001b[92mTest accuracy: 8707/10000 =  87.07 % ||| loss 0.3730095624923706\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.31199117049574854\n",
            "Batch #200 Loss: 0.3457139338552952\n",
            "Batch #300 Loss: 0.3143205916881561\n",
            "\u001b[92mTrain accuracy: 43019/48000 =  89.62 % ||| loss 0.27553990483283997\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10511/12000 =  87.59 % ||| loss 0.3417080044746399\u001b[0m\n",
            "\u001b[92mTest accuracy: 8702/10000 =  87.02 % ||| loss 0.368214875459671\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3204358798265457\n",
            "Batch #200 Loss: 0.30843677222728727\n",
            "Batch #300 Loss: 0.3224504071474075\n",
            "\u001b[92mTrain accuracy: 43230/48000 =  90.06 % ||| loss 0.2614557147026062\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10570/12000 =  88.08 % ||| loss 0.33016520738601685\u001b[0m\n",
            "\u001b[92mTest accuracy: 8700/10000 =  87.0 % ||| loss 0.3519982695579529\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_3</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_3</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_173146-Lenet5Dropout_1726089678.067411_4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_4' target=\"_blank\">Lenet5Dropout_1726089678.067411_4</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.6171752208471297\n",
            "Batch #200 Loss: 0.777950463294983\n",
            "Batch #300 Loss: 0.685126773416996\n",
            "\u001b[92mTrain accuracy: 38133/48000 =  79.44 % ||| loss 0.5164974927902222\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9586/12000 =  79.88 % ||| loss 0.5152369737625122\u001b[0m\n",
            "\u001b[92mTest accuracy: 7830/10000 =  78.3 % ||| loss 0.5371954441070557\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.5914046880602837\n",
            "Batch #200 Loss: 0.5618526414036751\n",
            "Batch #300 Loss: 0.5472726196050643\n",
            "\u001b[92mTrain accuracy: 39803/48000 =  82.92 % ||| loss 0.4438246190547943\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9948/12000 =  82.9 % ||| loss 0.44593846797943115\u001b[0m\n",
            "\u001b[92mTest accuracy: 8191/10000 =  81.91 % ||| loss 0.4679599702358246\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.521308706998825\n",
            "Batch #200 Loss: 0.524477511048317\n",
            "Batch #300 Loss: 0.5031998300552368\n",
            "\u001b[92mTrain accuracy: 39509/48000 =  82.31 % ||| loss 0.4440786838531494\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9853/12000 =  82.11 % ||| loss 0.4522899091243744\u001b[0m\n",
            "\u001b[92mTest accuracy: 8057/10000 =  80.57 % ||| loss 0.4866849184036255\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.4859222984313965\n",
            "Batch #200 Loss: 0.48313045293092727\n",
            "Batch #300 Loss: 0.47346260368824006\n",
            "\u001b[92mTrain accuracy: 41255/48000 =  85.95 % ||| loss 0.38111406564712524\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10260/12000 =  85.5 % ||| loss 0.4021551311016083\u001b[0m\n",
            "\u001b[92mTest accuracy: 8440/10000 =  84.4 % ||| loss 0.42496344447135925\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4532380896806717\n",
            "Batch #200 Loss: 0.4353305825591087\n",
            "Batch #300 Loss: 0.4400369480252266\n",
            "\u001b[92mTrain accuracy: 41513/48000 =  86.49 % ||| loss 0.36138656735420227\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10253/12000 =  85.44 % ||| loss 0.38694944977760315\u001b[0m\n",
            "\u001b[92mTest accuracy: 8467/10000 =  84.67 % ||| loss 0.4099122881889343\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.4098147675395012\n",
            "Batch #200 Loss: 0.4217849175632\n",
            "Batch #300 Loss: 0.4264402365684509\n",
            "\u001b[92mTrain accuracy: 41871/48000 =  87.23 % ||| loss 0.3460846245288849\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10360/12000 =  86.33 % ||| loss 0.3647049069404602\u001b[0m\n",
            "\u001b[92mTest accuracy: 8542/10000 =  85.42 % ||| loss 0.3978973627090454\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.40930433124303817\n",
            "Batch #200 Loss: 0.4210259303450584\n",
            "Batch #300 Loss: 0.41004529640078546\n",
            "\u001b[92mTrain accuracy: 42099/48000 =  87.71 % ||| loss 0.32332131266593933\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10381/12000 =  86.51 % ||| loss 0.3567216694355011\u001b[0m\n",
            "\u001b[92mTest accuracy: 8553/10000 =  85.53 % ||| loss 0.37924566864967346\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.40382265150547025\n",
            "Batch #200 Loss: 0.406295417547226\n",
            "Batch #300 Loss: 0.4100597323477268\n",
            "\u001b[92mTrain accuracy: 42056/48000 =  87.62 % ||| loss 0.3215995132923126\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10380/12000 =  86.5 % ||| loss 0.3526317775249481\u001b[0m\n",
            "\u001b[92mTest accuracy: 8594/10000 =  85.94 % ||| loss 0.38695135712623596\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3975639951229095\n",
            "Batch #200 Loss: 0.4021048554778099\n",
            "Batch #300 Loss: 0.4041438461840153\n",
            "\u001b[92mTrain accuracy: 42126/48000 =  87.76 % ||| loss 0.3171413242816925\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10406/12000 =  86.72 % ||| loss 0.3489101827144623\u001b[0m\n",
            "\u001b[92mTest accuracy: 8624/10000 =  86.24 % ||| loss 0.37644243240356445\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3867936006188393\n",
            "Batch #200 Loss: 0.3927266947925091\n",
            "Batch #300 Loss: 0.39253061920404436\n",
            "\u001b[92mTrain accuracy: 41539/48000 =  86.54 % ||| loss 0.3410922586917877\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10277/12000 =  85.64 % ||| loss 0.368564635515213\u001b[0m\n",
            "\u001b[92mTest accuracy: 8504/10000 =  85.04 % ||| loss 0.3958933651447296\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.38175211057066916\n",
            "Batch #200 Loss: 0.41116530805826185\n",
            "Batch #300 Loss: 0.38762461483478544\n",
            "\u001b[92mTrain accuracy: 42191/48000 =  87.9 % ||| loss 0.3218599557876587\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10411/12000 =  86.76 % ||| loss 0.3521343469619751\u001b[0m\n",
            "\u001b[92mTest accuracy: 8629/10000 =  86.29 % ||| loss 0.38216927647590637\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.37450829297304156\n",
            "Batch #200 Loss: 0.39066838175058366\n",
            "Batch #300 Loss: 0.41996889546513555\n",
            "\u001b[92mTrain accuracy: 41873/48000 =  87.24 % ||| loss 0.3462431728839874\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10305/12000 =  85.88 % ||| loss 0.3783765435218811\u001b[0m\n",
            "\u001b[92mTest accuracy: 8526/10000 =  85.26 % ||| loss 0.3976411819458008\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.3947428074479103\n",
            "Batch #200 Loss: 0.3960774213075638\n",
            "Batch #300 Loss: 0.3888675032556057\n",
            "\u001b[92mTrain accuracy: 42602/48000 =  88.75 % ||| loss 0.3088507354259491\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10531/12000 =  87.76 % ||| loss 0.34786832332611084\u001b[0m\n",
            "\u001b[92mTest accuracy: 8681/10000 =  86.81 % ||| loss 0.37166690826416016\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3997981649637222\n",
            "Batch #200 Loss: 0.38029880687594414\n",
            "Batch #300 Loss: 0.3801149359345436\n",
            "\u001b[92mTrain accuracy: 42367/48000 =  88.26 % ||| loss 0.3066888451576233\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10447/12000 =  87.06 % ||| loss 0.34162744879722595\u001b[0m\n",
            "\u001b[92mTest accuracy: 8580/10000 =  85.8 % ||| loss 0.3706901967525482\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.37869580924510954\n",
            "Batch #200 Loss: 0.38828446239233017\n",
            "Batch #300 Loss: 0.37840143784880637\n",
            "\u001b[92mTrain accuracy: 42011/48000 =  87.52 % ||| loss 0.31420084834098816\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10340/12000 =  86.17 % ||| loss 0.35520803928375244\u001b[0m\n",
            "\u001b[92mTest accuracy: 8514/10000 =  85.14 % ||| loss 0.38059815764427185\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.37585415959358215\n",
            "Batch #200 Loss: 0.39366993099451064\n",
            "Batch #300 Loss: 0.3607044605910778\n",
            "\u001b[92mTrain accuracy: 42306/48000 =  88.14 % ||| loss 0.3133217692375183\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10411/12000 =  86.76 % ||| loss 0.3549562394618988\u001b[0m\n",
            "\u001b[92mTest accuracy: 8575/10000 =  85.75 % ||| loss 0.39394745230674744\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3680757047235966\n",
            "Batch #200 Loss: 0.3836581484973431\n",
            "Batch #300 Loss: 0.40231483325362205\n",
            "\u001b[92mTrain accuracy: 41867/48000 =  87.22 % ||| loss 0.3292255401611328\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10313/12000 =  85.94 % ||| loss 0.37072521448135376\u001b[0m\n",
            "\u001b[92mTest accuracy: 8552/10000 =  85.52 % ||| loss 0.39892303943634033\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3894328786432743\n",
            "Batch #200 Loss: 0.38282216593623164\n",
            "Batch #300 Loss: 0.37512913063168524\n",
            "\u001b[92mTrain accuracy: 42655/48000 =  88.86 % ||| loss 0.29196906089782715\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10494/12000 =  87.45 % ||| loss 0.3339596688747406\u001b[0m\n",
            "\u001b[92mTest accuracy: 8702/10000 =  87.02 % ||| loss 0.358941912651062\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.37138927057385446\n",
            "Batch #200 Loss: 0.3546106983721256\n",
            "Batch #300 Loss: 0.3815265762805939\n",
            "\u001b[92mTrain accuracy: 42205/48000 =  87.93 % ||| loss 0.3189239203929901\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10360/12000 =  86.33 % ||| loss 0.36563122272491455\u001b[0m\n",
            "\u001b[92mTest accuracy: 8619/10000 =  86.19 % ||| loss 0.38621625304222107\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.35477477580308914\n",
            "Batch #200 Loss: 0.37254595160484316\n",
            "Batch #300 Loss: 0.3888685970008373\n",
            "\u001b[92mTrain accuracy: 42515/48000 =  88.57 % ||| loss 0.2944691479206085\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10480/12000 =  87.33 % ||| loss 0.3413870334625244\u001b[0m\n",
            "\u001b[92mTest accuracy: 8702/10000 =  87.02 % ||| loss 0.3718675971031189\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.3637105871737003\n",
            "Batch #200 Loss: 0.36916817933321\n",
            "Batch #300 Loss: 0.37236893162131307\n",
            "\u001b[92mTrain accuracy: 42079/48000 =  87.66 % ||| loss 0.31623074412345886\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10374/12000 =  86.45 % ||| loss 0.35930004715919495\u001b[0m\n",
            "\u001b[92mTest accuracy: 8594/10000 =  85.94 % ||| loss 0.3894636631011963\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.3717936934530735\n",
            "Batch #200 Loss: 0.3709072481095791\n",
            "Batch #300 Loss: 0.364586579054594\n",
            "\u001b[92mTrain accuracy: 42489/48000 =  88.52 % ||| loss 0.3034859597682953\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10388/12000 =  86.57 % ||| loss 0.3606160283088684\u001b[0m\n",
            "\u001b[92mTest accuracy: 8589/10000 =  85.89 % ||| loss 0.3832029104232788\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.3635273656249046\n",
            "Batch #200 Loss: 0.38550878822803497\n",
            "Batch #300 Loss: 0.36085405126214026\n",
            "\u001b[92mTrain accuracy: 41794/48000 =  87.07 % ||| loss 0.3409714102745056\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10317/12000 =  85.97 % ||| loss 0.3764832615852356\u001b[0m\n",
            "\u001b[92mTest accuracy: 8471/10000 =  84.71 % ||| loss 0.40679168701171875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.3672434192895889\n",
            "Batch #200 Loss: 0.37565103530883787\n",
            "Batch #300 Loss: 0.3738668315112591\n",
            "\u001b[92mTrain accuracy: 42400/48000 =  88.33 % ||| loss 0.2990354895591736\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10454/12000 =  87.12 % ||| loss 0.3520432710647583\u001b[0m\n",
            "\u001b[92mTest accuracy: 8620/10000 =  86.2 % ||| loss 0.38952377438545227\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3532012592256069\n",
            "Batch #200 Loss: 0.3734679774940014\n",
            "Batch #300 Loss: 0.39368452072143556\n",
            "\u001b[92mTrain accuracy: 42540/48000 =  88.62 % ||| loss 0.306100457906723\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10374/12000 =  86.45 % ||| loss 0.3677580952644348\u001b[0m\n",
            "\u001b[92mTest accuracy: 8594/10000 =  85.94 % ||| loss 0.39274221658706665\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_4</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_4</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_173342-Lenet5Dropout_1726089678.067411_5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_5' target=\"_blank\">Lenet5Dropout_1726089678.067411_5</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.6412147665023804\n",
            "Batch #200 Loss: 0.9084899467229843\n",
            "Batch #300 Loss: 0.7720858246088028\n",
            "\u001b[92mTrain accuracy: 36993/48000 =  77.07 % ||| loss 0.5932733416557312\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9231/12000 =  76.92 % ||| loss 0.5937412977218628\u001b[0m\n",
            "\u001b[92mTest accuracy: 7608/10000 =  76.08 % ||| loss 0.6149501800537109\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.672717048227787\n",
            "Batch #200 Loss: 0.6499810868501663\n",
            "Batch #300 Loss: 0.6248873081803322\n",
            "\u001b[92mTrain accuracy: 40033/48000 =  83.4 % ||| loss 0.45581507682800293\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10022/12000 =  83.52 % ||| loss 0.45736223459243774\u001b[0m\n",
            "\u001b[92mTest accuracy: 8229/10000 =  82.29 % ||| loss 0.49064525961875916\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5753944364190101\n",
            "Batch #200 Loss: 0.5828803011775017\n",
            "Batch #300 Loss: 0.5732207027077675\n",
            "\u001b[92mTrain accuracy: 40451/48000 =  84.27 % ||| loss 0.4410126805305481\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10095/12000 =  84.12 % ||| loss 0.443975567817688\u001b[0m\n",
            "\u001b[92mTest accuracy: 8314/10000 =  83.14 % ||| loss 0.4804660975933075\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5497040894627571\n",
            "Batch #200 Loss: 0.5389802917838097\n",
            "Batch #300 Loss: 0.5507268950343132\n",
            "\u001b[92mTrain accuracy: 40355/48000 =  84.07 % ||| loss 0.44247087836265564\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10120/12000 =  84.33 % ||| loss 0.4426440894603729\u001b[0m\n",
            "\u001b[92mTest accuracy: 8286/10000 =  82.86 % ||| loss 0.4783916771411896\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.524795092344284\n",
            "Batch #200 Loss: 0.5372419726848602\n",
            "Batch #300 Loss: 0.5495562550425529\n",
            "\u001b[92mTrain accuracy: 40923/48000 =  85.26 % ||| loss 0.400095671415329\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10244/12000 =  85.37 % ||| loss 0.407223641872406\u001b[0m\n",
            "\u001b[92mTest accuracy: 8404/10000 =  84.04 % ||| loss 0.45015594363212585\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5316812312602996\n",
            "Batch #200 Loss: 0.5243791851401329\n",
            "Batch #300 Loss: 0.524078366458416\n",
            "\u001b[92mTrain accuracy: 40930/48000 =  85.27 % ||| loss 0.4119254946708679\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10234/12000 =  85.28 % ||| loss 0.4231075048446655\u001b[0m\n",
            "\u001b[92mTest accuracy: 8401/10000 =  84.01 % ||| loss 0.45978042483329773\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5089319184422493\n",
            "Batch #200 Loss: 0.5201164332032203\n",
            "Batch #300 Loss: 0.5035627800226211\n",
            "\u001b[92mTrain accuracy: 40811/48000 =  85.02 % ||| loss 0.40998604893684387\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10212/12000 =  85.1 % ||| loss 0.42233309149742126\u001b[0m\n",
            "\u001b[92mTest accuracy: 8418/10000 =  84.18 % ||| loss 0.45909303426742554\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.5215914154052734\n",
            "Batch #200 Loss: 0.5105948868393898\n",
            "Batch #300 Loss: 0.5067481213808059\n",
            "\u001b[92mTrain accuracy: 40650/48000 =  84.69 % ||| loss 0.41318124532699585\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10146/12000 =  84.55 % ||| loss 0.42009812593460083\u001b[0m\n",
            "\u001b[92mTest accuracy: 8324/10000 =  83.24 % ||| loss 0.4540572762489319\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.5051696687936783\n",
            "Batch #200 Loss: 0.5042292168736457\n",
            "Batch #300 Loss: 0.4925076228380203\n",
            "\u001b[92mTrain accuracy: 40870/48000 =  85.15 % ||| loss 0.41967007517814636\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10207/12000 =  85.06 % ||| loss 0.4306941032409668\u001b[0m\n",
            "\u001b[92mTest accuracy: 8382/10000 =  83.82 % ||| loss 0.46474629640579224\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.5195414841175079\n",
            "Batch #200 Loss: 0.5011980071663856\n",
            "Batch #300 Loss: 0.5100853329896927\n",
            "\u001b[92mTrain accuracy: 41409/48000 =  86.27 % ||| loss 0.3739892244338989\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10305/12000 =  85.88 % ||| loss 0.3968229591846466\u001b[0m\n",
            "\u001b[92mTest accuracy: 8491/10000 =  84.91 % ||| loss 0.4346819519996643\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.4888370129466057\n",
            "Batch #200 Loss: 0.5118008288741112\n",
            "Batch #300 Loss: 0.4999809578061104\n",
            "\u001b[92mTrain accuracy: 41189/48000 =  85.81 % ||| loss 0.3844810426235199\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10273/12000 =  85.61 % ||| loss 0.4036441147327423\u001b[0m\n",
            "\u001b[92mTest accuracy: 8455/10000 =  84.55 % ||| loss 0.44140875339508057\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5093565380573273\n",
            "Batch #200 Loss: 0.48784978717565536\n",
            "Batch #300 Loss: 0.4850554355978966\n",
            "\u001b[92mTrain accuracy: 41529/48000 =  86.52 % ||| loss 0.3739355206489563\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10316/12000 =  85.97 % ||| loss 0.3950078785419464\u001b[0m\n",
            "\u001b[92mTest accuracy: 8515/10000 =  85.15 % ||| loss 0.43364208936691284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.49691260397434234\n",
            "Batch #200 Loss: 0.48650429159402847\n",
            "Batch #300 Loss: 0.509385416507721\n",
            "\u001b[92mTrain accuracy: 41430/48000 =  86.31 % ||| loss 0.372044175863266\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10282/12000 =  85.68 % ||| loss 0.3945294916629791\u001b[0m\n",
            "\u001b[92mTest accuracy: 8440/10000 =  84.4 % ||| loss 0.43143290281295776\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.48098535686731336\n",
            "Batch #200 Loss: 0.4971346890926361\n",
            "Batch #300 Loss: 0.5241709145903587\n",
            "\u001b[92mTrain accuracy: 41371/48000 =  86.19 % ||| loss 0.3911200761795044\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10289/12000 =  85.74 % ||| loss 0.40663257241249084\u001b[0m\n",
            "\u001b[92mTest accuracy: 8430/10000 =  84.3 % ||| loss 0.452709823846817\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.4966826143860817\n",
            "Batch #200 Loss: 0.5020648035407066\n",
            "Batch #300 Loss: 0.4707940700650215\n",
            "\u001b[92mTrain accuracy: 41223/48000 =  85.88 % ||| loss 0.38766393065452576\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10258/12000 =  85.48 % ||| loss 0.41064220666885376\u001b[0m\n",
            "\u001b[92mTest accuracy: 8434/10000 =  84.34 % ||| loss 0.44596627354621887\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.4930233910679817\n",
            "Batch #200 Loss: 0.5189776784181594\n",
            "Batch #300 Loss: 0.5050450518727303\n",
            "\u001b[92mTrain accuracy: 40968/48000 =  85.35 % ||| loss 0.3966429829597473\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10203/12000 =  85.02 % ||| loss 0.4101201295852661\u001b[0m\n",
            "\u001b[92mTest accuracy: 8376/10000 =  83.76 % ||| loss 0.4580426812171936\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5004487192630768\n",
            "Batch #200 Loss: 0.5049711260199546\n",
            "Batch #300 Loss: 0.4850850412249565\n",
            "\u001b[92mTrain accuracy: 40555/48000 =  84.49 % ||| loss 0.4121933579444885\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10122/12000 =  84.35 % ||| loss 0.4235568642616272\u001b[0m\n",
            "\u001b[92mTest accuracy: 8322/10000 =  83.22 % ||| loss 0.4644705355167389\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5035488085448742\n",
            "Batch #200 Loss: 0.48930770456790923\n",
            "Batch #300 Loss: 0.4924468243122101\n",
            "\u001b[92mTrain accuracy: 41217/48000 =  85.87 % ||| loss 0.3763275444507599\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10274/12000 =  85.62 % ||| loss 0.3999674320220947\u001b[0m\n",
            "\u001b[92mTest accuracy: 8429/10000 =  84.29 % ||| loss 0.4486730992794037\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.49189056158065797\n",
            "Batch #200 Loss: 0.48916323989629745\n",
            "Batch #300 Loss: 0.5005907922983169\n",
            "\u001b[92mTrain accuracy: 41345/48000 =  86.14 % ||| loss 0.3752257227897644\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10294/12000 =  85.78 % ||| loss 0.4005131125450134\u001b[0m\n",
            "\u001b[92mTest accuracy: 8455/10000 =  84.55 % ||| loss 0.4483332931995392\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.49710211604833604\n",
            "Batch #200 Loss: 0.4988610225915909\n",
            "Batch #300 Loss: 0.5148025760054589\n",
            "\u001b[92mTrain accuracy: 40964/48000 =  85.34 % ||| loss 0.39750730991363525\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10183/12000 =  84.86 % ||| loss 0.4178701341152191\u001b[0m\n",
            "\u001b[92mTest accuracy: 8364/10000 =  83.64 % ||| loss 0.4626903235912323\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.5031613951921463\n",
            "Batch #200 Loss: 0.5415828272700309\n",
            "Batch #300 Loss: 0.49755282402038575\n",
            "\u001b[92mTrain accuracy: 40868/48000 =  85.14 % ||| loss 0.4036051034927368\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10166/12000 =  84.72 % ||| loss 0.4199138283729553\u001b[0m\n",
            "\u001b[92mTest accuracy: 8390/10000 =  83.9 % ||| loss 0.45855972170829773\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.48983068436384203\n",
            "Batch #200 Loss: 0.49204882204532624\n",
            "Batch #300 Loss: 0.4946673095226288\n",
            "\u001b[92mTrain accuracy: 40920/48000 =  85.25 % ||| loss 0.41177958250045776\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10183/12000 =  84.86 % ||| loss 0.4398016035556793\u001b[0m\n",
            "\u001b[92mTest accuracy: 8393/10000 =  83.93 % ||| loss 0.46674424409866333\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.507509921491146\n",
            "Batch #200 Loss: 0.5713879972696304\n",
            "Batch #300 Loss: 0.5386899447441101\n",
            "\u001b[92mTrain accuracy: 41431/48000 =  86.31 % ||| loss 0.3791778087615967\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10284/12000 =  85.7 % ||| loss 0.40528765320777893\u001b[0m\n",
            "\u001b[92mTest accuracy: 8458/10000 =  84.58 % ||| loss 0.43321412801742554\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.48390451073646545\n",
            "Batch #200 Loss: 0.5137535548210144\n",
            "Batch #300 Loss: 0.524388556778431\n",
            "\u001b[92mTrain accuracy: 40299/48000 =  83.96 % ||| loss 0.41629910469055176\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9989/12000 =  83.24 % ||| loss 0.4343244731426239\u001b[0m\n",
            "\u001b[92mTest accuracy: 8252/10000 =  82.52 % ||| loss 0.4706907272338867\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.49882892072200774\n",
            "Batch #200 Loss: 0.5080979481339455\n",
            "Batch #300 Loss: 0.5103232982754707\n",
            "\u001b[92mTrain accuracy: 41620/48000 =  86.71 % ||| loss 0.3656925857067108\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10306/12000 =  85.88 % ||| loss 0.396584689617157\u001b[0m\n",
            "\u001b[92mTest accuracy: 8482/10000 =  84.82 % ||| loss 0.4318489730358124\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_5</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_5</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_173539-Lenet5Dropout_1726089678.067411_6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_6' target=\"_blank\">Lenet5Dropout_1726089678.067411_6</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.9824959194660188\n",
            "Batch #200 Loss: 0.8396472549438476\n",
            "Batch #300 Loss: 0.6438273441791534\n",
            "\u001b[92mTrain accuracy: 37935/48000 =  79.03 % ||| loss 0.5305349826812744\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9455/12000 =  78.79 % ||| loss 0.5310707688331604\u001b[0m\n",
            "\u001b[92mTest accuracy: 7846/10000 =  78.46 % ||| loss 0.553923487663269\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.5717031094431877\n",
            "Batch #200 Loss: 0.5390240147709846\n",
            "Batch #300 Loss: 0.5050092589855194\n",
            "\u001b[92mTrain accuracy: 39836/48000 =  82.99 % ||| loss 0.44965437054634094\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9967/12000 =  83.06 % ||| loss 0.4526224732398987\u001b[0m\n",
            "\u001b[92mTest accuracy: 8185/10000 =  81.85 % ||| loss 0.4792141914367676\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.46577175438404084\n",
            "Batch #200 Loss: 0.44619956344366074\n",
            "Batch #300 Loss: 0.441606302857399\n",
            "\u001b[92mTrain accuracy: 41410/48000 =  86.27 % ||| loss 0.36287757754325867\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10303/12000 =  85.86 % ||| loss 0.37238815426826477\u001b[0m\n",
            "\u001b[92mTest accuracy: 8517/10000 =  85.17 % ||| loss 0.4034166634082794\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.4090545292198658\n",
            "Batch #200 Loss: 0.419629622399807\n",
            "Batch #300 Loss: 0.3911468726396561\n",
            "\u001b[92mTrain accuracy: 41975/48000 =  87.45 % ||| loss 0.33500906825065613\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10424/12000 =  86.87 % ||| loss 0.3557949960231781\u001b[0m\n",
            "\u001b[92mTest accuracy: 8666/10000 =  86.66 % ||| loss 0.3702327013015747\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.37767302423715593\n",
            "Batch #200 Loss: 0.3797096453607082\n",
            "Batch #300 Loss: 0.373308295160532\n",
            "\u001b[92mTrain accuracy: 42002/48000 =  87.5 % ||| loss 0.328007310628891\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10435/12000 =  86.96 % ||| loss 0.3489101827144623\u001b[0m\n",
            "\u001b[92mTest accuracy: 8629/10000 =  86.29 % ||| loss 0.3745673596858978\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.36287811934947967\n",
            "Batch #200 Loss: 0.35910422295331956\n",
            "Batch #300 Loss: 0.36059646353125574\n",
            "\u001b[92mTrain accuracy: 42292/48000 =  88.11 % ||| loss 0.31262001395225525\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10473/12000 =  87.28 % ||| loss 0.33678218722343445\u001b[0m\n",
            "\u001b[92mTest accuracy: 8651/10000 =  86.51 % ||| loss 0.3621442914009094\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3524834094941616\n",
            "Batch #200 Loss: 0.34313409313559534\n",
            "Batch #300 Loss: 0.33444790333509444\n",
            "\u001b[92mTrain accuracy: 42622/48000 =  88.8 % ||| loss 0.29908108711242676\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10542/12000 =  87.85 % ||| loss 0.32355746626853943\u001b[0m\n",
            "\u001b[92mTest accuracy: 8736/10000 =  87.36 % ||| loss 0.3468688428401947\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3254381874203682\n",
            "Batch #200 Loss: 0.3374531316757202\n",
            "Batch #300 Loss: 0.32344602957367896\n",
            "\u001b[92mTrain accuracy: 42950/48000 =  89.48 % ||| loss 0.28202328085899353\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10601/12000 =  88.34 % ||| loss 0.312808096408844\u001b[0m\n",
            "\u001b[92mTest accuracy: 8764/10000 =  87.64 % ||| loss 0.33608609437942505\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3206143249571323\n",
            "Batch #200 Loss: 0.3248048722743988\n",
            "Batch #300 Loss: 0.3316887664794922\n",
            "\u001b[92mTrain accuracy: 42759/48000 =  89.08 % ||| loss 0.28584688901901245\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10572/12000 =  88.1 % ||| loss 0.32191741466522217\u001b[0m\n",
            "\u001b[92mTest accuracy: 8738/10000 =  87.38 % ||| loss 0.34465593099594116\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3089876116812229\n",
            "Batch #200 Loss: 0.310013774484396\n",
            "Batch #300 Loss: 0.314158369153738\n",
            "\u001b[92mTrain accuracy: 43208/48000 =  90.02 % ||| loss 0.26368555426597595\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10655/12000 =  88.79 % ||| loss 0.30150681734085083\u001b[0m\n",
            "\u001b[92mTest accuracy: 8802/10000 =  88.02 % ||| loss 0.3272075057029724\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.2922446919977665\n",
            "Batch #200 Loss: 0.3076289904117584\n",
            "Batch #300 Loss: 0.30860338613390925\n",
            "\u001b[92mTrain accuracy: 43272/48000 =  90.15 % ||| loss 0.2589268982410431\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10641/12000 =  88.67 % ||| loss 0.3056686520576477\u001b[0m\n",
            "\u001b[92mTest accuracy: 8844/10000 =  88.44 % ||| loss 0.32397305965423584\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.28588194742798806\n",
            "Batch #200 Loss: 0.29865140914916993\n",
            "Batch #300 Loss: 0.30424240350723264\n",
            "\u001b[92mTrain accuracy: 43210/48000 =  90.02 % ||| loss 0.26191434264183044\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10616/12000 =  88.47 % ||| loss 0.30837687849998474\u001b[0m\n",
            "\u001b[92mTest accuracy: 8798/10000 =  87.98 % ||| loss 0.3282954692840576\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.29387861147522926\n",
            "Batch #200 Loss: 0.2875798793137074\n",
            "Batch #300 Loss: 0.29325790420174597\n",
            "\u001b[92mTrain accuracy: 43547/48000 =  90.72 % ||| loss 0.24159356951713562\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10662/12000 =  88.85 % ||| loss 0.2923732101917267\u001b[0m\n",
            "\u001b[92mTest accuracy: 8835/10000 =  88.35 % ||| loss 0.3162247836589813\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.27928670346736906\n",
            "Batch #200 Loss: 0.2878787288069725\n",
            "Batch #300 Loss: 0.2874939608573914\n",
            "\u001b[92mTrain accuracy: 43592/48000 =  90.82 % ||| loss 0.24342234432697296\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10644/12000 =  88.7 % ||| loss 0.3050639033317566\u001b[0m\n",
            "\u001b[92mTest accuracy: 8834/10000 =  88.34 % ||| loss 0.3281536400318146\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.2824757684767246\n",
            "Batch #200 Loss: 0.2754365086555481\n",
            "Batch #300 Loss: 0.2885523845255375\n",
            "\u001b[92mTrain accuracy: 43734/48000 =  91.11 % ||| loss 0.22962850332260132\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10720/12000 =  89.33 % ||| loss 0.2908184826374054\u001b[0m\n",
            "\u001b[92mTest accuracy: 8854/10000 =  88.54 % ||| loss 0.3173082172870636\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.26177763268351556\n",
            "Batch #200 Loss: 0.2777475318312645\n",
            "Batch #300 Loss: 0.27964970290660857\n",
            "\u001b[92mTrain accuracy: 43534/48000 =  90.7 % ||| loss 0.24330653250217438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10646/12000 =  88.72 % ||| loss 0.3038627505302429\u001b[0m\n",
            "\u001b[92mTest accuracy: 8838/10000 =  88.38 % ||| loss 0.3256934583187103\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.2656839874386787\n",
            "Batch #200 Loss: 0.2825916473567486\n",
            "Batch #300 Loss: 0.26675847083330156\n",
            "\u001b[92mTrain accuracy: 43661/48000 =  90.96 % ||| loss 0.23880472779273987\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10674/12000 =  88.95 % ||| loss 0.30106449127197266\u001b[0m\n",
            "\u001b[92mTest accuracy: 8821/10000 =  88.21 % ||| loss 0.32205840945243835\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.273131303191185\n",
            "Batch #200 Loss: 0.27568495512008667\n",
            "Batch #300 Loss: 0.2629665644466877\n",
            "\u001b[92mTrain accuracy: 44078/48000 =  91.83 % ||| loss 0.21730594336986542\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10733/12000 =  89.44 % ||| loss 0.28419819474220276\u001b[0m\n",
            "\u001b[92mTest accuracy: 8877/10000 =  88.77 % ||| loss 0.31078821420669556\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.2628458569943905\n",
            "Batch #200 Loss: 0.2703126285970211\n",
            "Batch #300 Loss: 0.25677810966968534\n",
            "\u001b[92mTrain accuracy: 43550/48000 =  90.73 % ||| loss 0.24008223414421082\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10621/12000 =  88.51 % ||| loss 0.32186758518218994\u001b[0m\n",
            "\u001b[92mTest accuracy: 8817/10000 =  88.17 % ||| loss 0.33872848749160767\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.25310180492699147\n",
            "Batch #200 Loss: 0.2607687066495419\n",
            "Batch #300 Loss: 0.2556067006289959\n",
            "\u001b[92mTrain accuracy: 43792/48000 =  91.23 % ||| loss 0.22979842126369476\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10676/12000 =  88.97 % ||| loss 0.30365508794784546\u001b[0m\n",
            "\u001b[92mTest accuracy: 8849/10000 =  88.49 % ||| loss 0.32720354199409485\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.2501125404238701\n",
            "Batch #200 Loss: 0.24957133889198302\n",
            "Batch #300 Loss: 0.2596162885427475\n",
            "\u001b[92mTrain accuracy: 44148/48000 =  91.97 % ||| loss 0.20951853692531586\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10731/12000 =  89.42 % ||| loss 0.29095694422721863\u001b[0m\n",
            "\u001b[92mTest accuracy: 8884/10000 =  88.84 % ||| loss 0.3134908974170685\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.2470192002505064\n",
            "Batch #200 Loss: 0.25674947664141656\n",
            "Batch #300 Loss: 0.25532651782035826\n",
            "\u001b[92mTrain accuracy: 44183/48000 =  92.05 % ||| loss 0.20660658180713654\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10766/12000 =  89.72 % ||| loss 0.28492265939712524\u001b[0m\n",
            "\u001b[92mTest accuracy: 8892/10000 =  88.92 % ||| loss 0.3156380355358124\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.24209576673805713\n",
            "Batch #200 Loss: 0.25892841517925264\n",
            "Batch #300 Loss: 0.2582892441749573\n",
            "\u001b[92mTrain accuracy: 43752/48000 =  91.15 % ||| loss 0.22177134454250336\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10664/12000 =  88.87 % ||| loss 0.3057713210582733\u001b[0m\n",
            "\u001b[92mTest accuracy: 8821/10000 =  88.21 % ||| loss 0.33519941568374634\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.24366106882691382\n",
            "Batch #200 Loss: 0.24934051990509032\n",
            "Batch #300 Loss: 0.24923186928033828\n",
            "\u001b[92mTrain accuracy: 44266/48000 =  92.22 % ||| loss 0.19830112159252167\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10695/12000 =  89.12 % ||| loss 0.29939931631088257\u001b[0m\n",
            "\u001b[92mTest accuracy: 8899/10000 =  88.99 % ||| loss 0.3186517655849457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.23627269692718983\n",
            "Batch #200 Loss: 0.24444681368768215\n",
            "Batch #300 Loss: 0.2471487943828106\n",
            "\u001b[92mTrain accuracy: 44389/48000 =  92.48 % ||| loss 0.19595180451869965\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10755/12000 =  89.62 % ||| loss 0.2938084602355957\u001b[0m\n",
            "\u001b[92mTest accuracy: 8886/10000 =  88.86 % ||| loss 0.32257959246635437\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_6</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_6</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_173735-Lenet5Dropout_1726089678.067411_7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_7' target=\"_blank\">Lenet5Dropout_1726089678.067411_7</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.7712514501810075\n",
            "Batch #200 Loss: 0.8896890413761139\n",
            "Batch #300 Loss: 0.7316977906227112\n",
            "\u001b[92mTrain accuracy: 37634/48000 =  78.4 % ||| loss 0.5751945972442627\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9379/12000 =  78.16 % ||| loss 0.5744116306304932\u001b[0m\n",
            "\u001b[92mTest accuracy: 7777/10000 =  77.77 % ||| loss 0.5928961634635925\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6090329691767693\n",
            "Batch #200 Loss: 0.5742661020159722\n",
            "Batch #300 Loss: 0.5555963069200516\n",
            "\u001b[92mTrain accuracy: 40019/48000 =  83.37 % ||| loss 0.44939371943473816\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10021/12000 =  83.51 % ||| loss 0.454563707113266\u001b[0m\n",
            "\u001b[92mTest accuracy: 8196/10000 =  81.96 % ||| loss 0.47546786069869995\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5037410092353821\n",
            "Batch #200 Loss: 0.5015230721235275\n",
            "Batch #300 Loss: 0.48398117899894716\n",
            "\u001b[92mTrain accuracy: 40992/48000 =  85.4 % ||| loss 0.39009329676628113\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10252/12000 =  85.43 % ||| loss 0.39817920327186584\u001b[0m\n",
            "\u001b[92mTest accuracy: 8460/10000 =  84.6 % ||| loss 0.42074236273765564\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.44782005161046984\n",
            "Batch #200 Loss: 0.4490047749876976\n",
            "Batch #300 Loss: 0.4437795749306679\n",
            "\u001b[92mTrain accuracy: 41015/48000 =  85.45 % ||| loss 0.3829847276210785\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10232/12000 =  85.27 % ||| loss 0.3970491588115692\u001b[0m\n",
            "\u001b[92mTest accuracy: 8439/10000 =  84.39 % ||| loss 0.4187944531440735\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4279873728752136\n",
            "Batch #200 Loss: 0.4140081407129765\n",
            "Batch #300 Loss: 0.41615608483552935\n",
            "\u001b[92mTrain accuracy: 41645/48000 =  86.76 % ||| loss 0.3525807559490204\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10365/12000 =  86.38 % ||| loss 0.36763301491737366\u001b[0m\n",
            "\u001b[92mTest accuracy: 8603/10000 =  86.03 % ||| loss 0.3795181214809418\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.3965329070389271\n",
            "Batch #200 Loss: 0.40664982184767723\n",
            "Batch #300 Loss: 0.39871020406484603\n",
            "\u001b[92mTrain accuracy: 42111/48000 =  87.73 % ||| loss 0.332327663898468\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10424/12000 =  86.87 % ||| loss 0.3513449430465698\u001b[0m\n",
            "\u001b[92mTest accuracy: 8645/10000 =  86.45 % ||| loss 0.369001179933548\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.38921518698334695\n",
            "Batch #200 Loss: 0.37693371653556823\n",
            "Batch #300 Loss: 0.3910946847498417\n",
            "\u001b[92mTrain accuracy: 42375/48000 =  88.28 % ||| loss 0.3131007254123688\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10533/12000 =  87.78 % ||| loss 0.33514609932899475\u001b[0m\n",
            "\u001b[92mTest accuracy: 8716/10000 =  87.16 % ||| loss 0.3525954484939575\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3657698212563992\n",
            "Batch #200 Loss: 0.3631482067704201\n",
            "Batch #300 Loss: 0.3761532385647297\n",
            "\u001b[92mTrain accuracy: 42520/48000 =  88.58 % ||| loss 0.30879849195480347\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10550/12000 =  87.92 % ||| loss 0.3323325514793396\u001b[0m\n",
            "\u001b[92mTest accuracy: 8684/10000 =  86.84 % ||| loss 0.346468061208725\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3676348964869976\n",
            "Batch #200 Loss: 0.36036948740482333\n",
            "Batch #300 Loss: 0.35668972313404085\n",
            "\u001b[92mTrain accuracy: 42681/48000 =  88.92 % ||| loss 0.2923908233642578\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10572/12000 =  88.1 % ||| loss 0.32238492369651794\u001b[0m\n",
            "\u001b[92mTest accuracy: 8779/10000 =  87.79 % ||| loss 0.33451107144355774\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.35831684976816175\n",
            "Batch #200 Loss: 0.34218604877591136\n",
            "Batch #300 Loss: 0.36254186034202573\n",
            "\u001b[92mTrain accuracy: 42322/48000 =  88.17 % ||| loss 0.3107130229473114\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10466/12000 =  87.22 % ||| loss 0.3441285192966461\u001b[0m\n",
            "\u001b[92mTest accuracy: 8667/10000 =  86.67 % ||| loss 0.3566243648529053\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.35500498905777933\n",
            "Batch #200 Loss: 0.3434788180887699\n",
            "Batch #300 Loss: 0.3463160139322281\n",
            "\u001b[92mTrain accuracy: 42695/48000 =  88.95 % ||| loss 0.2883734703063965\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10548/12000 =  87.9 % ||| loss 0.3170320391654968\u001b[0m\n",
            "\u001b[92mTest accuracy: 8745/10000 =  87.45 % ||| loss 0.3353337347507477\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.32449687257409093\n",
            "Batch #200 Loss: 0.34514119058847426\n",
            "Batch #300 Loss: 0.3468346802890301\n",
            "\u001b[92mTrain accuracy: 43120/48000 =  89.83 % ||| loss 0.2743151783943176\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10636/12000 =  88.63 % ||| loss 0.3118675947189331\u001b[0m\n",
            "\u001b[92mTest accuracy: 8797/10000 =  87.97 % ||| loss 0.32603079080581665\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.3342337822914124\n",
            "Batch #200 Loss: 0.33453911870718\n",
            "Batch #300 Loss: 0.34413908019661904\n",
            "\u001b[92mTrain accuracy: 43120/48000 =  89.83 % ||| loss 0.27416038513183594\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10625/12000 =  88.54 % ||| loss 0.3128697872161865\u001b[0m\n",
            "\u001b[92mTest accuracy: 8824/10000 =  88.24 % ||| loss 0.3263971209526062\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.33754613518714904\n",
            "Batch #200 Loss: 0.31990663707256317\n",
            "Batch #300 Loss: 0.3266745863854885\n",
            "\u001b[92mTrain accuracy: 43123/48000 =  89.84 % ||| loss 0.26880353689193726\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10622/12000 =  88.52 % ||| loss 0.3108823895454407\u001b[0m\n",
            "\u001b[92mTest accuracy: 8819/10000 =  88.19 % ||| loss 0.3269731402397156\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3140210418403149\n",
            "Batch #200 Loss: 0.3312862075865269\n",
            "Batch #300 Loss: 0.31929989516735074\n",
            "\u001b[92mTrain accuracy: 43251/48000 =  90.11 % ||| loss 0.2623138427734375\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10635/12000 =  88.62 % ||| loss 0.30461639165878296\u001b[0m\n",
            "\u001b[92mTest accuracy: 8833/10000 =  88.33 % ||| loss 0.3249371647834778\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.30406898483633993\n",
            "Batch #200 Loss: 0.32997853085398676\n",
            "Batch #300 Loss: 0.32109509125351904\n",
            "\u001b[92mTrain accuracy: 43442/48000 =  90.5 % ||| loss 0.25383520126342773\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10713/12000 =  89.28 % ||| loss 0.2984894812107086\u001b[0m\n",
            "\u001b[92mTest accuracy: 8872/10000 =  88.72 % ||| loss 0.319928377866745\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3091371101140976\n",
            "Batch #200 Loss: 0.3282062368094921\n",
            "Batch #300 Loss: 0.323858640640974\n",
            "\u001b[92mTrain accuracy: 43251/48000 =  90.11 % ||| loss 0.2600077688694\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10651/12000 =  88.76 % ||| loss 0.30235013365745544\u001b[0m\n",
            "\u001b[92mTest accuracy: 8829/10000 =  88.29 % ||| loss 0.3239489495754242\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.30840644299983977\n",
            "Batch #200 Loss: 0.30417646169662477\n",
            "Batch #300 Loss: 0.31072270587086676\n",
            "\u001b[92mTrain accuracy: 43614/48000 =  90.86 % ||| loss 0.25116950273513794\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10701/12000 =  89.18 % ||| loss 0.29886189103126526\u001b[0m\n",
            "\u001b[92mTest accuracy: 8861/10000 =  88.61 % ||| loss 0.31498220562934875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.30931909143924713\n",
            "Batch #200 Loss: 0.2985385926067829\n",
            "Batch #300 Loss: 0.317553179115057\n",
            "\u001b[92mTrain accuracy: 43561/48000 =  90.75 % ||| loss 0.2449372559785843\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10690/12000 =  89.08 % ||| loss 0.29520586133003235\u001b[0m\n",
            "\u001b[92mTest accuracy: 8855/10000 =  88.55 % ||| loss 0.3107379376888275\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.30120687663555146\n",
            "Batch #200 Loss: 0.30516322180628774\n",
            "Batch #300 Loss: 0.3042412532866001\n",
            "\u001b[92mTrain accuracy: 43428/48000 =  90.48 % ||| loss 0.25332021713256836\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10684/12000 =  89.03 % ||| loss 0.30467626452445984\u001b[0m\n",
            "\u001b[92mTest accuracy: 8827/10000 =  88.27 % ||| loss 0.3287852108478546\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.29462841898202896\n",
            "Batch #200 Loss: 0.3023956467211246\n",
            "Batch #300 Loss: 0.30502668499946595\n",
            "\u001b[92mTrain accuracy: 43425/48000 =  90.47 % ||| loss 0.2477557510137558\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10655/12000 =  88.79 % ||| loss 0.3001990020275116\u001b[0m\n",
            "\u001b[92mTest accuracy: 8814/10000 =  88.14 % ||| loss 0.31581148505210876\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.2924301755428314\n",
            "Batch #200 Loss: 0.293669121414423\n",
            "Batch #300 Loss: 0.30191135019063947\n",
            "\u001b[92mTrain accuracy: 43490/48000 =  90.6 % ||| loss 0.24619872868061066\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10680/12000 =  89.0 % ||| loss 0.2995847165584564\u001b[0m\n",
            "\u001b[92mTest accuracy: 8836/10000 =  88.36 % ||| loss 0.31619083881378174\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.2783889244496822\n",
            "Batch #200 Loss: 0.2897638615965843\n",
            "Batch #300 Loss: 0.30788912206888197\n",
            "\u001b[92mTrain accuracy: 43508/48000 =  90.64 % ||| loss 0.24498775601387024\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10687/12000 =  89.06 % ||| loss 0.29712551832199097\u001b[0m\n",
            "\u001b[92mTest accuracy: 8844/10000 =  88.44 % ||| loss 0.3182689845561981\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.2875104530155659\n",
            "Batch #200 Loss: 0.2864523158967495\n",
            "Batch #300 Loss: 0.2963345701992512\n",
            "\u001b[92mTrain accuracy: 43634/48000 =  90.9 % ||| loss 0.23715227842330933\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10713/12000 =  89.28 % ||| loss 0.2942523658275604\u001b[0m\n",
            "\u001b[92mTest accuracy: 8836/10000 =  88.36 % ||| loss 0.31691601872444153\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.291252431422472\n",
            "Batch #200 Loss: 0.28784223198890685\n",
            "Batch #300 Loss: 0.2957638584822416\n",
            "\u001b[92mTrain accuracy: 43398/48000 =  90.41 % ||| loss 0.24670198559761047\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10602/12000 =  88.35 % ||| loss 0.30849263072013855\u001b[0m\n",
            "\u001b[92mTest accuracy: 8826/10000 =  88.26 % ||| loss 0.32181599736213684\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_7</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_7</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_173931-Lenet5Dropout_1726089678.067411_8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_8' target=\"_blank\">Lenet5Dropout_1726089678.067411_8</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.8766273939609528\n",
            "Batch #200 Loss: 1.007956805229187\n",
            "Batch #300 Loss: 0.7964230436086654\n",
            "\u001b[92mTrain accuracy: 37583/48000 =  78.3 % ||| loss 0.5595898628234863\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9365/12000 =  78.04 % ||| loss 0.5566816329956055\u001b[0m\n",
            "\u001b[92mTest accuracy: 7751/10000 =  77.51 % ||| loss 0.5763892531394958\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6732967433333397\n",
            "Batch #200 Loss: 0.6365870577096939\n",
            "Batch #300 Loss: 0.5885412496328354\n",
            "\u001b[92mTrain accuracy: 39541/48000 =  82.38 % ||| loss 0.4671024680137634\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9870/12000 =  82.25 % ||| loss 0.4690774083137512\u001b[0m\n",
            "\u001b[92mTest accuracy: 8165/10000 =  81.65 % ||| loss 0.4869346022605896\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5610774600505829\n",
            "Batch #200 Loss: 0.5377027532458305\n",
            "Batch #300 Loss: 0.5296247437596321\n",
            "\u001b[92mTrain accuracy: 40781/48000 =  84.96 % ||| loss 0.41717153787612915\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10178/12000 =  84.82 % ||| loss 0.427100270986557\u001b[0m\n",
            "\u001b[92mTest accuracy: 8373/10000 =  83.73 % ||| loss 0.44274699687957764\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5123540264368057\n",
            "Batch #200 Loss: 0.4879695349931717\n",
            "Batch #300 Loss: 0.4859131908416748\n",
            "\u001b[92mTrain accuracy: 41506/48000 =  86.47 % ||| loss 0.3644006848335266\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10344/12000 =  86.2 % ||| loss 0.3787175118923187\u001b[0m\n",
            "\u001b[92mTest accuracy: 8557/10000 =  85.57 % ||| loss 0.3954184055328369\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.45074408829212187\n",
            "Batch #200 Loss: 0.4500764366984367\n",
            "Batch #300 Loss: 0.44907188415527344\n",
            "\u001b[92mTrain accuracy: 41744/48000 =  86.97 % ||| loss 0.3488897383213043\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10389/12000 =  86.58 % ||| loss 0.36568453907966614\u001b[0m\n",
            "\u001b[92mTest accuracy: 8580/10000 =  85.8 % ||| loss 0.3782028257846832\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.43146740198135375\n",
            "Batch #200 Loss: 0.4373406544327736\n",
            "Batch #300 Loss: 0.42469718262553213\n",
            "\u001b[92mTrain accuracy: 42157/48000 =  87.83 % ||| loss 0.3322197198867798\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10484/12000 =  87.37 % ||| loss 0.35046058893203735\u001b[0m\n",
            "\u001b[92mTest accuracy: 8666/10000 =  86.66 % ||| loss 0.36983776092529297\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.43535603523254396\n",
            "Batch #200 Loss: 0.4031131701171398\n",
            "Batch #300 Loss: 0.41341581761837004\n",
            "\u001b[92mTrain accuracy: 42209/48000 =  87.94 % ||| loss 0.322590172290802\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10486/12000 =  87.38 % ||| loss 0.34311217069625854\u001b[0m\n",
            "\u001b[92mTest accuracy: 8651/10000 =  86.51 % ||| loss 0.3625119924545288\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.39820974335074427\n",
            "Batch #200 Loss: 0.40797225773334506\n",
            "Batch #300 Loss: 0.3938405938446522\n",
            "\u001b[92mTrain accuracy: 41972/48000 =  87.44 % ||| loss 0.3302638828754425\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10422/12000 =  86.85 % ||| loss 0.35372698307037354\u001b[0m\n",
            "\u001b[92mTest accuracy: 8594/10000 =  85.94 % ||| loss 0.3752065598964691\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.39024595469236373\n",
            "Batch #200 Loss: 0.3930117291212082\n",
            "Batch #300 Loss: 0.39335392609238623\n",
            "\u001b[92mTrain accuracy: 42621/48000 =  88.79 % ||| loss 0.3016590178012848\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10517/12000 =  87.64 % ||| loss 0.3326311409473419\u001b[0m\n",
            "\u001b[92mTest accuracy: 8722/10000 =  87.22 % ||| loss 0.34843865036964417\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3884205511212349\n",
            "Batch #200 Loss: 0.3869799360632896\n",
            "Batch #300 Loss: 0.3803930488228798\n",
            "\u001b[92mTrain accuracy: 42380/48000 =  88.29 % ||| loss 0.3048948645591736\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10485/12000 =  87.38 % ||| loss 0.336700439453125\u001b[0m\n",
            "\u001b[92mTest accuracy: 8686/10000 =  86.86 % ||| loss 0.35424304008483887\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.3761486205458641\n",
            "Batch #200 Loss: 0.3730251060426235\n",
            "Batch #300 Loss: 0.37136042192578317\n",
            "\u001b[92mTrain accuracy: 42556/48000 =  88.66 % ||| loss 0.30371004343032837\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10491/12000 =  87.42 % ||| loss 0.33602482080459595\u001b[0m\n",
            "\u001b[92mTest accuracy: 8719/10000 =  87.19 % ||| loss 0.3587411940097809\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3726439429819584\n",
            "Batch #200 Loss: 0.36721627324819567\n",
            "Batch #300 Loss: 0.3752529962360859\n",
            "\u001b[92mTrain accuracy: 42494/48000 =  88.53 % ||| loss 0.29233604669570923\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10472/12000 =  87.27 % ||| loss 0.3291061222553253\u001b[0m\n",
            "\u001b[92mTest accuracy: 8676/10000 =  86.76 % ||| loss 0.3536645770072937\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.3615133309364319\n",
            "Batch #200 Loss: 0.36154072314500807\n",
            "Batch #300 Loss: 0.3542130844295025\n",
            "\u001b[92mTrain accuracy: 42725/48000 =  89.01 % ||| loss 0.28836753964424133\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10529/12000 =  87.74 % ||| loss 0.32503315806388855\u001b[0m\n",
            "\u001b[92mTest accuracy: 8741/10000 =  87.41 % ||| loss 0.3417753577232361\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.35721696361899374\n",
            "Batch #200 Loss: 0.35492858320474624\n",
            "Batch #300 Loss: 0.3490393078327179\n",
            "\u001b[92mTrain accuracy: 42400/48000 =  88.33 % ||| loss 0.2983660101890564\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10413/12000 =  86.78 % ||| loss 0.33683162927627563\u001b[0m\n",
            "\u001b[92mTest accuracy: 8666/10000 =  86.66 % ||| loss 0.361018568277359\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.34498923987150193\n",
            "Batch #200 Loss: 0.34963950142264366\n",
            "Batch #300 Loss: 0.34993869334459304\n",
            "\u001b[92mTrain accuracy: 42965/48000 =  89.51 % ||| loss 0.2800138294696808\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10554/12000 =  87.95 % ||| loss 0.31912219524383545\u001b[0m\n",
            "\u001b[92mTest accuracy: 8758/10000 =  87.58 % ||| loss 0.3398600220680237\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3478106126189232\n",
            "Batch #200 Loss: 0.3431164103746414\n",
            "Batch #300 Loss: 0.3446564300358295\n",
            "\u001b[92mTrain accuracy: 42918/48000 =  89.41 % ||| loss 0.2741536796092987\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10555/12000 =  87.96 % ||| loss 0.3196234405040741\u001b[0m\n",
            "\u001b[92mTest accuracy: 8785/10000 =  87.85 % ||| loss 0.33273133635520935\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3363139574229717\n",
            "Batch #200 Loss: 0.34277472957968713\n",
            "Batch #300 Loss: 0.3519556026160717\n",
            "\u001b[92mTrain accuracy: 43196/48000 =  89.99 % ||| loss 0.26336631178855896\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10589/12000 =  88.24 % ||| loss 0.31436505913734436\u001b[0m\n",
            "\u001b[92mTest accuracy: 8823/10000 =  88.23 % ||| loss 0.330694317817688\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3448819477856159\n",
            "Batch #200 Loss: 0.33885430082678797\n",
            "Batch #300 Loss: 0.3438505493104458\n",
            "\u001b[92mTrain accuracy: 43158/48000 =  89.91 % ||| loss 0.2608911991119385\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10628/12000 =  88.57 % ||| loss 0.30902713537216187\u001b[0m\n",
            "\u001b[92mTest accuracy: 8802/10000 =  88.02 % ||| loss 0.3267463743686676\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.344547483175993\n",
            "Batch #200 Loss: 0.33212207406759264\n",
            "Batch #300 Loss: 0.3329115377366543\n",
            "\u001b[92mTrain accuracy: 43239/48000 =  90.08 % ||| loss 0.2603358328342438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10579/12000 =  88.16 % ||| loss 0.3147122859954834\u001b[0m\n",
            "\u001b[92mTest accuracy: 8819/10000 =  88.19 % ||| loss 0.3325057923793793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.3241700042784214\n",
            "Batch #200 Loss: 0.34207134410738943\n",
            "Batch #300 Loss: 0.33579143434762954\n",
            "\u001b[92mTrain accuracy: 43417/48000 =  90.45 % ||| loss 0.25270140171051025\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10601/12000 =  88.34 % ||| loss 0.3032568395137787\u001b[0m\n",
            "\u001b[92mTest accuracy: 8844/10000 =  88.44 % ||| loss 0.3251321017742157\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.3266533049941063\n",
            "Batch #200 Loss: 0.3351765863597393\n",
            "Batch #300 Loss: 0.3325442823767662\n",
            "\u001b[92mTrain accuracy: 43347/48000 =  90.31 % ||| loss 0.25188741087913513\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10618/12000 =  88.48 % ||| loss 0.30706390738487244\u001b[0m\n",
            "\u001b[92mTest accuracy: 8819/10000 =  88.19 % ||| loss 0.32340437173843384\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.3271294642984867\n",
            "Batch #200 Loss: 0.33102618008852003\n",
            "Batch #300 Loss: 0.32498985931277274\n",
            "\u001b[92mTrain accuracy: 43456/48000 =  90.53 % ||| loss 0.24655188620090485\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10634/12000 =  88.62 % ||| loss 0.3019894063472748\u001b[0m\n",
            "\u001b[92mTest accuracy: 8837/10000 =  88.37 % ||| loss 0.32415449619293213\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.32034130111336706\n",
            "Batch #200 Loss: 0.3263895556330681\n",
            "Batch #300 Loss: 0.33446722775697707\n",
            "\u001b[92mTrain accuracy: 43479/48000 =  90.58 % ||| loss 0.24834850430488586\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10665/12000 =  88.88 % ||| loss 0.30242449045181274\u001b[0m\n",
            "\u001b[92mTest accuracy: 8839/10000 =  88.39 % ||| loss 0.3217719495296478\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.31982306808233263\n",
            "Batch #200 Loss: 0.31699233651161196\n",
            "Batch #300 Loss: 0.332330275028944\n",
            "\u001b[92mTrain accuracy: 43353/48000 =  90.32 % ||| loss 0.247536763548851\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10626/12000 =  88.55 % ||| loss 0.3046383857727051\u001b[0m\n",
            "\u001b[92mTest accuracy: 8824/10000 =  88.24 % ||| loss 0.33029016852378845\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.32280726701021195\n",
            "Batch #200 Loss: 0.3109368559718132\n",
            "Batch #300 Loss: 0.33873994544148445\n",
            "\u001b[92mTrain accuracy: 43356/48000 =  90.33 % ||| loss 0.2512020170688629\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10587/12000 =  88.22 % ||| loss 0.30920857191085815\u001b[0m\n",
            "\u001b[92mTest accuracy: 8797/10000 =  87.97 % ||| loss 0.33180785179138184\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_8</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_8</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_174124-Lenet5Dropout_1726089678.067411_9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_9' target=\"_blank\">Lenet5Dropout_1726089678.067411_9</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_9' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3033336663246153\n",
            "Batch #200 Loss: 2.3000900554656982\n",
            "Batch #300 Loss: 2.2965333342552183\n",
            "\u001b[92mTrain accuracy: 11771/48000 =  24.52 % ||| loss 2.2855422496795654\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2906/12000 =  24.22 % ||| loss 2.2860493659973145\u001b[0m\n",
            "\u001b[92mTest accuracy: 2465/10000 =  24.65 % ||| loss 2.2855279445648193\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.279065420627594\n",
            "Batch #200 Loss: 2.248549163341522\n",
            "Batch #300 Loss: 2.1449309968948365\n",
            "\u001b[92mTrain accuracy: 27577/48000 =  57.45 % ||| loss 1.5830230712890625\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6904/12000 =  57.53 % ||| loss 1.583611011505127\u001b[0m\n",
            "\u001b[92mTest accuracy: 5675/10000 =  56.75 % ||| loss 1.5841600894927979\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.444243037700653\n",
            "Batch #200 Loss: 1.208057553768158\n",
            "Batch #300 Loss: 1.1027187240123748\n",
            "\u001b[92mTrain accuracy: 30659/48000 =  63.87 % ||| loss 0.9635626673698425\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7658/12000 =  63.82 % ||| loss 0.9529137015342712\u001b[0m\n",
            "\u001b[92mTest accuracy: 6333/10000 =  63.33 % ||| loss 0.9726263880729675\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.02159452855587\n",
            "Batch #200 Loss: 0.9869830983877182\n",
            "Batch #300 Loss: 0.9745002442598343\n",
            "\u001b[92mTrain accuracy: 32212/48000 =  67.11 % ||| loss 0.8563713431358337\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8085/12000 =  67.38 % ||| loss 0.8465654253959656\u001b[0m\n",
            "\u001b[92mTest accuracy: 6649/10000 =  66.49 % ||| loss 0.867885172367096\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.922961307168007\n",
            "Batch #200 Loss: 0.9110740506649018\n",
            "Batch #300 Loss: 0.877885012626648\n",
            "\u001b[92mTrain accuracy: 33608/48000 =  70.02 % ||| loss 0.790911078453064\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8408/12000 =  70.07 % ||| loss 0.7793522477149963\u001b[0m\n",
            "\u001b[92mTest accuracy: 6928/10000 =  69.28 % ||| loss 0.8070636987686157\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.8468022042512894\n",
            "Batch #200 Loss: 0.8375922346115112\n",
            "Batch #300 Loss: 0.8352007853984833\n",
            "\u001b[92mTrain accuracy: 34293/48000 =  71.44 % ||| loss 0.7474315762519836\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8619/12000 =  71.83 % ||| loss 0.736897885799408\u001b[0m\n",
            "\u001b[92mTest accuracy: 7094/10000 =  70.94 % ||| loss 0.7627863883972168\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.8096512299776077\n",
            "Batch #200 Loss: 0.8047354120016098\n",
            "Batch #300 Loss: 0.7922223782539368\n",
            "\u001b[92mTrain accuracy: 34728/48000 =  72.35 % ||| loss 0.7264437079429626\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8707/12000 =  72.56 % ||| loss 0.7172530889511108\u001b[0m\n",
            "\u001b[92mTest accuracy: 7178/10000 =  71.78 % ||| loss 0.751808762550354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.7802060067653656\n",
            "Batch #200 Loss: 0.7613330864906311\n",
            "Batch #300 Loss: 0.7727755630016326\n",
            "\u001b[92mTrain accuracy: 35305/48000 =  73.55 % ||| loss 0.6913468241691589\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8818/12000 =  73.48 % ||| loss 0.683228611946106\u001b[0m\n",
            "\u001b[92mTest accuracy: 7256/10000 =  72.56 % ||| loss 0.7146144509315491\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.756574639081955\n",
            "Batch #200 Loss: 0.7310888952016831\n",
            "Batch #300 Loss: 0.742112804055214\n",
            "\u001b[92mTrain accuracy: 35738/48000 =  74.45 % ||| loss 0.6627777218818665\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8924/12000 =  74.37 % ||| loss 0.654813826084137\u001b[0m\n",
            "\u001b[92mTest accuracy: 7387/10000 =  73.87 % ||| loss 0.6875038743019104\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.7231416475772857\n",
            "Batch #200 Loss: 0.7076030880212784\n",
            "Batch #300 Loss: 0.7204091811180114\n",
            "\u001b[92mTrain accuracy: 36022/48000 =  75.05 % ||| loss 0.6441141963005066\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9011/12000 =  75.09 % ||| loss 0.6356287002563477\u001b[0m\n",
            "\u001b[92mTest accuracy: 7458/10000 =  74.58 % ||| loss 0.6704204678535461\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.713810317516327\n",
            "Batch #200 Loss: 0.6845561593770981\n",
            "Batch #300 Loss: 0.6804330235719681\n",
            "\u001b[92mTrain accuracy: 36668/48000 =  76.39 % ||| loss 0.6260586977005005\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9146/12000 =  76.22 % ||| loss 0.6173028945922852\u001b[0m\n",
            "\u001b[92mTest accuracy: 7568/10000 =  75.68 % ||| loss 0.6543204188346863\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.6870190918445587\n",
            "Batch #200 Loss: 0.6756973278522491\n",
            "Batch #300 Loss: 0.6636548554897308\n",
            "\u001b[92mTrain accuracy: 36715/48000 =  76.49 % ||| loss 0.6140803694725037\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9184/12000 =  76.53 % ||| loss 0.6043218374252319\u001b[0m\n",
            "\u001b[92mTest accuracy: 7572/10000 =  75.72 % ||| loss 0.6382108926773071\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.6553704783320426\n",
            "Batch #200 Loss: 0.6551723036170006\n",
            "Batch #300 Loss: 0.6612084871530532\n",
            "\u001b[92mTrain accuracy: 37142/48000 =  77.38 % ||| loss 0.5862313508987427\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9254/12000 =  77.12 % ||| loss 0.5787085890769958\u001b[0m\n",
            "\u001b[92mTest accuracy: 7638/10000 =  76.38 % ||| loss 0.6155941486358643\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.627114631831646\n",
            "Batch #200 Loss: 0.6465347722172737\n",
            "Batch #300 Loss: 0.6300925549864769\n",
            "\u001b[92mTrain accuracy: 37424/48000 =  77.97 % ||| loss 0.5797141194343567\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9377/12000 =  78.14 % ||| loss 0.5712782144546509\u001b[0m\n",
            "\u001b[92mTest accuracy: 7691/10000 =  76.91 % ||| loss 0.6064532995223999\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.6214173918962479\n",
            "Batch #200 Loss: 0.6323800280690193\n",
            "Batch #300 Loss: 0.6113450941443443\n",
            "\u001b[92mTrain accuracy: 38231/48000 =  79.65 % ||| loss 0.5601771473884583\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9574/12000 =  79.78 % ||| loss 0.5531440377235413\u001b[0m\n",
            "\u001b[92mTest accuracy: 7881/10000 =  78.81 % ||| loss 0.5825791358947754\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.6066962295770645\n",
            "Batch #200 Loss: 0.6029107066988945\n",
            "Batch #300 Loss: 0.6103128865361214\n",
            "\u001b[92mTrain accuracy: 38370/48000 =  79.94 % ||| loss 0.5426020622253418\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9604/12000 =  80.03 % ||| loss 0.5365587472915649\u001b[0m\n",
            "\u001b[92mTest accuracy: 7906/10000 =  79.06 % ||| loss 0.572891116142273\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.6024679327011109\n",
            "Batch #200 Loss: 0.5803023958206177\n",
            "Batch #300 Loss: 0.5843106347322464\n",
            "\u001b[92mTrain accuracy: 38311/48000 =  79.81 % ||| loss 0.5361172556877136\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9562/12000 =  79.68 % ||| loss 0.5319141149520874\u001b[0m\n",
            "\u001b[92mTest accuracy: 7888/10000 =  78.88 % ||| loss 0.5592582821846008\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5766226586699486\n",
            "Batch #200 Loss: 0.5778282427787781\n",
            "Batch #300 Loss: 0.5720153689384461\n",
            "\u001b[92mTrain accuracy: 38519/48000 =  80.25 % ||| loss 0.5232598185539246\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9632/12000 =  80.27 % ||| loss 0.5173164010047913\u001b[0m\n",
            "\u001b[92mTest accuracy: 7946/10000 =  79.46 % ||| loss 0.5499590039253235\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5734050390124321\n",
            "Batch #200 Loss: 0.5594888392090798\n",
            "Batch #300 Loss: 0.5622439256310463\n",
            "\u001b[92mTrain accuracy: 39168/48000 =  81.6 % ||| loss 0.5019789338111877\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9805/12000 =  81.71 % ||| loss 0.4977533519268036\u001b[0m\n",
            "\u001b[92mTest accuracy: 8075/10000 =  80.75 % ||| loss 0.5330013036727905\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.5515080198645592\n",
            "Batch #200 Loss: 0.5516698700189591\n",
            "Batch #300 Loss: 0.5536371096968651\n",
            "\u001b[92mTrain accuracy: 39205/48000 =  81.68 % ||| loss 0.49518343806266785\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9852/12000 =  82.1 % ||| loss 0.49136441946029663\u001b[0m\n",
            "\u001b[92mTest accuracy: 8086/10000 =  80.86 % ||| loss 0.5254672169685364\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.5451167500019074\n",
            "Batch #200 Loss: 0.542669115960598\n",
            "Batch #300 Loss: 0.5375603243708611\n",
            "\u001b[92mTrain accuracy: 39395/48000 =  82.07 % ||| loss 0.48510441184043884\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9871/12000 =  82.26 % ||| loss 0.4837488532066345\u001b[0m\n",
            "\u001b[92mTest accuracy: 8130/10000 =  81.3 % ||| loss 0.5123406648635864\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.5296454748511314\n",
            "Batch #200 Loss: 0.5272028067708016\n",
            "Batch #300 Loss: 0.5279626575112343\n",
            "\u001b[92mTrain accuracy: 39529/48000 =  82.35 % ||| loss 0.4743121266365051\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9906/12000 =  82.55 % ||| loss 0.4730384945869446\u001b[0m\n",
            "\u001b[92mTest accuracy: 8142/10000 =  81.42 % ||| loss 0.5018807649612427\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5264869654178619\n",
            "Batch #200 Loss: 0.5203361511230469\n",
            "Batch #300 Loss: 0.5273924362659455\n",
            "\u001b[92mTrain accuracy: 39253/48000 =  81.78 % ||| loss 0.47742587327957153\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9845/12000 =  82.04 % ||| loss 0.4755832552909851\u001b[0m\n",
            "\u001b[92mTest accuracy: 8092/10000 =  80.92 % ||| loss 0.506488561630249\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5043094128370285\n",
            "Batch #200 Loss: 0.507749179303646\n",
            "Batch #300 Loss: 0.5149119332432747\n",
            "\u001b[92mTrain accuracy: 39701/48000 =  82.71 % ||| loss 0.46612146496772766\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9983/12000 =  83.19 % ||| loss 0.4662746787071228\u001b[0m\n",
            "\u001b[92mTest accuracy: 8236/10000 =  82.36 % ||| loss 0.4976344108581543\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5098640438914299\n",
            "Batch #200 Loss: 0.49907590210437774\n",
            "Batch #300 Loss: 0.4966747862100601\n",
            "\u001b[92mTrain accuracy: 40072/48000 =  83.48 % ||| loss 0.4511016309261322\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10030/12000 =  83.58 % ||| loss 0.4525686502456665\u001b[0m\n",
            "\u001b[92mTest accuracy: 8262/10000 =  82.62 % ||| loss 0.4792870581150055\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_9</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_9' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_9</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_174320-Lenet5Dropout_1726089678.067411_10</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_10' target=\"_blank\">Lenet5Dropout_1726089678.067411_10</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_10' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_10</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3029132890701294\n",
            "Batch #200 Loss: 2.2947415924072265\n",
            "Batch #300 Loss: 2.2840937423706054\n",
            "\u001b[92mTrain accuracy: 12659/48000 =  26.37 % ||| loss 2.2423694133758545\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3215/12000 =  26.79 % ||| loss 2.2415082454681396\u001b[0m\n",
            "\u001b[92mTest accuracy: 2637/10000 =  26.37 % ||| loss 2.2426230907440186\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2176147150993346\n",
            "Batch #200 Loss: 2.043466320037842\n",
            "Batch #300 Loss: 1.659579142332077\n",
            "\u001b[92mTrain accuracy: 26146/48000 =  54.47 % ||| loss 1.1869004964828491\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6533/12000 =  54.44 % ||| loss 1.1847658157348633\u001b[0m\n",
            "\u001b[92mTest accuracy: 5452/10000 =  54.52 % ||| loss 1.1945688724517822\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.270567148923874\n",
            "Batch #200 Loss: 1.1931154203414918\n",
            "Batch #300 Loss: 1.1311702412366866\n",
            "\u001b[92mTrain accuracy: 29571/48000 =  61.61 % ||| loss 0.9632584452629089\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7414/12000 =  61.78 % ||| loss 0.9558082222938538\u001b[0m\n",
            "\u001b[92mTest accuracy: 6085/10000 =  60.85 % ||| loss 0.9725015163421631\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.073018221259117\n",
            "Batch #200 Loss: 1.0251739621162415\n",
            "Batch #300 Loss: 1.0107758903503419\n",
            "\u001b[92mTrain accuracy: 32444/48000 =  67.59 % ||| loss 0.8516312837600708\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8134/12000 =  67.78 % ||| loss 0.8418756127357483\u001b[0m\n",
            "\u001b[92mTest accuracy: 6743/10000 =  67.43 % ||| loss 0.8644113540649414\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.9580113226175309\n",
            "Batch #200 Loss: 0.9313018697500229\n",
            "Batch #300 Loss: 0.9212013584375381\n",
            "\u001b[92mTrain accuracy: 33467/48000 =  69.72 % ||| loss 0.7950655221939087\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8364/12000 =  69.7 % ||| loss 0.7844067215919495\u001b[0m\n",
            "\u001b[92mTest accuracy: 6934/10000 =  69.34 % ||| loss 0.8066117763519287\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.877149897813797\n",
            "Batch #200 Loss: 0.8533775687217713\n",
            "Batch #300 Loss: 0.8616653960943222\n",
            "\u001b[92mTrain accuracy: 34489/48000 =  71.85 % ||| loss 0.7395467162132263\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8622/12000 =  71.85 % ||| loss 0.7303537130355835\u001b[0m\n",
            "\u001b[92mTest accuracy: 7105/10000 =  71.05 % ||| loss 0.7579060792922974\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.8430499625205994\n",
            "Batch #200 Loss: 0.8198524463176727\n",
            "Batch #300 Loss: 0.8264945131540299\n",
            "\u001b[92mTrain accuracy: 35024/48000 =  72.97 % ||| loss 0.7007746696472168\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8785/12000 =  73.21 % ||| loss 0.6883155107498169\u001b[0m\n",
            "\u001b[92mTest accuracy: 7281/10000 =  72.81 % ||| loss 0.7166932225227356\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.7972637414932251\n",
            "Batch #200 Loss: 0.7796836900711059\n",
            "Batch #300 Loss: 0.773636269569397\n",
            "\u001b[92mTrain accuracy: 35691/48000 =  74.36 % ||| loss 0.6665838360786438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8937/12000 =  74.48 % ||| loss 0.657008707523346\u001b[0m\n",
            "\u001b[92mTest accuracy: 7365/10000 =  73.65 % ||| loss 0.6917227506637573\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.7608678913116456\n",
            "Batch #200 Loss: 0.7531255847215652\n",
            "Batch #300 Loss: 0.7312376320362091\n",
            "\u001b[92mTrain accuracy: 36262/48000 =  75.55 % ||| loss 0.6463236212730408\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9049/12000 =  75.41 % ||| loss 0.6373091340065002\u001b[0m\n",
            "\u001b[92mTest accuracy: 7455/10000 =  74.55 % ||| loss 0.6734639406204224\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.7241261565685272\n",
            "Batch #200 Loss: 0.719996423125267\n",
            "Batch #300 Loss: 0.7135417848825455\n",
            "\u001b[92mTrain accuracy: 36499/48000 =  76.04 % ||| loss 0.6194204688072205\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9103/12000 =  75.86 % ||| loss 0.6108919382095337\u001b[0m\n",
            "\u001b[92mTest accuracy: 7524/10000 =  75.24 % ||| loss 0.6418406367301941\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6967963379621506\n",
            "Batch #200 Loss: 0.7072352087497711\n",
            "Batch #300 Loss: 0.6768141663074494\n",
            "\u001b[92mTrain accuracy: 36920/48000 =  76.92 % ||| loss 0.5974651575088501\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9232/12000 =  76.93 % ||| loss 0.5892859697341919\u001b[0m\n",
            "\u001b[92mTest accuracy: 7635/10000 =  76.35 % ||| loss 0.6181882619857788\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.6785139816999436\n",
            "Batch #200 Loss: 0.6801334807276725\n",
            "Batch #300 Loss: 0.6772919410467148\n",
            "\u001b[92mTrain accuracy: 37282/48000 =  77.67 % ||| loss 0.5787355303764343\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9330/12000 =  77.75 % ||| loss 0.5713376402854919\u001b[0m\n",
            "\u001b[92mTest accuracy: 7712/10000 =  77.12 % ||| loss 0.6036466360092163\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.6606628841161728\n",
            "Batch #200 Loss: 0.6614967319369316\n",
            "Batch #300 Loss: 0.6623172068595886\n",
            "\u001b[92mTrain accuracy: 37159/48000 =  77.41 % ||| loss 0.5721452832221985\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9290/12000 =  77.42 % ||| loss 0.5629594326019287\u001b[0m\n",
            "\u001b[92mTest accuracy: 7680/10000 =  76.8 % ||| loss 0.5924222469329834\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.6468360581994057\n",
            "Batch #200 Loss: 0.6507873430848121\n",
            "Batch #300 Loss: 0.6370333224534989\n",
            "\u001b[92mTrain accuracy: 37323/48000 =  77.76 % ||| loss 0.5596993565559387\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9319/12000 =  77.66 % ||| loss 0.5554898977279663\u001b[0m\n",
            "\u001b[92mTest accuracy: 7680/10000 =  76.8 % ||| loss 0.5843344330787659\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.6339007973670959\n",
            "Batch #200 Loss: 0.6313261356949806\n",
            "Batch #300 Loss: 0.6211991992592811\n",
            "\u001b[92mTrain accuracy: 37820/48000 =  78.79 % ||| loss 0.5467494130134583\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9438/12000 =  78.65 % ||| loss 0.5423646569252014\u001b[0m\n",
            "\u001b[92mTest accuracy: 7801/10000 =  78.01 % ||| loss 0.5678156018257141\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.6138845026493073\n",
            "Batch #200 Loss: 0.6252817314863205\n",
            "Batch #300 Loss: 0.6170734831690788\n",
            "\u001b[92mTrain accuracy: 37878/48000 =  78.91 % ||| loss 0.5308319330215454\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9487/12000 =  79.06 % ||| loss 0.526850163936615\u001b[0m\n",
            "\u001b[92mTest accuracy: 7833/10000 =  78.33 % ||| loss 0.5539795160293579\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.58705685287714\n",
            "Batch #200 Loss: 0.6128855666518211\n",
            "Batch #300 Loss: 0.6190769571065903\n",
            "\u001b[92mTrain accuracy: 38384/48000 =  79.97 % ||| loss 0.5240378975868225\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9635/12000 =  80.29 % ||| loss 0.5183243751525879\u001b[0m\n",
            "\u001b[92mTest accuracy: 7910/10000 =  79.1 % ||| loss 0.5520623922348022\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5977791357040405\n",
            "Batch #200 Loss: 0.5954167944192886\n",
            "Batch #300 Loss: 0.5819996348023415\n",
            "\u001b[92mTrain accuracy: 38336/48000 =  79.87 % ||| loss 0.512661337852478\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9572/12000 =  79.77 % ||| loss 0.5103381872177124\u001b[0m\n",
            "\u001b[92mTest accuracy: 7877/10000 =  78.77 % ||| loss 0.5399643182754517\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5885585606098175\n",
            "Batch #200 Loss: 0.5755491665005684\n",
            "Batch #300 Loss: 0.5848014554381371\n",
            "\u001b[92mTrain accuracy: 38350/48000 =  79.9 % ||| loss 0.5178772807121277\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9563/12000 =  79.69 % ||| loss 0.5177215337753296\u001b[0m\n",
            "\u001b[92mTest accuracy: 7878/10000 =  78.78 % ||| loss 0.544735848903656\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.5806709605455399\n",
            "Batch #200 Loss: 0.5668926295638085\n",
            "Batch #300 Loss: 0.5821418136358261\n",
            "\u001b[92mTrain accuracy: 38729/48000 =  80.69 % ||| loss 0.5012782216072083\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9701/12000 =  80.84 % ||| loss 0.5003088116645813\u001b[0m\n",
            "\u001b[92mTest accuracy: 7952/10000 =  79.52 % ||| loss 0.533630907535553\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.5773521900177002\n",
            "Batch #200 Loss: 0.5506555193662643\n",
            "Batch #300 Loss: 0.5627045154571533\n",
            "\u001b[92mTrain accuracy: 38988/48000 =  81.23 % ||| loss 0.49143606424331665\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9765/12000 =  81.38 % ||| loss 0.4877419173717499\u001b[0m\n",
            "\u001b[92mTest accuracy: 8050/10000 =  80.5 % ||| loss 0.5248656868934631\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.5601327642798424\n",
            "Batch #200 Loss: 0.5492866361141204\n",
            "Batch #300 Loss: 0.5758523389697074\n",
            "\u001b[92mTrain accuracy: 39270/48000 =  81.81 % ||| loss 0.4869764745235443\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9867/12000 =  82.23 % ||| loss 0.4847807288169861\u001b[0m\n",
            "\u001b[92mTest accuracy: 8111/10000 =  81.11 % ||| loss 0.516706109046936\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5490375334024429\n",
            "Batch #200 Loss: 0.5483709660172462\n",
            "Batch #300 Loss: 0.5550344321131706\n",
            "\u001b[92mTrain accuracy: 39318/48000 =  81.91 % ||| loss 0.48155659437179565\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9820/12000 =  81.83 % ||| loss 0.4827979505062103\u001b[0m\n",
            "\u001b[92mTest accuracy: 8043/10000 =  80.43 % ||| loss 0.5107240676879883\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5358828935027122\n",
            "Batch #200 Loss: 0.5422131204605103\n",
            "Batch #300 Loss: 0.5526785814762115\n",
            "\u001b[92mTrain accuracy: 39680/48000 =  82.67 % ||| loss 0.4708145260810852\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9913/12000 =  82.61 % ||| loss 0.4724936783313751\u001b[0m\n",
            "\u001b[92mTest accuracy: 8142/10000 =  81.42 % ||| loss 0.501754641532898\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5258701059222222\n",
            "Batch #200 Loss: 0.5352379184961319\n",
            "Batch #300 Loss: 0.5401348066329956\n",
            "\u001b[92mTrain accuracy: 39748/48000 =  82.81 % ||| loss 0.4642903208732605\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9921/12000 =  82.67 % ||| loss 0.46773844957351685\u001b[0m\n",
            "\u001b[92mTest accuracy: 8168/10000 =  81.68 % ||| loss 0.4970511794090271\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_10</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_10' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_10</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_174517-Lenet5Dropout_1726089678.067411_11</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_11' target=\"_blank\">Lenet5Dropout_1726089678.067411_11</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_11' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302472746372223\n",
            "Batch #200 Loss: 2.3026048278808595\n",
            "Batch #300 Loss: 2.301436734199524\n",
            "\u001b[92mTrain accuracy: 7857/48000 =  16.37 % ||| loss 2.2986011505126953\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1992/12000 =  16.6 % ||| loss 2.29844331741333\u001b[0m\n",
            "\u001b[92mTest accuracy: 1638/10000 =  16.38 % ||| loss 2.298693895339966\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2988020014762878\n",
            "Batch #200 Loss: 2.2968963861465452\n",
            "Batch #300 Loss: 2.2943359184265137\n",
            "\u001b[92mTrain accuracy: 13504/48000 =  28.13 % ||| loss 2.2874248027801514\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3348/12000 =  27.9 % ||| loss 2.287351131439209\u001b[0m\n",
            "\u001b[92mTest accuracy: 2816/10000 =  28.16 % ||| loss 2.28755784034729\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.286556158065796\n",
            "Batch #200 Loss: 2.27801442861557\n",
            "Batch #300 Loss: 2.2607378649711607\n",
            "\u001b[92mTrain accuracy: 21371/48000 =  44.52 % ||| loss 2.1772170066833496\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5315/12000 =  44.29 % ||| loss 2.1776182651519775\u001b[0m\n",
            "\u001b[92mTest accuracy: 4447/10000 =  44.47 % ||| loss 2.1778907775878906\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.1273396492004393\n",
            "Batch #200 Loss: 1.8367696678638459\n",
            "Batch #300 Loss: 1.5688823580741882\n",
            "\u001b[92mTrain accuracy: 26980/48000 =  56.21 % ||| loss 1.1720836162567139\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6765/12000 =  56.38 % ||| loss 1.1672217845916748\u001b[0m\n",
            "\u001b[92mTest accuracy: 5636/10000 =  56.36 % ||| loss 1.1801069974899292\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 1.3127783489227296\n",
            "Batch #200 Loss: 1.2648205280303955\n",
            "Batch #300 Loss: 1.2104690229892732\n",
            "\u001b[92mTrain accuracy: 30796/48000 =  64.16 % ||| loss 0.9885024428367615\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7753/12000 =  64.61 % ||| loss 0.9814542531967163\u001b[0m\n",
            "\u001b[92mTest accuracy: 6326/10000 =  63.26 % ||| loss 1.0009814500808716\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 1.1369377732276917\n",
            "Batch #200 Loss: 1.087455130815506\n",
            "Batch #300 Loss: 1.0667861449718474\n",
            "\u001b[92mTrain accuracy: 31430/48000 =  65.48 % ||| loss 0.8760126233100891\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7878/12000 =  65.65 % ||| loss 0.8663883805274963\u001b[0m\n",
            "\u001b[92mTest accuracy: 6464/10000 =  64.64 % ||| loss 0.8913725018501282\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 1.028787423968315\n",
            "Batch #200 Loss: 0.9998360615968704\n",
            "Batch #300 Loss: 0.9723597919940948\n",
            "\u001b[92mTrain accuracy: 32779/48000 =  68.29 % ||| loss 0.8142585158348083\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8230/12000 =  68.58 % ||| loss 0.8044534921646118\u001b[0m\n",
            "\u001b[92mTest accuracy: 6767/10000 =  67.67 % ||| loss 0.8290534615516663\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.9433261835575104\n",
            "Batch #200 Loss: 0.9459473025798798\n",
            "Batch #300 Loss: 0.9475078475475311\n",
            "\u001b[92mTrain accuracy: 33978/48000 =  70.79 % ||| loss 0.7720134854316711\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8567/12000 =  71.39 % ||| loss 0.7619085311889648\u001b[0m\n",
            "\u001b[92mTest accuracy: 7024/10000 =  70.24 % ||| loss 0.7919198870658875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.9060931527614593\n",
            "Batch #200 Loss: 0.8902850788831711\n",
            "Batch #300 Loss: 0.8980190187692643\n",
            "\u001b[92mTrain accuracy: 34380/48000 =  71.62 % ||| loss 0.7407687902450562\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8614/12000 =  71.78 % ||| loss 0.7287288308143616\u001b[0m\n",
            "\u001b[92mTest accuracy: 7105/10000 =  71.05 % ||| loss 0.7538074851036072\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.8815444421768188\n",
            "Batch #200 Loss: 0.8699590104818344\n",
            "Batch #300 Loss: 0.8591828203201294\n",
            "\u001b[92mTrain accuracy: 34809/48000 =  72.52 % ||| loss 0.722741961479187\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8748/12000 =  72.9 % ||| loss 0.712812066078186\u001b[0m\n",
            "\u001b[92mTest accuracy: 7197/10000 =  71.97 % ||| loss 0.7482710480690002\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.8673609614372253\n",
            "Batch #200 Loss: 0.8449609041213989\n",
            "Batch #300 Loss: 0.8339497977495194\n",
            "\u001b[92mTrain accuracy: 35238/48000 =  73.41 % ||| loss 0.7050337791442871\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8835/12000 =  73.62 % ||| loss 0.6950280070304871\u001b[0m\n",
            "\u001b[92mTest accuracy: 7246/10000 =  72.46 % ||| loss 0.7273736000061035\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.8273170316219329\n",
            "Batch #200 Loss: 0.8052308911085129\n",
            "Batch #300 Loss: 0.820013924241066\n",
            "\u001b[92mTrain accuracy: 35787/48000 =  74.56 % ||| loss 0.6789020299911499\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8916/12000 =  74.3 % ||| loss 0.6679213643074036\u001b[0m\n",
            "\u001b[92mTest accuracy: 7369/10000 =  73.69 % ||| loss 0.7021155953407288\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.801889443397522\n",
            "Batch #200 Loss: 0.8101826333999633\n",
            "Batch #300 Loss: 0.7942222547531128\n",
            "\u001b[92mTrain accuracy: 35920/48000 =  74.83 % ||| loss 0.6733044981956482\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8992/12000 =  74.93 % ||| loss 0.6619751453399658\u001b[0m\n",
            "\u001b[92mTest accuracy: 7384/10000 =  73.84 % ||| loss 0.6968737244606018\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.7873735970258713\n",
            "Batch #200 Loss: 0.7975572794675827\n",
            "Batch #300 Loss: 0.7697211086750031\n",
            "\u001b[92mTrain accuracy: 36270/48000 =  75.56 % ||| loss 0.6467458009719849\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9061/12000 =  75.51 % ||| loss 0.6361894607543945\u001b[0m\n",
            "\u001b[92mTest accuracy: 7449/10000 =  74.49 % ||| loss 0.6707471013069153\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.7697851365804672\n",
            "Batch #200 Loss: 0.7715091460943222\n",
            "Batch #300 Loss: 0.7596769684553146\n",
            "\u001b[92mTrain accuracy: 36556/48000 =  76.16 % ||| loss 0.6349536776542664\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9143/12000 =  76.19 % ||| loss 0.6244113445281982\u001b[0m\n",
            "\u001b[92mTest accuracy: 7496/10000 =  74.96 % ||| loss 0.6558730006217957\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.759388097524643\n",
            "Batch #200 Loss: 0.741635730266571\n",
            "Batch #300 Loss: 0.7467172056436538\n",
            "\u001b[92mTrain accuracy: 36798/48000 =  76.66 % ||| loss 0.6218805313110352\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9188/12000 =  76.57 % ||| loss 0.6120228171348572\u001b[0m\n",
            "\u001b[92mTest accuracy: 7575/10000 =  75.75 % ||| loss 0.6472354531288147\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.7461391347646713\n",
            "Batch #200 Loss: 0.7368366849422455\n",
            "Batch #300 Loss: 0.7339555060863495\n",
            "\u001b[92mTrain accuracy: 36874/48000 =  76.82 % ||| loss 0.6073393821716309\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9166/12000 =  76.38 % ||| loss 0.5970910787582397\u001b[0m\n",
            "\u001b[92mTest accuracy: 7577/10000 =  75.77 % ||| loss 0.6324029564857483\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.7270913010835648\n",
            "Batch #200 Loss: 0.7265631806850433\n",
            "Batch #300 Loss: 0.7233750033378601\n",
            "\u001b[92mTrain accuracy: 37000/48000 =  77.08 % ||| loss 0.5979037284851074\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9252/12000 =  77.1 % ||| loss 0.5879108905792236\u001b[0m\n",
            "\u001b[92mTest accuracy: 7604/10000 =  76.04 % ||| loss 0.6198545694351196\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.7146557152271271\n",
            "Batch #200 Loss: 0.7100042057037353\n",
            "Batch #300 Loss: 0.7018693640828133\n",
            "\u001b[92mTrain accuracy: 37035/48000 =  77.16 % ||| loss 0.5910071730613708\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9262/12000 =  77.18 % ||| loss 0.5790852904319763\u001b[0m\n",
            "\u001b[92mTest accuracy: 7617/10000 =  76.17 % ||| loss 0.6154696345329285\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.6973900973796845\n",
            "Batch #200 Loss: 0.687755116224289\n",
            "Batch #300 Loss: 0.6887204539775849\n",
            "\u001b[92mTrain accuracy: 37267/48000 =  77.64 % ||| loss 0.5854339003562927\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9294/12000 =  77.45 % ||| loss 0.5745035409927368\u001b[0m\n",
            "\u001b[92mTest accuracy: 7663/10000 =  76.63 % ||| loss 0.6098347306251526\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.689224348962307\n",
            "Batch #200 Loss: 0.6988590556383133\n",
            "Batch #300 Loss: 0.687230173945427\n",
            "\u001b[92mTrain accuracy: 37413/48000 =  77.94 % ||| loss 0.5729354023933411\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9356/12000 =  77.97 % ||| loss 0.565413236618042\u001b[0m\n",
            "\u001b[92mTest accuracy: 7694/10000 =  76.94 % ||| loss 0.5983196496963501\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.6804582750797272\n",
            "Batch #200 Loss: 0.6855604934692383\n",
            "Batch #300 Loss: 0.6878820741176606\n",
            "\u001b[92mTrain accuracy: 37604/48000 =  78.34 % ||| loss 0.5671143531799316\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9426/12000 =  78.55 % ||| loss 0.5576239228248596\u001b[0m\n",
            "\u001b[92mTest accuracy: 7779/10000 =  77.79 % ||| loss 0.5936154723167419\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.6828471323847771\n",
            "Batch #200 Loss: 0.6544583961367607\n",
            "Batch #300 Loss: 0.6579192027449607\n",
            "\u001b[92mTrain accuracy: 37676/48000 =  78.49 % ||| loss 0.5589329600334167\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9427/12000 =  78.56 % ||| loss 0.5508150458335876\u001b[0m\n",
            "\u001b[92mTest accuracy: 7786/10000 =  77.86 % ||| loss 0.5874406099319458\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.6583363074064255\n",
            "Batch #200 Loss: 0.6577212715148926\n",
            "Batch #300 Loss: 0.6594257712364197\n",
            "\u001b[92mTrain accuracy: 37724/48000 =  78.59 % ||| loss 0.5550901889801025\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9440/12000 =  78.67 % ||| loss 0.5466914176940918\u001b[0m\n",
            "\u001b[92mTest accuracy: 7735/10000 =  77.35 % ||| loss 0.5862848162651062\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.6535035082697869\n",
            "Batch #200 Loss: 0.6539954808354378\n",
            "Batch #300 Loss: 0.6567070457339287\n",
            "\u001b[92mTrain accuracy: 37808/48000 =  78.77 % ||| loss 0.546232283115387\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9423/12000 =  78.53 % ||| loss 0.5391210913658142\u001b[0m\n",
            "\u001b[92mTest accuracy: 7769/10000 =  77.69 % ||| loss 0.5773168206214905\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_11</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_11' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_11</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_174709-Lenet5Dropout_1726089678.067411_12</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_12' target=\"_blank\">Lenet5Dropout_1726089678.067411_12</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_12' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_12</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2930842280387878\n",
            "Batch #200 Loss: 1.5487552517652512\n",
            "Batch #300 Loss: 0.9388796639442444\n",
            "\u001b[92mTrain accuracy: 35061/48000 =  73.04 % ||| loss 0.721900463104248\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8779/12000 =  73.16 % ||| loss 0.7107893228530884\u001b[0m\n",
            "\u001b[92mTest accuracy: 7228/10000 =  72.28 % ||| loss 0.7420721054077148\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7322213330864906\n",
            "Batch #200 Loss: 0.6614228868484497\n",
            "Batch #300 Loss: 0.6301858749985695\n",
            "\u001b[92mTrain accuracy: 37769/48000 =  78.69 % ||| loss 0.5376186966896057\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9445/12000 =  78.71 % ||| loss 0.5346426367759705\u001b[0m\n",
            "\u001b[92mTest accuracy: 7821/10000 =  78.21 % ||| loss 0.5604287981987\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5921942186355591\n",
            "Batch #200 Loss: 0.5611498329043388\n",
            "Batch #300 Loss: 0.5532870057225228\n",
            "\u001b[92mTrain accuracy: 39484/48000 =  82.26 % ||| loss 0.47566792368888855\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9887/12000 =  82.39 % ||| loss 0.47583454847335815\u001b[0m\n",
            "\u001b[92mTest accuracy: 8138/10000 =  81.38 % ||| loss 0.49927109479904175\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5165130028128624\n",
            "Batch #200 Loss: 0.4934663346409798\n",
            "Batch #300 Loss: 0.48730927884578706\n",
            "\u001b[92mTrain accuracy: 40282/48000 =  83.92 % ||| loss 0.43435773253440857\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10053/12000 =  83.78 % ||| loss 0.44080644845962524\u001b[0m\n",
            "\u001b[92mTest accuracy: 8305/10000 =  83.05 % ||| loss 0.46818292140960693\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.48013205856084823\n",
            "Batch #200 Loss: 0.4755468130111694\n",
            "Batch #300 Loss: 0.4734055581688881\n",
            "\u001b[92mTrain accuracy: 40836/48000 =  85.08 % ||| loss 0.4023277163505554\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10213/12000 =  85.11 % ||| loss 0.4092690348625183\u001b[0m\n",
            "\u001b[92mTest accuracy: 8424/10000 =  84.24 % ||| loss 0.4305122494697571\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.4399285784363747\n",
            "Batch #200 Loss: 0.4351922994852066\n",
            "Batch #300 Loss: 0.4279395395517349\n",
            "\u001b[92mTrain accuracy: 41502/48000 =  86.46 % ||| loss 0.37231147289276123\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10301/12000 =  85.84 % ||| loss 0.38511496782302856\u001b[0m\n",
            "\u001b[92mTest accuracy: 8517/10000 =  85.17 % ||| loss 0.4093799889087677\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.4239908194541931\n",
            "Batch #200 Loss: 0.4045111009478569\n",
            "Batch #300 Loss: 0.41155424803495405\n",
            "\u001b[92mTrain accuracy: 41513/48000 =  86.49 % ||| loss 0.36573097109794617\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10321/12000 =  86.01 % ||| loss 0.38017845153808594\u001b[0m\n",
            "\u001b[92mTest accuracy: 8520/10000 =  85.2 % ||| loss 0.40419942140579224\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.38999152451753616\n",
            "Batch #200 Loss: 0.3934530732035637\n",
            "Batch #300 Loss: 0.3953230607509613\n",
            "\u001b[92mTrain accuracy: 41650/48000 =  86.77 % ||| loss 0.354361891746521\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10364/12000 =  86.37 % ||| loss 0.3701004683971405\u001b[0m\n",
            "\u001b[92mTest accuracy: 8565/10000 =  85.65 % ||| loss 0.395445853471756\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.39024921745061875\n",
            "Batch #200 Loss: 0.3807776629924774\n",
            "Batch #300 Loss: 0.37493631318211557\n",
            "\u001b[92mTrain accuracy: 42244/48000 =  88.01 % ||| loss 0.3238668143749237\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10466/12000 =  87.22 % ||| loss 0.3440733253955841\u001b[0m\n",
            "\u001b[92mTest accuracy: 8677/10000 =  86.77 % ||| loss 0.3642781674861908\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3624126052856445\n",
            "Batch #200 Loss: 0.35942830085754396\n",
            "Batch #300 Loss: 0.3701349286735058\n",
            "\u001b[92mTrain accuracy: 42236/48000 =  87.99 % ||| loss 0.32555094361305237\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10478/12000 =  87.32 % ||| loss 0.34406059980392456\u001b[0m\n",
            "\u001b[92mTest accuracy: 8653/10000 =  86.53 % ||| loss 0.3676193654537201\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.34202256754040716\n",
            "Batch #200 Loss: 0.35203490242362023\n",
            "Batch #300 Loss: 0.37129950627684594\n",
            "\u001b[92mTrain accuracy: 42619/48000 =  88.79 % ||| loss 0.303608775138855\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10525/12000 =  87.71 % ||| loss 0.32661008834838867\u001b[0m\n",
            "\u001b[92mTest accuracy: 8710/10000 =  87.1 % ||| loss 0.35140275955200195\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3485796415805817\n",
            "Batch #200 Loss: 0.3406668436527252\n",
            "Batch #300 Loss: 0.34615874737501146\n",
            "\u001b[92mTrain accuracy: 42628/48000 =  88.81 % ||| loss 0.30442601442337036\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10549/12000 =  87.91 % ||| loss 0.33063241839408875\u001b[0m\n",
            "\u001b[92mTest accuracy: 8712/10000 =  87.12 % ||| loss 0.36934328079223633\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.33538651809096337\n",
            "Batch #200 Loss: 0.3373477551341057\n",
            "Batch #300 Loss: 0.3422732965648174\n",
            "\u001b[92mTrain accuracy: 42929/48000 =  89.44 % ||| loss 0.2838793992996216\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10625/12000 =  88.54 % ||| loss 0.31489887833595276\u001b[0m\n",
            "\u001b[92mTest accuracy: 8783/10000 =  87.83 % ||| loss 0.3376465141773224\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3191140581667423\n",
            "Batch #200 Loss: 0.34297352224588395\n",
            "Batch #300 Loss: 0.3299823716282845\n",
            "\u001b[92mTrain accuracy: 42692/48000 =  88.94 % ||| loss 0.2910081446170807\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10575/12000 =  88.12 % ||| loss 0.32196497917175293\u001b[0m\n",
            "\u001b[92mTest accuracy: 8754/10000 =  87.54 % ||| loss 0.3439101576805115\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.31869650945067407\n",
            "Batch #200 Loss: 0.33024511322379113\n",
            "Batch #300 Loss: 0.3257869175076485\n",
            "\u001b[92mTrain accuracy: 43137/48000 =  89.87 % ||| loss 0.2732791602611542\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10674/12000 =  88.95 % ||| loss 0.30255529284477234\u001b[0m\n",
            "\u001b[92mTest accuracy: 8797/10000 =  87.97 % ||| loss 0.32186463475227356\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.31415734976530074\n",
            "Batch #200 Loss: 0.33015074297785757\n",
            "Batch #300 Loss: 0.31131956323981286\n",
            "\u001b[92mTrain accuracy: 43119/48000 =  89.83 % ||| loss 0.2741459012031555\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10652/12000 =  88.77 % ||| loss 0.3048225939273834\u001b[0m\n",
            "\u001b[92mTest accuracy: 8814/10000 =  88.14 % ||| loss 0.3313019573688507\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.2872280031442642\n",
            "Batch #200 Loss: 0.3177918566763401\n",
            "Batch #300 Loss: 0.3209675069153309\n",
            "\u001b[92mTrain accuracy: 43280/48000 =  90.17 % ||| loss 0.26284560561180115\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10666/12000 =  88.88 % ||| loss 0.2979816198348999\u001b[0m\n",
            "\u001b[92mTest accuracy: 8834/10000 =  88.34 % ||| loss 0.319252073764801\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3000684042274952\n",
            "Batch #200 Loss: 0.29212609767913816\n",
            "Batch #300 Loss: 0.321818431019783\n",
            "\u001b[92mTrain accuracy: 43440/48000 =  90.5 % ||| loss 0.2594088017940521\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10711/12000 =  89.26 % ||| loss 0.2939660847187042\u001b[0m\n",
            "\u001b[92mTest accuracy: 8859/10000 =  88.59 % ||| loss 0.31733694672584534\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.29119938984513283\n",
            "Batch #200 Loss: 0.2946911269426346\n",
            "Batch #300 Loss: 0.30236887201666834\n",
            "\u001b[92mTrain accuracy: 43306/48000 =  90.22 % ||| loss 0.2621108889579773\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10662/12000 =  88.85 % ||| loss 0.299898236989975\u001b[0m\n",
            "\u001b[92mTest accuracy: 8809/10000 =  88.09 % ||| loss 0.3275253176689148\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.2935183684527874\n",
            "Batch #200 Loss: 0.2948449963331223\n",
            "Batch #300 Loss: 0.2950402289628983\n",
            "\u001b[92mTrain accuracy: 43359/48000 =  90.33 % ||| loss 0.25781145691871643\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10698/12000 =  89.15 % ||| loss 0.29788920283317566\u001b[0m\n",
            "\u001b[92mTest accuracy: 8855/10000 =  88.55 % ||| loss 0.31765127182006836\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.2860978764295578\n",
            "Batch #200 Loss: 0.2815830382704735\n",
            "Batch #300 Loss: 0.28786697432398795\n",
            "\u001b[92mTrain accuracy: 43607/48000 =  90.85 % ||| loss 0.24618695676326752\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10736/12000 =  89.47 % ||| loss 0.2880061864852905\u001b[0m\n",
            "\u001b[92mTest accuracy: 8881/10000 =  88.81 % ||| loss 0.3102038502693176\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.28421167671680453\n",
            "Batch #200 Loss: 0.2759081339836121\n",
            "Batch #300 Loss: 0.29233440697193147\n",
            "\u001b[92mTrain accuracy: 43842/48000 =  91.34 % ||| loss 0.23726046085357666\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10748/12000 =  89.57 % ||| loss 0.2834750711917877\u001b[0m\n",
            "\u001b[92mTest accuracy: 8898/10000 =  88.98 % ||| loss 0.3052055239677429\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.272569967508316\n",
            "Batch #200 Loss: 0.2859946037828922\n",
            "Batch #300 Loss: 0.28859730407595635\n",
            "\u001b[92mTrain accuracy: 43644/48000 =  90.92 % ||| loss 0.24442757666110992\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10722/12000 =  89.35 % ||| loss 0.2912205159664154\u001b[0m\n",
            "\u001b[92mTest accuracy: 8877/10000 =  88.77 % ||| loss 0.31570112705230713\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.26955594822764395\n",
            "Batch #200 Loss: 0.2777319672703743\n",
            "Batch #300 Loss: 0.2807210801541805\n",
            "\u001b[92mTrain accuracy: 43836/48000 =  91.33 % ||| loss 0.2338501513004303\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10741/12000 =  89.51 % ||| loss 0.28838294744491577\u001b[0m\n",
            "\u001b[92mTest accuracy: 8891/10000 =  88.91 % ||| loss 0.3101346492767334\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.2705651395022869\n",
            "Batch #200 Loss: 0.27537287160754204\n",
            "Batch #300 Loss: 0.27319220051169396\n",
            "\u001b[92mTrain accuracy: 43893/48000 =  91.44 % ||| loss 0.23072491586208344\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10719/12000 =  89.33 % ||| loss 0.28516119718551636\u001b[0m\n",
            "\u001b[92mTest accuracy: 8891/10000 =  88.91 % ||| loss 0.30284520983695984\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_12</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_12' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_12</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_174904-Lenet5Dropout_1726089678.067411_13</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_13' target=\"_blank\">Lenet5Dropout_1726089678.067411_13</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_13' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_13</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.292239580154419\n",
            "Batch #200 Loss: 1.6019746100902557\n",
            "Batch #300 Loss: 1.0108595031499863\n",
            "\u001b[92mTrain accuracy: 33830/48000 =  70.48 % ||| loss 0.7268637418746948\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8475/12000 =  70.62 % ||| loss 0.7180624008178711\u001b[0m\n",
            "\u001b[92mTest accuracy: 6982/10000 =  69.82 % ||| loss 0.7426716685295105\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.8081876158714294\n",
            "Batch #200 Loss: 0.740462658405304\n",
            "Batch #300 Loss: 0.705547005534172\n",
            "\u001b[92mTrain accuracy: 37590/48000 =  78.31 % ||| loss 0.5878807902336121\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9444/12000 =  78.7 % ||| loss 0.5822332501411438\u001b[0m\n",
            "\u001b[92mTest accuracy: 7720/10000 =  77.2 % ||| loss 0.6061921119689941\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6438669848442078\n",
            "Batch #200 Loss: 0.6253316137194633\n",
            "Batch #300 Loss: 0.6032554110884667\n",
            "\u001b[92mTrain accuracy: 38123/48000 =  79.42 % ||| loss 0.508217990398407\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9493/12000 =  79.11 % ||| loss 0.5114622116088867\u001b[0m\n",
            "\u001b[92mTest accuracy: 7890/10000 =  78.9 % ||| loss 0.5315969586372375\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5747508931159974\n",
            "Batch #200 Loss: 0.5714432740211487\n",
            "Batch #300 Loss: 0.5472258070111274\n",
            "\u001b[92mTrain accuracy: 39735/48000 =  82.78 % ||| loss 0.45705321431159973\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9939/12000 =  82.83 % ||| loss 0.4630184471607208\u001b[0m\n",
            "\u001b[92mTest accuracy: 8220/10000 =  82.2 % ||| loss 0.48833635449409485\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5382251912355422\n",
            "Batch #200 Loss: 0.5306775921583176\n",
            "Batch #300 Loss: 0.5045824432373047\n",
            "\u001b[92mTrain accuracy: 39717/48000 =  82.74 % ||| loss 0.44788140058517456\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9893/12000 =  82.44 % ||| loss 0.45274636149406433\u001b[0m\n",
            "\u001b[92mTest accuracy: 8150/10000 =  81.5 % ||| loss 0.4736366271972656\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5076368513703347\n",
            "Batch #200 Loss: 0.49606492489576337\n",
            "Batch #300 Loss: 0.4906360426545143\n",
            "\u001b[92mTrain accuracy: 40624/48000 =  84.63 % ||| loss 0.41234251856803894\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10135/12000 =  84.46 % ||| loss 0.41860976815223694\u001b[0m\n",
            "\u001b[92mTest accuracy: 8359/10000 =  83.59 % ||| loss 0.4408165514469147\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.4685989558696747\n",
            "Batch #200 Loss: 0.4713878986239433\n",
            "Batch #300 Loss: 0.46416489124298094\n",
            "\u001b[92mTrain accuracy: 40916/48000 =  85.24 % ||| loss 0.39150965213775635\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10174/12000 =  84.78 % ||| loss 0.4043218195438385\u001b[0m\n",
            "\u001b[92mTest accuracy: 8406/10000 =  84.06 % ||| loss 0.4241895079612732\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.4628486344218254\n",
            "Batch #200 Loss: 0.4545071178674698\n",
            "Batch #300 Loss: 0.4361437597870827\n",
            "\u001b[92mTrain accuracy: 41407/48000 =  86.26 % ||| loss 0.3754979968070984\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10335/12000 =  86.12 % ||| loss 0.3871466815471649\u001b[0m\n",
            "\u001b[92mTest accuracy: 8509/10000 =  85.09 % ||| loss 0.41250351071357727\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.4358536237478256\n",
            "Batch #200 Loss: 0.4249441269040108\n",
            "Batch #300 Loss: 0.43255563780665396\n",
            "\u001b[92mTrain accuracy: 41267/48000 =  85.97 % ||| loss 0.3753332793712616\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10204/12000 =  85.03 % ||| loss 0.3901948034763336\u001b[0m\n",
            "\u001b[92mTest accuracy: 8462/10000 =  84.62 % ||| loss 0.41887298226356506\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.4186761143803597\n",
            "Batch #200 Loss: 0.4093949741125107\n",
            "Batch #300 Loss: 0.4174537932872772\n",
            "\u001b[92mTrain accuracy: 41963/48000 =  87.42 % ||| loss 0.3413477838039398\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10392/12000 =  86.6 % ||| loss 0.35837915539741516\u001b[0m\n",
            "\u001b[92mTest accuracy: 8600/10000 =  86.0 % ||| loss 0.3845081031322479\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.41297462105751037\n",
            "Batch #200 Loss: 0.3898645968735218\n",
            "Batch #300 Loss: 0.4083671486377716\n",
            "\u001b[92mTrain accuracy: 41836/48000 =  87.16 % ||| loss 0.33598092198371887\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10430/12000 =  86.92 % ||| loss 0.351540207862854\u001b[0m\n",
            "\u001b[92mTest accuracy: 8591/10000 =  85.91 % ||| loss 0.38094058632850647\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.39238588690757753\n",
            "Batch #200 Loss: 0.3882075162231922\n",
            "Batch #300 Loss: 0.39452860414981844\n",
            "\u001b[92mTrain accuracy: 42185/48000 =  87.89 % ||| loss 0.3239807188510895\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10457/12000 =  87.14 % ||| loss 0.3428715765476227\u001b[0m\n",
            "\u001b[92mTest accuracy: 8627/10000 =  86.27 % ||| loss 0.36626484990119934\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.3879810325801373\n",
            "Batch #200 Loss: 0.37512573570013047\n",
            "Batch #300 Loss: 0.3856426125764847\n",
            "\u001b[92mTrain accuracy: 42017/48000 =  87.54 % ||| loss 0.32603585720062256\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10418/12000 =  86.82 % ||| loss 0.34549668431282043\u001b[0m\n",
            "\u001b[92mTest accuracy: 8607/10000 =  86.07 % ||| loss 0.3707732856273651\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3740283128619194\n",
            "Batch #200 Loss: 0.3781874060630798\n",
            "Batch #300 Loss: 0.3706579057872295\n",
            "\u001b[92mTrain accuracy: 42257/48000 =  88.04 % ||| loss 0.3186066746711731\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10434/12000 =  86.95 % ||| loss 0.3431544601917267\u001b[0m\n",
            "\u001b[92mTest accuracy: 8629/10000 =  86.29 % ||| loss 0.3706214129924774\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3660765936970711\n",
            "Batch #200 Loss: 0.3673612229526043\n",
            "Batch #300 Loss: 0.3745572020113468\n",
            "\u001b[92mTrain accuracy: 42317/48000 =  88.16 % ||| loss 0.3103293478488922\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10485/12000 =  87.38 % ||| loss 0.33513355255126953\u001b[0m\n",
            "\u001b[92mTest accuracy: 8644/10000 =  86.44 % ||| loss 0.3582245409488678\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.35383113235235214\n",
            "Batch #200 Loss: 0.3675513043999672\n",
            "Batch #300 Loss: 0.36292701959609985\n",
            "\u001b[92mTrain accuracy: 42466/48000 =  88.47 % ||| loss 0.3114860951900482\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10506/12000 =  87.55 % ||| loss 0.3396237790584564\u001b[0m\n",
            "\u001b[92mTest accuracy: 8702/10000 =  87.02 % ||| loss 0.36997419595718384\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.35878659576177596\n",
            "Batch #200 Loss: 0.370282334536314\n",
            "Batch #300 Loss: 0.35751053333282473\n",
            "\u001b[92mTrain accuracy: 42267/48000 =  88.06 % ||| loss 0.3123795986175537\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10492/12000 =  87.43 % ||| loss 0.3390311002731323\u001b[0m\n",
            "\u001b[92mTest accuracy: 8636/10000 =  86.36 % ||| loss 0.37178122997283936\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.35377623766660693\n",
            "Batch #200 Loss: 0.34883124113082886\n",
            "Batch #300 Loss: 0.36076085925102236\n",
            "\u001b[92mTrain accuracy: 42510/48000 =  88.56 % ||| loss 0.3012199103832245\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10504/12000 =  87.53 % ||| loss 0.3299344778060913\u001b[0m\n",
            "\u001b[92mTest accuracy: 8681/10000 =  86.81 % ||| loss 0.3604463040828705\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.34806897640228274\n",
            "Batch #200 Loss: 0.3522931270301342\n",
            "Batch #300 Loss: 0.3253891226649284\n",
            "\u001b[92mTrain accuracy: 42437/48000 =  88.41 % ||| loss 0.301114022731781\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10478/12000 =  87.32 % ||| loss 0.33162882924079895\u001b[0m\n",
            "\u001b[92mTest accuracy: 8665/10000 =  86.65 % ||| loss 0.3541857600212097\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.3488862888514996\n",
            "Batch #200 Loss: 0.3470969400554895\n",
            "Batch #300 Loss: 0.34285168662667276\n",
            "\u001b[92mTrain accuracy: 42950/48000 =  89.48 % ||| loss 0.28045883774757385\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10621/12000 =  88.51 % ||| loss 0.3102157413959503\u001b[0m\n",
            "\u001b[92mTest accuracy: 8749/10000 =  87.49 % ||| loss 0.3398701250553131\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.33634151697158815\n",
            "Batch #200 Loss: 0.33799679681658745\n",
            "Batch #300 Loss: 0.32761728644371035\n",
            "\u001b[92mTrain accuracy: 42953/48000 =  89.49 % ||| loss 0.27917754650115967\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10575/12000 =  88.12 % ||| loss 0.3140451908111572\u001b[0m\n",
            "\u001b[92mTest accuracy: 8746/10000 =  87.46 % ||| loss 0.3387976884841919\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.3343958221375942\n",
            "Batch #200 Loss: 0.34401861593127253\n",
            "Batch #300 Loss: 0.32532404363155365\n",
            "\u001b[92mTrain accuracy: 43024/48000 =  89.63 % ||| loss 0.27586808800697327\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10621/12000 =  88.51 % ||| loss 0.30990007519721985\u001b[0m\n",
            "\u001b[92mTest accuracy: 8777/10000 =  87.77 % ||| loss 0.3342636823654175\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.3327004039287567\n",
            "Batch #200 Loss: 0.33052778273820876\n",
            "Batch #300 Loss: 0.33642358645796777\n",
            "\u001b[92mTrain accuracy: 43161/48000 =  89.92 % ||| loss 0.27098026871681213\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10600/12000 =  88.33 % ||| loss 0.3103012144565582\u001b[0m\n",
            "\u001b[92mTest accuracy: 8764/10000 =  87.64 % ||| loss 0.34163087606430054\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.32851706579327583\n",
            "Batch #200 Loss: 0.32181363463401796\n",
            "Batch #300 Loss: 0.33140644803643227\n",
            "\u001b[92mTrain accuracy: 43220/48000 =  90.04 % ||| loss 0.2664201557636261\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10658/12000 =  88.82 % ||| loss 0.3036418557167053\u001b[0m\n",
            "\u001b[92mTest accuracy: 8777/10000 =  87.77 % ||| loss 0.33268702030181885\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3225846195220947\n",
            "Batch #200 Loss: 0.31829728811979296\n",
            "Batch #300 Loss: 0.3194454000890255\n",
            "\u001b[92mTrain accuracy: 43358/48000 =  90.33 % ||| loss 0.26031625270843506\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10685/12000 =  89.04 % ||| loss 0.2975856363773346\u001b[0m\n",
            "\u001b[92mTest accuracy: 8789/10000 =  87.89 % ||| loss 0.3271455466747284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_13</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_13' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_13</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_175056-Lenet5Dropout_1726089678.067411_14</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_14' target=\"_blank\">Lenet5Dropout_1726089678.067411_14</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_14' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_14</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2984039306640627\n",
            "Batch #200 Loss: 1.9061475574970246\n",
            "Batch #300 Loss: 1.2030072563886642\n",
            "\u001b[92mTrain accuracy: 33515/48000 =  69.82 % ||| loss 0.8113908171653748\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8442/12000 =  70.35 % ||| loss 0.7977558970451355\u001b[0m\n",
            "\u001b[92mTest accuracy: 6958/10000 =  69.58 % ||| loss 0.8265979886054993\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.9150068873167038\n",
            "Batch #200 Loss: 0.8363337326049805\n",
            "Batch #300 Loss: 0.7886206364631653\n",
            "\u001b[92mTrain accuracy: 36054/48000 =  75.11 % ||| loss 0.6314713358879089\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9023/12000 =  75.19 % ||| loss 0.620453417301178\u001b[0m\n",
            "\u001b[92mTest accuracy: 7389/10000 =  73.89 % ||| loss 0.6569212675094604\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.7483969360589982\n",
            "Batch #200 Loss: 0.6823857933282852\n",
            "Batch #300 Loss: 0.6766733705997467\n",
            "\u001b[92mTrain accuracy: 37905/48000 =  78.97 % ||| loss 0.536143958568573\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9490/12000 =  79.08 % ||| loss 0.5335988998413086\u001b[0m\n",
            "\u001b[92mTest accuracy: 7840/10000 =  78.4 % ||| loss 0.5555206537246704\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.6333259645104409\n",
            "Batch #200 Loss: 0.6341357460618019\n",
            "Batch #300 Loss: 0.59680409938097\n",
            "\u001b[92mTrain accuracy: 38504/48000 =  80.22 % ||| loss 0.49191564321517944\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9618/12000 =  80.15 % ||| loss 0.49670782685279846\u001b[0m\n",
            "\u001b[92mTest accuracy: 7928/10000 =  79.28 % ||| loss 0.5120814442634583\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.6007413867115975\n",
            "Batch #200 Loss: 0.5737072387337685\n",
            "Batch #300 Loss: 0.5791100460290909\n",
            "\u001b[92mTrain accuracy: 39202/48000 =  81.67 % ||| loss 0.47571101784706116\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9800/12000 =  81.67 % ||| loss 0.48322972655296326\u001b[0m\n",
            "\u001b[92mTest accuracy: 8103/10000 =  81.03 % ||| loss 0.49750393629074097\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5636183139681816\n",
            "Batch #200 Loss: 0.5428029829263687\n",
            "Batch #300 Loss: 0.5507165336608887\n",
            "\u001b[92mTrain accuracy: 39794/48000 =  82.9 % ||| loss 0.453145295381546\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9950/12000 =  82.92 % ||| loss 0.46196362376213074\u001b[0m\n",
            "\u001b[92mTest accuracy: 8199/10000 =  81.99 % ||| loss 0.4818341135978699\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.519484413266182\n",
            "Batch #200 Loss: 0.5206019416451454\n",
            "Batch #300 Loss: 0.5282253414392472\n",
            "\u001b[92mTrain accuracy: 40449/48000 =  84.27 % ||| loss 0.42502209544181824\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10085/12000 =  84.04 % ||| loss 0.4351232051849365\u001b[0m\n",
            "\u001b[92mTest accuracy: 8322/10000 =  83.22 % ||| loss 0.46322911977767944\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.4965368553996086\n",
            "Batch #200 Loss: 0.5023018282651901\n",
            "Batch #300 Loss: 0.49937851637601854\n",
            "\u001b[92mTrain accuracy: 40407/48000 =  84.18 % ||| loss 0.42854008078575134\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10084/12000 =  84.03 % ||| loss 0.4372130036354065\u001b[0m\n",
            "\u001b[92mTest accuracy: 8294/10000 =  82.94 % ||| loss 0.46554502844810486\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.4836704933643341\n",
            "Batch #200 Loss: 0.4957777118682861\n",
            "Batch #300 Loss: 0.476323519051075\n",
            "\u001b[92mTrain accuracy: 41405/48000 =  86.26 % ||| loss 0.3761448860168457\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10308/12000 =  85.9 % ||| loss 0.3863154351711273\u001b[0m\n",
            "\u001b[92mTest accuracy: 8522/10000 =  85.22 % ||| loss 0.40772107243537903\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.4506310471892357\n",
            "Batch #200 Loss: 0.46039619982242586\n",
            "Batch #300 Loss: 0.4655922156572342\n",
            "\u001b[92mTrain accuracy: 41702/48000 =  86.88 % ||| loss 0.3591046631336212\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10390/12000 =  86.58 % ||| loss 0.37206852436065674\u001b[0m\n",
            "\u001b[92mTest accuracy: 8571/10000 =  85.71 % ||| loss 0.39991891384124756\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.4445621907711029\n",
            "Batch #200 Loss: 0.44053045839071275\n",
            "Batch #300 Loss: 0.43643118768930433\n",
            "\u001b[92mTrain accuracy: 41786/48000 =  87.05 % ||| loss 0.3429054319858551\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10394/12000 =  86.62 % ||| loss 0.3593667149543762\u001b[0m\n",
            "\u001b[92mTest accuracy: 8575/10000 =  85.75 % ||| loss 0.38084980845451355\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.4231841316819191\n",
            "Batch #200 Loss: 0.42135017693042753\n",
            "Batch #300 Loss: 0.44117470920085905\n",
            "\u001b[92mTrain accuracy: 42128/48000 =  87.77 % ||| loss 0.333975225687027\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10451/12000 =  87.09 % ||| loss 0.35252290964126587\u001b[0m\n",
            "\u001b[92mTest accuracy: 8661/10000 =  86.61 % ||| loss 0.3738517165184021\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.4117866393923759\n",
            "Batch #200 Loss: 0.4096736317873001\n",
            "Batch #300 Loss: 0.41133813709020617\n",
            "\u001b[92mTrain accuracy: 41890/48000 =  87.27 % ||| loss 0.3564945161342621\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10322/12000 =  86.02 % ||| loss 0.37796613574028015\u001b[0m\n",
            "\u001b[92mTest accuracy: 8580/10000 =  85.8 % ||| loss 0.4035506844520569\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.40717393457889556\n",
            "Batch #200 Loss: 0.3848731815814972\n",
            "Batch #300 Loss: 0.40973835974931716\n",
            "\u001b[92mTrain accuracy: 42399/48000 =  88.33 % ||| loss 0.31499019265174866\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10497/12000 =  87.48 % ||| loss 0.33918702602386475\u001b[0m\n",
            "\u001b[92mTest accuracy: 8680/10000 =  86.8 % ||| loss 0.35516268014907837\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3972764457762241\n",
            "Batch #200 Loss: 0.3695921379327774\n",
            "Batch #300 Loss: 0.4008008049428463\n",
            "\u001b[92mTrain accuracy: 42484/48000 =  88.51 % ||| loss 0.3069458305835724\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10547/12000 =  87.89 % ||| loss 0.3287905752658844\u001b[0m\n",
            "\u001b[92mTest accuracy: 8701/10000 =  87.01 % ||| loss 0.34766873717308044\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.39132922545075416\n",
            "Batch #200 Loss: 0.39373094514012336\n",
            "Batch #300 Loss: 0.38500434324145316\n",
            "\u001b[92mTrain accuracy: 42567/48000 =  88.68 % ||| loss 0.30777522921562195\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10531/12000 =  87.76 % ||| loss 0.3334716260433197\u001b[0m\n",
            "\u001b[92mTest accuracy: 8719/10000 =  87.19 % ||| loss 0.35607296228408813\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3724543453752995\n",
            "Batch #200 Loss: 0.38128673493862153\n",
            "Batch #300 Loss: 0.38076993510127066\n",
            "\u001b[92mTrain accuracy: 42632/48000 =  88.82 % ||| loss 0.29952460527420044\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10572/12000 =  88.1 % ||| loss 0.3245784342288971\u001b[0m\n",
            "\u001b[92mTest accuracy: 8734/10000 =  87.34 % ||| loss 0.35220396518707275\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.37367081210017206\n",
            "Batch #200 Loss: 0.3742568039894104\n",
            "Batch #300 Loss: 0.36123767867684364\n",
            "\u001b[92mTrain accuracy: 42938/48000 =  89.45 % ||| loss 0.2851320207118988\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10642/12000 =  88.68 % ||| loss 0.31045377254486084\u001b[0m\n",
            "\u001b[92mTest accuracy: 8792/10000 =  87.92 % ||| loss 0.3297339975833893\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.3571076296269894\n",
            "Batch #200 Loss: 0.37617369458079336\n",
            "Batch #300 Loss: 0.36300598323345185\n",
            "\u001b[92mTrain accuracy: 42847/48000 =  89.26 % ||| loss 0.28368330001831055\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10579/12000 =  88.16 % ||| loss 0.3152635991573334\u001b[0m\n",
            "\u001b[92mTest accuracy: 8783/10000 =  87.83 % ||| loss 0.3365013599395752\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.3399779112637043\n",
            "Batch #200 Loss: 0.3593968929350376\n",
            "Batch #300 Loss: 0.3702875706553459\n",
            "\u001b[92mTrain accuracy: 42845/48000 =  89.26 % ||| loss 0.28265616297721863\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10592/12000 =  88.27 % ||| loss 0.3114655613899231\u001b[0m\n",
            "\u001b[92mTest accuracy: 8764/10000 =  87.64 % ||| loss 0.3341027796268463\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.3512295436859131\n",
            "Batch #200 Loss: 0.36262687340378763\n",
            "Batch #300 Loss: 0.34837578132748603\n",
            "\u001b[92mTrain accuracy: 43153/48000 =  89.9 % ||| loss 0.27282172441482544\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10663/12000 =  88.86 % ||| loss 0.30605441331863403\u001b[0m\n",
            "\u001b[92mTest accuracy: 8824/10000 =  88.24 % ||| loss 0.3256957232952118\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.33721498802304267\n",
            "Batch #200 Loss: 0.3546625858545303\n",
            "Batch #300 Loss: 0.36152270182967183\n",
            "\u001b[92mTrain accuracy: 42996/48000 =  89.58 % ||| loss 0.2768245041370392\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10578/12000 =  88.15 % ||| loss 0.31508511304855347\u001b[0m\n",
            "\u001b[92mTest accuracy: 8787/10000 =  87.87 % ||| loss 0.333342045545578\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.34656057253479955\n",
            "Batch #200 Loss: 0.3534357418119907\n",
            "Batch #300 Loss: 0.3472815243899822\n",
            "\u001b[92mTrain accuracy: 43273/48000 =  90.15 % ||| loss 0.2654075622558594\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10666/12000 =  88.88 % ||| loss 0.3012191951274872\u001b[0m\n",
            "\u001b[92mTest accuracy: 8847/10000 =  88.47 % ||| loss 0.3214508593082428\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.33894967049360275\n",
            "Batch #200 Loss: 0.33335571199655534\n",
            "Batch #300 Loss: 0.34956970512866975\n",
            "\u001b[92mTrain accuracy: 43161/48000 =  89.92 % ||| loss 0.264679491519928\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10639/12000 =  88.66 % ||| loss 0.30193954706192017\u001b[0m\n",
            "\u001b[92mTest accuracy: 8801/10000 =  88.01 % ||| loss 0.3295327126979828\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.33111770942807195\n",
            "Batch #200 Loss: 0.3419155539572239\n",
            "Batch #300 Loss: 0.3344904014468193\n",
            "\u001b[92mTrain accuracy: 43258/48000 =  90.12 % ||| loss 0.26516714692115784\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10695/12000 =  89.12 % ||| loss 0.30346521735191345\u001b[0m\n",
            "\u001b[92mTest accuracy: 8822/10000 =  88.22 % ||| loss 0.3248736560344696\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_14</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_14' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_14</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_175247-Lenet5Dropout_1726089678.067411_15</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_15' target=\"_blank\">Lenet5Dropout_1726089678.067411_15</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_15' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_15</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2981918478012084\n",
            "Batch #200 Loss: 2.235715296268463\n",
            "Batch #300 Loss: 1.4495684218406677\n",
            "\u001b[92mTrain accuracy: 30499/48000 =  63.54 % ||| loss 0.9619793891906738\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7631/12000 =  63.59 % ||| loss 0.9488258361816406\u001b[0m\n",
            "\u001b[92mTest accuracy: 6276/10000 =  62.76 % ||| loss 0.9764296412467957\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.9808559530973434\n",
            "Batch #200 Loss: 0.9008180791139603\n",
            "Batch #300 Loss: 0.8659695881605148\n",
            "\u001b[92mTrain accuracy: 34557/48000 =  71.99 % ||| loss 0.7414976954460144\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8680/12000 =  72.33 % ||| loss 0.7308724522590637\u001b[0m\n",
            "\u001b[92mTest accuracy: 7106/10000 =  71.06 % ||| loss 0.765552282333374\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.7935422527790069\n",
            "Batch #200 Loss: 0.7754050087928772\n",
            "Batch #300 Loss: 0.7452546048164368\n",
            "\u001b[92mTrain accuracy: 35425/48000 =  73.8 % ||| loss 0.6691430807113647\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8850/12000 =  73.75 % ||| loss 0.657646119594574\u001b[0m\n",
            "\u001b[92mTest accuracy: 7273/10000 =  72.73 % ||| loss 0.6980518102645874\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.7077549958229065\n",
            "Batch #200 Loss: 0.7099146747589111\n",
            "Batch #300 Loss: 0.6897044754028321\n",
            "\u001b[92mTrain accuracy: 37180/48000 =  77.46 % ||| loss 0.5967152714729309\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9309/12000 =  77.58 % ||| loss 0.5877497792243958\u001b[0m\n",
            "\u001b[92mTest accuracy: 7684/10000 =  76.84 % ||| loss 0.6209962368011475\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.6627132618427276\n",
            "Batch #200 Loss: 0.6433577546477318\n",
            "Batch #300 Loss: 0.6542525231838227\n",
            "\u001b[92mTrain accuracy: 37720/48000 =  78.58 % ||| loss 0.5670731663703918\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9438/12000 =  78.65 % ||| loss 0.5657436847686768\u001b[0m\n",
            "\u001b[92mTest accuracy: 7744/10000 =  77.44 % ||| loss 0.5957322716712952\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.6261346274614334\n",
            "Batch #200 Loss: 0.5989443328976631\n",
            "Batch #300 Loss: 0.6018123817443848\n",
            "\u001b[92mTrain accuracy: 38506/48000 =  80.22 % ||| loss 0.527971625328064\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9641/12000 =  80.34 % ||| loss 0.5242694020271301\u001b[0m\n",
            "\u001b[92mTest accuracy: 7961/10000 =  79.61 % ||| loss 0.5603068470954895\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5863452917337417\n",
            "Batch #200 Loss: 0.5790581059455872\n",
            "Batch #300 Loss: 0.5680769890546798\n",
            "\u001b[92mTrain accuracy: 38937/48000 =  81.12 % ||| loss 0.514459490776062\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9733/12000 =  81.11 % ||| loss 0.5142923593521118\u001b[0m\n",
            "\u001b[92mTest accuracy: 8004/10000 =  80.04 % ||| loss 0.5481871366500854\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.5502775910496712\n",
            "Batch #200 Loss: 0.5586967298388481\n",
            "Batch #300 Loss: 0.5465115377306938\n",
            "\u001b[92mTrain accuracy: 38853/48000 =  80.94 % ||| loss 0.500728189945221\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9648/12000 =  80.4 % ||| loss 0.505003035068512\u001b[0m\n",
            "\u001b[92mTest accuracy: 8011/10000 =  80.11 % ||| loss 0.5292156934738159\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.5287683421373367\n",
            "Batch #200 Loss: 0.5384725931286812\n",
            "Batch #300 Loss: 0.5164076516032219\n",
            "\u001b[92mTrain accuracy: 39828/48000 =  82.97 % ||| loss 0.45996028184890747\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9968/12000 =  83.07 % ||| loss 0.46249666810035706\u001b[0m\n",
            "\u001b[92mTest accuracy: 8223/10000 =  82.23 % ||| loss 0.4935169517993927\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.5260091498494148\n",
            "Batch #200 Loss: 0.5108541351556778\n",
            "Batch #300 Loss: 0.4959836980700493\n",
            "\u001b[92mTrain accuracy: 40149/48000 =  83.64 % ||| loss 0.446506142616272\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10023/12000 =  83.53 % ||| loss 0.4485156238079071\u001b[0m\n",
            "\u001b[92mTest accuracy: 8257/10000 =  82.57 % ||| loss 0.47671860456466675\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.49503460168838503\n",
            "Batch #200 Loss: 0.4819781658053398\n",
            "Batch #300 Loss: 0.49234163016080856\n",
            "\u001b[92mTrain accuracy: 39987/48000 =  83.31 % ||| loss 0.4481818377971649\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10014/12000 =  83.45 % ||| loss 0.4509297311306\u001b[0m\n",
            "\u001b[92mTest accuracy: 8226/10000 =  82.26 % ||| loss 0.4839094281196594\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.4955296361446381\n",
            "Batch #200 Loss: 0.48312721759080884\n",
            "Batch #300 Loss: 0.46427014231681824\n",
            "\u001b[92mTrain accuracy: 40544/48000 =  84.47 % ||| loss 0.41586413979530334\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10131/12000 =  84.42 % ||| loss 0.42399752140045166\u001b[0m\n",
            "\u001b[92mTest accuracy: 8338/10000 =  83.38 % ||| loss 0.45178940892219543\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.45808026522397993\n",
            "Batch #200 Loss: 0.46277678191661836\n",
            "Batch #300 Loss: 0.46115628838539124\n",
            "\u001b[92mTrain accuracy: 40801/48000 =  85.0 % ||| loss 0.40727120637893677\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10216/12000 =  85.13 % ||| loss 0.41673997044563293\u001b[0m\n",
            "\u001b[92mTest accuracy: 8399/10000 =  83.99 % ||| loss 0.44881942868232727\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.4448636141419411\n",
            "Batch #200 Loss: 0.4528204590082169\n",
            "Batch #300 Loss: 0.45144374430179596\n",
            "\u001b[92mTrain accuracy: 41127/48000 =  85.68 % ||| loss 0.3902130424976349\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10255/12000 =  85.46 % ||| loss 0.3992869555950165\u001b[0m\n",
            "\u001b[92mTest accuracy: 8445/10000 =  84.45 % ||| loss 0.4235610067844391\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.4532438859343529\n",
            "Batch #200 Loss: 0.42406406596302987\n",
            "Batch #300 Loss: 0.4556424805521965\n",
            "\u001b[92mTrain accuracy: 41269/48000 =  85.98 % ||| loss 0.38338789343833923\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10280/12000 =  85.67 % ||| loss 0.3936520516872406\u001b[0m\n",
            "\u001b[92mTest accuracy: 8447/10000 =  84.47 % ||| loss 0.42044660449028015\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.42505489438772204\n",
            "Batch #200 Loss: 0.4265209758281708\n",
            "Batch #300 Loss: 0.4386835509538651\n",
            "\u001b[92mTrain accuracy: 40978/48000 =  85.37 % ||| loss 0.3915833830833435\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10231/12000 =  85.26 % ||| loss 0.4022800028324127\u001b[0m\n",
            "\u001b[92mTest accuracy: 8417/10000 =  84.17 % ||| loss 0.43418800830841064\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.42251637026667593\n",
            "Batch #200 Loss: 0.4288712242245674\n",
            "Batch #300 Loss: 0.43407674968242643\n",
            "\u001b[92mTrain accuracy: 41259/48000 =  85.96 % ||| loss 0.3784627318382263\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10288/12000 =  85.73 % ||| loss 0.3905338644981384\u001b[0m\n",
            "\u001b[92mTest accuracy: 8485/10000 =  84.85 % ||| loss 0.41739127039909363\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.4101189313828945\n",
            "Batch #200 Loss: 0.4259214970469475\n",
            "Batch #300 Loss: 0.4063720311224461\n",
            "\u001b[92mTrain accuracy: 41563/48000 =  86.59 % ||| loss 0.36654096841812134\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10322/12000 =  86.02 % ||| loss 0.38024812936782837\u001b[0m\n",
            "\u001b[92mTest accuracy: 8545/10000 =  85.45 % ||| loss 0.4021585285663605\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.41014114558696746\n",
            "Batch #200 Loss: 0.40762223437428474\n",
            "Batch #300 Loss: 0.4022106865048409\n",
            "\u001b[92mTrain accuracy: 41695/48000 =  86.86 % ||| loss 0.3508586883544922\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10384/12000 =  86.53 % ||| loss 0.3689994812011719\u001b[0m\n",
            "\u001b[92mTest accuracy: 8575/10000 =  85.75 % ||| loss 0.3922465443611145\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.404406398832798\n",
            "Batch #200 Loss: 0.4030961073935032\n",
            "Batch #300 Loss: 0.4016262027621269\n",
            "\u001b[92mTrain accuracy: 41719/48000 =  86.91 % ||| loss 0.3499995172023773\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10389/12000 =  86.58 % ||| loss 0.3644523322582245\u001b[0m\n",
            "\u001b[92mTest accuracy: 8548/10000 =  85.48 % ||| loss 0.39047038555145264\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.3955162915587425\n",
            "Batch #200 Loss: 0.3935633444786072\n",
            "Batch #300 Loss: 0.38635492995381354\n",
            "\u001b[92mTrain accuracy: 41936/48000 =  87.37 % ||| loss 0.33901870250701904\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10406/12000 =  86.72 % ||| loss 0.35824301838874817\u001b[0m\n",
            "\u001b[92mTest accuracy: 8596/10000 =  85.96 % ||| loss 0.3800181746482849\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.38720942914485934\n",
            "Batch #200 Loss: 0.3827418987452984\n",
            "Batch #300 Loss: 0.3799675172567368\n",
            "\u001b[92mTrain accuracy: 41738/48000 =  86.95 % ||| loss 0.34812361001968384\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10400/12000 =  86.67 % ||| loss 0.365261435508728\u001b[0m\n",
            "\u001b[92mTest accuracy: 8570/10000 =  85.7 % ||| loss 0.3913843333721161\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.3706902079284191\n",
            "Batch #200 Loss: 0.3856367690861225\n",
            "Batch #300 Loss: 0.38654126197099686\n",
            "\u001b[92mTrain accuracy: 42011/48000 =  87.52 % ||| loss 0.33485451340675354\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10437/12000 =  86.98 % ||| loss 0.3523605167865753\u001b[0m\n",
            "\u001b[92mTest accuracy: 8588/10000 =  85.88 % ||| loss 0.3826247453689575\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.3763509924709797\n",
            "Batch #200 Loss: 0.3730517590045929\n",
            "Batch #300 Loss: 0.370059053003788\n",
            "\u001b[92mTrain accuracy: 42098/48000 =  87.7 % ||| loss 0.3291555941104889\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10481/12000 =  87.34 % ||| loss 0.34820544719696045\u001b[0m\n",
            "\u001b[92mTest accuracy: 8624/10000 =  86.24 % ||| loss 0.3699607849121094\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.36873233526945115\n",
            "Batch #200 Loss: 0.37858966082334516\n",
            "Batch #300 Loss: 0.37508866891264914\n",
            "\u001b[92mTrain accuracy: 42055/48000 =  87.61 % ||| loss 0.331380158662796\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10461/12000 =  87.17 % ||| loss 0.35067853331565857\u001b[0m\n",
            "\u001b[92mTest accuracy: 8608/10000 =  86.08 % ||| loss 0.37665945291519165\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_15</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_15' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_15</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_175434-Lenet5Dropout_1726089678.067411_16</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_16' target=\"_blank\">Lenet5Dropout_1726089678.067411_16</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_16' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_16</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3002835273742677\n",
            "Batch #200 Loss: 2.2702112364768983\n",
            "Batch #300 Loss: 1.820875004529953\n",
            "\u001b[92mTrain accuracy: 29809/48000 =  62.1 % ||| loss 1.00649094581604\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7489/12000 =  62.41 % ||| loss 0.9983837008476257\u001b[0m\n",
            "\u001b[92mTest accuracy: 6143/10000 =  61.43 % ||| loss 1.0158307552337646\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 1.0981285363435744\n",
            "Batch #200 Loss: 1.017714842557907\n",
            "Batch #300 Loss: 0.941678272485733\n",
            "\u001b[92mTrain accuracy: 34128/48000 =  71.1 % ||| loss 0.764413595199585\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8517/12000 =  70.97 % ||| loss 0.7553154826164246\u001b[0m\n",
            "\u001b[92mTest accuracy: 7044/10000 =  70.44 % ||| loss 0.7726393938064575\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.8366768759489059\n",
            "Batch #200 Loss: 0.804571772813797\n",
            "Batch #300 Loss: 0.7936809897422791\n",
            "\u001b[92mTrain accuracy: 35902/48000 =  74.8 % ||| loss 0.6695428490638733\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8968/12000 =  74.73 % ||| loss 0.6617663502693176\u001b[0m\n",
            "\u001b[92mTest accuracy: 7396/10000 =  73.96 % ||| loss 0.6906523108482361\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.7466053545475007\n",
            "Batch #200 Loss: 0.7191743385791779\n",
            "Batch #300 Loss: 0.7145767021179199\n",
            "\u001b[92mTrain accuracy: 36556/48000 =  76.16 % ||| loss 0.6082959771156311\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9166/12000 =  76.38 % ||| loss 0.5993128418922424\u001b[0m\n",
            "\u001b[92mTest accuracy: 7512/10000 =  75.12 % ||| loss 0.6309086680412292\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.7017593643069268\n",
            "Batch #200 Loss: 0.6667555719614029\n",
            "Batch #300 Loss: 0.6613973587751388\n",
            "\u001b[92mTrain accuracy: 37184/48000 =  77.47 % ||| loss 0.5717496871948242\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9325/12000 =  77.71 % ||| loss 0.5617139339447021\u001b[0m\n",
            "\u001b[92mTest accuracy: 7656/10000 =  76.56 % ||| loss 0.595348060131073\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.6503043648600578\n",
            "Batch #200 Loss: 0.6401003772020339\n",
            "Batch #300 Loss: 0.6173027691245079\n",
            "\u001b[92mTrain accuracy: 37786/48000 =  78.72 % ||| loss 0.5369240045547485\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9477/12000 =  78.97 % ||| loss 0.5310540795326233\u001b[0m\n",
            "\u001b[92mTest accuracy: 7809/10000 =  78.09 % ||| loss 0.5591832995414734\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.6112733352184295\n",
            "Batch #200 Loss: 0.6104362487792969\n",
            "Batch #300 Loss: 0.5695827504992486\n",
            "\u001b[92mTrain accuracy: 38284/48000 =  79.76 % ||| loss 0.5154750347137451\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9560/12000 =  79.67 % ||| loss 0.5111880302429199\u001b[0m\n",
            "\u001b[92mTest accuracy: 7902/10000 =  79.02 % ||| loss 0.5356097221374512\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.5781701222062111\n",
            "Batch #200 Loss: 0.567398248910904\n",
            "Batch #300 Loss: 0.5694238740205765\n",
            "\u001b[92mTrain accuracy: 39416/48000 =  82.12 % ||| loss 0.48129549622535706\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9891/12000 =  82.42 % ||| loss 0.47986558079719543\u001b[0m\n",
            "\u001b[92mTest accuracy: 8124/10000 =  81.24 % ||| loss 0.5058405995368958\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.5542513114213944\n",
            "Batch #200 Loss: 0.559928797185421\n",
            "Batch #300 Loss: 0.5412863358855248\n",
            "\u001b[92mTrain accuracy: 39583/48000 =  82.46 % ||| loss 0.4690401554107666\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9919/12000 =  82.66 % ||| loss 0.46820372343063354\u001b[0m\n",
            "\u001b[92mTest accuracy: 8173/10000 =  81.73 % ||| loss 0.49152833223342896\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.5414618968963623\n",
            "Batch #200 Loss: 0.534636872112751\n",
            "Batch #300 Loss: 0.5310245871543884\n",
            "\u001b[92mTrain accuracy: 39757/48000 =  82.83 % ||| loss 0.4601966142654419\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9981/12000 =  83.17 % ||| loss 0.45840272307395935\u001b[0m\n",
            "\u001b[92mTest accuracy: 8230/10000 =  82.3 % ||| loss 0.4870580732822418\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.5133490291237831\n",
            "Batch #200 Loss: 0.5286857098340988\n",
            "Batch #300 Loss: 0.5266698145866394\n",
            "\u001b[92mTrain accuracy: 39839/48000 =  83.0 % ||| loss 0.44712597131729126\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9972/12000 =  83.1 % ||| loss 0.4513601064682007\u001b[0m\n",
            "\u001b[92mTest accuracy: 8227/10000 =  82.27 % ||| loss 0.47612279653549194\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5067413774132729\n",
            "Batch #200 Loss: 0.5061172226071358\n",
            "Batch #300 Loss: 0.5138098356127739\n",
            "\u001b[92mTrain accuracy: 40395/48000 =  84.16 % ||| loss 0.4273664057254791\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10109/12000 =  84.24 % ||| loss 0.4340830445289612\u001b[0m\n",
            "\u001b[92mTest accuracy: 8357/10000 =  83.57 % ||| loss 0.4537472724914551\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5102233016490936\n",
            "Batch #200 Loss: 0.49108184278011324\n",
            "Batch #300 Loss: 0.4910987988114357\n",
            "\u001b[92mTrain accuracy: 40426/48000 =  84.22 % ||| loss 0.42908841371536255\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10116/12000 =  84.3 % ||| loss 0.4342302680015564\u001b[0m\n",
            "\u001b[92mTest accuracy: 8345/10000 =  83.45 % ||| loss 0.46419236063957214\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.4943110629916191\n",
            "Batch #200 Loss: 0.49111758947372436\n",
            "Batch #300 Loss: 0.4809326589107513\n",
            "\u001b[92mTrain accuracy: 40622/48000 =  84.63 % ||| loss 0.41241079568862915\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10169/12000 =  84.74 % ||| loss 0.41827717423439026\u001b[0m\n",
            "\u001b[92mTest accuracy: 8394/10000 =  83.94 % ||| loss 0.443359911441803\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.4715705427527428\n",
            "Batch #200 Loss: 0.4819233354926109\n",
            "Batch #300 Loss: 0.4706632706522942\n",
            "\u001b[92mTrain accuracy: 40842/48000 =  85.09 % ||| loss 0.4023437201976776\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10191/12000 =  84.92 % ||| loss 0.4102681279182434\u001b[0m\n",
            "\u001b[92mTest accuracy: 8415/10000 =  84.15 % ||| loss 0.4311985969543457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.45415566235780713\n",
            "Batch #200 Loss: 0.4632939872145653\n",
            "Batch #300 Loss: 0.4682622477412224\n",
            "\u001b[92mTrain accuracy: 40946/48000 =  85.3 % ||| loss 0.40004777908325195\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10230/12000 =  85.25 % ||| loss 0.40698835253715515\u001b[0m\n",
            "\u001b[92mTest accuracy: 8439/10000 =  84.39 % ||| loss 0.42766818404197693\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.44982280761003496\n",
            "Batch #200 Loss: 0.455101757645607\n",
            "Batch #300 Loss: 0.45947050124406813\n",
            "\u001b[92mTrain accuracy: 41190/48000 =  85.81 % ||| loss 0.377291738986969\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10270/12000 =  85.58 % ||| loss 0.3877304494380951\u001b[0m\n",
            "\u001b[92mTest accuracy: 8495/10000 =  84.95 % ||| loss 0.40852466225624084\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.4616169789433479\n",
            "Batch #200 Loss: 0.42752011835575104\n",
            "Batch #300 Loss: 0.43557133585214614\n",
            "\u001b[92mTrain accuracy: 41356/48000 =  86.16 % ||| loss 0.3786507248878479\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10299/12000 =  85.82 % ||| loss 0.3882874846458435\u001b[0m\n",
            "\u001b[92mTest accuracy: 8522/10000 =  85.22 % ||| loss 0.4127423167228699\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.43326025307178495\n",
            "Batch #200 Loss: 0.4400122582912445\n",
            "Batch #300 Loss: 0.43084234490990636\n",
            "\u001b[92mTrain accuracy: 41546/48000 =  86.55 % ||| loss 0.36342111229896545\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10363/12000 =  86.36 % ||| loss 0.37741225957870483\u001b[0m\n",
            "\u001b[92mTest accuracy: 8549/10000 =  85.49 % ||| loss 0.39776960015296936\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.41605387032032015\n",
            "Batch #200 Loss: 0.4321883836388588\n",
            "Batch #300 Loss: 0.4270248357951641\n",
            "\u001b[92mTrain accuracy: 41611/48000 =  86.69 % ||| loss 0.3637576401233673\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10352/12000 =  86.27 % ||| loss 0.3762199282646179\u001b[0m\n",
            "\u001b[92mTest accuracy: 8549/10000 =  85.49 % ||| loss 0.400199830532074\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.4168332585692406\n",
            "Batch #200 Loss: 0.4134622475504875\n",
            "Batch #300 Loss: 0.4295612791180611\n",
            "\u001b[92mTrain accuracy: 41658/48000 =  86.79 % ||| loss 0.35259127616882324\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10406/12000 =  86.72 % ||| loss 0.3647420406341553\u001b[0m\n",
            "\u001b[92mTest accuracy: 8580/10000 =  85.8 % ||| loss 0.3882945477962494\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.4262935474514961\n",
            "Batch #200 Loss: 0.4054847827553749\n",
            "Batch #300 Loss: 0.4178870689868927\n",
            "\u001b[92mTrain accuracy: 41802/48000 =  87.09 % ||| loss 0.34441372752189636\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10424/12000 =  86.87 % ||| loss 0.35931870341300964\u001b[0m\n",
            "\u001b[92mTest accuracy: 8616/10000 =  86.16 % ||| loss 0.38291555643081665\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.4026763442158699\n",
            "Batch #200 Loss: 0.41288881897926333\n",
            "Batch #300 Loss: 0.40148712426424027\n",
            "\u001b[92mTrain accuracy: 41424/48000 =  86.3 % ||| loss 0.3690190017223358\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10281/12000 =  85.67 % ||| loss 0.38167524337768555\u001b[0m\n",
            "\u001b[92mTest accuracy: 8519/10000 =  85.19 % ||| loss 0.4089193344116211\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.4048555764555931\n",
            "Batch #200 Loss: 0.412041994035244\n",
            "Batch #300 Loss: 0.39378314808011056\n",
            "\u001b[92mTrain accuracy: 41929/48000 =  87.35 % ||| loss 0.3376603424549103\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10412/12000 =  86.77 % ||| loss 0.35438627004623413\u001b[0m\n",
            "\u001b[92mTest accuracy: 8613/10000 =  86.13 % ||| loss 0.3781009912490845\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3909531611204147\n",
            "Batch #200 Loss: 0.40000902384519577\n",
            "Batch #300 Loss: 0.395892346650362\n",
            "\u001b[92mTrain accuracy: 42108/48000 =  87.72 % ||| loss 0.3272702097892761\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10496/12000 =  87.47 % ||| loss 0.3431108891963959\u001b[0m\n",
            "\u001b[92mTest accuracy: 8678/10000 =  86.78 % ||| loss 0.36610960960388184\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_16</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_16' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_16</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_175628-Lenet5Dropout_1726089678.067411_17</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_17' target=\"_blank\">Lenet5Dropout_1726089678.067411_17</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_17' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_17</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.299523129463196\n",
            "Batch #200 Loss: 2.267747206687927\n",
            "Batch #300 Loss: 1.9125357246398926\n",
            "\u001b[92mTrain accuracy: 26165/48000 =  54.51 % ||| loss 1.1201890707015991\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6508/12000 =  54.23 % ||| loss 1.114729404449463\u001b[0m\n",
            "\u001b[92mTest accuracy: 5424/10000 =  54.24 % ||| loss 1.1302223205566406\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 1.2521839994192123\n",
            "Batch #200 Loss: 1.1287223958969117\n",
            "Batch #300 Loss: 1.0401094794273376\n",
            "\u001b[92mTrain accuracy: 32192/48000 =  67.07 % ||| loss 0.818182110786438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8063/12000 =  67.19 % ||| loss 0.8069257736206055\u001b[0m\n",
            "\u001b[92mTest accuracy: 6643/10000 =  66.43 % ||| loss 0.833156943321228\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.9521954101324082\n",
            "Batch #200 Loss: 0.9098548126220704\n",
            "Batch #300 Loss: 0.8724913889169693\n",
            "\u001b[92mTrain accuracy: 34642/48000 =  72.17 % ||| loss 0.6911850571632385\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8727/12000 =  72.72 % ||| loss 0.6753812432289124\u001b[0m\n",
            "\u001b[92mTest accuracy: 7143/10000 =  71.43 % ||| loss 0.7067257761955261\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.8154110354185105\n",
            "Batch #200 Loss: 0.8044275277853012\n",
            "Batch #300 Loss: 0.7767972964048385\n",
            "\u001b[92mTrain accuracy: 36148/48000 =  75.31 % ||| loss 0.630447268486023\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9045/12000 =  75.38 % ||| loss 0.6212805509567261\u001b[0m\n",
            "\u001b[92mTest accuracy: 7439/10000 =  74.39 % ||| loss 0.6554710865020752\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.7560066145658493\n",
            "Batch #200 Loss: 0.7306237989664077\n",
            "Batch #300 Loss: 0.7184398418664932\n",
            "\u001b[92mTrain accuracy: 37039/48000 =  77.16 % ||| loss 0.5745434761047363\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9294/12000 =  77.45 % ||| loss 0.5638419389724731\u001b[0m\n",
            "\u001b[92mTest accuracy: 7635/10000 =  76.35 % ||| loss 0.5931801199913025\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.6783640363812447\n",
            "Batch #200 Loss: 0.6882864898443222\n",
            "Batch #300 Loss: 0.6823781034350396\n",
            "\u001b[92mTrain accuracy: 37514/48000 =  78.15 % ||| loss 0.5497815608978271\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9396/12000 =  78.3 % ||| loss 0.5418905019760132\u001b[0m\n",
            "\u001b[92mTest accuracy: 7753/10000 =  77.53 % ||| loss 0.568830668926239\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.6455666837096214\n",
            "Batch #200 Loss: 0.6510455200076103\n",
            "Batch #300 Loss: 0.6406149038672447\n",
            "\u001b[92mTrain accuracy: 37912/48000 =  78.98 % ||| loss 0.518930971622467\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9486/12000 =  79.05 % ||| loss 0.516279935836792\u001b[0m\n",
            "\u001b[92mTest accuracy: 7807/10000 =  78.07 % ||| loss 0.5419416427612305\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.626346335709095\n",
            "Batch #200 Loss: 0.6245837408304215\n",
            "Batch #300 Loss: 0.6029268431663514\n",
            "\u001b[92mTrain accuracy: 37614/48000 =  78.36 % ||| loss 0.5214133858680725\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9398/12000 =  78.32 % ||| loss 0.5233263969421387\u001b[0m\n",
            "\u001b[92mTest accuracy: 7733/10000 =  77.33 % ||| loss 0.5487502217292786\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.6157677072286606\n",
            "Batch #200 Loss: 0.5856098353862762\n",
            "Batch #300 Loss: 0.5875984805822373\n",
            "\u001b[92mTrain accuracy: 38371/48000 =  79.94 % ||| loss 0.49443745613098145\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9595/12000 =  79.96 % ||| loss 0.49409496784210205\u001b[0m\n",
            "\u001b[92mTest accuracy: 7925/10000 =  79.25 % ||| loss 0.5180903077125549\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.576997677385807\n",
            "Batch #200 Loss: 0.5798672768473625\n",
            "Batch #300 Loss: 0.5874210312962532\n",
            "\u001b[92mTrain accuracy: 39756/48000 =  82.83 % ||| loss 0.46865832805633545\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9924/12000 =  82.7 % ||| loss 0.46848005056381226\u001b[0m\n",
            "\u001b[92mTest accuracy: 8197/10000 =  81.97 % ||| loss 0.49289417266845703\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.5645397725701332\n",
            "Batch #200 Loss: 0.5652220797538757\n",
            "Batch #300 Loss: 0.5470664981007576\n",
            "\u001b[92mTrain accuracy: 39769/48000 =  82.85 % ||| loss 0.45988208055496216\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9955/12000 =  82.96 % ||| loss 0.46036630868911743\u001b[0m\n",
            "\u001b[92mTest accuracy: 8172/10000 =  81.72 % ||| loss 0.4838060438632965\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.570240113735199\n",
            "Batch #200 Loss: 0.5457643264532089\n",
            "Batch #300 Loss: 0.5540763521194458\n",
            "\u001b[92mTrain accuracy: 40105/48000 =  83.55 % ||| loss 0.4475336968898773\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10022/12000 =  83.52 % ||| loss 0.45344534516334534\u001b[0m\n",
            "\u001b[92mTest accuracy: 8225/10000 =  82.25 % ||| loss 0.47582849860191345\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5401956567168236\n",
            "Batch #200 Loss: 0.551862872838974\n",
            "Batch #300 Loss: 0.5295481270551682\n",
            "\u001b[92mTrain accuracy: 40316/48000 =  83.99 % ||| loss 0.4381536543369293\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10049/12000 =  83.74 % ||| loss 0.4418887197971344\u001b[0m\n",
            "\u001b[92mTest accuracy: 8297/10000 =  82.97 % ||| loss 0.4693578779697418\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.533218689262867\n",
            "Batch #200 Loss: 0.5369642260670662\n",
            "Batch #300 Loss: 0.5236258620023727\n",
            "\u001b[92mTrain accuracy: 40519/48000 =  84.41 % ||| loss 0.43089884519577026\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10104/12000 =  84.2 % ||| loss 0.4388431906700134\u001b[0m\n",
            "\u001b[92mTest accuracy: 8313/10000 =  83.13 % ||| loss 0.4622395634651184\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5266459962725639\n",
            "Batch #200 Loss: 0.5164499861001969\n",
            "Batch #300 Loss: 0.522162972688675\n",
            "\u001b[92mTrain accuracy: 40341/48000 =  84.04 % ||| loss 0.42434683442115784\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10039/12000 =  83.66 % ||| loss 0.43315595388412476\u001b[0m\n",
            "\u001b[92mTest accuracy: 8290/10000 =  82.9 % ||| loss 0.4521126449108124\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.5064594498276711\n",
            "Batch #200 Loss: 0.5048094329237938\n",
            "Batch #300 Loss: 0.5170916339755058\n",
            "\u001b[92mTrain accuracy: 40723/48000 =  84.84 % ||| loss 0.4101389944553375\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10142/12000 =  84.52 % ||| loss 0.4187643229961395\u001b[0m\n",
            "\u001b[92mTest accuracy: 8386/10000 =  83.86 % ||| loss 0.44012022018432617\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.4941974401473999\n",
            "Batch #200 Loss: 0.5093039500713349\n",
            "Batch #300 Loss: 0.4889103990793228\n",
            "\u001b[92mTrain accuracy: 40681/48000 =  84.75 % ||| loss 0.4162420928478241\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10123/12000 =  84.36 % ||| loss 0.4279225468635559\u001b[0m\n",
            "\u001b[92mTest accuracy: 8355/10000 =  83.55 % ||| loss 0.444830060005188\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.4984032168984413\n",
            "Batch #200 Loss: 0.4859146043658257\n",
            "Batch #300 Loss: 0.490497068464756\n",
            "\u001b[92mTrain accuracy: 40962/48000 =  85.34 % ||| loss 0.40042147040367126\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10197/12000 =  84.97 % ||| loss 0.4105076789855957\u001b[0m\n",
            "\u001b[92mTest accuracy: 8434/10000 =  84.34 % ||| loss 0.43303391337394714\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.4939709582924843\n",
            "Batch #200 Loss: 0.48610260725021365\n",
            "Batch #300 Loss: 0.48355943530797957\n",
            "\u001b[92mTrain accuracy: 41095/48000 =  85.61 % ||| loss 0.3858366012573242\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10189/12000 =  84.91 % ||| loss 0.3962901532649994\u001b[0m\n",
            "\u001b[92mTest accuracy: 8450/10000 =  84.5 % ||| loss 0.4176429510116577\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.4835161754488945\n",
            "Batch #200 Loss: 0.4839586663246155\n",
            "Batch #300 Loss: 0.471165746152401\n",
            "\u001b[92mTrain accuracy: 41304/48000 =  86.05 % ||| loss 0.3807613253593445\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10248/12000 =  85.4 % ||| loss 0.39356499910354614\u001b[0m\n",
            "\u001b[92mTest accuracy: 8505/10000 =  85.05 % ||| loss 0.4134160280227661\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.477588087618351\n",
            "Batch #200 Loss: 0.463500908613205\n",
            "Batch #300 Loss: 0.4534525415301323\n",
            "\u001b[92mTrain accuracy: 41274/48000 =  85.99 % ||| loss 0.375846266746521\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10263/12000 =  85.52 % ||| loss 0.38829782605171204\u001b[0m\n",
            "\u001b[92mTest accuracy: 8462/10000 =  84.62 % ||| loss 0.41037923097610474\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.4592486709356308\n",
            "Batch #200 Loss: 0.4588327929377556\n",
            "Batch #300 Loss: 0.45241510510444644\n",
            "\u001b[92mTrain accuracy: 41488/48000 =  86.43 % ||| loss 0.36447709798812866\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10325/12000 =  86.04 % ||| loss 0.3773348331451416\u001b[0m\n",
            "\u001b[92mTest accuracy: 8516/10000 =  85.16 % ||| loss 0.3997156023979187\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.4559656593203545\n",
            "Batch #200 Loss: 0.45081104576587677\n",
            "Batch #300 Loss: 0.462359656393528\n",
            "\u001b[92mTrain accuracy: 41596/48000 =  86.66 % ||| loss 0.36020606756210327\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10341/12000 =  86.17 % ||| loss 0.3722512125968933\u001b[0m\n",
            "\u001b[92mTest accuracy: 8530/10000 =  85.3 % ||| loss 0.3950806260108948\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.4541584327816963\n",
            "Batch #200 Loss: 0.4383358311653137\n",
            "Batch #300 Loss: 0.4545937034487724\n",
            "\u001b[92mTrain accuracy: 41592/48000 =  86.65 % ||| loss 0.3531125485897064\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10333/12000 =  86.11 % ||| loss 0.3654693067073822\u001b[0m\n",
            "\u001b[92mTest accuracy: 8536/10000 =  85.36 % ||| loss 0.3882657587528229\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.43323143899440764\n",
            "Batch #200 Loss: 0.4453753699362278\n",
            "Batch #300 Loss: 0.4413467055559158\n",
            "\u001b[92mTrain accuracy: 41758/48000 =  87.0 % ||| loss 0.3476719856262207\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10374/12000 =  86.45 % ||| loss 0.36317458748817444\u001b[0m\n",
            "\u001b[92mTest accuracy: 8540/10000 =  85.4 % ||| loss 0.3870938718318939\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_17</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_17' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_17</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_175821-Lenet5Dropout_1726089678.067411_18</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_18' target=\"_blank\">Lenet5Dropout_1726089678.067411_18</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_18' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_18</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3052330088615416\n",
            "Batch #200 Loss: 2.305075056552887\n",
            "Batch #300 Loss: 2.304750189781189\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3034653663635254\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3029797077178955\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3035731315612793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3039874696731566\n",
            "Batch #200 Loss: 2.3038005805015564\n",
            "Batch #300 Loss: 2.3022647213935854\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.301784038543701\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3013253211975098\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3017232418060303\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3021254205703734\n",
            "Batch #200 Loss: 2.301208062171936\n",
            "Batch #300 Loss: 2.3009827518463135\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3000435829162598\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.299607992172241\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.300213098526001\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.300044176578522\n",
            "Batch #200 Loss: 2.299707109928131\n",
            "Batch #300 Loss: 2.3000559282302855\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.2982237339019775\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.2978262901306152\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2981841564178467\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.298227436542511\n",
            "Batch #200 Loss: 2.2989144420623777\n",
            "Batch #300 Loss: 2.2981280064582825\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.2962496280670166\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.2958462238311768\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2964158058166504\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.2971161508560183\n",
            "Batch #200 Loss: 2.2973402547836304\n",
            "Batch #300 Loss: 2.294773070812225\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.293989658355713\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.2935776710510254\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.293933153152466\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.295246183872223\n",
            "Batch #200 Loss: 2.294506776332855\n",
            "Batch #300 Loss: 2.2927555322647093\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.2912821769714355\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.290858745574951\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2912380695343018\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.2918052196502687\n",
            "Batch #200 Loss: 2.291554160118103\n",
            "Batch #300 Loss: 2.2903248977661135\n",
            "\u001b[92mTrain accuracy: 4803/48000 =  10.01 % ||| loss 2.2878475189208984\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.2874562740325928\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.28774356842041\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.2890696334838867\n",
            "Batch #200 Loss: 2.2880474376678466\n",
            "Batch #300 Loss: 2.287078754901886\n",
            "\u001b[92mTrain accuracy: 5071/48000 =  10.56 % ||| loss 2.2833595275878906\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1258/12000 =  10.48 % ||| loss 2.282961845397949\u001b[0m\n",
            "\u001b[92mTest accuracy: 1063/10000 =  10.63 % ||| loss 2.2832279205322266\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.2843072032928466\n",
            "Batch #200 Loss: 2.2835531640052795\n",
            "Batch #300 Loss: 2.2805665063858034\n",
            "\u001b[92mTrain accuracy: 6531/48000 =  13.61 % ||| loss 2.277202606201172\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1641/12000 =  13.68 % ||| loss 2.276757001876831\u001b[0m\n",
            "\u001b[92mTest accuracy: 1367/10000 =  13.67 % ||| loss 2.2773280143737793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.2785808134078978\n",
            "Batch #200 Loss: 2.2765028858184815\n",
            "Batch #300 Loss: 2.2749757075309756\n",
            "\u001b[92mTrain accuracy: 10828/48000 =  22.56 % ||| loss 2.268648862838745\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2722/12000 =  22.68 % ||| loss 2.2681703567504883\u001b[0m\n",
            "\u001b[92mTest accuracy: 2234/10000 =  22.34 % ||| loss 2.2687668800354004\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.269169054031372\n",
            "Batch #200 Loss: 2.268235814571381\n",
            "Batch #300 Loss: 2.264577202796936\n",
            "\u001b[92mTrain accuracy: 11855/48000 =  24.7 % ||| loss 2.2561557292938232\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2966/12000 =  24.72 % ||| loss 2.255580425262451\u001b[0m\n",
            "\u001b[92mTest accuracy: 2454/10000 =  24.54 % ||| loss 2.256666898727417\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.258417432308197\n",
            "Batch #200 Loss: 2.25393447637558\n",
            "Batch #300 Loss: 2.2501294445991515\n",
            "\u001b[92mTrain accuracy: 12708/48000 =  26.47 % ||| loss 2.2373387813568115\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3174/12000 =  26.45 % ||| loss 2.236581563949585\u001b[0m\n",
            "\u001b[92mTest accuracy: 2647/10000 =  26.47 % ||| loss 2.237452507019043\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.2407439804077147\n",
            "Batch #200 Loss: 2.2315675711631773\n",
            "Batch #300 Loss: 2.2251654958724973\n",
            "\u001b[92mTrain accuracy: 13042/48000 =  27.17 % ||| loss 2.206756830215454\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3260/12000 =  27.17 % ||| loss 2.2058234214782715\u001b[0m\n",
            "\u001b[92mTest accuracy: 2713/10000 =  27.13 % ||| loss 2.20674991607666\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.2119145131111146\n",
            "Batch #200 Loss: 2.198798987865448\n",
            "Batch #300 Loss: 2.1862354254722596\n",
            "\u001b[92mTrain accuracy: 16077/48000 =  33.49 % ||| loss 2.154111862182617\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4048/12000 =  33.73 % ||| loss 2.152855157852173\u001b[0m\n",
            "\u001b[92mTest accuracy: 3355/10000 =  33.55 % ||| loss 2.1541998386383057\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.1558941531181337\n",
            "Batch #200 Loss: 2.136301865577698\n",
            "Batch #300 Loss: 2.1123604369163513\n",
            "\u001b[92mTrain accuracy: 17464/48000 =  36.38 % ||| loss 2.055662155151367\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4401/12000 =  36.68 % ||| loss 2.054086446762085\u001b[0m\n",
            "\u001b[92mTest accuracy: 3632/10000 =  36.32 % ||| loss 2.056307792663574\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.060647599697113\n",
            "Batch #200 Loss: 2.023109052181244\n",
            "Batch #300 Loss: 1.9781354665756226\n",
            "\u001b[92mTrain accuracy: 17814/48000 =  37.11 % ||| loss 1.876620888710022\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4462/12000 =  37.18 % ||| loss 1.8745619058609009\u001b[0m\n",
            "\u001b[92mTest accuracy: 3690/10000 =  36.9 % ||| loss 1.8775064945220947\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 1.898550250530243\n",
            "Batch #200 Loss: 1.8469610679149628\n",
            "Batch #300 Loss: 1.7914212334156037\n",
            "\u001b[92mTrain accuracy: 20092/48000 =  41.86 % ||| loss 1.6576623916625977\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5039/12000 =  41.99 % ||| loss 1.6550562381744385\u001b[0m\n",
            "\u001b[92mTest accuracy: 4202/10000 =  42.02 % ||| loss 1.6580253839492798\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 1.7087093877792359\n",
            "Batch #200 Loss: 1.6631861019134522\n",
            "Batch #300 Loss: 1.6246450757980346\n",
            "\u001b[92mTrain accuracy: 23966/48000 =  49.93 % ||| loss 1.4836113452911377\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5974/12000 =  49.78 % ||| loss 1.4807502031326294\u001b[0m\n",
            "\u001b[92mTest accuracy: 5003/10000 =  50.03 % ||| loss 1.4875470399856567\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 1.5681770622730256\n",
            "Batch #200 Loss: 1.5278789269924165\n",
            "Batch #300 Loss: 1.4978138661384583\n",
            "\u001b[92mTrain accuracy: 26160/48000 =  54.5 % ||| loss 1.3613865375518799\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6521/12000 =  54.34 % ||| loss 1.3577680587768555\u001b[0m\n",
            "\u001b[92mTest accuracy: 5442/10000 =  54.42 % ||| loss 1.3651471138000488\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 1.4515571045875548\n",
            "Batch #200 Loss: 1.4370592164993286\n",
            "Batch #300 Loss: 1.4124094331264496\n",
            "\u001b[92mTrain accuracy: 26964/48000 =  56.17 % ||| loss 1.2716790437698364\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6733/12000 =  56.11 % ||| loss 1.2675138711929321\u001b[0m\n",
            "\u001b[92mTest accuracy: 5603/10000 =  56.03 % ||| loss 1.2758724689483643\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 1.3788275337219238\n",
            "Batch #200 Loss: 1.3518387353420258\n",
            "Batch #300 Loss: 1.3467985606193542\n",
            "\u001b[92mTrain accuracy: 27350/48000 =  56.98 % ||| loss 1.198887586593628\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6822/12000 =  56.85 % ||| loss 1.1939592361450195\u001b[0m\n",
            "\u001b[92mTest accuracy: 5657/10000 =  56.57 % ||| loss 1.2029513120651245\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 1.3036009550094605\n",
            "Batch #200 Loss: 1.2860173797607422\n",
            "Batch #300 Loss: 1.2787235045433045\n",
            "\u001b[92mTrain accuracy: 28056/48000 =  58.45 % ||| loss 1.142611026763916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7021/12000 =  58.51 % ||| loss 1.1373084783554077\u001b[0m\n",
            "\u001b[92mTest accuracy: 5786/10000 =  57.86 % ||| loss 1.1505017280578613\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 1.2460203230381013\n",
            "Batch #200 Loss: 1.230145684480667\n",
            "Batch #300 Loss: 1.2278620898723602\n",
            "\u001b[92mTrain accuracy: 28517/48000 =  59.41 % ||| loss 1.099247694015503\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7134/12000 =  59.45 % ||| loss 1.0938552618026733\u001b[0m\n",
            "\u001b[92mTest accuracy: 5872/10000 =  58.72 % ||| loss 1.1102319955825806\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 1.1961502981185914\n",
            "Batch #200 Loss: 1.186146057844162\n",
            "Batch #300 Loss: 1.1859182059764861\n",
            "\u001b[92mTrain accuracy: 29084/48000 =  60.59 % ||| loss 1.0632747411727905\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7265/12000 =  60.54 % ||| loss 1.0562877655029297\u001b[0m\n",
            "\u001b[92mTest accuracy: 6005/10000 =  60.05 % ||| loss 1.0712370872497559\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_18</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_18' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_18</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_180012-Lenet5Dropout_1726089678.067411_19</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_19' target=\"_blank\">Lenet5Dropout_1726089678.067411_19</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_19' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_19</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.304156789779663\n",
            "Batch #200 Loss: 2.3047387838363647\n",
            "Batch #300 Loss: 2.303223149776459\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3031301498413086\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3033132553100586\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3034002780914307\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3031985759735107\n",
            "Batch #200 Loss: 2.302802927494049\n",
            "Batch #300 Loss: 2.302635734081268\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3021910190582275\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3023765087127686\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302273988723755\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.30299535036087\n",
            "Batch #200 Loss: 2.301881890296936\n",
            "Batch #300 Loss: 2.3020022439956667\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3011727333068848\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.301316976547241\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3009705543518066\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3016068887710572\n",
            "Batch #200 Loss: 2.30183260679245\n",
            "Batch #300 Loss: 2.3007148623466493\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.300137996673584\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.300246000289917\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.300367832183838\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.300381407737732\n",
            "Batch #200 Loss: 2.2998796629905702\n",
            "Batch #300 Loss: 2.299506177902222\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.2990360260009766\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.299121379852295\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.299138307571411\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.299317467212677\n",
            "Batch #200 Loss: 2.2985139536857604\n",
            "Batch #300 Loss: 2.2988357496261598\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.297842502593994\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.29793119430542\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2980377674102783\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.2986953353881834\n",
            "Batch #200 Loss: 2.2976419568061828\n",
            "Batch #300 Loss: 2.2972427558898927\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.2965362071990967\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.296598196029663\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.296600818634033\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.297428562641144\n",
            "Batch #200 Loss: 2.297111496925354\n",
            "Batch #300 Loss: 2.2955604076385496\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.2950310707092285\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.295069932937622\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2951748371124268\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.295164306163788\n",
            "Batch #200 Loss: 2.2951984167099\n",
            "Batch #300 Loss: 2.2944708967208864\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.293217182159424\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.2932028770446777\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.293390989303589\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.2939644265174866\n",
            "Batch #200 Loss: 2.2927019667625426\n",
            "Batch #300 Loss: 2.293093254566193\n",
            "\u001b[92mTrain accuracy: 4839/48000 =  10.08 % ||| loss 2.2910029888153076\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1178/12000 =  9.817 % ||| loss 2.290968894958496\u001b[0m\n",
            "\u001b[92mTest accuracy: 1008/10000 =  10.08 % ||| loss 2.2911839485168457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.291305480003357\n",
            "Batch #200 Loss: 2.2911888575553894\n",
            "Batch #300 Loss: 2.290027756690979\n",
            "\u001b[92mTrain accuracy: 5655/48000 =  11.78 % ||| loss 2.2880330085754395\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1384/12000 =  11.53 % ||| loss 2.287921190261841\u001b[0m\n",
            "\u001b[92mTest accuracy: 1167/10000 =  11.67 % ||| loss 2.2880380153656006\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.2890647411346436\n",
            "Batch #200 Loss: 2.2880049538612366\n",
            "Batch #300 Loss: 2.286643443107605\n",
            "\u001b[92mTrain accuracy: 8980/48000 =  18.71 % ||| loss 2.2841432094573975\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2230/12000 =  18.58 % ||| loss 2.283928632736206\u001b[0m\n",
            "\u001b[92mTest accuracy: 1825/10000 =  18.25 % ||| loss 2.2842156887054443\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.2847581362724303\n",
            "Batch #200 Loss: 2.2846739649772645\n",
            "Batch #300 Loss: 2.2830729556083678\n",
            "\u001b[92mTrain accuracy: 12430/48000 =  25.9 % ||| loss 2.279062032699585\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3116/12000 =  25.97 % ||| loss 2.278801679611206\u001b[0m\n",
            "\u001b[92mTest accuracy: 2548/10000 =  25.48 % ||| loss 2.279404640197754\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.2802478289604187\n",
            "Batch #200 Loss: 2.2788342237472534\n",
            "Batch #300 Loss: 2.276963884830475\n",
            "\u001b[92mTrain accuracy: 15637/48000 =  32.58 % ||| loss 2.272068738937378\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3879/12000 =  32.32 % ||| loss 2.2717013359069824\u001b[0m\n",
            "\u001b[92mTest accuracy: 3248/10000 =  32.48 % ||| loss 2.2723169326782227\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.275042691230774\n",
            "Batch #200 Loss: 2.270987753868103\n",
            "Batch #300 Loss: 2.269405772686005\n",
            "\u001b[92mTrain accuracy: 18255/48000 =  38.03 % ||| loss 2.262141227722168\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4584/12000 =  38.2 % ||| loss 2.2616171836853027\u001b[0m\n",
            "\u001b[92mTest accuracy: 3764/10000 =  37.64 % ||| loss 2.2626216411590576\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.264324178695679\n",
            "Batch #200 Loss: 2.2620697689056395\n",
            "Batch #300 Loss: 2.25709849357605\n",
            "\u001b[92mTrain accuracy: 19609/48000 =  40.85 % ||| loss 2.247232675552368\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4904/12000 =  40.87 % ||| loss 2.2465248107910156\u001b[0m\n",
            "\u001b[92mTest accuracy: 3995/10000 =  39.95 % ||| loss 2.2474148273468018\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.2491146421432493\n",
            "Batch #200 Loss: 2.2453464674949646\n",
            "Batch #300 Loss: 2.237679660320282\n",
            "\u001b[92mTrain accuracy: 20201/48000 =  42.09 % ||| loss 2.223050355911255\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5094/12000 =  42.45 % ||| loss 2.2221009731292725\u001b[0m\n",
            "\u001b[92mTest accuracy: 4186/10000 =  41.86 % ||| loss 2.223414182662964\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.228184242248535\n",
            "Batch #200 Loss: 2.2200254249572753\n",
            "Batch #300 Loss: 2.20758455991745\n",
            "\u001b[92mTrain accuracy: 20955/48000 =  43.66 % ||| loss 2.1822140216827393\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5281/12000 =  44.01 % ||| loss 2.1808881759643555\u001b[0m\n",
            "\u001b[92mTest accuracy: 4326/10000 =  43.26 % ||| loss 2.18282413482666\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.188319284915924\n",
            "Batch #200 Loss: 2.1762183570861815\n",
            "Batch #300 Loss: 2.1579836773872376\n",
            "\u001b[92mTrain accuracy: 21411/48000 =  44.61 % ||| loss 2.109590768814087\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5351/12000 =  44.59 % ||| loss 2.1075656414031982\u001b[0m\n",
            "\u001b[92mTest accuracy: 4408/10000 =  44.08 % ||| loss 2.110574245452881\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.120737874507904\n",
            "Batch #200 Loss: 2.095014922618866\n",
            "Batch #300 Loss: 2.0628760385513307\n",
            "\u001b[92mTrain accuracy: 21963/48000 =  45.76 % ||| loss 1.9773519039154053\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5481/12000 =  45.67 % ||| loss 1.9741817712783813\u001b[0m\n",
            "\u001b[92mTest accuracy: 4519/10000 =  45.19 % ||| loss 1.977867841720581\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 1.9970046067237854\n",
            "Batch #200 Loss: 1.9632061636447906\n",
            "Batch #300 Loss: 1.9114048814773559\n",
            "\u001b[92mTrain accuracy: 23758/48000 =  49.5 % ||| loss 1.7675989866256714\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5924/12000 =  49.37 % ||| loss 1.762813687324524\u001b[0m\n",
            "\u001b[92mTest accuracy: 4863/10000 =  48.63 % ||| loss 1.7714146375656128\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 1.8248515772819518\n",
            "Batch #200 Loss: 1.7751851570606232\n",
            "Batch #300 Loss: 1.712868241071701\n",
            "\u001b[92mTrain accuracy: 27012/48000 =  56.27 % ||| loss 1.5343713760375977\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6798/12000 =  56.65 % ||| loss 1.5282827615737915\u001b[0m\n",
            "\u001b[92mTest accuracy: 5566/10000 =  55.66 % ||| loss 1.5396993160247803\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 1.6320371127128601\n",
            "Batch #200 Loss: 1.598702986240387\n",
            "Batch #300 Loss: 1.5563271844387054\n",
            "\u001b[92mTrain accuracy: 27362/48000 =  57.0 % ||| loss 1.3649405241012573\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6863/12000 =  57.19 % ||| loss 1.3580964803695679\u001b[0m\n",
            "\u001b[92mTest accuracy: 5688/10000 =  56.88 % ||| loss 1.3709256649017334\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 1.5029650235176086\n",
            "Batch #200 Loss: 1.4701508843898774\n",
            "Batch #300 Loss: 1.4396899271011352\n",
            "\u001b[92mTrain accuracy: 28002/48000 =  58.34 % ||| loss 1.2573795318603516\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7087/12000 =  59.06 % ||| loss 1.2503472566604614\u001b[0m\n",
            "\u001b[92mTest accuracy: 5812/10000 =  58.12 % ||| loss 1.2681450843811035\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 1.402133584022522\n",
            "Batch #200 Loss: 1.3823522686958314\n",
            "Batch #300 Loss: 1.3684362530708314\n",
            "\u001b[92mTrain accuracy: 28731/48000 =  59.86 % ||| loss 1.186804175376892\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7224/12000 =  60.2 % ||| loss 1.1794968843460083\u001b[0m\n",
            "\u001b[92mTest accuracy: 5964/10000 =  59.64 % ||| loss 1.1995750665664673\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_19</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_19' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_19</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_180201-Lenet5Dropout_1726089678.067411_20</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_20' target=\"_blank\">Lenet5Dropout_1726089678.067411_20</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_20' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_20</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3080807304382325\n",
            "Batch #200 Loss: 2.3062538957595824\n",
            "Batch #300 Loss: 2.3052624464035034\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3050026893615723\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3056421279907227\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3050649166107178\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3057748341560362\n",
            "Batch #200 Loss: 2.305383906364441\n",
            "Batch #300 Loss: 2.3044244480133056\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.303408622741699\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3040335178375244\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303633689880371\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3043923711776735\n",
            "Batch #200 Loss: 2.3047240781784057\n",
            "Batch #300 Loss: 2.3019101643562316\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3018064498901367\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302371025085449\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.301849842071533\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3025662159919738\n",
            "Batch #200 Loss: 2.3025816607475282\n",
            "Batch #300 Loss: 2.302550344467163\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.300076961517334\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3005499839782715\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.299962043762207\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.300773696899414\n",
            "Batch #200 Loss: 2.3002349495887757\n",
            "Batch #300 Loss: 2.3002270913124083\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.2980473041534424\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.2984957695007324\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.298072576522827\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.298397510051727\n",
            "Batch #200 Loss: 2.298351082801819\n",
            "Batch #300 Loss: 2.297915997505188\n",
            "\u001b[92mTrain accuracy: 4811/48000 =  10.02 % ||| loss 2.2956995964050293\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.2960925102233887\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2960870265960693\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.2973756766319275\n",
            "Batch #200 Loss: 2.2951000118255616\n",
            "Batch #300 Loss: 2.295828025341034\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.293062448501587\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.293405771255493\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2931268215179443\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.2969132232666016\n",
            "Batch #200 Loss: 2.2936090874671935\n",
            "Batch #300 Loss: 2.293230638504028\n",
            "\u001b[92mTrain accuracy: 4840/48000 =  10.08 % ||| loss 2.289963483810425\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1196/12000 =  9.967 % ||| loss 2.2902681827545166\u001b[0m\n",
            "\u001b[92mTest accuracy: 1007/10000 =  10.07 % ||| loss 2.290163040161133\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.291856906414032\n",
            "Batch #200 Loss: 2.291052370071411\n",
            "Batch #300 Loss: 2.291054186820984\n",
            "\u001b[92mTrain accuracy: 4990/48000 =  10.4 % ||| loss 2.286208152770996\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1239/12000 =  10.32 % ||| loss 2.2864561080932617\u001b[0m\n",
            "\u001b[92mTest accuracy: 1036/10000 =  10.36 % ||| loss 2.286478281021118\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.2895045018196107\n",
            "Batch #200 Loss: 2.2876611089706422\n",
            "Batch #300 Loss: 2.286337447166443\n",
            "\u001b[92mTrain accuracy: 5356/48000 =  11.16 % ||| loss 2.2815535068511963\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1325/12000 =  11.04 % ||| loss 2.281689167022705\u001b[0m\n",
            "\u001b[92mTest accuracy: 1110/10000 =  11.1 % ||| loss 2.2815446853637695\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.2835127258300782\n",
            "Batch #200 Loss: 2.283203685283661\n",
            "Batch #300 Loss: 2.282588171958923\n",
            "\u001b[92mTrain accuracy: 5752/48000 =  11.98 % ||| loss 2.275632858276367\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1425/12000 =  11.88 % ||| loss 2.275709629058838\u001b[0m\n",
            "\u001b[92mTest accuracy: 1178/10000 =  11.78 % ||| loss 2.2758264541625977\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.2814735174179077\n",
            "Batch #200 Loss: 2.2772041583061218\n",
            "Batch #300 Loss: 2.276296947002411\n",
            "\u001b[92mTrain accuracy: 6206/48000 =  12.93 % ||| loss 2.268054246902466\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1534/12000 =  12.78 % ||| loss 2.2680351734161377\u001b[0m\n",
            "\u001b[92mTest accuracy: 1272/10000 =  12.72 % ||| loss 2.268123149871826\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.272884533405304\n",
            "Batch #200 Loss: 2.270887670516968\n",
            "Batch #300 Loss: 2.2680905628204346\n",
            "\u001b[92mTrain accuracy: 6901/48000 =  14.38 % ||| loss 2.25774884223938\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1700/12000 =  14.17 % ||| loss 2.2576122283935547\u001b[0m\n",
            "\u001b[92mTest accuracy: 1414/10000 =  14.14 % ||| loss 2.2578651905059814\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.2625706958770753\n",
            "Batch #200 Loss: 2.2634626770019532\n",
            "Batch #300 Loss: 2.2573981189727785\n",
            "\u001b[92mTrain accuracy: 8941/48000 =  18.63 % ||| loss 2.243271589279175\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2255/12000 =  18.79 % ||| loss 2.242997407913208\u001b[0m\n",
            "\u001b[92mTest accuracy: 1860/10000 =  18.6 % ||| loss 2.243252754211426\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.253568525314331\n",
            "Batch #200 Loss: 2.2479422807693483\n",
            "Batch #300 Loss: 2.2433503246307374\n",
            "\u001b[92mTrain accuracy: 11844/48000 =  24.68 % ||| loss 2.222027063369751\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2939/12000 =  24.49 % ||| loss 2.2214956283569336\u001b[0m\n",
            "\u001b[92mTest accuracy: 2499/10000 =  24.99 % ||| loss 2.2222137451171875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.233998403549194\n",
            "Batch #200 Loss: 2.229158754348755\n",
            "Batch #300 Loss: 2.2183888292312623\n",
            "\u001b[92mTrain accuracy: 14038/48000 =  29.25 % ||| loss 2.1904027462005615\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3530/12000 =  29.42 % ||| loss 2.1896355152130127\u001b[0m\n",
            "\u001b[92mTest accuracy: 2977/10000 =  29.77 % ||| loss 2.1903045177459717\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.2075593137741087\n",
            "Batch #200 Loss: 2.1966762256622316\n",
            "Batch #300 Loss: 2.1856460046768187\n",
            "\u001b[92mTrain accuracy: 15124/48000 =  31.51 % ||| loss 2.141477108001709\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3779/12000 =  31.49 % ||| loss 2.140350341796875\u001b[0m\n",
            "\u001b[92mTest accuracy: 3148/10000 =  31.48 % ||| loss 2.141901969909668\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.1639593601226808\n",
            "Batch #200 Loss: 2.1501678323745725\n",
            "Batch #300 Loss: 2.1300204372406006\n",
            "\u001b[92mTrain accuracy: 15259/48000 =  31.79 % ||| loss 2.0634727478027344\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3830/12000 =  31.92 % ||| loss 2.0620276927948\u001b[0m\n",
            "\u001b[92mTest accuracy: 3155/10000 =  31.55 % ||| loss 2.063474416732788\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.1001676058769227\n",
            "Batch #200 Loss: 2.0822209548950195\n",
            "Batch #300 Loss: 2.0524722957611083\n",
            "\u001b[92mTrain accuracy: 15303/48000 =  31.88 % ||| loss 1.948024868965149\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3866/12000 =  32.22 % ||| loss 1.9461337327957153\u001b[0m\n",
            "\u001b[92mTest accuracy: 3180/10000 =  31.8 % ||| loss 1.9466674327850342\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.008289248943329\n",
            "Batch #200 Loss: 1.9723059237003326\n",
            "Batch #300 Loss: 1.9355142378807069\n",
            "\u001b[92mTrain accuracy: 16691/48000 =  34.77 % ||| loss 1.8027843236923218\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4183/12000 =  34.86 % ||| loss 1.8006442785263062\u001b[0m\n",
            "\u001b[92mTest accuracy: 3475/10000 =  34.75 % ||| loss 1.8024351596832275\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 1.8986620461940766\n",
            "Batch #200 Loss: 1.860723922252655\n",
            "Batch #300 Loss: 1.8299197709560395\n",
            "\u001b[92mTrain accuracy: 18678/48000 =  38.91 % ||| loss 1.6620076894760132\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4682/12000 =  39.02 % ||| loss 1.6598601341247559\u001b[0m\n",
            "\u001b[92mTest accuracy: 3912/10000 =  39.12 % ||| loss 1.6633350849151611\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 1.7801758337020874\n",
            "Batch #200 Loss: 1.7549611687660218\n",
            "Batch #300 Loss: 1.7220579445362092\n",
            "\u001b[92mTrain accuracy: 22191/48000 =  46.23 % ||| loss 1.5349093675613403\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5601/12000 =  46.67 % ||| loss 1.5330387353897095\u001b[0m\n",
            "\u001b[92mTest accuracy: 4645/10000 =  46.45 % ||| loss 1.5370728969573975\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 1.675975512266159\n",
            "Batch #200 Loss: 1.6511075282096863\n",
            "Batch #300 Loss: 1.633564326763153\n",
            "\u001b[92mTrain accuracy: 24470/48000 =  50.98 % ||| loss 1.4293190240859985\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6147/12000 =  51.23 % ||| loss 1.4268124103546143\u001b[0m\n",
            "\u001b[92mTest accuracy: 5098/10000 =  50.98 % ||| loss 1.4329557418823242\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 1.597205560207367\n",
            "Batch #200 Loss: 1.5675114762783051\n",
            "Batch #300 Loss: 1.548061069250107\n",
            "\u001b[92mTrain accuracy: 25537/48000 =  53.2 % ||| loss 1.3415355682373047\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6365/12000 =  53.04 % ||| loss 1.3391751050949097\u001b[0m\n",
            "\u001b[92mTest accuracy: 5293/10000 =  52.93 % ||| loss 1.3462616205215454\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 1.5210560286045074\n",
            "Batch #200 Loss: 1.5000727999210357\n",
            "Batch #300 Loss: 1.4872958219051362\n",
            "\u001b[92mTrain accuracy: 25462/48000 =  53.05 % ||| loss 1.2726588249206543\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6336/12000 =  52.8 % ||| loss 1.2700632810592651\u001b[0m\n",
            "\u001b[92mTest accuracy: 5271/10000 =  52.71 % ||| loss 1.2752186059951782\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_20</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_20' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_20</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_180349-Lenet5Dropout_1726089678.067411_21</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_21' target=\"_blank\">Lenet5Dropout_1726089678.067411_21</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_21' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_21</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302004919052124\n",
            "Batch #200 Loss: 2.300049057006836\n",
            "Batch #300 Loss: 2.2960684084892273\n",
            "\u001b[92mTrain accuracy: 8049/48000 =  16.77 % ||| loss 2.2871272563934326\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2048/12000 =  17.07 % ||| loss 2.2870049476623535\u001b[0m\n",
            "\u001b[92mTest accuracy: 1645/10000 =  16.45 % ||| loss 2.287207841873169\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2819033789634706\n",
            "Batch #200 Loss: 2.2607675290107725\n",
            "Batch #300 Loss: 2.193160107135773\n",
            "\u001b[92mTrain accuracy: 17036/48000 =  35.49 % ||| loss 1.8106846809387207\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4282/12000 =  35.68 % ||| loss 1.8095017671585083\u001b[0m\n",
            "\u001b[92mTest accuracy: 3550/10000 =  35.5 % ||| loss 1.811556100845337\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.6486110234260558\n",
            "Batch #200 Loss: 1.3261486518383026\n",
            "Batch #300 Loss: 1.1977864146232604\n",
            "\u001b[92mTrain accuracy: 28519/48000 =  59.41 % ||| loss 1.0182231664657593\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7193/12000 =  59.94 % ||| loss 1.0118577480316162\u001b[0m\n",
            "\u001b[92mTest accuracy: 5938/10000 =  59.38 % ||| loss 1.0262699127197266\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.0838676917552947\n",
            "Batch #200 Loss: 1.0406336599588395\n",
            "Batch #300 Loss: 1.0067362010478973\n",
            "\u001b[92mTrain accuracy: 31523/48000 =  65.67 % ||| loss 0.8840962648391724\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7895/12000 =  65.79 % ||| loss 0.8772405982017517\u001b[0m\n",
            "\u001b[92mTest accuracy: 6495/10000 =  64.95 % ||| loss 0.9036495089530945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.9716215229034424\n",
            "Batch #200 Loss: 0.9240270590782166\n",
            "Batch #300 Loss: 0.9225865375995635\n",
            "\u001b[92mTrain accuracy: 33137/48000 =  69.04 % ||| loss 0.8155412673950195\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8310/12000 =  69.25 % ||| loss 0.8064930438995361\u001b[0m\n",
            "\u001b[92mTest accuracy: 6837/10000 =  68.37 % ||| loss 0.8279800415039062\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.8715564942359925\n",
            "Batch #200 Loss: 0.8607140707969666\n",
            "Batch #300 Loss: 0.8583654755353928\n",
            "\u001b[92mTrain accuracy: 34166/48000 =  71.18 % ||| loss 0.7604650259017944\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8528/12000 =  71.07 % ||| loss 0.754713237285614\u001b[0m\n",
            "\u001b[92mTest accuracy: 7038/10000 =  70.38 % ||| loss 0.7816144227981567\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.8191377782821655\n",
            "Batch #200 Loss: 0.8208404803276061\n",
            "Batch #300 Loss: 0.8008913481235505\n",
            "\u001b[92mTrain accuracy: 35128/48000 =  73.18 % ||| loss 0.7154610753059387\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8806/12000 =  73.38 % ||| loss 0.7076097726821899\u001b[0m\n",
            "\u001b[92mTest accuracy: 7234/10000 =  72.34 % ||| loss 0.7382311224937439\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.7812896037101745\n",
            "Batch #200 Loss: 0.7716663885116577\n",
            "Batch #300 Loss: 0.7627731424570083\n",
            "\u001b[92mTrain accuracy: 35232/48000 =  73.4 % ||| loss 0.6890130043029785\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8767/12000 =  73.06 % ||| loss 0.6818995475769043\u001b[0m\n",
            "\u001b[92mTest accuracy: 7227/10000 =  72.27 % ||| loss 0.7084445357322693\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.7330569106340409\n",
            "Batch #200 Loss: 0.7281716114282608\n",
            "Batch #300 Loss: 0.7283545285463333\n",
            "\u001b[92mTrain accuracy: 35829/48000 =  74.64 % ||| loss 0.6604993343353271\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8934/12000 =  74.45 % ||| loss 0.6533669233322144\u001b[0m\n",
            "\u001b[92mTest accuracy: 7392/10000 =  73.92 % ||| loss 0.6805506348609924\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.7063356912136078\n",
            "Batch #200 Loss: 0.6958159935474396\n",
            "Batch #300 Loss: 0.7149129414558411\n",
            "\u001b[92mTrain accuracy: 36697/48000 =  76.45 % ||| loss 0.6204608082771301\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9179/12000 =  76.49 % ||| loss 0.6150082349777222\u001b[0m\n",
            "\u001b[92mTest accuracy: 7579/10000 =  75.79 % ||| loss 0.6433085799217224\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6795150971412659\n",
            "Batch #200 Loss: 0.680963362455368\n",
            "Batch #300 Loss: 0.6726891601085663\n",
            "\u001b[92mTrain accuracy: 36621/48000 =  76.29 % ||| loss 0.604220449924469\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9110/12000 =  75.92 % ||| loss 0.6004258990287781\u001b[0m\n",
            "\u001b[92mTest accuracy: 7534/10000 =  75.34 % ||| loss 0.6308552622795105\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.6640996313095093\n",
            "Batch #200 Loss: 0.654743320941925\n",
            "Batch #300 Loss: 0.6444432753324508\n",
            "\u001b[92mTrain accuracy: 37145/48000 =  77.39 % ||| loss 0.5820817351341248\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9267/12000 =  77.22 % ||| loss 0.5782379508018494\u001b[0m\n",
            "\u001b[92mTest accuracy: 7665/10000 =  76.65 % ||| loss 0.6035683751106262\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.6344093427062034\n",
            "Batch #200 Loss: 0.6301709520816803\n",
            "Batch #300 Loss: 0.6277379289269447\n",
            "\u001b[92mTrain accuracy: 37142/48000 =  77.38 % ||| loss 0.5746862292289734\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9282/12000 =  77.35 % ||| loss 0.573076605796814\u001b[0m\n",
            "\u001b[92mTest accuracy: 7636/10000 =  76.36 % ||| loss 0.6012387275695801\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.6177037322521209\n",
            "Batch #200 Loss: 0.6254410290718079\n",
            "Batch #300 Loss: 0.6163583135604859\n",
            "\u001b[92mTrain accuracy: 37559/48000 =  78.25 % ||| loss 0.563549816608429\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9357/12000 =  77.98 % ||| loss 0.5618419051170349\u001b[0m\n",
            "\u001b[92mTest accuracy: 7755/10000 =  77.55 % ||| loss 0.5875377655029297\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5937675696611404\n",
            "Batch #200 Loss: 0.6183446672558784\n",
            "Batch #300 Loss: 0.604124596118927\n",
            "\u001b[92mTrain accuracy: 37644/48000 =  78.42 % ||| loss 0.549975574016571\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9387/12000 =  78.22 % ||| loss 0.549301266670227\u001b[0m\n",
            "\u001b[92mTest accuracy: 7736/10000 =  77.36 % ||| loss 0.573482871055603\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.6033817574381828\n",
            "Batch #200 Loss: 0.5970458495616913\n",
            "Batch #300 Loss: 0.5786550357937813\n",
            "\u001b[92mTrain accuracy: 38224/48000 =  79.63 % ||| loss 0.532318115234375\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9529/12000 =  79.41 % ||| loss 0.5303041338920593\u001b[0m\n",
            "\u001b[92mTest accuracy: 7880/10000 =  78.8 % ||| loss 0.5593186616897583\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5910642674565315\n",
            "Batch #200 Loss: 0.5829497009515763\n",
            "Batch #300 Loss: 0.5785090798139572\n",
            "\u001b[92mTrain accuracy: 38181/48000 =  79.54 % ||| loss 0.5225664377212524\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9532/12000 =  79.43 % ||| loss 0.5238184332847595\u001b[0m\n",
            "\u001b[92mTest accuracy: 7856/10000 =  78.56 % ||| loss 0.5476679801940918\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5790546599030495\n",
            "Batch #200 Loss: 0.5690860310196877\n",
            "Batch #300 Loss: 0.5701966911554337\n",
            "\u001b[92mTrain accuracy: 38723/48000 =  80.67 % ||| loss 0.5079882144927979\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9653/12000 =  80.44 % ||| loss 0.5100429058074951\u001b[0m\n",
            "\u001b[92mTest accuracy: 7980/10000 =  79.8 % ||| loss 0.5382152199745178\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5654727786779403\n",
            "Batch #200 Loss: 0.5511687669157982\n",
            "Batch #300 Loss: 0.548999317586422\n",
            "\u001b[92mTrain accuracy: 39061/48000 =  81.38 % ||| loss 0.4988529086112976\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9793/12000 =  81.61 % ||| loss 0.49835309386253357\u001b[0m\n",
            "\u001b[92mTest accuracy: 8048/10000 =  80.48 % ||| loss 0.5262663960456848\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.5463877016305924\n",
            "Batch #200 Loss: 0.554054688513279\n",
            "Batch #300 Loss: 0.5503892505168915\n",
            "\u001b[92mTrain accuracy: 38959/48000 =  81.16 % ||| loss 0.4913872182369232\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9743/12000 =  81.19 % ||| loss 0.493105947971344\u001b[0m\n",
            "\u001b[92mTest accuracy: 8030/10000 =  80.3 % ||| loss 0.5171202421188354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.5313532018661499\n",
            "Batch #200 Loss: 0.5383852547407151\n",
            "Batch #300 Loss: 0.5373648208379745\n",
            "\u001b[92mTrain accuracy: 39140/48000 =  81.54 % ||| loss 0.4941273629665375\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9777/12000 =  81.47 % ||| loss 0.49434149265289307\u001b[0m\n",
            "\u001b[92mTest accuracy: 8079/10000 =  80.79 % ||| loss 0.5249246954917908\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.5295102676749229\n",
            "Batch #200 Loss: 0.5326606231927872\n",
            "Batch #300 Loss: 0.5261558875441551\n",
            "\u001b[92mTrain accuracy: 39493/48000 =  82.28 % ||| loss 0.4747600555419922\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9865/12000 =  82.21 % ||| loss 0.476612389087677\u001b[0m\n",
            "\u001b[92mTest accuracy: 8179/10000 =  81.79 % ||| loss 0.5104480981826782\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5332155427336693\n",
            "Batch #200 Loss: 0.5203532260656357\n",
            "Batch #300 Loss: 0.5121176010370254\n",
            "\u001b[92mTrain accuracy: 39570/48000 =  82.44 % ||| loss 0.466613233089447\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9882/12000 =  82.35 % ||| loss 0.4700055718421936\u001b[0m\n",
            "\u001b[92mTest accuracy: 8193/10000 =  81.93 % ||| loss 0.4950968623161316\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5143575102090836\n",
            "Batch #200 Loss: 0.5124665856361389\n",
            "Batch #300 Loss: 0.5184794652462006\n",
            "\u001b[92mTrain accuracy: 39733/48000 =  82.78 % ||| loss 0.4599698483943939\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9956/12000 =  82.97 % ||| loss 0.46158578991889954\u001b[0m\n",
            "\u001b[92mTest accuracy: 8198/10000 =  81.98 % ||| loss 0.49361762404441833\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5066967707872391\n",
            "Batch #200 Loss: 0.4934578809142113\n",
            "Batch #300 Loss: 0.5059051445126533\n",
            "\u001b[92mTrain accuracy: 39969/48000 =  83.27 % ||| loss 0.4550870656967163\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9963/12000 =  83.03 % ||| loss 0.4602165222167969\u001b[0m\n",
            "\u001b[92mTest accuracy: 8262/10000 =  82.62 % ||| loss 0.48373326659202576\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_21</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_21' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_21</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_180540-Lenet5Dropout_1726089678.067411_22</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_22' target=\"_blank\">Lenet5Dropout_1726089678.067411_22</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_22' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_22</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3031957173347473\n",
            "Batch #200 Loss: 2.3016427850723264\n",
            "Batch #300 Loss: 2.298863034248352\n",
            "\u001b[92mTrain accuracy: 9603/48000 =  20.01 % ||| loss 2.2932567596435547\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2401/12000 =  20.01 % ||| loss 2.293024778366089\u001b[0m\n",
            "\u001b[92mTest accuracy: 2003/10000 =  20.03 % ||| loss 2.293376922607422\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2918781733512876\n",
            "Batch #200 Loss: 2.2841585159301756\n",
            "Batch #300 Loss: 2.26588853597641\n",
            "\u001b[92mTrain accuracy: 12902/48000 =  26.88 % ||| loss 2.19295597076416\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3284/12000 =  27.37 % ||| loss 2.1922879219055176\u001b[0m\n",
            "\u001b[92mTest accuracy: 2661/10000 =  26.61 % ||| loss 2.1924285888671875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.1426299905776975\n",
            "Batch #200 Loss: 1.876887593269348\n",
            "Batch #300 Loss: 1.547897175550461\n",
            "\u001b[92mTrain accuracy: 28933/48000 =  60.28 % ||| loss 1.1582708358764648\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7248/12000 =  60.4 % ||| loss 1.1537045240402222\u001b[0m\n",
            "\u001b[92mTest accuracy: 5960/10000 =  59.6 % ||| loss 1.1736797094345093\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.2598108005523683\n",
            "Batch #200 Loss: 1.2080735206604003\n",
            "Batch #300 Loss: 1.1512054139375687\n",
            "\u001b[92mTrain accuracy: 30576/48000 =  63.7 % ||| loss 0.9709230065345764\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7677/12000 =  63.98 % ||| loss 0.9616153836250305\u001b[0m\n",
            "\u001b[92mTest accuracy: 6293/10000 =  62.93 % ||| loss 0.9848668575286865\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 1.0842387402057647\n",
            "Batch #200 Loss: 1.049381715655327\n",
            "Batch #300 Loss: 1.0255852645635606\n",
            "\u001b[92mTrain accuracy: 31979/48000 =  66.62 % ||| loss 0.8793468475341797\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7985/12000 =  66.54 % ||| loss 0.8659063577651978\u001b[0m\n",
            "\u001b[92mTest accuracy: 6597/10000 =  65.97 % ||| loss 0.8924073576927185\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.991679927110672\n",
            "Batch #200 Loss: 0.9791392397880554\n",
            "Batch #300 Loss: 0.938893346786499\n",
            "\u001b[92mTrain accuracy: 33186/48000 =  69.14 % ||| loss 0.822170078754425\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8327/12000 =  69.39 % ||| loss 0.8101223111152649\u001b[0m\n",
            "\u001b[92mTest accuracy: 6850/10000 =  68.5 % ||| loss 0.836557149887085\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.9273000031709671\n",
            "Batch #200 Loss: 0.9187438863515854\n",
            "Batch #300 Loss: 0.8999035340547562\n",
            "\u001b[92mTrain accuracy: 33344/48000 =  69.47 % ||| loss 0.795153021812439\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8374/12000 =  69.78 % ||| loss 0.7820380926132202\u001b[0m\n",
            "\u001b[92mTest accuracy: 6893/10000 =  68.93 % ||| loss 0.8174039721488953\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.8925190126895904\n",
            "Batch #200 Loss: 0.8731960785388947\n",
            "Batch #300 Loss: 0.8532698655128479\n",
            "\u001b[92mTrain accuracy: 34030/48000 =  70.9 % ||| loss 0.7526476979255676\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8566/12000 =  71.38 % ||| loss 0.7378481030464172\u001b[0m\n",
            "\u001b[92mTest accuracy: 7026/10000 =  70.26 % ||| loss 0.7701845169067383\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.8661730098724365\n",
            "Batch #200 Loss: 0.8334365183115006\n",
            "Batch #300 Loss: 0.8244387429952621\n",
            "\u001b[92mTrain accuracy: 34447/48000 =  71.76 % ||| loss 0.7283610105514526\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8660/12000 =  72.17 % ||| loss 0.7141009569168091\u001b[0m\n",
            "\u001b[92mTest accuracy: 7137/10000 =  71.37 % ||| loss 0.7449803948402405\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.829420799612999\n",
            "Batch #200 Loss: 0.8091312474012375\n",
            "Batch #300 Loss: 0.8091587030887604\n",
            "\u001b[92mTrain accuracy: 34863/48000 =  72.63 % ||| loss 0.709783673286438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8725/12000 =  72.71 % ||| loss 0.6986611485481262\u001b[0m\n",
            "\u001b[92mTest accuracy: 7187/10000 =  71.87 % ||| loss 0.7316509485244751\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.7965992760658264\n",
            "Batch #200 Loss: 0.8026047652959823\n",
            "Batch #300 Loss: 0.806151008605957\n",
            "\u001b[92mTrain accuracy: 35166/48000 =  73.26 % ||| loss 0.6897179484367371\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8833/12000 =  73.61 % ||| loss 0.6760709881782532\u001b[0m\n",
            "\u001b[92mTest accuracy: 7298/10000 =  72.98 % ||| loss 0.7055222988128662\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.7852461248636246\n",
            "Batch #200 Loss: 0.7511153274774551\n",
            "Batch #300 Loss: 0.7656805562973023\n",
            "\u001b[92mTrain accuracy: 35525/48000 =  74.01 % ||| loss 0.6715823411941528\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8863/12000 =  73.86 % ||| loss 0.6604158878326416\u001b[0m\n",
            "\u001b[92mTest accuracy: 7329/10000 =  73.29 % ||| loss 0.6919582486152649\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.7460232543945312\n",
            "Batch #200 Loss: 0.755995381474495\n",
            "Batch #300 Loss: 0.7472185230255127\n",
            "\u001b[92mTrain accuracy: 35734/48000 =  74.45 % ||| loss 0.6674829125404358\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8930/12000 =  74.42 % ||| loss 0.6539673805236816\u001b[0m\n",
            "\u001b[92mTest accuracy: 7358/10000 =  73.58 % ||| loss 0.6885645985603333\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.7403523313999176\n",
            "Batch #200 Loss: 0.7382536756992341\n",
            "Batch #300 Loss: 0.7272830605506897\n",
            "\u001b[92mTrain accuracy: 36153/48000 =  75.32 % ||| loss 0.6330768465995789\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9037/12000 =  75.31 % ||| loss 0.6213573813438416\u001b[0m\n",
            "\u001b[92mTest accuracy: 7463/10000 =  74.63 % ||| loss 0.6543559432029724\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.7163584953546525\n",
            "Batch #200 Loss: 0.7056318312883377\n",
            "Batch #300 Loss: 0.7077151846885681\n",
            "\u001b[92mTrain accuracy: 36079/48000 =  75.16 % ||| loss 0.6275917291641235\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9007/12000 =  75.06 % ||| loss 0.6162266135215759\u001b[0m\n",
            "\u001b[92mTest accuracy: 7451/10000 =  74.51 % ||| loss 0.6497742533683777\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.6915030798316002\n",
            "Batch #200 Loss: 0.6949758011102677\n",
            "Batch #300 Loss: 0.7040849030017853\n",
            "\u001b[92mTrain accuracy: 36777/48000 =  76.62 % ||| loss 0.6048227548599243\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9177/12000 =  76.48 % ||| loss 0.597920298576355\u001b[0m\n",
            "\u001b[92mTest accuracy: 7574/10000 =  75.74 % ||| loss 0.6308881044387817\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.6823480644822121\n",
            "Batch #200 Loss: 0.680474442243576\n",
            "Batch #300 Loss: 0.6780797612667083\n",
            "\u001b[92mTrain accuracy: 37046/48000 =  77.18 % ||| loss 0.5950096845626831\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9245/12000 =  77.04 % ||| loss 0.588660478591919\u001b[0m\n",
            "\u001b[92mTest accuracy: 7648/10000 =  76.48 % ||| loss 0.6152756810188293\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.6638606470823288\n",
            "Batch #200 Loss: 0.6810094285011291\n",
            "Batch #300 Loss: 0.6574379616975784\n",
            "\u001b[92mTrain accuracy: 36510/48000 =  76.06 % ||| loss 0.5934067368507385\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9144/12000 =  76.2 % ||| loss 0.5828453898429871\u001b[0m\n",
            "\u001b[92mTest accuracy: 7547/10000 =  75.47 % ||| loss 0.6133800745010376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.6541686901450157\n",
            "Batch #200 Loss: 0.6669759476184844\n",
            "Batch #300 Loss: 0.6439519649744034\n",
            "\u001b[92mTrain accuracy: 37369/48000 =  77.85 % ||| loss 0.5713248252868652\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9367/12000 =  78.06 % ||| loss 0.5608429312705994\u001b[0m\n",
            "\u001b[92mTest accuracy: 7720/10000 =  77.2 % ||| loss 0.5960400104522705\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.6390307509899139\n",
            "Batch #200 Loss: 0.6367042678594589\n",
            "Batch #300 Loss: 0.6442214378714561\n",
            "\u001b[92mTrain accuracy: 37879/48000 =  78.91 % ||| loss 0.5543702840805054\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9473/12000 =  78.94 % ||| loss 0.5468494892120361\u001b[0m\n",
            "\u001b[92mTest accuracy: 7839/10000 =  78.39 % ||| loss 0.5854306221008301\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.6293389782309532\n",
            "Batch #200 Loss: 0.6269733637571335\n",
            "Batch #300 Loss: 0.6260144844651222\n",
            "\u001b[92mTrain accuracy: 37855/48000 =  78.86 % ||| loss 0.5464101433753967\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9449/12000 =  78.74 % ||| loss 0.5392951369285583\u001b[0m\n",
            "\u001b[92mTest accuracy: 7815/10000 =  78.15 % ||| loss 0.5691290497779846\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.6172954145073891\n",
            "Batch #200 Loss: 0.60470707654953\n",
            "Batch #300 Loss: 0.6224065878987313\n",
            "\u001b[92mTrain accuracy: 37763/48000 =  78.67 % ||| loss 0.5446350574493408\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9450/12000 =  78.75 % ||| loss 0.5374338626861572\u001b[0m\n",
            "\u001b[92mTest accuracy: 7762/10000 =  77.62 % ||| loss 0.5711943507194519\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.6199586400389672\n",
            "Batch #200 Loss: 0.6048292455077171\n",
            "Batch #300 Loss: 0.6023715496063232\n",
            "\u001b[92mTrain accuracy: 38409/48000 =  80.02 % ||| loss 0.5245056748390198\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9609/12000 =  80.08 % ||| loss 0.5213276147842407\u001b[0m\n",
            "\u001b[92mTest accuracy: 7929/10000 =  79.29 % ||| loss 0.5497115254402161\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5928476238250733\n",
            "Batch #200 Loss: 0.5986269867420196\n",
            "Batch #300 Loss: 0.5999386909604073\n",
            "\u001b[92mTrain accuracy: 38266/48000 =  79.72 % ||| loss 0.5238762497901917\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9565/12000 =  79.71 % ||| loss 0.5200530290603638\u001b[0m\n",
            "\u001b[92mTest accuracy: 7895/10000 =  78.95 % ||| loss 0.5485695004463196\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5925132536888122\n",
            "Batch #200 Loss: 0.5890298667550087\n",
            "Batch #300 Loss: 0.5876875510811805\n",
            "\u001b[92mTrain accuracy: 38814/48000 =  80.86 % ||| loss 0.5169987678527832\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9681/12000 =  80.67 % ||| loss 0.5149828791618347\u001b[0m\n",
            "\u001b[92mTest accuracy: 7970/10000 =  79.7 % ||| loss 0.5426154732704163\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_22</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_22' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_22</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_180734-Lenet5Dropout_1726089678.067411_23</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_23' target=\"_blank\">Lenet5Dropout_1726089678.067411_23</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_23' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_23</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3034586811065676\n",
            "Batch #200 Loss: 2.299958028793335\n",
            "Batch #300 Loss: 2.2970788741111754\n",
            "\u001b[92mTrain accuracy: 11427/48000 =  23.81 % ||| loss 2.289463758468628\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2820/12000 =  23.5 % ||| loss 2.2895541191101074\u001b[0m\n",
            "\u001b[92mTest accuracy: 2414/10000 =  24.14 % ||| loss 2.289508819580078\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2876560568809508\n",
            "Batch #200 Loss: 2.280543658733368\n",
            "Batch #300 Loss: 2.2631268763542174\n",
            "\u001b[92mTrain accuracy: 13523/48000 =  28.17 % ||| loss 2.198188304901123\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3339/12000 =  27.82 % ||| loss 2.198840379714966\u001b[0m\n",
            "\u001b[92mTest accuracy: 2803/10000 =  28.03 % ||| loss 2.198333978652954\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.1608894729614256\n",
            "Batch #200 Loss: 1.9624082839488983\n",
            "Batch #300 Loss: 1.6949434864521027\n",
            "\u001b[92mTrain accuracy: 26100/48000 =  54.37 % ||| loss 1.2337709665298462\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6504/12000 =  54.2 % ||| loss 1.2297533750534058\u001b[0m\n",
            "\u001b[92mTest accuracy: 5403/10000 =  54.03 % ||| loss 1.2399940490722656\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.3669256925582887\n",
            "Batch #200 Loss: 1.2883367669582366\n",
            "Batch #300 Loss: 1.2444124484062196\n",
            "\u001b[92mTrain accuracy: 29555/48000 =  61.57 % ||| loss 1.0120633840560913\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7384/12000 =  61.53 % ||| loss 1.0046067237854004\u001b[0m\n",
            "\u001b[92mTest accuracy: 6057/10000 =  60.57 % ||| loss 1.023098349571228\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 1.1701329571008683\n",
            "Batch #200 Loss: 1.149089870452881\n",
            "Batch #300 Loss: 1.0904278248548507\n",
            "\u001b[92mTrain accuracy: 31350/48000 =  65.31 % ||| loss 0.909702718257904\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7871/12000 =  65.59 % ||| loss 0.9009873270988464\u001b[0m\n",
            "\u001b[92mTest accuracy: 6446/10000 =  64.46 % ||| loss 0.9212688207626343\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 1.0492814725637436\n",
            "Batch #200 Loss: 1.0296411472558975\n",
            "Batch #300 Loss: 1.01379645049572\n",
            "\u001b[92mTrain accuracy: 33152/48000 =  69.07 % ||| loss 0.8170168995857239\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8285/12000 =  69.04 % ||| loss 0.8056779503822327\u001b[0m\n",
            "\u001b[92mTest accuracy: 6833/10000 =  68.33 % ||| loss 0.8321787118911743\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.9742949390411377\n",
            "Batch #200 Loss: 0.961187436580658\n",
            "Batch #300 Loss: 0.9322874689102173\n",
            "\u001b[92mTrain accuracy: 34317/48000 =  71.49 % ||| loss 0.7761028409004211\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8593/12000 =  71.61 % ||| loss 0.7649056315422058\u001b[0m\n",
            "\u001b[92mTest accuracy: 7058/10000 =  70.58 % ||| loss 0.7940685749053955\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.9235909426212311\n",
            "Batch #200 Loss: 0.9122702741622924\n",
            "Batch #300 Loss: 0.8922745817899704\n",
            "\u001b[92mTrain accuracy: 34512/48000 =  71.9 % ||| loss 0.7314273118972778\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8652/12000 =  72.1 % ||| loss 0.7190183997154236\u001b[0m\n",
            "\u001b[92mTest accuracy: 7095/10000 =  70.95 % ||| loss 0.7512089014053345\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.8780766308307648\n",
            "Batch #200 Loss: 0.8653470170497894\n",
            "Batch #300 Loss: 0.8586990523338318\n",
            "\u001b[92mTrain accuracy: 35098/48000 =  73.12 % ||| loss 0.7140697240829468\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8790/12000 =  73.25 % ||| loss 0.7022404670715332\u001b[0m\n",
            "\u001b[92mTest accuracy: 7221/10000 =  72.21 % ||| loss 0.7376174330711365\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.8454147654771805\n",
            "Batch #200 Loss: 0.8395796465873718\n",
            "Batch #300 Loss: 0.8296152365207672\n",
            "\u001b[92mTrain accuracy: 35382/48000 =  73.71 % ||| loss 0.6879033446311951\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8842/12000 =  73.68 % ||| loss 0.6772951483726501\u001b[0m\n",
            "\u001b[92mTest accuracy: 7297/10000 =  72.97 % ||| loss 0.7076731324195862\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.812091469168663\n",
            "Batch #200 Loss: 0.8051403856277466\n",
            "Batch #300 Loss: 0.7997160196304322\n",
            "\u001b[92mTrain accuracy: 35822/48000 =  74.63 % ||| loss 0.6645636558532715\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8937/12000 =  74.48 % ||| loss 0.654822826385498\u001b[0m\n",
            "\u001b[92mTest accuracy: 7407/10000 =  74.07 % ||| loss 0.6843417882919312\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.7815787082910538\n",
            "Batch #200 Loss: 0.7869538235664367\n",
            "Batch #300 Loss: 0.7751931440830231\n",
            "\u001b[92mTrain accuracy: 35770/48000 =  74.52 % ||| loss 0.658205509185791\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8930/12000 =  74.42 % ||| loss 0.6461220979690552\u001b[0m\n",
            "\u001b[92mTest accuracy: 7376/10000 =  73.76 % ||| loss 0.6827343106269836\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.7835978376865387\n",
            "Batch #200 Loss: 0.7607710772752762\n",
            "Batch #300 Loss: 0.766570930480957\n",
            "\u001b[92mTrain accuracy: 36197/48000 =  75.41 % ||| loss 0.6399363279342651\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9058/12000 =  75.48 % ||| loss 0.6283119916915894\u001b[0m\n",
            "\u001b[92mTest accuracy: 7475/10000 =  74.75 % ||| loss 0.663897693157196\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.7464784252643585\n",
            "Batch #200 Loss: 0.7565237379074097\n",
            "Batch #300 Loss: 0.734758950471878\n",
            "\u001b[92mTrain accuracy: 36539/48000 =  76.12 % ||| loss 0.6142145991325378\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9103/12000 =  75.86 % ||| loss 0.6048319339752197\u001b[0m\n",
            "\u001b[92mTest accuracy: 7530/10000 =  75.3 % ||| loss 0.6379882097244263\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.7339226368069649\n",
            "Batch #200 Loss: 0.7325411057472229\n",
            "Batch #300 Loss: 0.7261530256271362\n",
            "\u001b[92mTrain accuracy: 36863/48000 =  76.8 % ||| loss 0.6064928770065308\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9212/12000 =  76.77 % ||| loss 0.595539927482605\u001b[0m\n",
            "\u001b[92mTest accuracy: 7607/10000 =  76.07 % ||| loss 0.632763147354126\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.7202670747041702\n",
            "Batch #200 Loss: 0.704279482960701\n",
            "Batch #300 Loss: 0.7109315508604049\n",
            "\u001b[92mTrain accuracy: 36930/48000 =  76.94 % ||| loss 0.5878470540046692\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9215/12000 =  76.79 % ||| loss 0.5799587965011597\u001b[0m\n",
            "\u001b[92mTest accuracy: 7619/10000 =  76.19 % ||| loss 0.612698495388031\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.706594979763031\n",
            "Batch #200 Loss: 0.6963983088731766\n",
            "Batch #300 Loss: 0.703303427696228\n",
            "\u001b[92mTrain accuracy: 37204/48000 =  77.51 % ||| loss 0.5771124958992004\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9292/12000 =  77.43 % ||| loss 0.5678533911705017\u001b[0m\n",
            "\u001b[92mTest accuracy: 7686/10000 =  76.86 % ||| loss 0.6049172878265381\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.6762023913860321\n",
            "Batch #200 Loss: 0.711419295668602\n",
            "Batch #300 Loss: 0.6814339047670365\n",
            "\u001b[92mTrain accuracy: 37325/48000 =  77.76 % ||| loss 0.5667877197265625\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9328/12000 =  77.73 % ||| loss 0.5611655712127686\u001b[0m\n",
            "\u001b[92mTest accuracy: 7721/10000 =  77.21 % ||| loss 0.5890604257583618\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.6710997450351716\n",
            "Batch #200 Loss: 0.6682919982075691\n",
            "Batch #300 Loss: 0.6747958359122276\n",
            "\u001b[92mTrain accuracy: 37562/48000 =  78.25 % ||| loss 0.5671388506889343\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9404/12000 =  78.37 % ||| loss 0.5581141114234924\u001b[0m\n",
            "\u001b[92mTest accuracy: 7720/10000 =  77.2 % ||| loss 0.5897876620292664\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.6600655549764634\n",
            "Batch #200 Loss: 0.6617869606614113\n",
            "Batch #300 Loss: 0.6551529541611671\n",
            "\u001b[92mTrain accuracy: 37676/48000 =  78.49 % ||| loss 0.5464574098587036\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9400/12000 =  78.33 % ||| loss 0.5411865711212158\u001b[0m\n",
            "\u001b[92mTest accuracy: 7785/10000 =  77.85 % ||| loss 0.5715698003768921\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.6715583175420761\n",
            "Batch #200 Loss: 0.633120000064373\n",
            "Batch #300 Loss: 0.657578001320362\n",
            "\u001b[92mTrain accuracy: 37778/48000 =  78.7 % ||| loss 0.5388064980506897\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9446/12000 =  78.72 % ||| loss 0.5323600769042969\u001b[0m\n",
            "\u001b[92mTest accuracy: 7799/10000 =  77.99 % ||| loss 0.5643928050994873\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.6423453143239022\n",
            "Batch #200 Loss: 0.6382239082455635\n",
            "Batch #300 Loss: 0.6459825813770295\n",
            "\u001b[92mTrain accuracy: 38060/48000 =  79.29 % ||| loss 0.5342419147491455\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9497/12000 =  79.14 % ||| loss 0.5278325080871582\u001b[0m\n",
            "\u001b[92mTest accuracy: 7855/10000 =  78.55 % ||| loss 0.5568987727165222\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.6351100212335586\n",
            "Batch #200 Loss: 0.638839984536171\n",
            "Batch #300 Loss: 0.6298315459489823\n",
            "\u001b[92mTrain accuracy: 38091/48000 =  79.36 % ||| loss 0.5240551829338074\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9508/12000 =  79.23 % ||| loss 0.5208157300949097\u001b[0m\n",
            "\u001b[92mTest accuracy: 7849/10000 =  78.49 % ||| loss 0.5511537194252014\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.6321063348650933\n",
            "Batch #200 Loss: 0.6208265167474747\n",
            "Batch #300 Loss: 0.6125228270888329\n",
            "\u001b[92mTrain accuracy: 38130/48000 =  79.44 % ||| loss 0.5187112092971802\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9546/12000 =  79.55 % ||| loss 0.5156787633895874\u001b[0m\n",
            "\u001b[92mTest accuracy: 7888/10000 =  78.88 % ||| loss 0.5411713719367981\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.6149583235383034\n",
            "Batch #200 Loss: 0.6167840304970741\n",
            "Batch #300 Loss: 0.6195934957265854\n",
            "\u001b[92mTrain accuracy: 38355/48000 =  79.91 % ||| loss 0.5073096752166748\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9571/12000 =  79.76 % ||| loss 0.5056638121604919\u001b[0m\n",
            "\u001b[92mTest accuracy: 7917/10000 =  79.17 % ||| loss 0.5314136743545532\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_23</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_23' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_23</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_180927-Lenet5Dropout_1726089678.067411_24</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_24' target=\"_blank\">Lenet5Dropout_1726089678.067411_24</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_24' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_24</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.305298342704773\n",
            "Batch #200 Loss: 2.304196720123291\n",
            "Batch #300 Loss: 2.3036145782470703\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3022007942199707\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302476167678833\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302110195159912\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.302502634525299\n",
            "Batch #200 Loss: 2.3007868719100952\n",
            "Batch #300 Loss: 2.3010552644729616\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.299436330795288\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.2996349334716797\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.299431085586548\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.2995705437660217\n",
            "Batch #200 Loss: 2.2989328575134276\n",
            "Batch #300 Loss: 2.2979751348495485\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.296754837036133\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.2969248294830322\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2967796325683594\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.296886670589447\n",
            "Batch #200 Loss: 2.29597140789032\n",
            "Batch #300 Loss: 2.2948605370521546\n",
            "\u001b[92mTrain accuracy: 4876/48000 =  10.16 % ||| loss 2.2930943965911865\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1192/12000 =  9.933 % ||| loss 2.293254852294922\u001b[0m\n",
            "\u001b[92mTest accuracy: 1007/10000 =  10.07 % ||| loss 2.2930119037628174\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.2930657625198365\n",
            "Batch #200 Loss: 2.2915741705894472\n",
            "Batch #300 Loss: 2.2898448157310485\n",
            "\u001b[92mTrain accuracy: 10049/48000 =  20.94 % ||| loss 2.286752223968506\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2498/12000 =  20.82 % ||| loss 2.286877155303955\u001b[0m\n",
            "\u001b[92mTest accuracy: 2069/10000 =  20.69 % ||| loss 2.2866904735565186\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.2864084362983705\n",
            "Batch #200 Loss: 2.283478629589081\n",
            "Batch #300 Loss: 2.280384976863861\n",
            "\u001b[92mTrain accuracy: 18694/48000 =  38.95 % ||| loss 2.2735912799835205\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4665/12000 =  38.88 % ||| loss 2.27374529838562\u001b[0m\n",
            "\u001b[92mTest accuracy: 3883/10000 =  38.83 % ||| loss 2.2735815048217773\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.2730816054344176\n",
            "Batch #200 Loss: 2.2653246307373047\n",
            "Batch #300 Loss: 2.2578849530220033\n",
            "\u001b[92mTrain accuracy: 19265/48000 =  40.14 % ||| loss 2.238455295562744\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4843/12000 =  40.36 % ||| loss 2.238558769226074\u001b[0m\n",
            "\u001b[92mTest accuracy: 4021/10000 =  40.21 % ||| loss 2.2383534908294678\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.2330002331733705\n",
            "Batch #200 Loss: 2.2073748874664307\n",
            "Batch #300 Loss: 2.167693727016449\n",
            "\u001b[92mTrain accuracy: 17701/48000 =  36.88 % ||| loss 2.0497515201568604\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4427/12000 =  36.89 % ||| loss 2.0498135089874268\u001b[0m\n",
            "\u001b[92mTest accuracy: 3667/10000 =  36.67 % ||| loss 2.049387216567993\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 1.9976038706302643\n",
            "Batch #200 Loss: 1.8185665893554688\n",
            "Batch #300 Loss: 1.6281582736968994\n",
            "\u001b[92mTrain accuracy: 24937/48000 =  51.95 % ||| loss 1.3259707689285278\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6207/12000 =  51.73 % ||| loss 1.3223116397857666\u001b[0m\n",
            "\u001b[92mTest accuracy: 5159/10000 =  51.59 % ||| loss 1.3311412334442139\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 1.3816505753993988\n",
            "Batch #200 Loss: 1.3027388906478883\n",
            "Batch #300 Loss: 1.239855535030365\n",
            "\u001b[92mTrain accuracy: 28534/48000 =  59.45 % ||| loss 1.066555380821228\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7187/12000 =  59.89 % ||| loss 1.058576226234436\u001b[0m\n",
            "\u001b[92mTest accuracy: 5932/10000 =  59.32 % ||| loss 1.0760911703109741\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 1.1535753417015076\n",
            "Batch #200 Loss: 1.1312056231498717\n",
            "Batch #300 Loss: 1.0988082134723662\n",
            "\u001b[92mTrain accuracy: 30685/48000 =  63.93 % ||| loss 0.9666553735733032\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7731/12000 =  64.42 % ||| loss 0.9568188190460205\u001b[0m\n",
            "\u001b[92mTest accuracy: 6309/10000 =  63.09 % ||| loss 0.9813843965530396\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 1.0565708035230637\n",
            "Batch #200 Loss: 1.0480415993928909\n",
            "Batch #300 Loss: 1.0091999137401582\n",
            "\u001b[92mTrain accuracy: 31628/48000 =  65.89 % ||| loss 0.903383731842041\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7988/12000 =  66.57 % ||| loss 0.8922825455665588\u001b[0m\n",
            "\u001b[92mTest accuracy: 6546/10000 =  65.46 % ||| loss 0.9151556491851807\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.9809150594472885\n",
            "Batch #200 Loss: 0.9618626344203949\n",
            "Batch #300 Loss: 0.9618772727251053\n",
            "\u001b[92mTrain accuracy: 32638/48000 =  68.0 % ||| loss 0.8529019951820374\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8175/12000 =  68.12 % ||| loss 0.8407258987426758\u001b[0m\n",
            "\u001b[92mTest accuracy: 6759/10000 =  67.59 % ||| loss 0.8691036105155945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.9366410142183303\n",
            "Batch #200 Loss: 0.9265755903720856\n",
            "Batch #300 Loss: 0.8991037160158157\n",
            "\u001b[92mTrain accuracy: 33483/48000 =  69.76 % ||| loss 0.8111962676048279\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8427/12000 =  70.23 % ||| loss 0.7982286214828491\u001b[0m\n",
            "\u001b[92mTest accuracy: 6943/10000 =  69.43 % ||| loss 0.825337290763855\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.8963658595085144\n",
            "Batch #200 Loss: 0.8836770766973495\n",
            "Batch #300 Loss: 0.8672871577739716\n",
            "\u001b[92mTrain accuracy: 34031/48000 =  70.9 % ||| loss 0.780600905418396\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8551/12000 =  71.26 % ||| loss 0.7695916295051575\u001b[0m\n",
            "\u001b[92mTest accuracy: 7024/10000 =  70.24 % ||| loss 0.7976175546646118\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.8573990947008133\n",
            "Batch #200 Loss: 0.8358809214830398\n",
            "Batch #300 Loss: 0.8474728673696518\n",
            "\u001b[92mTrain accuracy: 34168/48000 =  71.18 % ||| loss 0.7558842897415161\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8551/12000 =  71.26 % ||| loss 0.7447646856307983\u001b[0m\n",
            "\u001b[92mTest accuracy: 7075/10000 =  70.75 % ||| loss 0.7712947726249695\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.8332859665155411\n",
            "Batch #200 Loss: 0.8207710909843445\n",
            "Batch #300 Loss: 0.8149541074037552\n",
            "\u001b[92mTrain accuracy: 34805/48000 =  72.51 % ||| loss 0.7345694899559021\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8715/12000 =  72.62 % ||| loss 0.7247303128242493\u001b[0m\n",
            "\u001b[92mTest accuracy: 7174/10000 =  71.74 % ||| loss 0.7511992454528809\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.8023390698432923\n",
            "Batch #200 Loss: 0.8079904782772064\n",
            "Batch #300 Loss: 0.7925353956222534\n",
            "\u001b[92mTrain accuracy: 34907/48000 =  72.72 % ||| loss 0.7166902422904968\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8748/12000 =  72.9 % ||| loss 0.7061728835105896\u001b[0m\n",
            "\u001b[92mTest accuracy: 7213/10000 =  72.13 % ||| loss 0.7328636646270752\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.7805266487598419\n",
            "Batch #200 Loss: 0.7882450926303863\n",
            "Batch #300 Loss: 0.7835474526882171\n",
            "\u001b[92mTrain accuracy: 35127/48000 =  73.18 % ||| loss 0.7024123668670654\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8799/12000 =  73.32 % ||| loss 0.6914556622505188\u001b[0m\n",
            "\u001b[92mTest accuracy: 7265/10000 =  72.65 % ||| loss 0.7213769555091858\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.7527832674980164\n",
            "Batch #200 Loss: 0.7723847109079361\n",
            "Batch #300 Loss: 0.7713024288415908\n",
            "\u001b[92mTrain accuracy: 35221/48000 =  73.38 % ||| loss 0.6938655376434326\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8793/12000 =  73.28 % ||| loss 0.6835436820983887\u001b[0m\n",
            "\u001b[92mTest accuracy: 7280/10000 =  72.8 % ||| loss 0.7118272185325623\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.7392557871341705\n",
            "Batch #200 Loss: 0.7455173075199127\n",
            "Batch #300 Loss: 0.750080270767212\n",
            "\u001b[92mTrain accuracy: 35731/48000 =  74.44 % ||| loss 0.6772741079330444\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8962/12000 =  74.68 % ||| loss 0.667789101600647\u001b[0m\n",
            "\u001b[92mTest accuracy: 7396/10000 =  73.96 % ||| loss 0.6953049898147583\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.7491012892127037\n",
            "Batch #200 Loss: 0.7226241201162338\n",
            "Batch #300 Loss: 0.7358585441112518\n",
            "\u001b[92mTrain accuracy: 35764/48000 =  74.51 % ||| loss 0.6637531518936157\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8915/12000 =  74.29 % ||| loss 0.656134843826294\u001b[0m\n",
            "\u001b[92mTest accuracy: 7391/10000 =  73.91 % ||| loss 0.6842932105064392\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.7248249918222427\n",
            "Batch #200 Loss: 0.7255403828620911\n",
            "Batch #300 Loss: 0.7274712616205216\n",
            "\u001b[92mTrain accuracy: 35943/48000 =  74.88 % ||| loss 0.6569045186042786\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8997/12000 =  74.98 % ||| loss 0.6469478607177734\u001b[0m\n",
            "\u001b[92mTest accuracy: 7419/10000 =  74.19 % ||| loss 0.6818795800209045\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.7221530872583389\n",
            "Batch #200 Loss: 0.7131618016958237\n",
            "Batch #300 Loss: 0.7079304087162018\n",
            "\u001b[92mTrain accuracy: 36198/48000 =  75.41 % ||| loss 0.6434028148651123\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9065/12000 =  75.54 % ||| loss 0.6359423995018005\u001b[0m\n",
            "\u001b[92mTest accuracy: 7504/10000 =  75.04 % ||| loss 0.6711385846138\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.7088997048139573\n",
            "Batch #200 Loss: 0.7089643931388855\n",
            "Batch #300 Loss: 0.6868333280086517\n",
            "\u001b[92mTrain accuracy: 36264/48000 =  75.55 % ||| loss 0.6347239017486572\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9086/12000 =  75.72 % ||| loss 0.6272216439247131\u001b[0m\n",
            "\u001b[92mTest accuracy: 7505/10000 =  75.05 % ||| loss 0.6573055386543274\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_24</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_24' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_24</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_181120-Lenet5Dropout_1726089678.067411_25</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_25' target=\"_blank\">Lenet5Dropout_1726089678.067411_25</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_25' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_25</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.30631352186203\n",
            "Batch #200 Loss: 2.3044763588905335\n",
            "Batch #300 Loss: 2.3036380290985106\n",
            "\u001b[92mTrain accuracy: 5108/48000 =  10.64 % ||| loss 2.3029117584228516\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1276/12000 =  10.63 % ||| loss 2.3032891750335693\u001b[0m\n",
            "\u001b[92mTest accuracy: 1069/10000 =  10.69 % ||| loss 2.303349256515503\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.302632908821106\n",
            "Batch #200 Loss: 2.3028359961509706\n",
            "Batch #300 Loss: 2.30217561006546\n",
            "\u001b[92mTrain accuracy: 6205/48000 =  12.93 % ||| loss 2.300732374191284\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1525/12000 =  12.71 % ||| loss 2.301046848297119\u001b[0m\n",
            "\u001b[92mTest accuracy: 1287/10000 =  12.87 % ||| loss 2.300969362258911\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3007480812072756\n",
            "Batch #200 Loss: 2.300320703983307\n",
            "Batch #300 Loss: 2.2998569416999817\n",
            "\u001b[92mTrain accuracy: 7347/48000 =  15.31 % ||| loss 2.2982981204986572\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1804/12000 =  15.03 % ||| loss 2.298567056655884\u001b[0m\n",
            "\u001b[92mTest accuracy: 1548/10000 =  15.48 % ||| loss 2.298419713973999\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.2981941318511963\n",
            "Batch #200 Loss: 2.2977909088134765\n",
            "Batch #300 Loss: 2.29697909116745\n",
            "\u001b[92mTrain accuracy: 8063/48000 =  16.8 % ||| loss 2.2949447631835938\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2004/12000 =  16.7 % ||| loss 2.295163869857788\u001b[0m\n",
            "\u001b[92mTest accuracy: 1689/10000 =  16.89 % ||| loss 2.295036554336548\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.2951040625572205\n",
            "Batch #200 Loss: 2.293902635574341\n",
            "Batch #300 Loss: 2.2928194069862364\n",
            "\u001b[92mTrain accuracy: 8338/48000 =  17.37 % ||| loss 2.2896463871002197\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2078/12000 =  17.32 % ||| loss 2.2898547649383545\u001b[0m\n",
            "\u001b[92mTest accuracy: 1757/10000 =  17.57 % ||| loss 2.289795160293579\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.2888894963264463\n",
            "Batch #200 Loss: 2.2878381395339966\n",
            "Batch #300 Loss: 2.2847366285324098\n",
            "\u001b[92mTrain accuracy: 11673/48000 =  24.32 % ||| loss 2.278414011001587\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2914/12000 =  24.28 % ||| loss 2.2785916328430176\u001b[0m\n",
            "\u001b[92mTest accuracy: 2443/10000 =  24.43 % ||| loss 2.278563976287842\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.278237826824188\n",
            "Batch #200 Loss: 2.2725667262077334\n",
            "Batch #300 Loss: 2.267558856010437\n",
            "\u001b[92mTrain accuracy: 14200/48000 =  29.58 % ||| loss 2.248469829559326\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3550/12000 =  29.58 % ||| loss 2.2486519813537598\u001b[0m\n",
            "\u001b[92mTest accuracy: 2955/10000 =  29.55 % ||| loss 2.2484500408172607\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.2472458195686342\n",
            "Batch #200 Loss: 2.229272971153259\n",
            "Batch #300 Loss: 2.201310293674469\n",
            "\u001b[92mTrain accuracy: 15518/48000 =  32.33 % ||| loss 2.1173315048217773\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3955/12000 =  32.96 % ||| loss 2.1176979541778564\u001b[0m\n",
            "\u001b[92mTest accuracy: 3232/10000 =  32.32 % ||| loss 2.118004083633423\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.097413556575775\n",
            "Batch #200 Loss: 1.981073603630066\n",
            "Batch #300 Loss: 1.8358041393756865\n",
            "\u001b[92mTrain accuracy: 23799/48000 =  49.58 % ||| loss 1.5410237312316895\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5972/12000 =  49.77 % ||| loss 1.5412265062332153\u001b[0m\n",
            "\u001b[92mTest accuracy: 4936/10000 =  49.36 % ||| loss 1.5423495769500732\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 1.5986888253688811\n",
            "Batch #200 Loss: 1.493971792459488\n",
            "Batch #300 Loss: 1.418121359348297\n",
            "\u001b[92mTrain accuracy: 28627/48000 =  59.64 % ||| loss 1.1775048971176147\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7196/12000 =  59.97 % ||| loss 1.1746491193771362\u001b[0m\n",
            "\u001b[92mTest accuracy: 5962/10000 =  59.62 % ||| loss 1.1832246780395508\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 1.318617603778839\n",
            "Batch #200 Loss: 1.2794037139415741\n",
            "Batch #300 Loss: 1.2504838186502456\n",
            "\u001b[92mTrain accuracy: 28606/48000 =  59.6 % ||| loss 1.0634669065475464\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7175/12000 =  59.79 % ||| loss 1.0588449239730835\u001b[0m\n",
            "\u001b[92mTest accuracy: 5922/10000 =  59.22 % ||| loss 1.0744335651397705\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 1.1960013008117676\n",
            "Batch #200 Loss: 1.1903226864337921\n",
            "Batch #300 Loss: 1.1609424757957458\n",
            "\u001b[92mTrain accuracy: 30016/48000 =  62.53 % ||| loss 1.0001779794692993\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7524/12000 =  62.7 % ||| loss 0.9930494427680969\u001b[0m\n",
            "\u001b[92mTest accuracy: 6196/10000 =  61.96 % ||| loss 1.0087074041366577\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 1.1268635416030883\n",
            "Batch #200 Loss: 1.110693119764328\n",
            "Batch #300 Loss: 1.1046377259492874\n",
            "\u001b[92mTrain accuracy: 30482/48000 =  63.5 % ||| loss 0.9519704580307007\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7680/12000 =  64.0 % ||| loss 0.9420713186264038\u001b[0m\n",
            "\u001b[92mTest accuracy: 6304/10000 =  63.04 % ||| loss 0.9644454717636108\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 1.0775578951835632\n",
            "Batch #200 Loss: 1.0632070696353912\n",
            "Batch #300 Loss: 1.058324284553528\n",
            "\u001b[92mTrain accuracy: 31534/48000 =  65.7 % ||| loss 0.9158642292022705\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7901/12000 =  65.84 % ||| loss 0.9043632745742798\u001b[0m\n",
            "\u001b[92mTest accuracy: 6525/10000 =  65.25 % ||| loss 0.9287621378898621\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 1.0238900202512742\n",
            "Batch #200 Loss: 1.0290328615903854\n",
            "Batch #300 Loss: 1.0101300233602524\n",
            "\u001b[92mTrain accuracy: 32036/48000 =  66.74 % ||| loss 0.8793990612030029\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8049/12000 =  67.07 % ||| loss 0.8678378462791443\u001b[0m\n",
            "\u001b[92mTest accuracy: 6649/10000 =  66.49 % ||| loss 0.8937823176383972\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 1.0021565192937851\n",
            "Batch #200 Loss: 0.9866895592212677\n",
            "Batch #300 Loss: 0.9828416454792023\n",
            "\u001b[92mTrain accuracy: 32584/48000 =  67.88 % ||| loss 0.8514132499694824\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8182/12000 =  68.18 % ||| loss 0.8390215635299683\u001b[0m\n",
            "\u001b[92mTest accuracy: 6739/10000 =  67.39 % ||| loss 0.8639034032821655\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.9578977864980698\n",
            "Batch #200 Loss: 0.9541808122396469\n",
            "Batch #300 Loss: 0.9573278939723968\n",
            "\u001b[92mTrain accuracy: 32993/48000 =  68.74 % ||| loss 0.829431414604187\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8282/12000 =  69.02 % ||| loss 0.8162332773208618\u001b[0m\n",
            "\u001b[92mTest accuracy: 6813/10000 =  68.13 % ||| loss 0.8406627178192139\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.9374816900491715\n",
            "Batch #200 Loss: 0.9428514343500137\n",
            "Batch #300 Loss: 0.9219646501541138\n",
            "\u001b[92mTrain accuracy: 33373/48000 =  69.53 % ||| loss 0.8042780160903931\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8363/12000 =  69.69 % ||| loss 0.7912898659706116\u001b[0m\n",
            "\u001b[92mTest accuracy: 6889/10000 =  68.89 % ||| loss 0.8198503851890564\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.9204017019271851\n",
            "Batch #200 Loss: 0.906678524017334\n",
            "Batch #300 Loss: 0.9008993238210679\n",
            "\u001b[92mTrain accuracy: 33669/48000 =  70.14 % ||| loss 0.7864253520965576\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8469/12000 =  70.58 % ||| loss 0.772459089756012\u001b[0m\n",
            "\u001b[92mTest accuracy: 6962/10000 =  69.62 % ||| loss 0.8034846782684326\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.8985742193460464\n",
            "Batch #200 Loss: 0.8831112688779831\n",
            "Batch #300 Loss: 0.881262708902359\n",
            "\u001b[92mTrain accuracy: 33805/48000 =  70.43 % ||| loss 0.7697851657867432\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8497/12000 =  70.81 % ||| loss 0.7564784288406372\u001b[0m\n",
            "\u001b[92mTest accuracy: 6975/10000 =  69.75 % ||| loss 0.784970223903656\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.86623978972435\n",
            "Batch #200 Loss: 0.8649242329597473\n",
            "Batch #300 Loss: 0.8703412628173828\n",
            "\u001b[92mTrain accuracy: 34198/48000 =  71.25 % ||| loss 0.7550748586654663\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8607/12000 =  71.73 % ||| loss 0.7414588928222656\u001b[0m\n",
            "\u001b[92mTest accuracy: 7061/10000 =  70.61 % ||| loss 0.7748060822486877\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.8659721475839615\n",
            "Batch #200 Loss: 0.8483975517749787\n",
            "Batch #300 Loss: 0.8496224826574326\n",
            "\u001b[92mTrain accuracy: 34223/48000 =  71.3 % ||| loss 0.7443780899047852\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8610/12000 =  71.75 % ||| loss 0.7313510179519653\u001b[0m\n",
            "\u001b[92mTest accuracy: 7085/10000 =  70.85 % ||| loss 0.758578360080719\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.8499890667200088\n",
            "Batch #200 Loss: 0.8385347557067871\n",
            "Batch #300 Loss: 0.8338766843080521\n",
            "\u001b[92mTrain accuracy: 34655/48000 =  72.2 % ||| loss 0.7294305562973022\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8733/12000 =  72.78 % ||| loss 0.7165054678916931\u001b[0m\n",
            "\u001b[92mTest accuracy: 7136/10000 =  71.36 % ||| loss 0.7496832609176636\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.8209319180250167\n",
            "Batch #200 Loss: 0.8385502934455872\n",
            "Batch #300 Loss: 0.8297241181135178\n",
            "\u001b[92mTrain accuracy: 34672/48000 =  72.23 % ||| loss 0.7260724902153015\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8728/12000 =  72.73 % ||| loss 0.7131286859512329\u001b[0m\n",
            "\u001b[92mTest accuracy: 7161/10000 =  71.61 % ||| loss 0.7463006377220154\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.8178640937805176\n",
            "Batch #200 Loss: 0.8231515300273895\n",
            "Batch #300 Loss: 0.8163569849729538\n",
            "\u001b[92mTrain accuracy: 34929/48000 =  72.77 % ||| loss 0.7130471467971802\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8774/12000 =  73.12 % ||| loss 0.7010862231254578\u001b[0m\n",
            "\u001b[92mTest accuracy: 7210/10000 =  72.1 % ||| loss 0.7326392531394958\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_25</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_25' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_25</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_181312-Lenet5Dropout_1726089678.067411_26</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_26' target=\"_blank\">Lenet5Dropout_1726089678.067411_26</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_26' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_26</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.303386025428772\n",
            "Batch #200 Loss: 2.303778166770935\n",
            "Batch #300 Loss: 2.303469355106354\n",
            "\u001b[92mTrain accuracy: 4834/48000 =  10.07 % ||| loss 2.302485942840576\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1194/12000 =  9.95 % ||| loss 2.3026676177978516\u001b[0m\n",
            "\u001b[92mTest accuracy: 999/10000 =  9.99 % ||| loss 2.3026962280273438\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3024053502082826\n",
            "Batch #200 Loss: 2.3026165914535524\n",
            "Batch #300 Loss: 2.3020855522155763\n",
            "\u001b[92mTrain accuracy: 4890/48000 =  10.19 % ||| loss 2.3012583255767822\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1210/12000 =  10.08 % ||| loss 2.301400899887085\u001b[0m\n",
            "\u001b[92mTest accuracy: 1007/10000 =  10.07 % ||| loss 2.301506519317627\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.301396429538727\n",
            "Batch #200 Loss: 2.3004092144966126\n",
            "Batch #300 Loss: 2.301378710269928\n",
            "\u001b[92mTrain accuracy: 4999/48000 =  10.41 % ||| loss 2.299896478652954\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1241/12000 =  10.34 % ||| loss 2.3000500202178955\u001b[0m\n",
            "\u001b[92mTest accuracy: 1038/10000 =  10.38 % ||| loss 2.300008773803711\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.30015310049057\n",
            "Batch #200 Loss: 2.2997525095939637\n",
            "Batch #300 Loss: 2.2994803047180175\n",
            "\u001b[92mTrain accuracy: 5014/48000 =  10.45 % ||| loss 2.29817533493042\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1234/12000 =  10.28 % ||| loss 2.2983243465423584\u001b[0m\n",
            "\u001b[92mTest accuracy: 1037/10000 =  10.37 % ||| loss 2.298217535018921\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.2982872033119204\n",
            "Batch #200 Loss: 2.297267198562622\n",
            "Batch #300 Loss: 2.29722749710083\n",
            "\u001b[92mTrain accuracy: 6843/48000 =  14.26 % ||| loss 2.295682668685913\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1677/12000 =  13.98 % ||| loss 2.295821189880371\u001b[0m\n",
            "\u001b[92mTest accuracy: 1423/10000 =  14.23 % ||| loss 2.295607566833496\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.2962763047218324\n",
            "Batch #200 Loss: 2.294554500579834\n",
            "Batch #300 Loss: 2.2943775725364683\n",
            "\u001b[92mTrain accuracy: 10632/48000 =  22.15 % ||| loss 2.291761636734009\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2632/12000 =  21.93 % ||| loss 2.2918779850006104\u001b[0m\n",
            "\u001b[92mTest accuracy: 2192/10000 =  21.92 % ||| loss 2.2919092178344727\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.2925510120391848\n",
            "Batch #200 Loss: 2.2910403990745545\n",
            "Batch #300 Loss: 2.2894747281074523\n",
            "\u001b[92mTrain accuracy: 11997/48000 =  24.99 % ||| loss 2.284860849380493\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2996/12000 =  24.97 % ||| loss 2.284930944442749\u001b[0m\n",
            "\u001b[92mTest accuracy: 2520/10000 =  25.2 % ||| loss 2.2850453853607178\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.285579104423523\n",
            "Batch #200 Loss: 2.283046588897705\n",
            "Batch #300 Loss: 2.279672303199768\n",
            "\u001b[92mTrain accuracy: 11689/48000 =  24.35 % ||| loss 2.270806312561035\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2929/12000 =  24.41 % ||| loss 2.2707929611206055\u001b[0m\n",
            "\u001b[92mTest accuracy: 2463/10000 =  24.63 % ||| loss 2.2709403038024902\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.2728811430931093\n",
            "Batch #200 Loss: 2.267111933231354\n",
            "Batch #300 Loss: 2.259738359451294\n",
            "\u001b[92mTrain accuracy: 11479/48000 =  23.91 % ||| loss 2.239403247833252\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2894/12000 =  24.12 % ||| loss 2.239318609237671\u001b[0m\n",
            "\u001b[92mTest accuracy: 2440/10000 =  24.4 % ||| loss 2.2394464015960693\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.2414411520957946\n",
            "Batch #200 Loss: 2.2287264466285706\n",
            "Batch #300 Loss: 2.2117099261283872\n",
            "\u001b[92mTrain accuracy: 13848/48000 =  28.85 % ||| loss 2.1531808376312256\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3501/12000 =  29.18 % ||| loss 2.152773380279541\u001b[0m\n",
            "\u001b[92mTest accuracy: 2882/10000 =  28.82 % ||| loss 2.1531898975372314\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.164543302059174\n",
            "Batch #200 Loss: 2.1145935893058776\n",
            "Batch #300 Loss: 2.058737802505493\n",
            "\u001b[92mTrain accuracy: 19608/48000 =  40.85 % ||| loss 1.8477940559387207\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4941/12000 =  41.17 % ||| loss 1.8478084802627563\u001b[0m\n",
            "\u001b[92mTest accuracy: 4066/10000 =  40.66 % ||| loss 1.8482584953308105\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 1.8912237632274627\n",
            "Batch #200 Loss: 1.7876159536838532\n",
            "Batch #300 Loss: 1.6925268149375916\n",
            "\u001b[92mTrain accuracy: 24744/48000 =  51.55 % ||| loss 1.3893558979034424\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6163/12000 =  51.36 % ||| loss 1.3884036540985107\u001b[0m\n",
            "\u001b[92mTest accuracy: 5146/10000 =  51.46 % ||| loss 1.3901209831237793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 1.551878787279129\n",
            "Batch #200 Loss: 1.4891940820217133\n",
            "Batch #300 Loss: 1.4323891282081604\n",
            "\u001b[92mTrain accuracy: 26163/48000 =  54.51 % ||| loss 1.1883119344711304\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6529/12000 =  54.41 % ||| loss 1.1852989196777344\u001b[0m\n",
            "\u001b[92mTest accuracy: 5450/10000 =  54.5 % ||| loss 1.1902860403060913\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 1.373681801557541\n",
            "Batch #200 Loss: 1.3416459333896638\n",
            "Batch #300 Loss: 1.3311796045303346\n",
            "\u001b[92mTrain accuracy: 26280/48000 =  54.75 % ||| loss 1.1065788269042969\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6562/12000 =  54.68 % ||| loss 1.1024137735366821\u001b[0m\n",
            "\u001b[92mTest accuracy: 5487/10000 =  54.87 % ||| loss 1.112445592880249\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 1.301444925069809\n",
            "Batch #200 Loss: 1.2607175242900848\n",
            "Batch #300 Loss: 1.2429552268981934\n",
            "\u001b[92mTrain accuracy: 27899/48000 =  58.12 % ||| loss 1.0497978925704956\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6977/12000 =  58.14 % ||| loss 1.0451898574829102\u001b[0m\n",
            "\u001b[92mTest accuracy: 5794/10000 =  57.94 % ||| loss 1.05780029296875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 1.21680473446846\n",
            "Batch #200 Loss: 1.2075893139839173\n",
            "Batch #300 Loss: 1.1992363154888153\n",
            "\u001b[92mTrain accuracy: 28951/48000 =  60.31 % ||| loss 1.0097371339797974\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7192/12000 =  59.93 % ||| loss 1.0041205883026123\u001b[0m\n",
            "\u001b[92mTest accuracy: 5968/10000 =  59.68 % ||| loss 1.0199813842773438\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 1.1644750720262527\n",
            "Batch #200 Loss: 1.1496979594230652\n",
            "Batch #300 Loss: 1.147714815735817\n",
            "\u001b[92mTrain accuracy: 29673/48000 =  61.82 % ||| loss 0.9741513133049011\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7388/12000 =  61.57 % ||| loss 0.9688349962234497\u001b[0m\n",
            "\u001b[92mTest accuracy: 6140/10000 =  61.4 % ||| loss 0.9876545667648315\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 1.132426655292511\n",
            "Batch #200 Loss: 1.1240294831991196\n",
            "Batch #300 Loss: 1.1083607971668243\n",
            "\u001b[92mTrain accuracy: 30926/48000 =  64.43 % ||| loss 0.9271926283836365\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7763/12000 =  64.69 % ||| loss 0.9198559522628784\u001b[0m\n",
            "\u001b[92mTest accuracy: 6337/10000 =  63.37 % ||| loss 0.9382776021957397\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 1.078769894838333\n",
            "Batch #200 Loss: 1.0773156225681304\n",
            "Batch #300 Loss: 1.0716408067941665\n",
            "\u001b[92mTrain accuracy: 31718/48000 =  66.08 % ||| loss 0.8949174880981445\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7976/12000 =  66.47 % ||| loss 0.8865280747413635\u001b[0m\n",
            "\u001b[92mTest accuracy: 6486/10000 =  64.86 % ||| loss 0.9050812721252441\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 1.0595479589700698\n",
            "Batch #200 Loss: 1.0425221067667008\n",
            "Batch #300 Loss: 1.0334928351640702\n",
            "\u001b[92mTrain accuracy: 32312/48000 =  67.32 % ||| loss 0.8637752532958984\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8106/12000 =  67.55 % ||| loss 0.8550542593002319\u001b[0m\n",
            "\u001b[92mTest accuracy: 6676/10000 =  66.76 % ||| loss 0.8758152723312378\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 1.016862763762474\n",
            "Batch #200 Loss: 1.01345372736454\n",
            "Batch #300 Loss: 1.0131473284959793\n",
            "\u001b[92mTrain accuracy: 33263/48000 =  69.3 % ||| loss 0.8370161652565002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8328/12000 =  69.4 % ||| loss 0.8281005620956421\u001b[0m\n",
            "\u001b[92mTest accuracy: 6879/10000 =  68.79 % ||| loss 0.8509662747383118\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.9905558931827545\n",
            "Batch #200 Loss: 0.9817322587966919\n",
            "Batch #300 Loss: 0.9879549688100815\n",
            "\u001b[92mTrain accuracy: 33359/48000 =  69.5 % ||| loss 0.8181434869766235\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8399/12000 =  69.99 % ||| loss 0.8077614307403564\u001b[0m\n",
            "\u001b[92mTest accuracy: 6911/10000 =  69.11 % ||| loss 0.8344631791114807\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.9674986839294434\n",
            "Batch #200 Loss: 0.9612089937925339\n",
            "Batch #300 Loss: 0.9644413137435913\n",
            "\u001b[92mTrain accuracy: 33760/48000 =  70.33 % ||| loss 0.7901223301887512\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8494/12000 =  70.78 % ||| loss 0.7798293828964233\u001b[0m\n",
            "\u001b[92mTest accuracy: 7001/10000 =  70.01 % ||| loss 0.8065145611763\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.9515053278207779\n",
            "Batch #200 Loss: 0.9481943166255951\n",
            "Batch #300 Loss: 0.9230357694625855\n",
            "\u001b[92mTrain accuracy: 34106/48000 =  71.05 % ||| loss 0.7707836627960205\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8584/12000 =  71.53 % ||| loss 0.7592923045158386\u001b[0m\n",
            "\u001b[92mTest accuracy: 7077/10000 =  70.77 % ||| loss 0.7844303846359253\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.9243038755655288\n",
            "Batch #200 Loss: 0.9212775939702987\n",
            "Batch #300 Loss: 0.9129740399122238\n",
            "\u001b[92mTrain accuracy: 34387/48000 =  71.64 % ||| loss 0.7531232833862305\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8675/12000 =  72.29 % ||| loss 0.7416381239891052\u001b[0m\n",
            "\u001b[92mTest accuracy: 7126/10000 =  71.26 % ||| loss 0.7689589262008667\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726089678.067411_26</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_26' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726089678.067411_26</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!!!!! Hyper Param Tuning Finished!!!!!!!!!!!\n",
            "Best Model: Lenet5Dropout(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "\n",
            "HyperParams: {'learning_rate': 0.001, 'momentum': 0.7, 'dropout': 0.5}\n",
            "\n",
            "Accuracies: {'train': 0.9247708333333333, 'val': 0.89625, 'test': 0.8886}\n"
          ]
        }
      ],
      "source": [
        "class Lenet5Dropout(Lenet5):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(kwargs['dropout'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.max_pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.max_pool1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    \n",
        "param_grid = {\n",
        "  'learning_rate':[0.1, 0.01,0.001],\n",
        "  'momentum':[0, 0.9, 0.7],\n",
        "  'dropout':[0.2, 0.35, 0.5]\n",
        "}\n",
        "\n",
        "best_dropout = hyperparameter_tuning(Lenet5Dropout, dataloaders, device, 25, **param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiiV2yXT4CFj"
      },
      "source": [
        "#### Using Weight Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHirXkQmNu2V",
        "outputId": "cd74126e-da53-4703-9710-93a3b01165ba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:Lenet5Decay_1726099462.4277499_1) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-accuracy</td><td>▁▇█</td></tr><tr><td>Test-loss</td><td>█▂▁</td></tr><tr><td>Train-accuracy</td><td>▁▇█</td></tr><tr><td>Train-loss</td><td>█▂▁</td></tr><tr><td>Validation-accuracy</td><td>▁▇█</td></tr><tr><td>Validation-loss</td><td>█▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-accuracy</td><td>0.7501</td></tr><tr><td>Test-loss</td><td>0.64233</td></tr><tr><td>Train-accuracy</td><td>0.76269</td></tr><tr><td>Train-loss</td><td>0.6189</td></tr><tr><td>Validation-accuracy</td><td>0.76142</td></tr><tr><td>Validation-loss</td><td>0.61987</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099462.4277499_1</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099462.4277499_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099462.4277499_1</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240911_200615-Lenet5Decay_1726099462.4277499_1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.18.0 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:Lenet5Decay_1726099462.4277499_1). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_200735-Lenet5Decay_1726099655.2927098_0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_0' target=\"_blank\">Lenet5Decay_1726099655.2927098_0</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3031405782699585\n",
            "Batch #200 Loss: 2.3027668833732604\n",
            "Batch #300 Loss: 2.3026126289367674\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3026208877563477\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3028647899627686\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025918006896973\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3026922512054444\n",
            "Batch #200 Loss: 2.3028743290901184\n",
            "Batch #300 Loss: 2.302789170742035\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3026485443115234\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.302687406539917\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027217388153076\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3028176736831667\n",
            "Batch #200 Loss: 2.3027800273895265\n",
            "Batch #300 Loss: 2.302765038013458\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3026368618011475\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302731513977051\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302649974822998\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3027770709991455\n",
            "Batch #200 Loss: 2.3027488088607786\n",
            "Batch #300 Loss: 2.3026648950576782\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3026962280273438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3026673793792725\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302727222442627\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3027393889427183\n",
            "Batch #200 Loss: 2.3027036714553835\n",
            "Batch #300 Loss: 2.3027579188346863\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.302657127380371\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.3028311729431152\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027215003967285\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3027583384513854\n",
            "Batch #200 Loss: 2.3027138090133668\n",
            "Batch #300 Loss: 2.302750208377838\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3026442527770996\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3029065132141113\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027637004852295\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.302763159275055\n",
            "Batch #200 Loss: 2.3027539610862733\n",
            "Batch #300 Loss: 2.3027349638938905\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302583932876587\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3027360439300537\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025851249694824\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.3026124382019044\n",
            "Batch #200 Loss: 2.3028064703941347\n",
            "Batch #300 Loss: 2.3027108907699585\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302618980407715\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302690267562866\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026552200317383\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.302813081741333\n",
            "Batch #200 Loss: 2.3027609610557556\n",
            "Batch #300 Loss: 2.3028287053108216\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302631139755249\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302649736404419\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025975227355957\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.3026997375488283\n",
            "Batch #200 Loss: 2.302797701358795\n",
            "Batch #300 Loss: 2.30269508600235\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3026344776153564\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.3026230335235596\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026461601257324\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.3026285576820373\n",
            "Batch #200 Loss: 2.3027092266082763\n",
            "Batch #300 Loss: 2.3027026438713074\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302666425704956\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3029608726501465\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302736282348633\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.3026688528060912\n",
            "Batch #200 Loss: 2.302620356082916\n",
            "Batch #300 Loss: 2.302674217224121\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3026609420776367\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3028385639190674\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026599884033203\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.302751522064209\n",
            "Batch #200 Loss: 2.3026403832435607\n",
            "Batch #300 Loss: 2.302616662979126\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302652597427368\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3027663230895996\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027076721191406\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.3026522064208983\n",
            "Batch #200 Loss: 2.3026785159111025\n",
            "Batch #300 Loss: 2.302810745239258\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3026466369628906\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3027749061584473\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302701950073242\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.302639293670654\n",
            "Batch #200 Loss: 2.302714793682098\n",
            "Batch #300 Loss: 2.3026777338981628\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302645206451416\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3026018142700195\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026227951049805\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.302779619693756\n",
            "Batch #200 Loss: 2.3026996660232544\n",
            "Batch #300 Loss: 2.3027843403816224\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025941848754883\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026859760284424\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302626132965088\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.3027495193481444\n",
            "Batch #200 Loss: 2.3026692128181456\n",
            "Batch #300 Loss: 2.302806315422058\n",
            "\u001b[92mTrain accuracy: 4739/48000 =  9.873 % ||| loss 2.3026740550994873\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1261/12000 =  10.51 % ||| loss 2.3025565147399902\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302750587463379\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.302845778465271\n",
            "Batch #200 Loss: 2.3026854610443115\n",
            "Batch #300 Loss: 2.3027668404579162\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3026604652404785\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302623748779297\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027122020721436\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.3027947545051575\n",
            "Batch #200 Loss: 2.30267067193985\n",
            "Batch #300 Loss: 2.3028467392921446\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302645444869995\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.30289888381958\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302726984024048\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.302750997543335\n",
            "Batch #200 Loss: 2.3027952313423157\n",
            "Batch #300 Loss: 2.3027145075798034\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3026716709136963\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026773929595947\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027100563049316\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.3027621841430665\n",
            "Batch #200 Loss: 2.3027381014823916\n",
            "Batch #300 Loss: 2.3027218174934387\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3026294708251953\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302708625793457\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026010990142822\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.3028108620643617\n",
            "Batch #200 Loss: 2.302651264667511\n",
            "Batch #300 Loss: 2.3028095984458923\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3026533126831055\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3027591705322266\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30263614654541\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.3028254461288453\n",
            "Batch #200 Loss: 2.3027366518974306\n",
            "Batch #300 Loss: 2.302751052379608\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.30259370803833\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302680253982544\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026046752929688\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.3027025961875918\n",
            "Batch #200 Loss: 2.3026259398460387\n",
            "Batch #300 Loss: 2.302659020423889\n",
            "\u001b[92mTrain accuracy: 4739/48000 =  9.873 % ||| loss 2.302668809890747\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1261/12000 =  10.51 % ||| loss 2.302520990371704\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026108741760254\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.302722918987274\n",
            "Batch #200 Loss: 2.3025874042510988\n",
            "Batch #300 Loss: 2.3028172707557677\n",
            "\u001b[92mTrain accuracy: 4739/48000 =  9.873 % ||| loss 2.302669048309326\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1261/12000 =  10.51 % ||| loss 2.3026182651519775\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302647352218628\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_0</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_0</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_200931-Lenet5Decay_1726099655.2927098_1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_1' target=\"_blank\">Lenet5Decay_1726099655.2927098_1</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.290131318569183\n",
            "Batch #200 Loss: 1.5586581337451935\n",
            "Batch #300 Loss: 0.9959647607803345\n",
            "\u001b[92mTrain accuracy: 32793/48000 =  68.32 % ||| loss 0.7931789755821228\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8229/12000 =  68.58 % ||| loss 0.7851133942604065\u001b[0m\n",
            "\u001b[92mTest accuracy: 6663/10000 =  66.63 % ||| loss 0.8192341327667236\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7569173628091812\n",
            "Batch #200 Loss: 0.6825532457232475\n",
            "Batch #300 Loss: 0.6687143355607986\n",
            "\u001b[92mTrain accuracy: 36739/48000 =  76.54 % ||| loss 0.6129377484321594\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9227/12000 =  76.89 % ||| loss 0.6080062389373779\u001b[0m\n",
            "\u001b[92mTest accuracy: 7543/10000 =  75.43 % ||| loss 0.6338198184967041\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6192936497926712\n",
            "Batch #200 Loss: 0.5922738710045814\n",
            "Batch #300 Loss: 0.5873033449053764\n",
            "\u001b[92mTrain accuracy: 37563/48000 =  78.26 % ||| loss 0.5783735513687134\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9375/12000 =  78.12 % ||| loss 0.5785331130027771\u001b[0m\n",
            "\u001b[92mTest accuracy: 7704/10000 =  77.04 % ||| loss 0.6090949773788452\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5566032019257545\n",
            "Batch #200 Loss: 0.5687618970870971\n",
            "Batch #300 Loss: 0.5534434267878532\n",
            "\u001b[92mTrain accuracy: 39138/48000 =  81.54 % ||| loss 0.5208113193511963\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9783/12000 =  81.53 % ||| loss 0.5188984274864197\u001b[0m\n",
            "\u001b[92mTest accuracy: 8063/10000 =  80.63 % ||| loss 0.5535877346992493\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5276484113931655\n",
            "Batch #200 Loss: 0.5377102768421174\n",
            "Batch #300 Loss: 0.5143043971061707\n",
            "\u001b[92mTrain accuracy: 39859/48000 =  83.04 % ||| loss 0.4749927222728729\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9980/12000 =  83.17 % ||| loss 0.4776759743690491\u001b[0m\n",
            "\u001b[92mTest accuracy: 8203/10000 =  82.03 % ||| loss 0.5007375478744507\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5151052206754685\n",
            "Batch #200 Loss: 0.5101408126950264\n",
            "Batch #300 Loss: 0.5078723123669624\n",
            "\u001b[92mTrain accuracy: 38859/48000 =  80.96 % ||| loss 0.5162498950958252\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9744/12000 =  81.2 % ||| loss 0.5193778276443481\u001b[0m\n",
            "\u001b[92mTest accuracy: 7976/10000 =  79.76 % ||| loss 0.5399239659309387\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.48959040492773054\n",
            "Batch #200 Loss: 0.4966725552082062\n",
            "Batch #300 Loss: 0.4917629188299179\n",
            "\u001b[92mTrain accuracy: 40327/48000 =  84.01 % ||| loss 0.4605264365673065\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10076/12000 =  83.97 % ||| loss 0.46038708090782166\u001b[0m\n",
            "\u001b[92mTest accuracy: 8322/10000 =  83.22 % ||| loss 0.48756393790245056\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.48581176578998564\n",
            "Batch #200 Loss: 0.4993704956769943\n",
            "Batch #300 Loss: 0.4716658115386963\n",
            "\u001b[92mTrain accuracy: 40559/48000 =  84.5 % ||| loss 0.43967342376708984\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10132/12000 =  84.43 % ||| loss 0.4420650899410248\u001b[0m\n",
            "\u001b[92mTest accuracy: 8388/10000 =  83.88 % ||| loss 0.46103620529174805\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.47409997463226317\n",
            "Batch #200 Loss: 0.4866616067290306\n",
            "Batch #300 Loss: 0.47062438040971755\n",
            "\u001b[92mTrain accuracy: 40734/48000 =  84.86 % ||| loss 0.42677897214889526\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10161/12000 =  84.67 % ||| loss 0.42777395248413086\u001b[0m\n",
            "\u001b[92mTest accuracy: 8420/10000 =  84.2 % ||| loss 0.4481218159198761\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.47427054345607755\n",
            "Batch #200 Loss: 0.44368304550647736\n",
            "Batch #300 Loss: 0.47331429958343507\n",
            "\u001b[92mTrain accuracy: 40528/48000 =  84.43 % ||| loss 0.4344240725040436\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10085/12000 =  84.04 % ||| loss 0.4403783082962036\u001b[0m\n",
            "\u001b[92mTest accuracy: 8366/10000 =  83.66 % ||| loss 0.46045732498168945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.4636929824948311\n",
            "Batch #200 Loss: 0.4656700855493546\n",
            "Batch #300 Loss: 0.4587662735581398\n",
            "\u001b[92mTrain accuracy: 40705/48000 =  84.8 % ||| loss 0.42442387342453003\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10165/12000 =  84.71 % ||| loss 0.42943739891052246\u001b[0m\n",
            "\u001b[92mTest accuracy: 8395/10000 =  83.95 % ||| loss 0.4517291188240051\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.45596461564302443\n",
            "Batch #200 Loss: 0.4491451013088226\n",
            "Batch #300 Loss: 0.44844024091959\n",
            "\u001b[92mTrain accuracy: 40960/48000 =  85.33 % ||| loss 0.41369688510894775\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10188/12000 =  84.9 % ||| loss 0.4216051995754242\u001b[0m\n",
            "\u001b[92mTest accuracy: 8446/10000 =  84.46 % ||| loss 0.4380130469799042\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.4393657423555851\n",
            "Batch #200 Loss: 0.4489237394928932\n",
            "Batch #300 Loss: 0.46306190729141233\n",
            "\u001b[92mTrain accuracy: 39028/48000 =  81.31 % ||| loss 0.49200817942619324\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9743/12000 =  81.19 % ||| loss 0.49699851870536804\u001b[0m\n",
            "\u001b[92mTest accuracy: 8054/10000 =  80.54 % ||| loss 0.5228181481361389\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.45456174343824385\n",
            "Batch #200 Loss: 0.4385573327541351\n",
            "Batch #300 Loss: 0.4511425846815109\n",
            "\u001b[92mTrain accuracy: 41066/48000 =  85.55 % ||| loss 0.40727588534355164\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10251/12000 =  85.42 % ||| loss 0.413908988237381\u001b[0m\n",
            "\u001b[92mTest accuracy: 8511/10000 =  85.11 % ||| loss 0.4311535060405731\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.43475763604044915\n",
            "Batch #200 Loss: 0.4449640539288521\n",
            "Batch #300 Loss: 0.449829138815403\n",
            "\u001b[92mTrain accuracy: 40417/48000 =  84.2 % ||| loss 0.434375524520874\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10067/12000 =  83.89 % ||| loss 0.4397515654563904\u001b[0m\n",
            "\u001b[92mTest accuracy: 8301/10000 =  83.01 % ||| loss 0.463548481464386\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.43499205738306046\n",
            "Batch #200 Loss: 0.43327306061983106\n",
            "Batch #300 Loss: 0.44149614483118055\n",
            "\u001b[92mTrain accuracy: 39915/48000 =  83.16 % ||| loss 0.46005144715309143\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9968/12000 =  83.07 % ||| loss 0.4660305976867676\u001b[0m\n",
            "\u001b[92mTest accuracy: 8226/10000 =  82.26 % ||| loss 0.4888525903224945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.4345648416876793\n",
            "Batch #200 Loss: 0.44174746811389926\n",
            "Batch #300 Loss: 0.4398748502135277\n",
            "\u001b[92mTrain accuracy: 40800/48000 =  85.0 % ||| loss 0.41843611001968384\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10204/12000 =  85.03 % ||| loss 0.4236941337585449\u001b[0m\n",
            "\u001b[92mTest accuracy: 8413/10000 =  84.13 % ||| loss 0.4418331980705261\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.42660398572683333\n",
            "Batch #200 Loss: 0.43776065707206724\n",
            "Batch #300 Loss: 0.4380516502261162\n",
            "\u001b[92mTrain accuracy: 41228/48000 =  85.89 % ||| loss 0.40635210275650024\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10280/12000 =  85.67 % ||| loss 0.4131729006767273\u001b[0m\n",
            "\u001b[92mTest accuracy: 8500/10000 =  85.0 % ||| loss 0.431745320558548\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.44703444063663483\n",
            "Batch #200 Loss: 0.4191353763639927\n",
            "Batch #300 Loss: 0.4147038134932518\n",
            "\u001b[92mTrain accuracy: 41016/48000 =  85.45 % ||| loss 0.4082276225090027\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10199/12000 =  84.99 % ||| loss 0.4170870780944824\u001b[0m\n",
            "\u001b[92mTest accuracy: 8460/10000 =  84.6 % ||| loss 0.4301615357398987\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.4255447933077812\n",
            "Batch #200 Loss: 0.4419885250926018\n",
            "Batch #300 Loss: 0.43105662912130355\n",
            "\u001b[92mTrain accuracy: 41166/48000 =  85.76 % ||| loss 0.405569851398468\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10260/12000 =  85.5 % ||| loss 0.4110656976699829\u001b[0m\n",
            "\u001b[92mTest accuracy: 8478/10000 =  84.78 % ||| loss 0.43665361404418945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.42084646910429\n",
            "Batch #200 Loss: 0.42956114500761033\n",
            "Batch #300 Loss: 0.44449281960725784\n",
            "\u001b[92mTrain accuracy: 39567/48000 =  82.43 % ||| loss 0.464509516954422\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9926/12000 =  82.72 % ||| loss 0.4673610031604767\u001b[0m\n",
            "\u001b[92mTest accuracy: 8164/10000 =  81.64 % ||| loss 0.4920275807380676\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.41909699350595475\n",
            "Batch #200 Loss: 0.4188967701792717\n",
            "Batch #300 Loss: 0.43249612629413603\n",
            "\u001b[92mTrain accuracy: 40573/48000 =  84.53 % ||| loss 0.4255514442920685\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10145/12000 =  84.54 % ||| loss 0.42780765891075134\u001b[0m\n",
            "\u001b[92mTest accuracy: 8366/10000 =  83.66 % ||| loss 0.45829343795776367\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.4246896940469742\n",
            "Batch #200 Loss: 0.42529221311211585\n",
            "Batch #300 Loss: 0.4166442821919918\n",
            "\u001b[92mTrain accuracy: 39942/48000 =  83.21 % ||| loss 0.4519309997558594\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9952/12000 =  82.93 % ||| loss 0.46047285199165344\u001b[0m\n",
            "\u001b[92mTest accuracy: 8188/10000 =  81.88 % ||| loss 0.47597557306289673\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.42358031153678893\n",
            "Batch #200 Loss: 0.4130374301970005\n",
            "Batch #300 Loss: 0.42739296436309815\n",
            "\u001b[92mTrain accuracy: 40520/48000 =  84.42 % ||| loss 0.4243209660053253\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10055/12000 =  83.79 % ||| loss 0.43162769079208374\u001b[0m\n",
            "\u001b[92mTest accuracy: 8352/10000 =  83.52 % ||| loss 0.4525485336780548\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.4219967243075371\n",
            "Batch #200 Loss: 0.432786445915699\n",
            "Batch #300 Loss: 0.4163092502951622\n",
            "\u001b[92mTrain accuracy: 40746/48000 =  84.89 % ||| loss 0.41646435856819153\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10163/12000 =  84.69 % ||| loss 0.4248832166194916\u001b[0m\n",
            "\u001b[92mTest accuracy: 8407/10000 =  84.07 % ||| loss 0.4412977993488312\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_1</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_1</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_201120-Lenet5Decay_1726099655.2927098_2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_2' target=\"_blank\">Lenet5Decay_1726099655.2927098_2</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.299186406135559\n",
            "Batch #200 Loss: 1.818533947467804\n",
            "Batch #300 Loss: 1.0130429512262344\n",
            "\u001b[92mTrain accuracy: 33370/48000 =  69.52 % ||| loss 0.7941453456878662\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8413/12000 =  70.11 % ||| loss 0.7830581665039062\u001b[0m\n",
            "\u001b[92mTest accuracy: 6935/10000 =  69.35 % ||| loss 0.8108181357383728\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7103325122594834\n",
            "Batch #200 Loss: 0.6658174362778664\n",
            "Batch #300 Loss: 0.6189364120364189\n",
            "\u001b[92mTrain accuracy: 36928/48000 =  76.93 % ||| loss 0.5863221287727356\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9243/12000 =  77.03 % ||| loss 0.5796123147010803\u001b[0m\n",
            "\u001b[92mTest accuracy: 7596/10000 =  75.96 % ||| loss 0.6077520847320557\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5453636705875397\n",
            "Batch #200 Loss: 0.5313335946202278\n",
            "Batch #300 Loss: 0.5076456773281097\n",
            "\u001b[92mTrain accuracy: 38409/48000 =  80.02 % ||| loss 0.5095421671867371\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9562/12000 =  79.68 % ||| loss 0.5184505581855774\u001b[0m\n",
            "\u001b[92mTest accuracy: 7930/10000 =  79.3 % ||| loss 0.5392253994941711\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.4802525368332863\n",
            "Batch #200 Loss: 0.45566839098930356\n",
            "Batch #300 Loss: 0.4476513478159905\n",
            "\u001b[92mTrain accuracy: 40557/48000 =  84.49 % ||| loss 0.4228084683418274\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10142/12000 =  84.52 % ||| loss 0.43445855379104614\u001b[0m\n",
            "\u001b[92mTest accuracy: 8393/10000 =  83.93 % ||| loss 0.4459693431854248\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.41519455552101137\n",
            "Batch #200 Loss: 0.42582998767495156\n",
            "Batch #300 Loss: 0.4205797979235649\n",
            "\u001b[92mTrain accuracy: 41152/48000 =  85.73 % ||| loss 0.3864991366863251\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10257/12000 =  85.47 % ||| loss 0.39895009994506836\u001b[0m\n",
            "\u001b[92mTest accuracy: 8471/10000 =  84.71 % ||| loss 0.4179590046405792\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.39483627542853356\n",
            "Batch #200 Loss: 0.3819918204843998\n",
            "Batch #300 Loss: 0.40092945367097854\n",
            "\u001b[92mTrain accuracy: 41496/48000 =  86.45 % ||| loss 0.3666389286518097\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10316/12000 =  85.97 % ||| loss 0.3836295008659363\u001b[0m\n",
            "\u001b[92mTest accuracy: 8534/10000 =  85.34 % ||| loss 0.4015156924724579\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.365791275203228\n",
            "Batch #200 Loss: 0.37166483625769614\n",
            "Batch #300 Loss: 0.374946544021368\n",
            "\u001b[92mTrain accuracy: 41226/48000 =  85.89 % ||| loss 0.3772448003292084\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10173/12000 =  84.78 % ||| loss 0.40018123388290405\u001b[0m\n",
            "\u001b[92mTest accuracy: 8492/10000 =  84.92 % ||| loss 0.4109901785850525\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.35497539564967157\n",
            "Batch #200 Loss: 0.3556924122571945\n",
            "Batch #300 Loss: 0.3608337190747261\n",
            "\u001b[92mTrain accuracy: 41958/48000 =  87.41 % ||| loss 0.3372255861759186\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10391/12000 =  86.59 % ||| loss 0.3614770174026489\u001b[0m\n",
            "\u001b[92mTest accuracy: 8600/10000 =  86.0 % ||| loss 0.3871183395385742\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.34018570125103\n",
            "Batch #200 Loss: 0.34935829356312753\n",
            "Batch #300 Loss: 0.3446034088730812\n",
            "\u001b[92mTrain accuracy: 42226/48000 =  87.97 % ||| loss 0.32448461651802063\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10493/12000 =  87.44 % ||| loss 0.34696847200393677\u001b[0m\n",
            "\u001b[92mTest accuracy: 8641/10000 =  86.41 % ||| loss 0.3711700141429901\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3452318799495697\n",
            "Batch #200 Loss: 0.3200947007536888\n",
            "Batch #300 Loss: 0.3307000422477722\n",
            "\u001b[92mTrain accuracy: 42480/48000 =  88.5 % ||| loss 0.31830528378486633\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10511/12000 =  87.59 % ||| loss 0.3404950201511383\u001b[0m\n",
            "\u001b[92mTest accuracy: 8701/10000 =  87.01 % ||| loss 0.3583132028579712\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.32762610450387003\n",
            "Batch #200 Loss: 0.3129865051805973\n",
            "Batch #300 Loss: 0.321639946103096\n",
            "\u001b[92mTrain accuracy: 42263/48000 =  88.05 % ||| loss 0.32352063059806824\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10444/12000 =  87.03 % ||| loss 0.3529435694217682\u001b[0m\n",
            "\u001b[92mTest accuracy: 8677/10000 =  86.77 % ||| loss 0.3703252971172333\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.30991421893239024\n",
            "Batch #200 Loss: 0.31843902558088305\n",
            "Batch #300 Loss: 0.3158790583908558\n",
            "\u001b[92mTrain accuracy: 42513/48000 =  88.57 % ||| loss 0.3011742830276489\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10470/12000 =  87.25 % ||| loss 0.3345247805118561\u001b[0m\n",
            "\u001b[92mTest accuracy: 8666/10000 =  86.66 % ||| loss 0.3588554561138153\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.2931144750118256\n",
            "Batch #200 Loss: 0.3157538014650345\n",
            "Batch #300 Loss: 0.303588373363018\n",
            "\u001b[92mTrain accuracy: 42876/48000 =  89.33 % ||| loss 0.29316091537475586\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10558/12000 =  87.98 % ||| loss 0.32320770621299744\u001b[0m\n",
            "\u001b[92mTest accuracy: 8762/10000 =  87.62 % ||| loss 0.34415391087532043\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.29197272345423697\n",
            "Batch #200 Loss: 0.30556780129671096\n",
            "Batch #300 Loss: 0.2920748616755009\n",
            "\u001b[92mTrain accuracy: 42782/48000 =  89.13 % ||| loss 0.29741427302360535\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10537/12000 =  87.81 % ||| loss 0.33267325162887573\u001b[0m\n",
            "\u001b[92mTest accuracy: 8728/10000 =  87.28 % ||| loss 0.3564509153366089\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.29258365944027903\n",
            "Batch #200 Loss: 0.28632377803325654\n",
            "Batch #300 Loss: 0.29576302886009215\n",
            "\u001b[92mTrain accuracy: 43022/48000 =  89.63 % ||| loss 0.2789651155471802\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10615/12000 =  88.46 % ||| loss 0.31324607133865356\u001b[0m\n",
            "\u001b[92mTest accuracy: 8754/10000 =  87.54 % ||| loss 0.3360903859138489\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.28332002460956573\n",
            "Batch #200 Loss: 0.28769061774015425\n",
            "Batch #300 Loss: 0.28526701107621194\n",
            "\u001b[92mTrain accuracy: 43459/48000 =  90.54 % ||| loss 0.2615869343280792\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10668/12000 =  88.9 % ||| loss 0.3010278344154358\u001b[0m\n",
            "\u001b[92mTest accuracy: 8858/10000 =  88.58 % ||| loss 0.31814277172088623\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.2735870715230703\n",
            "Batch #200 Loss: 0.2788159209489822\n",
            "Batch #300 Loss: 0.28645327016711236\n",
            "\u001b[92mTrain accuracy: 43097/48000 =  89.79 % ||| loss 0.27298030257225037\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10588/12000 =  88.23 % ||| loss 0.31431612372398376\u001b[0m\n",
            "\u001b[92mTest accuracy: 8770/10000 =  87.7 % ||| loss 0.3391476571559906\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.27189974740147593\n",
            "Batch #200 Loss: 0.28096710398793223\n",
            "Batch #300 Loss: 0.2773492456972599\n",
            "\u001b[92mTrain accuracy: 43504/48000 =  90.63 % ||| loss 0.2571167051792145\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10669/12000 =  88.91 % ||| loss 0.30131542682647705\u001b[0m\n",
            "\u001b[92mTest accuracy: 8840/10000 =  88.4 % ||| loss 0.3291783034801483\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.2622650741040707\n",
            "Batch #200 Loss: 0.2749627491831779\n",
            "Batch #300 Loss: 0.267256438434124\n",
            "\u001b[92mTrain accuracy: 43766/48000 =  91.18 % ||| loss 0.2424699366092682\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10714/12000 =  89.28 % ||| loss 0.28993311524391174\u001b[0m\n",
            "\u001b[92mTest accuracy: 8882/10000 =  88.82 % ||| loss 0.31562864780426025\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.2636083656549454\n",
            "Batch #200 Loss: 0.2673990619927645\n",
            "Batch #300 Loss: 0.2686480888724327\n",
            "\u001b[92mTrain accuracy: 43455/48000 =  90.53 % ||| loss 0.25851941108703613\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10650/12000 =  88.75 % ||| loss 0.30868110060691833\u001b[0m\n",
            "\u001b[92mTest accuracy: 8798/10000 =  87.98 % ||| loss 0.3311404883861542\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.2596257008612156\n",
            "Batch #200 Loss: 0.26478919103741644\n",
            "Batch #300 Loss: 0.26212161034345627\n",
            "\u001b[92mTrain accuracy: 43828/48000 =  91.31 % ||| loss 0.24073559045791626\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10727/12000 =  89.39 % ||| loss 0.2903393805027008\u001b[0m\n",
            "\u001b[92mTest accuracy: 8902/10000 =  89.02 % ||| loss 0.32140013575553894\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.256131379455328\n",
            "Batch #200 Loss: 0.25394556418061254\n",
            "Batch #300 Loss: 0.2595844265073538\n",
            "\u001b[92mTrain accuracy: 43407/48000 =  90.43 % ||| loss 0.2613195478916168\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10636/12000 =  88.63 % ||| loss 0.31190991401672363\u001b[0m\n",
            "\u001b[92mTest accuracy: 8783/10000 =  87.83 % ||| loss 0.33533790707588196\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.25051626428961754\n",
            "Batch #200 Loss: 0.25236331097781656\n",
            "Batch #300 Loss: 0.25692368507385255\n",
            "\u001b[92mTrain accuracy: 43619/48000 =  90.87 % ||| loss 0.24794842302799225\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10681/12000 =  89.01 % ||| loss 0.29865604639053345\u001b[0m\n",
            "\u001b[92mTest accuracy: 8855/10000 =  88.55 % ||| loss 0.32017821073532104\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.25035609886050225\n",
            "Batch #200 Loss: 0.24346537306904792\n",
            "Batch #300 Loss: 0.2500885386765003\n",
            "\u001b[92mTrain accuracy: 43897/48000 =  91.45 % ||| loss 0.23355761170387268\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10712/12000 =  89.27 % ||| loss 0.2888920307159424\u001b[0m\n",
            "\u001b[92mTest accuracy: 8895/10000 =  88.95 % ||| loss 0.31439700722694397\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.24063888050615787\n",
            "Batch #200 Loss: 0.23454364366829394\n",
            "Batch #300 Loss: 0.2523556758463383\n",
            "\u001b[92mTrain accuracy: 43851/48000 =  91.36 % ||| loss 0.23264797031879425\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10701/12000 =  89.18 % ||| loss 0.29304906725883484\u001b[0m\n",
            "\u001b[92mTest accuracy: 8875/10000 =  88.75 % ||| loss 0.31160518527030945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_2</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_2</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_201308-Lenet5Decay_1726099655.2927098_3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_3' target=\"_blank\">Lenet5Decay_1726099655.2927098_3</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.303307902812958\n",
            "Batch #200 Loss: 2.3045531725883484\n",
            "Batch #300 Loss: 2.3031143665313722\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3029863834381104\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.303440809249878\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3031320571899414\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.303368215560913\n",
            "Batch #200 Loss: 2.303685231208801\n",
            "Batch #300 Loss: 2.3037444591522216\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.303478240966797\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3039236068725586\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3035500049591064\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.303538491725922\n",
            "Batch #200 Loss: 2.303683013916016\n",
            "Batch #300 Loss: 2.30288773059845\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3032889366149902\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.303260087966919\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3032898902893066\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3033980655670168\n",
            "Batch #200 Loss: 2.304191892147064\n",
            "Batch #300 Loss: 2.3033577156066896\n",
            "\u001b[92mTrain accuracy: 4739/48000 =  9.873 % ||| loss 2.302952527999878\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1261/12000 =  10.51 % ||| loss 2.3028857707977295\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3029801845550537\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3030661010742186\n",
            "Batch #200 Loss: 2.303920409679413\n",
            "Batch #300 Loss: 2.30339706659317\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3033618927001953\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.303280830383301\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3034539222717285\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.303608603477478\n",
            "Batch #200 Loss: 2.3031679153442384\n",
            "Batch #300 Loss: 2.303381450176239\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.303093910217285\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.303309202194214\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3031132221221924\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.3036987948417664\n",
            "Batch #200 Loss: 2.3033469915390015\n",
            "Batch #300 Loss: 2.3037322902679445\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.303269863128662\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.303992986679077\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3033254146575928\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.3037590336799623\n",
            "Batch #200 Loss: 2.303562877178192\n",
            "Batch #300 Loss: 2.3032715916633606\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3032259941101074\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.3033666610717773\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3031203746795654\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.3034602308273318\n",
            "Batch #200 Loss: 2.3033950567245483\n",
            "Batch #300 Loss: 2.3027141189575193\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.302834987640381\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.303061008453369\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028340339660645\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.303518178462982\n",
            "Batch #200 Loss: 2.3034982681274414\n",
            "Batch #300 Loss: 2.3036754822731016\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3031551837921143\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3033604621887207\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303281307220459\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.3035159802436826\n",
            "Batch #200 Loss: 2.3037676763534547\n",
            "Batch #300 Loss: 2.303379783630371\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3033156394958496\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.3033876419067383\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303406000137329\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.3033532643318178\n",
            "Batch #200 Loss: 2.3033455419540405\n",
            "Batch #300 Loss: 2.303817069530487\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.303227424621582\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.303276777267456\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303290367126465\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.30322286605835\n",
            "Batch #200 Loss: 2.3036627316474916\n",
            "Batch #300 Loss: 2.3037923479080202\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3027663230895996\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3030667304992676\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027899265289307\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.30374240398407\n",
            "Batch #200 Loss: 2.3035846519470216\n",
            "Batch #300 Loss: 2.3040343642234804\n",
            "\u001b[92mTrain accuracy: 4739/48000 =  9.873 % ||| loss 2.3034112453460693\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1261/12000 =  10.51 % ||| loss 2.3031766414642334\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3033814430236816\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.3037486600875856\n",
            "Batch #200 Loss: 2.3032712602615355\n",
            "Batch #300 Loss: 2.3038890027999877\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.303917407989502\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.303962469100952\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3039190769195557\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.303451397418976\n",
            "Batch #200 Loss: 2.3043247127532958\n",
            "Batch #300 Loss: 2.30381742477417\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.3033673763275146\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.3029370307922363\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303189754486084\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.303730731010437\n",
            "Batch #200 Loss: 2.303444027900696\n",
            "Batch #300 Loss: 2.3034362244606017\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.303879499435425\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3043417930603027\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303943395614624\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.3036834502220156\n",
            "Batch #200 Loss: 2.303226041793823\n",
            "Batch #300 Loss: 2.303719048500061\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3031058311462402\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3033299446105957\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303135633468628\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.303525598049164\n",
            "Batch #200 Loss: 2.3033402943611145\n",
            "Batch #300 Loss: 2.3035656595230103\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3030483722686768\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3032138347625732\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303105115890503\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.303109514713287\n",
            "Batch #200 Loss: 2.3038167786598205\n",
            "Batch #300 Loss: 2.302940685749054\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3029844760894775\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3030552864074707\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303025484085083\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.303660855293274\n",
            "Batch #200 Loss: 2.30399076461792\n",
            "Batch #300 Loss: 2.303654682636261\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.303640604019165\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.3029544353485107\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3035802841186523\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.3035151076316835\n",
            "Batch #200 Loss: 2.3040933847427367\n",
            "Batch #300 Loss: 2.304068021774292\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3031275272369385\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3033196926116943\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303133964538574\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.302966492176056\n",
            "Batch #200 Loss: 2.303265745639801\n",
            "Batch #300 Loss: 2.30316748380661\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3031845092773438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3036868572235107\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3032326698303223\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.3038617396354675\n",
            "Batch #200 Loss: 2.3040913915634156\n",
            "Batch #300 Loss: 2.303605625629425\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.3047103881835938\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.304036855697632\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.304730176925659\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3037738251686095\n",
            "Batch #200 Loss: 2.3035545539855957\n",
            "Batch #300 Loss: 2.303917102813721\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.303234100341797\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3032610416412354\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3031909465789795\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_3</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_3</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_201457-Lenet5Decay_1726099655.2927098_4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_4' target=\"_blank\">Lenet5Decay_1726099655.2927098_4</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.9323289185762405\n",
            "Batch #200 Loss: 0.8842926758527756\n",
            "Batch #300 Loss: 0.6906452947854995\n",
            "\u001b[92mTrain accuracy: 37710/48000 =  78.56 % ||| loss 0.5720733404159546\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9413/12000 =  78.44 % ||| loss 0.566231906414032\u001b[0m\n",
            "\u001b[92mTest accuracy: 7799/10000 =  77.99 % ||| loss 0.5820006132125854\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.599996054470539\n",
            "Batch #200 Loss: 0.5969297981262207\n",
            "Batch #300 Loss: 0.5676893186569214\n",
            "\u001b[92mTrain accuracy: 37989/48000 =  79.14 % ||| loss 0.5539823770523071\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9414/12000 =  78.45 % ||| loss 0.5630596876144409\u001b[0m\n",
            "\u001b[92mTest accuracy: 7836/10000 =  78.36 % ||| loss 0.5686607360839844\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5626416417956353\n",
            "Batch #200 Loss: 0.5612466371059418\n",
            "Batch #300 Loss: 0.5459073677659034\n",
            "\u001b[92mTrain accuracy: 39513/48000 =  82.32 % ||| loss 0.4755445122718811\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9862/12000 =  82.18 % ||| loss 0.47805798053741455\u001b[0m\n",
            "\u001b[92mTest accuracy: 8163/10000 =  81.63 % ||| loss 0.4986056685447693\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5334388306736946\n",
            "Batch #200 Loss: 0.5339227640628814\n",
            "Batch #300 Loss: 0.5569103187322617\n",
            "\u001b[92mTrain accuracy: 39188/48000 =  81.64 % ||| loss 0.5104632377624512\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9742/12000 =  81.18 % ||| loss 0.5085193514823914\u001b[0m\n",
            "\u001b[92mTest accuracy: 8077/10000 =  80.77 % ||| loss 0.5299162864685059\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5266450408101082\n",
            "Batch #200 Loss: 0.5608860543370247\n",
            "Batch #300 Loss: 0.5217689862847328\n",
            "\u001b[92mTrain accuracy: 38826/48000 =  80.89 % ||| loss 0.5209932327270508\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9750/12000 =  81.25 % ||| loss 0.5183264017105103\u001b[0m\n",
            "\u001b[92mTest accuracy: 8034/10000 =  80.34 % ||| loss 0.5410151481628418\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5557846459746361\n",
            "Batch #200 Loss: 0.5161044201254845\n",
            "Batch #300 Loss: 0.5233865588903427\n",
            "\u001b[92mTrain accuracy: 37513/48000 =  78.15 % ||| loss 0.6017994284629822\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9407/12000 =  78.39 % ||| loss 0.5973784923553467\u001b[0m\n",
            "\u001b[92mTest accuracy: 7786/10000 =  77.86 % ||| loss 0.6175616979598999\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5357544213533402\n",
            "Batch #200 Loss: 0.5190573492646218\n",
            "Batch #300 Loss: 0.5480484706163407\n",
            "\u001b[92mTrain accuracy: 39747/48000 =  82.81 % ||| loss 0.4905044734477997\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9892/12000 =  82.43 % ||| loss 0.4913996458053589\u001b[0m\n",
            "\u001b[92mTest accuracy: 8218/10000 =  82.18 % ||| loss 0.5031862258911133\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.5071003365516663\n",
            "Batch #200 Loss: 0.5291070002317428\n",
            "Batch #300 Loss: 0.5314130958914757\n",
            "\u001b[92mTrain accuracy: 38295/48000 =  79.78 % ||| loss 0.5575655698776245\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9558/12000 =  79.65 % ||| loss 0.5620740652084351\u001b[0m\n",
            "\u001b[92mTest accuracy: 7940/10000 =  79.4 % ||| loss 0.572687566280365\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.5184102603793144\n",
            "Batch #200 Loss: 0.5074984240531921\n",
            "Batch #300 Loss: 0.5233379009366036\n",
            "\u001b[92mTrain accuracy: 39386/48000 =  82.05 % ||| loss 0.5295151472091675\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9933/12000 =  82.78 % ||| loss 0.5223718881607056\u001b[0m\n",
            "\u001b[92mTest accuracy: 8161/10000 =  81.61 % ||| loss 0.5392716526985168\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.5089188566803933\n",
            "Batch #200 Loss: 0.5235710290074348\n",
            "Batch #300 Loss: 0.5275547844171524\n",
            "\u001b[92mTrain accuracy: 39261/48000 =  81.79 % ||| loss 0.5087111592292786\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9834/12000 =  81.95 % ||| loss 0.5048204064369202\u001b[0m\n",
            "\u001b[92mTest accuracy: 8118/10000 =  81.18 % ||| loss 0.522553563117981\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.5145706048607827\n",
            "Batch #200 Loss: 0.5144976532459259\n",
            "Batch #300 Loss: 0.49654428750276564\n",
            "\u001b[92mTrain accuracy: 39134/48000 =  81.53 % ||| loss 0.4970014989376068\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9767/12000 =  81.39 % ||| loss 0.5017306208610535\u001b[0m\n",
            "\u001b[92mTest accuracy: 8082/10000 =  80.82 % ||| loss 0.5191076993942261\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5279650154709816\n",
            "Batch #200 Loss: 0.4961051794886589\n",
            "Batch #300 Loss: 0.5133945965766906\n",
            "\u001b[92mTrain accuracy: 39658/48000 =  82.62 % ||| loss 0.49389490485191345\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9889/12000 =  82.41 % ||| loss 0.495294451713562\u001b[0m\n",
            "\u001b[92mTest accuracy: 8218/10000 =  82.18 % ||| loss 0.5031750202178955\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5173284894227982\n",
            "Batch #200 Loss: 0.524486529827118\n",
            "Batch #300 Loss: 0.5254127693176269\n",
            "\u001b[92mTrain accuracy: 38990/48000 =  81.23 % ||| loss 0.517787754535675\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9687/12000 =  80.73 % ||| loss 0.5214539170265198\u001b[0m\n",
            "\u001b[92mTest accuracy: 8067/10000 =  80.67 % ||| loss 0.5342424511909485\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.49635281145572663\n",
            "Batch #200 Loss: 0.5266153627634048\n",
            "Batch #300 Loss: 0.5057116335630417\n",
            "\u001b[92mTrain accuracy: 38857/48000 =  80.95 % ||| loss 0.528336763381958\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9680/12000 =  80.67 % ||| loss 0.5350115299224854\u001b[0m\n",
            "\u001b[92mTest accuracy: 8054/10000 =  80.54 % ||| loss 0.5471362471580505\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5175067621469498\n",
            "Batch #200 Loss: 0.5103601616621017\n",
            "Batch #300 Loss: 0.5205329492688179\n",
            "\u001b[92mTrain accuracy: 39212/48000 =  81.69 % ||| loss 0.5311795473098755\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9797/12000 =  81.64 % ||| loss 0.5320890545845032\u001b[0m\n",
            "\u001b[92mTest accuracy: 8102/10000 =  81.02 % ||| loss 0.5450935959815979\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.4862162819504738\n",
            "Batch #200 Loss: 0.5078963354229927\n",
            "Batch #300 Loss: 0.49023220837116244\n",
            "\u001b[92mTrain accuracy: 38506/48000 =  80.22 % ||| loss 0.5446016192436218\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9629/12000 =  80.24 % ||| loss 0.546418309211731\u001b[0m\n",
            "\u001b[92mTest accuracy: 7954/10000 =  79.54 % ||| loss 0.5547699928283691\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5031060817837715\n",
            "Batch #200 Loss: 0.5132694149017334\n",
            "Batch #300 Loss: 0.5051299795508385\n",
            "\u001b[92mTrain accuracy: 39833/48000 =  82.99 % ||| loss 0.49038663506507874\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9909/12000 =  82.58 % ||| loss 0.49168074131011963\u001b[0m\n",
            "\u001b[92mTest accuracy: 8250/10000 =  82.5 % ||| loss 0.5037617087364197\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5120273213088512\n",
            "Batch #200 Loss: 0.5209223714470863\n",
            "Batch #300 Loss: 0.5095246145129204\n",
            "\u001b[92mTrain accuracy: 39202/48000 =  81.67 % ||| loss 0.5191691517829895\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9800/12000 =  81.67 % ||| loss 0.5218117237091064\u001b[0m\n",
            "\u001b[92mTest accuracy: 8093/10000 =  80.93 % ||| loss 0.5414776802062988\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5065400359034539\n",
            "Batch #200 Loss: 0.5251234695315361\n",
            "Batch #300 Loss: 0.5007264840602875\n",
            "\u001b[92mTrain accuracy: 40131/48000 =  83.61 % ||| loss 0.46190762519836426\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9973/12000 =  83.11 % ||| loss 0.46890655159950256\u001b[0m\n",
            "\u001b[92mTest accuracy: 8292/10000 =  82.92 % ||| loss 0.48086991906166077\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.4937518805265427\n",
            "Batch #200 Loss: 0.5044094163179398\n",
            "Batch #300 Loss: 0.5075085258483887\n",
            "\u001b[92mTrain accuracy: 40024/48000 =  83.38 % ||| loss 0.4611130654811859\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9995/12000 =  83.29 % ||| loss 0.4606860280036926\u001b[0m\n",
            "\u001b[92mTest accuracy: 8264/10000 =  82.64 % ||| loss 0.47710660099983215\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.4982882696390152\n",
            "Batch #200 Loss: 0.5033468878269196\n",
            "Batch #300 Loss: 0.5232315114140511\n",
            "\u001b[92mTrain accuracy: 38794/48000 =  80.82 % ||| loss 0.5429462194442749\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9731/12000 =  81.09 % ||| loss 0.5407493114471436\u001b[0m\n",
            "\u001b[92mTest accuracy: 8023/10000 =  80.23 % ||| loss 0.5554817914962769\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.5048665803670883\n",
            "Batch #200 Loss: 0.5086665165424347\n",
            "Batch #300 Loss: 0.5166482776403427\n",
            "\u001b[92mTrain accuracy: 39476/48000 =  82.24 % ||| loss 0.5037785172462463\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9896/12000 =  82.47 % ||| loss 0.5014722943305969\u001b[0m\n",
            "\u001b[92mTest accuracy: 8161/10000 =  81.61 % ||| loss 0.5209529995918274\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.47817874640226365\n",
            "Batch #200 Loss: 0.5104743185639381\n",
            "Batch #300 Loss: 0.5127836456894874\n",
            "\u001b[92mTrain accuracy: 37874/48000 =  78.9 % ||| loss 0.5702769756317139\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9451/12000 =  78.76 % ||| loss 0.573942244052887\u001b[0m\n",
            "\u001b[92mTest accuracy: 7802/10000 =  78.02 % ||| loss 0.588883101940155\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5074711859226226\n",
            "Batch #200 Loss: 0.5201072135567665\n",
            "Batch #300 Loss: 0.4914460629224777\n",
            "\u001b[92mTrain accuracy: 37966/48000 =  79.1 % ||| loss 0.605734646320343\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9425/12000 =  78.54 % ||| loss 0.6054039597511292\u001b[0m\n",
            "\u001b[92mTest accuracy: 7845/10000 =  78.45 % ||| loss 0.625120222568512\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.502909599840641\n",
            "Batch #200 Loss: 0.5194837322831154\n",
            "Batch #300 Loss: 0.5005699726939201\n",
            "\u001b[92mTrain accuracy: 39542/48000 =  82.38 % ||| loss 0.5053303837776184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9915/12000 =  82.62 % ||| loss 0.5025652050971985\u001b[0m\n",
            "\u001b[92mTest accuracy: 8211/10000 =  82.11 % ||| loss 0.5206992030143738\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_4</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_4</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_201646-Lenet5Decay_1726099655.2927098_5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_5' target=\"_blank\">Lenet5Decay_1726099655.2927098_5</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.8215619057416916\n",
            "Batch #200 Loss: 0.7727319848537445\n",
            "Batch #300 Loss: 0.5599298894405365\n",
            "\u001b[92mTrain accuracy: 39359/48000 =  82.0 % ||| loss 0.48273444175720215\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9858/12000 =  82.15 % ||| loss 0.47915929555892944\u001b[0m\n",
            "\u001b[92mTest accuracy: 8120/10000 =  81.2 % ||| loss 0.5086513757705688\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.508467261493206\n",
            "Batch #200 Loss: 0.48538962185382845\n",
            "Batch #300 Loss: 0.47197129875421523\n",
            "\u001b[92mTrain accuracy: 40518/48000 =  84.41 % ||| loss 0.41611912846565247\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10106/12000 =  84.22 % ||| loss 0.42256611585617065\u001b[0m\n",
            "\u001b[92mTest accuracy: 8361/10000 =  83.61 % ||| loss 0.44148629903793335\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.4343975070118904\n",
            "Batch #200 Loss: 0.41910492539405825\n",
            "Batch #300 Loss: 0.43263904213905335\n",
            "\u001b[92mTrain accuracy: 40862/48000 =  85.13 % ||| loss 0.3991777300834656\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10193/12000 =  84.94 % ||| loss 0.4092361330986023\u001b[0m\n",
            "\u001b[92mTest accuracy: 8415/10000 =  84.15 % ||| loss 0.42911234498023987\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.4066748967766762\n",
            "Batch #200 Loss: 0.4140439787507057\n",
            "Batch #300 Loss: 0.4104071882367134\n",
            "\u001b[92mTrain accuracy: 41237/48000 =  85.91 % ||| loss 0.3719378113746643\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10264/12000 =  85.53 % ||| loss 0.38115981221199036\u001b[0m\n",
            "\u001b[92mTest accuracy: 8511/10000 =  85.11 % ||| loss 0.4027473032474518\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.39588903725147245\n",
            "Batch #200 Loss: 0.3913705848157406\n",
            "Batch #300 Loss: 0.38421770319342613\n",
            "\u001b[92mTrain accuracy: 41005/48000 =  85.43 % ||| loss 0.3947543799877167\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10246/12000 =  85.38 % ||| loss 0.40995174646377563\u001b[0m\n",
            "\u001b[92mTest accuracy: 8422/10000 =  84.22 % ||| loss 0.4353366792201996\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.39185640797019006\n",
            "Batch #200 Loss: 0.3808508983254433\n",
            "Batch #300 Loss: 0.3698845957219601\n",
            "\u001b[92mTrain accuracy: 40960/48000 =  85.33 % ||| loss 0.38657382130622864\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10149/12000 =  84.58 % ||| loss 0.40169739723205566\u001b[0m\n",
            "\u001b[92mTest accuracy: 8413/10000 =  84.13 % ||| loss 0.428505003452301\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3708260428905487\n",
            "Batch #200 Loss: 0.38261164605617526\n",
            "Batch #300 Loss: 0.3617841970920563\n",
            "\u001b[92mTrain accuracy: 41083/48000 =  85.59 % ||| loss 0.37994030117988586\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10199/12000 =  84.99 % ||| loss 0.3963380753993988\u001b[0m\n",
            "\u001b[92mTest accuracy: 8443/10000 =  84.43 % ||| loss 0.41882649064064026\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.37619128480553626\n",
            "Batch #200 Loss: 0.37284557402133944\n",
            "Batch #300 Loss: 0.36829694122076034\n",
            "\u001b[92mTrain accuracy: 40590/48000 =  84.56 % ||| loss 0.4103844463825226\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10103/12000 =  84.19 % ||| loss 0.4266375005245209\u001b[0m\n",
            "\u001b[92mTest accuracy: 8335/10000 =  83.35 % ||| loss 0.4507388472557068\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.36442276850342753\n",
            "Batch #200 Loss: 0.3692423525452614\n",
            "Batch #300 Loss: 0.3578915365040302\n",
            "\u001b[92mTrain accuracy: 42230/48000 =  87.98 % ||| loss 0.33068421483039856\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10472/12000 =  87.27 % ||| loss 0.35011717677116394\u001b[0m\n",
            "\u001b[92mTest accuracy: 8655/10000 =  86.55 % ||| loss 0.37328532338142395\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.36446271821856496\n",
            "Batch #200 Loss: 0.3604451207816601\n",
            "Batch #300 Loss: 0.3687983889877796\n",
            "\u001b[92mTrain accuracy: 42264/48000 =  88.05 % ||| loss 0.32701247930526733\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10457/12000 =  87.14 % ||| loss 0.34935835003852844\u001b[0m\n",
            "\u001b[92mTest accuracy: 8645/10000 =  86.45 % ||| loss 0.37043297290802\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.34849738702178\n",
            "Batch #200 Loss: 0.3705974407494068\n",
            "Batch #300 Loss: 0.3572596842050552\n",
            "\u001b[92mTrain accuracy: 41342/48000 =  86.13 % ||| loss 0.3674944341182709\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10298/12000 =  85.82 % ||| loss 0.3844779133796692\u001b[0m\n",
            "\u001b[92mTest accuracy: 8495/10000 =  84.95 % ||| loss 0.4062778055667877\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.35331028550863264\n",
            "Batch #200 Loss: 0.3444491232931614\n",
            "Batch #300 Loss: 0.36838560849428176\n",
            "\u001b[92mTrain accuracy: 41623/48000 =  86.71 % ||| loss 0.34693342447280884\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10320/12000 =  86.0 % ||| loss 0.36778268218040466\u001b[0m\n",
            "\u001b[92mTest accuracy: 8564/10000 =  85.64 % ||| loss 0.39338815212249756\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.3282074983417988\n",
            "Batch #200 Loss: 0.35398515924811363\n",
            "Batch #300 Loss: 0.3592411369085312\n",
            "\u001b[92mTrain accuracy: 42081/48000 =  87.67 % ||| loss 0.32999947667121887\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10416/12000 =  86.8 % ||| loss 0.35686686635017395\u001b[0m\n",
            "\u001b[92mTest accuracy: 8628/10000 =  86.28 % ||| loss 0.37866610288619995\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.34922092005610467\n",
            "Batch #200 Loss: 0.3566479626297951\n",
            "Batch #300 Loss: 0.3382720065116882\n",
            "\u001b[92mTrain accuracy: 42378/48000 =  88.29 % ||| loss 0.31229013204574585\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10472/12000 =  87.27 % ||| loss 0.33635807037353516\u001b[0m\n",
            "\u001b[92mTest accuracy: 8690/10000 =  86.9 % ||| loss 0.35727840662002563\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3433532005548477\n",
            "Batch #200 Loss: 0.34422916293144223\n",
            "Batch #300 Loss: 0.35338604509830474\n",
            "\u001b[92mTrain accuracy: 42223/48000 =  87.96 % ||| loss 0.33137422800064087\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10430/12000 =  86.92 % ||| loss 0.35421785712242126\u001b[0m\n",
            "\u001b[92mTest accuracy: 8638/10000 =  86.38 % ||| loss 0.3914327025413513\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3377942341566086\n",
            "Batch #200 Loss: 0.34714041873812673\n",
            "Batch #300 Loss: 0.3579217344522476\n",
            "\u001b[92mTrain accuracy: 42360/48000 =  88.25 % ||| loss 0.3235152065753937\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10460/12000 =  87.17 % ||| loss 0.3430490791797638\u001b[0m\n",
            "\u001b[92mTest accuracy: 8679/10000 =  86.79 % ||| loss 0.3622184097766876\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3364825619757175\n",
            "Batch #200 Loss: 0.3469643357396126\n",
            "Batch #300 Loss: 0.35209559187293055\n",
            "\u001b[92mTrain accuracy: 42376/48000 =  88.28 % ||| loss 0.317977637052536\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10502/12000 =  87.52 % ||| loss 0.3374156057834625\u001b[0m\n",
            "\u001b[92mTest accuracy: 8703/10000 =  87.03 % ||| loss 0.3652181923389435\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3313742333650589\n",
            "Batch #200 Loss: 0.3498744282126427\n",
            "Batch #300 Loss: 0.3346977023780346\n",
            "\u001b[92mTrain accuracy: 41893/48000 =  87.28 % ||| loss 0.33311137557029724\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10359/12000 =  86.33 % ||| loss 0.3563750684261322\u001b[0m\n",
            "\u001b[92mTest accuracy: 8574/10000 =  85.74 % ||| loss 0.3904147446155548\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.33514421701431274\n",
            "Batch #200 Loss: 0.34128613874316216\n",
            "Batch #300 Loss: 0.3460941915214062\n",
            "\u001b[92mTrain accuracy: 42266/48000 =  88.05 % ||| loss 0.317596435546875\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10486/12000 =  87.38 % ||| loss 0.3450489342212677\u001b[0m\n",
            "\u001b[92mTest accuracy: 8675/10000 =  86.75 % ||| loss 0.3614684045314789\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.33448789492249487\n",
            "Batch #200 Loss: 0.34154202073812484\n",
            "Batch #300 Loss: 0.3476630885899067\n",
            "\u001b[92mTrain accuracy: 42362/48000 =  88.25 % ||| loss 0.3227801024913788\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10454/12000 =  87.12 % ||| loss 0.3428919017314911\u001b[0m\n",
            "\u001b[92mTest accuracy: 8660/10000 =  86.6 % ||| loss 0.37019261717796326\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.327674747556448\n",
            "Batch #200 Loss: 0.3363057045638561\n",
            "Batch #300 Loss: 0.35637586176395414\n",
            "\u001b[92mTrain accuracy: 41118/48000 =  85.66 % ||| loss 0.3773176372051239\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10185/12000 =  84.88 % ||| loss 0.39987093210220337\u001b[0m\n",
            "\u001b[92mTest accuracy: 8421/10000 =  84.21 % ||| loss 0.4205135405063629\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.33732680410146715\n",
            "Batch #200 Loss: 0.34065760537981987\n",
            "Batch #300 Loss: 0.3442478922009468\n",
            "\u001b[92mTrain accuracy: 42264/48000 =  88.05 % ||| loss 0.32225552201271057\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10506/12000 =  87.55 % ||| loss 0.34483829140663147\u001b[0m\n",
            "\u001b[92mTest accuracy: 8645/10000 =  86.45 % ||| loss 0.3653481602668762\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.35949001416563986\n",
            "Batch #200 Loss: 0.33107707619667054\n",
            "Batch #300 Loss: 0.33303600683808326\n",
            "\u001b[92mTrain accuracy: 42248/48000 =  88.02 % ||| loss 0.3202400803565979\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10461/12000 =  87.17 % ||| loss 0.34895938634872437\u001b[0m\n",
            "\u001b[92mTest accuracy: 8622/10000 =  86.22 % ||| loss 0.3721151053905487\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.3201676231622696\n",
            "Batch #200 Loss: 0.3191595494747162\n",
            "Batch #300 Loss: 0.34165219247341155\n",
            "\u001b[92mTrain accuracy: 42073/48000 =  87.65 % ||| loss 0.33291566371917725\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10449/12000 =  87.08 % ||| loss 0.3577219247817993\u001b[0m\n",
            "\u001b[92mTest accuracy: 8567/10000 =  85.67 % ||| loss 0.3843280076980591\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3288608339428902\n",
            "Batch #200 Loss: 0.3304286128282547\n",
            "Batch #300 Loss: 0.35397823035717013\n",
            "\u001b[92mTrain accuracy: 41779/48000 =  87.04 % ||| loss 0.35289523005485535\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10345/12000 =  86.21 % ||| loss 0.37876376509666443\u001b[0m\n",
            "\u001b[92mTest accuracy: 8542/10000 =  85.42 % ||| loss 0.3958487808704376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_5</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_5</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_201834-Lenet5Decay_1726099655.2927098_6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_6' target=\"_blank\">Lenet5Decay_1726099655.2927098_6</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.303066153526306\n",
            "Batch #200 Loss: 2.3029258632659912\n",
            "Batch #300 Loss: 2.302842655181885\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3026459217071533\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.302783489227295\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30271315574646\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3028874588012695\n",
            "Batch #200 Loss: 2.302865505218506\n",
            "Batch #300 Loss: 2.3029908967018127\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.302931308746338\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3028109073638916\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302868127822876\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3029451084136965\n",
            "Batch #200 Loss: 2.3029704475402832\n",
            "Batch #300 Loss: 2.302727634906769\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.30290150642395\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.302781581878662\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3029625415802\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3031395792961122\n",
            "Batch #200 Loss: 2.3032015538215638\n",
            "Batch #300 Loss: 2.3029668164253234\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302858829498291\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3027560710906982\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028316497802734\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3028481459617614\n",
            "Batch #200 Loss: 2.302947311401367\n",
            "Batch #300 Loss: 2.303021402359009\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3027472496032715\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3027353286743164\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027219772338867\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.302989854812622\n",
            "Batch #200 Loss: 2.30283127784729\n",
            "Batch #300 Loss: 2.302691876888275\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302943706512451\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3030881881713867\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302910089492798\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.302935314178467\n",
            "Batch #200 Loss: 2.3029780125617982\n",
            "Batch #300 Loss: 2.302969920635223\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3027684688568115\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3028292655944824\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302715539932251\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.3028241419792175\n",
            "Batch #200 Loss: 2.3031303548812865\n",
            "Batch #300 Loss: 2.3027623510360717\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.303025960922241\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3027894496917725\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303022861480713\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.302926881313324\n",
            "Batch #200 Loss: 2.302859358787537\n",
            "Batch #300 Loss: 2.3030541253089907\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.302837371826172\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.302532196044922\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027195930480957\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.3027710151672363\n",
            "Batch #200 Loss: 2.3029754114151\n",
            "Batch #300 Loss: 2.3028773355484007\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.30288028717041\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.3028969764709473\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302924871444702\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.3028670072555544\n",
            "Batch #200 Loss: 2.30264976978302\n",
            "Batch #300 Loss: 2.3029874086380007\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.3031117916107178\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.3027210235595703\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3029494285583496\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.30301855802536\n",
            "Batch #200 Loss: 2.302914960384369\n",
            "Batch #300 Loss: 2.303121862411499\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302736282348633\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302867889404297\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302807092666626\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.3026341724395754\n",
            "Batch #200 Loss: 2.302859170436859\n",
            "Batch #300 Loss: 2.302830092906952\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3031654357910156\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.3033995628356934\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3030152320861816\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.302877657413483\n",
            "Batch #200 Loss: 2.302993652820587\n",
            "Batch #300 Loss: 2.302942385673523\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302722215652466\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302912950515747\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027539253234863\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.3029755902290345\n",
            "Batch #200 Loss: 2.3028391933441164\n",
            "Batch #300 Loss: 2.30312162399292\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3027219772338867\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3030266761779785\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028969764709473\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.302911455631256\n",
            "Batch #200 Loss: 2.302921552658081\n",
            "Batch #300 Loss: 2.302985956668854\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3029210567474365\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3033158779144287\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303112030029297\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.303004460334778\n",
            "Batch #200 Loss: 2.3028919434547426\n",
            "Batch #300 Loss: 2.3031281518936155\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.30306339263916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.303009271621704\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3031134605407715\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.3031067872047424\n",
            "Batch #200 Loss: 2.3026575899124144\n",
            "Batch #300 Loss: 2.302920660972595\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.3029839992523193\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.302905797958374\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303086757659912\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.302995719909668\n",
            "Batch #200 Loss: 2.303043992519379\n",
            "Batch #300 Loss: 2.3028921937942504\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3027565479278564\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026158809661865\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027002811431885\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.3031779313087464\n",
            "Batch #200 Loss: 2.303134217262268\n",
            "Batch #300 Loss: 2.3032359528541564\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3027491569519043\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302818536758423\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027350902557373\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.3029157280921937\n",
            "Batch #200 Loss: 2.303020725250244\n",
            "Batch #300 Loss: 2.3031888151168824\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.302887201309204\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.302563190460205\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028786182403564\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.3030096912384033\n",
            "Batch #200 Loss: 2.3029861664772033\n",
            "Batch #300 Loss: 2.3027786660194396\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3029301166534424\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3028080463409424\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028481006622314\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.302972848415375\n",
            "Batch #200 Loss: 2.3029024052619933\n",
            "Batch #300 Loss: 2.302550573348999\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.302932024002075\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.30277419090271\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3029587268829346\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.303033289909363\n",
            "Batch #200 Loss: 2.3027234482765198\n",
            "Batch #300 Loss: 2.30270334482193\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3029391765594482\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.30302095413208\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028650283813477\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3028990244865417\n",
            "Batch #200 Loss: 2.3030292892456057\n",
            "Batch #300 Loss: 2.302996015548706\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3028817176818848\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3029398918151855\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028860092163086\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_6</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_6</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_202023-Lenet5Decay_1726099655.2927098_7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_7' target=\"_blank\">Lenet5Decay_1726099655.2927098_7</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.6668455958366395\n",
            "Batch #200 Loss: 0.8330574542284012\n",
            "Batch #300 Loss: 0.7059806996583938\n",
            "\u001b[92mTrain accuracy: 36724/48000 =  76.51 % ||| loss 0.6262487769126892\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9221/12000 =  76.84 % ||| loss 0.6165639758110046\u001b[0m\n",
            "\u001b[92mTest accuracy: 7582/10000 =  75.82 % ||| loss 0.6428982019424438\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6206953245401382\n",
            "Batch #200 Loss: 0.5850384211540223\n",
            "Batch #300 Loss: 0.5819150230288506\n",
            "\u001b[92mTrain accuracy: 39096/48000 =  81.45 % ||| loss 0.5190039277076721\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9761/12000 =  81.34 % ||| loss 0.5194871425628662\u001b[0m\n",
            "\u001b[92mTest accuracy: 8059/10000 =  80.59 % ||| loss 0.5407870411872864\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5494983330368995\n",
            "Batch #200 Loss: 0.5347753584384918\n",
            "Batch #300 Loss: 0.5274551817774773\n",
            "\u001b[92mTrain accuracy: 39285/48000 =  81.84 % ||| loss 0.503379762172699\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9836/12000 =  81.97 % ||| loss 0.49913910031318665\u001b[0m\n",
            "\u001b[92mTest accuracy: 8121/10000 =  81.21 % ||| loss 0.5251532196998596\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5210010969638824\n",
            "Batch #200 Loss: 0.49115961879491804\n",
            "Batch #300 Loss: 0.5043953436613083\n",
            "\u001b[92mTrain accuracy: 39769/48000 =  82.85 % ||| loss 0.4782477617263794\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9921/12000 =  82.67 % ||| loss 0.4762476980686188\u001b[0m\n",
            "\u001b[92mTest accuracy: 8186/10000 =  81.86 % ||| loss 0.500766396522522\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4796617656946182\n",
            "Batch #200 Loss: 0.47927074909210204\n",
            "Batch #300 Loss: 0.491842747926712\n",
            "\u001b[92mTrain accuracy: 40150/48000 =  83.65 % ||| loss 0.4387045204639435\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10045/12000 =  83.71 % ||| loss 0.44043371081352234\u001b[0m\n",
            "\u001b[92mTest accuracy: 8290/10000 =  82.9 % ||| loss 0.4623326063156128\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.4751312345266342\n",
            "Batch #200 Loss: 0.4616720446944237\n",
            "Batch #300 Loss: 0.4689055439829826\n",
            "\u001b[92mTrain accuracy: 40836/48000 =  85.08 % ||| loss 0.4222288727760315\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10160/12000 =  84.67 % ||| loss 0.4270138740539551\u001b[0m\n",
            "\u001b[92mTest accuracy: 8446/10000 =  84.46 % ||| loss 0.4439369738101959\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.4569741371273994\n",
            "Batch #200 Loss: 0.4578085854649544\n",
            "Batch #300 Loss: 0.47142247021198275\n",
            "\u001b[92mTrain accuracy: 40253/48000 =  83.86 % ||| loss 0.4533670246601105\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10038/12000 =  83.65 % ||| loss 0.4579455256462097\u001b[0m\n",
            "\u001b[92mTest accuracy: 8286/10000 =  82.86 % ||| loss 0.4749036431312561\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.4495185700058937\n",
            "Batch #200 Loss: 0.458030444085598\n",
            "Batch #300 Loss: 0.4612774610519409\n",
            "\u001b[92mTrain accuracy: 40595/48000 =  84.57 % ||| loss 0.435676634311676\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10167/12000 =  84.72 % ||| loss 0.4340757131576538\u001b[0m\n",
            "\u001b[92mTest accuracy: 8390/10000 =  83.9 % ||| loss 0.4527437686920166\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.4533291602134705\n",
            "Batch #200 Loss: 0.4471947619318962\n",
            "Batch #300 Loss: 0.4635289993882179\n",
            "\u001b[92mTrain accuracy: 40751/48000 =  84.9 % ||| loss 0.415250688791275\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10198/12000 =  84.98 % ||| loss 0.41770002245903015\u001b[0m\n",
            "\u001b[92mTest accuracy: 8425/10000 =  84.25 % ||| loss 0.4387534558773041\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.45613127112388613\n",
            "Batch #200 Loss: 0.4533540326356888\n",
            "Batch #300 Loss: 0.4368594497442245\n",
            "\u001b[92mTrain accuracy: 40892/48000 =  85.19 % ||| loss 0.41867971420288086\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10230/12000 =  85.25 % ||| loss 0.4196127951145172\u001b[0m\n",
            "\u001b[92mTest accuracy: 8454/10000 =  84.54 % ||| loss 0.4364842176437378\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.44950068950653077\n",
            "Batch #200 Loss: 0.44798900067806247\n",
            "Batch #300 Loss: 0.4503246268630028\n",
            "\u001b[92mTrain accuracy: 40367/48000 =  84.1 % ||| loss 0.4413915276527405\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10128/12000 =  84.4 % ||| loss 0.44326019287109375\u001b[0m\n",
            "\u001b[92mTest accuracy: 8340/10000 =  83.4 % ||| loss 0.4642273187637329\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.44267779469490054\n",
            "Batch #200 Loss: 0.44585614174604415\n",
            "Batch #300 Loss: 0.44588426530361175\n",
            "\u001b[92mTrain accuracy: 40590/48000 =  84.56 % ||| loss 0.42591825127601624\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10099/12000 =  84.16 % ||| loss 0.4370633363723755\u001b[0m\n",
            "\u001b[92mTest accuracy: 8381/10000 =  83.81 % ||| loss 0.44782015681266785\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.4344270347058773\n",
            "Batch #200 Loss: 0.4540720960497856\n",
            "Batch #300 Loss: 0.4544015219807625\n",
            "\u001b[92mTrain accuracy: 40785/48000 =  84.97 % ||| loss 0.44187378883361816\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10189/12000 =  84.91 % ||| loss 0.4452029764652252\u001b[0m\n",
            "\u001b[92mTest accuracy: 8422/10000 =  84.22 % ||| loss 0.46609386801719666\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.45573650002479554\n",
            "Batch #200 Loss: 0.42791236013174055\n",
            "Batch #300 Loss: 0.45118974715471266\n",
            "\u001b[92mTrain accuracy: 40479/48000 =  84.33 % ||| loss 0.4400765299797058\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10119/12000 =  84.33 % ||| loss 0.4392111003398895\u001b[0m\n",
            "\u001b[92mTest accuracy: 8377/10000 =  83.77 % ||| loss 0.4580709636211395\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.44535824060440066\n",
            "Batch #200 Loss: 0.44270203560590743\n",
            "Batch #300 Loss: 0.43686325088143346\n",
            "\u001b[92mTrain accuracy: 40907/48000 =  85.22 % ||| loss 0.41961565613746643\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10216/12000 =  85.13 % ||| loss 0.42556628584861755\u001b[0m\n",
            "\u001b[92mTest accuracy: 8434/10000 =  84.34 % ||| loss 0.44858691096305847\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.43417427003383635\n",
            "Batch #200 Loss: 0.4554149749875069\n",
            "Batch #300 Loss: 0.4437629196047783\n",
            "\u001b[92mTrain accuracy: 39545/48000 =  82.39 % ||| loss 0.47523656487464905\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9849/12000 =  82.08 % ||| loss 0.4805730879306793\u001b[0m\n",
            "\u001b[92mTest accuracy: 8193/10000 =  81.93 % ||| loss 0.5008090734481812\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.43826810777187347\n",
            "Batch #200 Loss: 0.447075779736042\n",
            "Batch #300 Loss: 0.4374942687153816\n",
            "\u001b[92mTrain accuracy: 38584/48000 =  80.38 % ||| loss 0.5208054780960083\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9618/12000 =  80.15 % ||| loss 0.5265094041824341\u001b[0m\n",
            "\u001b[92mTest accuracy: 7988/10000 =  79.88 % ||| loss 0.5401110053062439\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.42989728942513467\n",
            "Batch #200 Loss: 0.43974305748939513\n",
            "Batch #300 Loss: 0.4376527085900307\n",
            "\u001b[92mTrain accuracy: 40923/48000 =  85.26 % ||| loss 0.41334837675094604\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10182/12000 =  84.85 % ||| loss 0.4231005012989044\u001b[0m\n",
            "\u001b[92mTest accuracy: 8453/10000 =  84.53 % ||| loss 0.4358024001121521\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.4318432977795601\n",
            "Batch #200 Loss: 0.4416708129644394\n",
            "Batch #300 Loss: 0.43365903675556183\n",
            "\u001b[92mTrain accuracy: 38311/48000 =  79.81 % ||| loss 0.5300190448760986\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9508/12000 =  79.23 % ||| loss 0.5363561511039734\u001b[0m\n",
            "\u001b[92mTest accuracy: 7904/10000 =  79.04 % ||| loss 0.5572699308395386\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.43157396018505095\n",
            "Batch #200 Loss: 0.4369645968079567\n",
            "Batch #300 Loss: 0.4418521702289581\n",
            "\u001b[92mTrain accuracy: 40130/48000 =  83.6 % ||| loss 0.4537446200847626\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10015/12000 =  83.46 % ||| loss 0.4610942602157593\u001b[0m\n",
            "\u001b[92mTest accuracy: 8280/10000 =  82.8 % ||| loss 0.47252991795539856\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.45064673900604246\n",
            "Batch #200 Loss: 0.4182728120684624\n",
            "Batch #300 Loss: 0.4428459841012955\n",
            "\u001b[92mTrain accuracy: 40771/48000 =  84.94 % ||| loss 0.4175158739089966\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10204/12000 =  85.03 % ||| loss 0.42381367087364197\u001b[0m\n",
            "\u001b[92mTest accuracy: 8409/10000 =  84.09 % ||| loss 0.4421558082103729\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.4387955293059349\n",
            "Batch #200 Loss: 0.4258275693655014\n",
            "Batch #300 Loss: 0.44233089178800583\n",
            "\u001b[92mTrain accuracy: 39994/48000 =  83.32 % ||| loss 0.48589587211608887\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9923/12000 =  82.69 % ||| loss 0.4911002218723297\u001b[0m\n",
            "\u001b[92mTest accuracy: 8233/10000 =  82.33 % ||| loss 0.513480544090271\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.4238534826040268\n",
            "Batch #200 Loss: 0.4323047024011612\n",
            "Batch #300 Loss: 0.42571654796600344\n",
            "\u001b[92mTrain accuracy: 41354/48000 =  86.15 % ||| loss 0.39505329728126526\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10289/12000 =  85.74 % ||| loss 0.4037722647190094\u001b[0m\n",
            "\u001b[92mTest accuracy: 8541/10000 =  85.41 % ||| loss 0.41788366436958313\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.4322136688232422\n",
            "Batch #200 Loss: 0.4484093037247658\n",
            "Batch #300 Loss: 0.4401594744622707\n",
            "\u001b[92mTrain accuracy: 40042/48000 =  83.42 % ||| loss 0.4569208025932312\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9936/12000 =  82.8 % ||| loss 0.46415048837661743\u001b[0m\n",
            "\u001b[92mTest accuracy: 8274/10000 =  82.74 % ||| loss 0.4852111041545868\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.428977290391922\n",
            "Batch #200 Loss: 0.4299829122424126\n",
            "Batch #300 Loss: 0.4256443050503731\n",
            "\u001b[92mTrain accuracy: 41502/48000 =  86.46 % ||| loss 0.3946760296821594\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10366/12000 =  86.38 % ||| loss 0.3996809422969818\u001b[0m\n",
            "\u001b[92mTest accuracy: 8553/10000 =  85.53 % ||| loss 0.4216950237751007\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_7</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_7</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_202212-Lenet5Decay_1726099655.2927098_8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_8' target=\"_blank\">Lenet5Decay_1726099655.2927098_8</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.860411865711212\n",
            "Batch #200 Loss: 0.738541932106018\n",
            "Batch #300 Loss: 0.5861772632598877\n",
            "\u001b[92mTrain accuracy: 37848/48000 =  78.85 % ||| loss 0.5444328784942627\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9509/12000 =  79.24 % ||| loss 0.5384646058082581\u001b[0m\n",
            "\u001b[92mTest accuracy: 7768/10000 =  77.68 % ||| loss 0.5754517912864685\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.48061165273189543\n",
            "Batch #200 Loss: 0.4544718199968338\n",
            "Batch #300 Loss: 0.4250790268182755\n",
            "\u001b[92mTrain accuracy: 41066/48000 =  85.55 % ||| loss 0.39758220314979553\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10216/12000 =  85.13 % ||| loss 0.4059945046901703\u001b[0m\n",
            "\u001b[92mTest accuracy: 8442/10000 =  84.42 % ||| loss 0.42634692788124084\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.3890807856619358\n",
            "Batch #200 Loss: 0.3930794958770275\n",
            "Batch #300 Loss: 0.37304937332868576\n",
            "\u001b[92mTrain accuracy: 41575/48000 =  86.61 % ||| loss 0.3557290732860565\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10313/12000 =  85.94 % ||| loss 0.3720751702785492\u001b[0m\n",
            "\u001b[92mTest accuracy: 8554/10000 =  85.54 % ||| loss 0.39426562190055847\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.3450703710317612\n",
            "Batch #200 Loss: 0.3498839341104031\n",
            "Batch #300 Loss: 0.3545084916055202\n",
            "\u001b[92mTrain accuracy: 41691/48000 =  86.86 % ||| loss 0.35801783204078674\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10347/12000 =  86.22 % ||| loss 0.37694135308265686\u001b[0m\n",
            "\u001b[92mTest accuracy: 8554/10000 =  85.54 % ||| loss 0.39671608805656433\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.32960120171308516\n",
            "Batch #200 Loss: 0.33408864617347717\n",
            "Batch #300 Loss: 0.3281911712884903\n",
            "\u001b[92mTrain accuracy: 42281/48000 =  88.09 % ||| loss 0.3151692748069763\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10471/12000 =  87.26 % ||| loss 0.33848482370376587\u001b[0m\n",
            "\u001b[92mTest accuracy: 8677/10000 =  86.77 % ||| loss 0.3577152490615845\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.320694123506546\n",
            "Batch #200 Loss: 0.30834957882761954\n",
            "Batch #300 Loss: 0.3003071388602257\n",
            "\u001b[92mTrain accuracy: 42580/48000 =  88.71 % ||| loss 0.30518293380737305\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10481/12000 =  87.34 % ||| loss 0.33387428522109985\u001b[0m\n",
            "\u001b[92mTest accuracy: 8715/10000 =  87.15 % ||| loss 0.35578158497810364\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.30039215564727784\n",
            "Batch #200 Loss: 0.3016932421922684\n",
            "Batch #300 Loss: 0.3001759725809097\n",
            "\u001b[92mTrain accuracy: 43176/48000 =  89.95 % ||| loss 0.27370941638946533\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10615/12000 =  88.46 % ||| loss 0.306821346282959\u001b[0m\n",
            "\u001b[92mTest accuracy: 8848/10000 =  88.48 % ||| loss 0.32180848717689514\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.2875577758252621\n",
            "Batch #200 Loss: 0.2962341770529747\n",
            "Batch #300 Loss: 0.29282533407211303\n",
            "\u001b[92mTrain accuracy: 43110/48000 =  89.81 % ||| loss 0.2771201431751251\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10613/12000 =  88.44 % ||| loss 0.3112090528011322\u001b[0m\n",
            "\u001b[92mTest accuracy: 8824/10000 =  88.24 % ||| loss 0.32695019245147705\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.27906261771917346\n",
            "Batch #200 Loss: 0.2854996433109045\n",
            "Batch #300 Loss: 0.28040105298161505\n",
            "\u001b[92mTrain accuracy: 43201/48000 =  90.0 % ||| loss 0.2715584933757782\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10651/12000 =  88.76 % ||| loss 0.30854716897010803\u001b[0m\n",
            "\u001b[92mTest accuracy: 8833/10000 =  88.33 % ||| loss 0.32627543807029724\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.2788102860748768\n",
            "Batch #200 Loss: 0.2766083313524723\n",
            "Batch #300 Loss: 0.28225846409797667\n",
            "\u001b[92mTrain accuracy: 42587/48000 =  88.72 % ||| loss 0.3125447630882263\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10463/12000 =  87.19 % ||| loss 0.35843244194984436\u001b[0m\n",
            "\u001b[92mTest accuracy: 8702/10000 =  87.02 % ||| loss 0.3802497684955597\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.2737144835293293\n",
            "Batch #200 Loss: 0.279089652299881\n",
            "Batch #300 Loss: 0.27481448769569394\n",
            "\u001b[92mTrain accuracy: 43259/48000 =  90.12 % ||| loss 0.2650439739227295\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10627/12000 =  88.56 % ||| loss 0.3095758259296417\u001b[0m\n",
            "\u001b[92mTest accuracy: 8836/10000 =  88.36 % ||| loss 0.32714879512786865\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.2733074740320444\n",
            "Batch #200 Loss: 0.2660260756313801\n",
            "Batch #300 Loss: 0.27246632039546964\n",
            "\u001b[92mTrain accuracy: 43064/48000 =  89.72 % ||| loss 0.2781570255756378\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10538/12000 =  87.82 % ||| loss 0.3256385326385498\u001b[0m\n",
            "\u001b[92mTest accuracy: 8810/10000 =  88.1 % ||| loss 0.33466705679893494\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.2667867999523878\n",
            "Batch #200 Loss: 0.26202307298779487\n",
            "Batch #300 Loss: 0.25900477200746536\n",
            "\u001b[92mTrain accuracy: 43501/48000 =  90.63 % ||| loss 0.25100794434547424\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10692/12000 =  89.1 % ||| loss 0.2913585305213928\u001b[0m\n",
            "\u001b[92mTest accuracy: 8844/10000 =  88.44 % ||| loss 0.3180064857006073\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.2533497606217861\n",
            "Batch #200 Loss: 0.26942412704229357\n",
            "Batch #300 Loss: 0.2624554803967476\n",
            "\u001b[92mTrain accuracy: 43616/48000 =  90.87 % ||| loss 0.24648119509220123\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10716/12000 =  89.3 % ||| loss 0.29321590065956116\u001b[0m\n",
            "\u001b[92mTest accuracy: 8889/10000 =  88.89 % ||| loss 0.3090042173862457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.24693768233060837\n",
            "Batch #200 Loss: 0.2504714313149452\n",
            "Batch #300 Loss: 0.2554938817024231\n",
            "\u001b[92mTrain accuracy: 43288/48000 =  90.18 % ||| loss 0.26393675804138184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10617/12000 =  88.48 % ||| loss 0.30891239643096924\u001b[0m\n",
            "\u001b[92mTest accuracy: 8784/10000 =  87.84 % ||| loss 0.3293226361274719\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.24394094228744506\n",
            "Batch #200 Loss: 0.25602001391351226\n",
            "Batch #300 Loss: 0.25450886800885203\n",
            "\u001b[92mTrain accuracy: 43813/48000 =  91.28 % ||| loss 0.2389473021030426\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10655/12000 =  88.79 % ||| loss 0.29259970784187317\u001b[0m\n",
            "\u001b[92mTest accuracy: 8875/10000 =  88.75 % ||| loss 0.3134407103061676\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.24766347996890545\n",
            "Batch #200 Loss: 0.2555415144562721\n",
            "Batch #300 Loss: 0.24943296879529953\n",
            "\u001b[92mTrain accuracy: 44161/48000 =  92.0 % ||| loss 0.22099626064300537\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10754/12000 =  89.62 % ||| loss 0.27626463770866394\u001b[0m\n",
            "\u001b[92mTest accuracy: 8966/10000 =  89.66 % ||| loss 0.29178524017333984\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.24549491606652737\n",
            "Batch #200 Loss: 0.2438642743229866\n",
            "Batch #300 Loss: 0.2507819176465273\n",
            "\u001b[92mTrain accuracy: 44002/48000 =  91.67 % ||| loss 0.22641821205615997\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10723/12000 =  89.36 % ||| loss 0.2880595922470093\u001b[0m\n",
            "\u001b[92mTest accuracy: 8900/10000 =  89.0 % ||| loss 0.31210246682167053\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.2361075647175312\n",
            "Batch #200 Loss: 0.2344736126065254\n",
            "Batch #300 Loss: 0.2485398519039154\n",
            "\u001b[92mTrain accuracy: 43809/48000 =  91.27 % ||| loss 0.2372906357049942\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10668/12000 =  88.9 % ||| loss 0.3005535900592804\u001b[0m\n",
            "\u001b[92mTest accuracy: 8867/10000 =  88.67 % ||| loss 0.3165769577026367\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.23575138248503208\n",
            "Batch #200 Loss: 0.24553912378847598\n",
            "Batch #300 Loss: 0.24519194215536116\n",
            "\u001b[92mTrain accuracy: 43611/48000 =  90.86 % ||| loss 0.24836547672748566\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10635/12000 =  88.62 % ||| loss 0.3163216710090637\u001b[0m\n",
            "\u001b[92mTest accuracy: 8817/10000 =  88.17 % ||| loss 0.3316086530685425\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.23034682795405387\n",
            "Batch #200 Loss: 0.23619574993848802\n",
            "Batch #300 Loss: 0.2383931440114975\n",
            "\u001b[92mTrain accuracy: 43887/48000 =  91.43 % ||| loss 0.2358320951461792\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10644/12000 =  88.7 % ||| loss 0.3040072023868561\u001b[0m\n",
            "\u001b[92mTest accuracy: 8851/10000 =  88.51 % ||| loss 0.31898191571235657\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.2279741818457842\n",
            "Batch #200 Loss: 0.23790612429380417\n",
            "Batch #300 Loss: 0.23763637632131576\n",
            "\u001b[92mTrain accuracy: 44183/48000 =  92.05 % ||| loss 0.21833735704421997\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10730/12000 =  89.42 % ||| loss 0.2806435525417328\u001b[0m\n",
            "\u001b[92mTest accuracy: 8916/10000 =  89.16 % ||| loss 0.2984587848186493\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.22853522978723048\n",
            "Batch #200 Loss: 0.24125125914812087\n",
            "Batch #300 Loss: 0.23686263114213943\n",
            "\u001b[92mTrain accuracy: 44169/48000 =  92.02 % ||| loss 0.21987102925777435\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10771/12000 =  89.76 % ||| loss 0.28151148557662964\u001b[0m\n",
            "\u001b[92mTest accuracy: 8932/10000 =  89.32 % ||| loss 0.3004786968231201\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.2161764119565487\n",
            "Batch #200 Loss: 0.23432628192007543\n",
            "Batch #300 Loss: 0.23699577681720257\n",
            "\u001b[92mTrain accuracy: 44218/48000 =  92.12 % ||| loss 0.2117059975862503\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10744/12000 =  89.53 % ||| loss 0.2831956446170807\u001b[0m\n",
            "\u001b[92mTest accuracy: 8941/10000 =  89.41 % ||| loss 0.30121174454689026\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.2200552884489298\n",
            "Batch #200 Loss: 0.22799921579658985\n",
            "Batch #300 Loss: 0.2252901278436184\n",
            "\u001b[92mTrain accuracy: 43985/48000 =  91.64 % ||| loss 0.22571881115436554\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10671/12000 =  88.92 % ||| loss 0.29884710907936096\u001b[0m\n",
            "\u001b[92mTest accuracy: 8852/10000 =  88.52 % ||| loss 0.3236716687679291\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_8</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_8</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_202401-Lenet5Decay_1726099655.2927098_9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_9' target=\"_blank\">Lenet5Decay_1726099655.2927098_9</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_9' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3025435256958007\n",
            "Batch #200 Loss: 2.301024477481842\n",
            "Batch #300 Loss: 2.3010322046279907\n",
            "\u001b[92mTrain accuracy: 5856/48000 =  12.2 % ||| loss 2.3011600971221924\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1443/12000 =  12.03 % ||| loss 2.301351547241211\u001b[0m\n",
            "\u001b[92mTest accuracy: 1232/10000 =  12.32 % ||| loss 2.3011960983276367\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.301523425579071\n",
            "Batch #200 Loss: 2.3015068674087527\n",
            "Batch #300 Loss: 2.3018482398986815\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302196979522705\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302340030670166\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3022170066833496\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.302336072921753\n",
            "Batch #200 Loss: 2.3024191093444824\n",
            "Batch #300 Loss: 2.3024629044532774\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.30251145362854\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026158809661865\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302528142929077\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3026154327392576\n",
            "Batch #200 Loss: 2.302578098773956\n",
            "Batch #300 Loss: 2.302547652721405\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302568197250366\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302659034729004\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025965690612793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.302618854045868\n",
            "Batch #200 Loss: 2.3025697803497316\n",
            "Batch #300 Loss: 2.3026480650901795\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025753498077393\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026583194732666\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025903701782227\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3025998210906984\n",
            "Batch #200 Loss: 2.3025707268714903\n",
            "Batch #300 Loss: 2.302630522251129\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025736808776855\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026552200317383\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026018142700195\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.3026113510131836\n",
            "Batch #200 Loss: 2.3025979375839234\n",
            "Batch #300 Loss: 2.302628071308136\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025741577148438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302652597427368\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025832176208496\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.302585425376892\n",
            "Batch #200 Loss: 2.3025741457939146\n",
            "Batch #300 Loss: 2.302632393836975\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302574396133423\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026435375213623\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025894165039062\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.3026043820381163\n",
            "Batch #200 Loss: 2.302621262073517\n",
            "Batch #300 Loss: 2.3026178336143492\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026528358459473\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025691509246826\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.3025710725784303\n",
            "Batch #200 Loss: 2.302665138244629\n",
            "Batch #300 Loss: 2.302586352825165\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3026511669158936\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302582263946533\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.3025878715515136\n",
            "Batch #200 Loss: 2.3025654983520507\n",
            "Batch #300 Loss: 2.302649800777435\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3025760650634766\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3026440143585205\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026046752929688\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.3026240563392637\n",
            "Batch #200 Loss: 2.302571704387665\n",
            "Batch #300 Loss: 2.3026235485076905\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302654266357422\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025875091552734\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.3025936245918275\n",
            "Batch #200 Loss: 2.302607684135437\n",
            "Batch #300 Loss: 2.3026336288452147\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302574396133423\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302656412124634\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302581310272217\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.302560365200043\n",
            "Batch #200 Loss: 2.3025873708724975\n",
            "Batch #300 Loss: 2.3026079320907593\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025760650634766\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302640438079834\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302611827850342\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.3025988006591795\n",
            "Batch #200 Loss: 2.302603006362915\n",
            "Batch #300 Loss: 2.3026088428497316\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025753498077393\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302640438079834\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302591562271118\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.302574875354767\n",
            "Batch #200 Loss: 2.3026041102409365\n",
            "Batch #300 Loss: 2.30262455701828\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025758266448975\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302643299102783\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302595615386963\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.302553243637085\n",
            "Batch #200 Loss: 2.302620139122009\n",
            "Batch #300 Loss: 2.302656424045563\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302574872970581\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026435375213623\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025903701782227\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.3025639033317566\n",
            "Batch #200 Loss: 2.3026201033592226\n",
            "Batch #300 Loss: 2.302608516216278\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3025755882263184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302643060684204\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302584648132324\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.302591710090637\n",
            "Batch #200 Loss: 2.302569561004639\n",
            "Batch #300 Loss: 2.3026135087013246\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3025760650634766\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302640438079834\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302593469619751\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.302589144706726\n",
            "Batch #200 Loss: 2.30263738155365\n",
            "Batch #300 Loss: 2.3025718474388124\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026492595672607\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025901317596436\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.3025692820549013\n",
            "Batch #200 Loss: 2.3026523065567015\n",
            "Batch #300 Loss: 2.302605638504028\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302647829055786\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302581548690796\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.3026155924797056\n",
            "Batch #200 Loss: 2.302544674873352\n",
            "Batch #300 Loss: 2.3026101279258726\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3025755882263184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3026416301727295\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025848865509033\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.3026117944717406\n",
            "Batch #200 Loss: 2.3025810599327086\n",
            "Batch #300 Loss: 2.3025693106651306\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025758266448975\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302639961242676\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025975227355957\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.3025874710083007\n",
            "Batch #200 Loss: 2.3026212191581727\n",
            "Batch #300 Loss: 2.3026131415367126\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3025741577148438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302644729614258\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302577018737793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3026014947891236\n",
            "Batch #200 Loss: 2.3026229524612427\n",
            "Batch #300 Loss: 2.302581751346588\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302574872970581\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.30265212059021\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026092052459717\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_9</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_9' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_9</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_202549-Lenet5Decay_1726099655.2927098_10</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_10' target=\"_blank\">Lenet5Decay_1726099655.2927098_10</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_10' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_10</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302101001739502\n",
            "Batch #200 Loss: 2.3021844482421874\n",
            "Batch #300 Loss: 2.2999481916427613\n",
            "\u001b[92mTrain accuracy: 8405/48000 =  17.51 % ||| loss 2.297607183456421\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2086/12000 =  17.38 % ||| loss 2.2978508472442627\u001b[0m\n",
            "\u001b[92mTest accuracy: 1746/10000 =  17.46 % ||| loss 2.297661066055298\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2965636134147642\n",
            "Batch #200 Loss: 2.29402724981308\n",
            "Batch #300 Loss: 2.2897149181365966\n",
            "\u001b[92mTrain accuracy: 14603/48000 =  30.42 % ||| loss 2.2804293632507324\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3664/12000 =  30.53 % ||| loss 2.2803215980529785\u001b[0m\n",
            "\u001b[92mTest accuracy: 3053/10000 =  30.53 % ||| loss 2.280524253845215\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.2723759412765503\n",
            "Batch #200 Loss: 2.242512946128845\n",
            "Batch #300 Loss: 2.1288036060333253\n",
            "\u001b[92mTrain accuracy: 26913/48000 =  56.07 % ||| loss 1.463409662246704\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6752/12000 =  56.27 % ||| loss 1.459680438041687\u001b[0m\n",
            "\u001b[92mTest accuracy: 5549/10000 =  55.49 % ||| loss 1.4696390628814697\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.2610050761699676\n",
            "Batch #200 Loss: 1.1021564263105392\n",
            "Batch #300 Loss: 1.025560205578804\n",
            "\u001b[92mTrain accuracy: 29941/48000 =  62.38 % ||| loss 0.9687363505363464\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7519/12000 =  62.66 % ||| loss 0.9606505632400513\u001b[0m\n",
            "\u001b[92mTest accuracy: 6149/10000 =  61.49 % ||| loss 0.9871002435684204\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.9529929691553116\n",
            "Batch #200 Loss: 0.9467690849304199\n",
            "Batch #300 Loss: 0.9185373228788376\n",
            "\u001b[92mTrain accuracy: 32677/48000 =  68.08 % ||| loss 0.8617910146713257\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8163/12000 =  68.03 % ||| loss 0.8530552387237549\u001b[0m\n",
            "\u001b[92mTest accuracy: 6735/10000 =  67.35 % ||| loss 0.8799564242362976\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.865811864733696\n",
            "Batch #200 Loss: 0.8683234471082687\n",
            "Batch #300 Loss: 0.836714101433754\n",
            "\u001b[92mTrain accuracy: 33704/48000 =  70.22 % ||| loss 0.8239962458610535\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8418/12000 =  70.15 % ||| loss 0.8181928992271423\u001b[0m\n",
            "\u001b[92mTest accuracy: 6935/10000 =  69.35 % ||| loss 0.8392998576164246\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.8097830510139465\n",
            "Batch #200 Loss: 0.7876320987939834\n",
            "Batch #300 Loss: 0.7938566386699677\n",
            "\u001b[92mTrain accuracy: 33488/48000 =  69.77 % ||| loss 0.7693929076194763\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8402/12000 =  70.02 % ||| loss 0.7580797076225281\u001b[0m\n",
            "\u001b[92mTest accuracy: 6935/10000 =  69.35 % ||| loss 0.7890973091125488\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.7667760860919952\n",
            "Batch #200 Loss: 0.7524056005477905\n",
            "Batch #300 Loss: 0.7551059031486511\n",
            "\u001b[92mTrain accuracy: 35157/48000 =  73.24 % ||| loss 0.7129480838775635\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8829/12000 =  73.58 % ||| loss 0.7024312019348145\u001b[0m\n",
            "\u001b[92mTest accuracy: 7265/10000 =  72.65 % ||| loss 0.732708215713501\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.7338952904939652\n",
            "Batch #200 Loss: 0.730756171643734\n",
            "Batch #300 Loss: 0.720834094285965\n",
            "\u001b[92mTrain accuracy: 35387/48000 =  73.72 % ||| loss 0.6893489360809326\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8869/12000 =  73.91 % ||| loss 0.6787782907485962\u001b[0m\n",
            "\u001b[92mTest accuracy: 7318/10000 =  73.18 % ||| loss 0.7100561857223511\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.6999326485395432\n",
            "Batch #200 Loss: 0.7114257353544235\n",
            "Batch #300 Loss: 0.684498184621334\n",
            "\u001b[92mTrain accuracy: 35149/48000 =  73.23 % ||| loss 0.6854413151741028\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8804/12000 =  73.37 % ||| loss 0.6783964037895203\u001b[0m\n",
            "\u001b[92mTest accuracy: 7260/10000 =  72.6 % ||| loss 0.7094037532806396\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6757992768287658\n",
            "Batch #200 Loss: 0.688262549340725\n",
            "Batch #300 Loss: 0.674084703028202\n",
            "\u001b[92mTrain accuracy: 35999/48000 =  75.0 % ||| loss 0.6830577254295349\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9025/12000 =  75.21 % ||| loss 0.673728346824646\u001b[0m\n",
            "\u001b[92mTest accuracy: 7412/10000 =  74.12 % ||| loss 0.7085530757904053\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.6654757019877434\n",
            "Batch #200 Loss: 0.6613756811618805\n",
            "Batch #300 Loss: 0.6605891117453575\n",
            "\u001b[92mTrain accuracy: 35701/48000 =  74.38 % ||| loss 0.6578532457351685\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8954/12000 =  74.62 % ||| loss 0.6521252393722534\u001b[0m\n",
            "\u001b[92mTest accuracy: 7349/10000 =  73.49 % ||| loss 0.6836950778961182\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.6378853553533554\n",
            "Batch #200 Loss: 0.6532566168904305\n",
            "Batch #300 Loss: 0.6381994214653969\n",
            "\u001b[92mTrain accuracy: 35609/48000 =  74.19 % ||| loss 0.6742368340492249\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8927/12000 =  74.39 % ||| loss 0.6592145562171936\u001b[0m\n",
            "\u001b[92mTest accuracy: 7331/10000 =  73.31 % ||| loss 0.6988824605941772\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.6417832911014557\n",
            "Batch #200 Loss: 0.6220936495065689\n",
            "Batch #300 Loss: 0.6237330484390259\n",
            "\u001b[92mTrain accuracy: 36987/48000 =  77.06 % ||| loss 0.621778130531311\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9267/12000 =  77.22 % ||| loss 0.6150394678115845\u001b[0m\n",
            "\u001b[92mTest accuracy: 7590/10000 =  75.9 % ||| loss 0.6509845852851868\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.6220951101183891\n",
            "Batch #200 Loss: 0.6285162881016731\n",
            "Batch #300 Loss: 0.6122457191348076\n",
            "\u001b[92mTrain accuracy: 37185/48000 =  77.47 % ||| loss 0.5977288484573364\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9305/12000 =  77.54 % ||| loss 0.5929391384124756\u001b[0m\n",
            "\u001b[92mTest accuracy: 7636/10000 =  76.36 % ||| loss 0.6261956095695496\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.6262985935807228\n",
            "Batch #200 Loss: 0.6231302261352539\n",
            "Batch #300 Loss: 0.5928563222289085\n",
            "\u001b[92mTrain accuracy: 36415/48000 =  75.86 % ||| loss 0.623414933681488\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9142/12000 =  76.18 % ||| loss 0.6188157796859741\u001b[0m\n",
            "\u001b[92mTest accuracy: 7465/10000 =  74.65 % ||| loss 0.6607784032821655\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.6096098738908767\n",
            "Batch #200 Loss: 0.5983334213495255\n",
            "Batch #300 Loss: 0.5964837664365769\n",
            "\u001b[92mTrain accuracy: 37390/48000 =  77.9 % ||| loss 0.5912041664123535\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9367/12000 =  78.06 % ||| loss 0.5882503986358643\u001b[0m\n",
            "\u001b[92mTest accuracy: 7692/10000 =  76.92 % ||| loss 0.6140400767326355\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5935217615962028\n",
            "Batch #200 Loss: 0.6014975556731224\n",
            "Batch #300 Loss: 0.5830722495913505\n",
            "\u001b[92mTrain accuracy: 37903/48000 =  78.96 % ||| loss 0.5713354349136353\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9516/12000 =  79.3 % ||| loss 0.5662878751754761\u001b[0m\n",
            "\u001b[92mTest accuracy: 7806/10000 =  78.06 % ||| loss 0.6067242622375488\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5719524604082108\n",
            "Batch #200 Loss: 0.5967735293507576\n",
            "Batch #300 Loss: 0.570253182053566\n",
            "\u001b[92mTrain accuracy: 37687/48000 =  78.51 % ||| loss 0.5677467584609985\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9436/12000 =  78.63 % ||| loss 0.5631996989250183\u001b[0m\n",
            "\u001b[92mTest accuracy: 7740/10000 =  77.4 % ||| loss 0.5964518785476685\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.5820297569036483\n",
            "Batch #200 Loss: 0.5674976733326912\n",
            "Batch #300 Loss: 0.5748299050331116\n",
            "\u001b[92mTrain accuracy: 37743/48000 =  78.63 % ||| loss 0.5712913274765015\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9420/12000 =  78.5 % ||| loss 0.5690178871154785\u001b[0m\n",
            "\u001b[92mTest accuracy: 7781/10000 =  77.81 % ||| loss 0.6034500598907471\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.5641971057653428\n",
            "Batch #200 Loss: 0.5767861256003379\n",
            "Batch #300 Loss: 0.5797359326481819\n",
            "\u001b[92mTrain accuracy: 38171/48000 =  79.52 % ||| loss 0.5614790320396423\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9589/12000 =  79.91 % ||| loss 0.5564264059066772\u001b[0m\n",
            "\u001b[92mTest accuracy: 7874/10000 =  78.74 % ||| loss 0.591532826423645\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.564520615041256\n",
            "Batch #200 Loss: 0.5560070782899856\n",
            "Batch #300 Loss: 0.5793324413895607\n",
            "\u001b[92mTrain accuracy: 38415/48000 =  80.03 % ||| loss 0.5486121773719788\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9619/12000 =  80.16 % ||| loss 0.5458526015281677\u001b[0m\n",
            "\u001b[92mTest accuracy: 7902/10000 =  79.02 % ||| loss 0.5776059031486511\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5529246640205383\n",
            "Batch #200 Loss: 0.5555617877840996\n",
            "Batch #300 Loss: 0.5607827132940293\n",
            "\u001b[92mTrain accuracy: 37839/48000 =  78.83 % ||| loss 0.5511934161186218\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9481/12000 =  79.01 % ||| loss 0.5494531989097595\u001b[0m\n",
            "\u001b[92mTest accuracy: 7807/10000 =  78.07 % ||| loss 0.5804165601730347\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5529084798693656\n",
            "Batch #200 Loss: 0.5449591225385666\n",
            "Batch #300 Loss: 0.5444231894612312\n",
            "\u001b[92mTrain accuracy: 38163/48000 =  79.51 % ||| loss 0.5505204200744629\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9593/12000 =  79.94 % ||| loss 0.5443459153175354\u001b[0m\n",
            "\u001b[92mTest accuracy: 7850/10000 =  78.5 % ||| loss 0.5788400173187256\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5569763153791427\n",
            "Batch #200 Loss: 0.5534309279918671\n",
            "Batch #300 Loss: 0.5249758067727089\n",
            "\u001b[92mTrain accuracy: 38668/48000 =  80.56 % ||| loss 0.5414701104164124\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9680/12000 =  80.67 % ||| loss 0.5407263040542603\u001b[0m\n",
            "\u001b[92mTest accuracy: 7950/10000 =  79.5 % ||| loss 0.5700592994689941\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_10</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_10' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_10</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_202738-Lenet5Decay_1726099655.2927098_11</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_11' target=\"_blank\">Lenet5Decay_1726099655.2927098_11</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_11' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3015277409553527\n",
            "Batch #200 Loss: 2.2955612182617187\n",
            "Batch #300 Loss: 2.2840253281593323\n",
            "\u001b[92mTrain accuracy: 9410/48000 =  19.6 % ||| loss 2.2537264823913574\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2450/12000 =  20.42 % ||| loss 2.2525312900543213\u001b[0m\n",
            "\u001b[92mTest accuracy: 1935/10000 =  19.35 % ||| loss 2.2538199424743652\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2215667366981506\n",
            "Batch #200 Loss: 2.0343400061130525\n",
            "Batch #300 Loss: 1.4698891830444336\n",
            "\u001b[92mTrain accuracy: 27935/48000 =  58.2 % ||| loss 1.049044132232666\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6962/12000 =  58.02 % ||| loss 1.0420891046524048\u001b[0m\n",
            "\u001b[92mTest accuracy: 5733/10000 =  57.33 % ||| loss 1.0580642223358154\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.043628077507019\n",
            "Batch #200 Loss: 0.9792432141304016\n",
            "Batch #300 Loss: 0.96470055103302\n",
            "\u001b[92mTrain accuracy: 29947/48000 =  62.39 % ||| loss 0.9523767232894897\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7504/12000 =  62.53 % ||| loss 0.9444088339805603\u001b[0m\n",
            "\u001b[92mTest accuracy: 6182/10000 =  61.82 % ||| loss 0.9666578769683838\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.9108866691589356\n",
            "Batch #200 Loss: 0.8842577433586121\n",
            "Batch #300 Loss: 0.8336733174324036\n",
            "\u001b[92mTrain accuracy: 32817/48000 =  68.37 % ||| loss 0.8073743581771851\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8243/12000 =  68.69 % ||| loss 0.7957717180252075\u001b[0m\n",
            "\u001b[92mTest accuracy: 6745/10000 =  67.45 % ||| loss 0.823482096195221\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.8058652234077454\n",
            "Batch #200 Loss: 0.8085405617952347\n",
            "Batch #300 Loss: 0.7857083874940872\n",
            "\u001b[92mTrain accuracy: 34838/48000 =  72.58 % ||| loss 0.7401780486106873\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8741/12000 =  72.84 % ||| loss 0.7319638729095459\u001b[0m\n",
            "\u001b[92mTest accuracy: 7180/10000 =  71.8 % ||| loss 0.7557149529457092\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.753020076751709\n",
            "Batch #200 Loss: 0.7459307205677033\n",
            "Batch #300 Loss: 0.7280332738161087\n",
            "\u001b[92mTrain accuracy: 35372/48000 =  73.69 % ||| loss 0.7075998783111572\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8846/12000 =  73.72 % ||| loss 0.6986611485481262\u001b[0m\n",
            "\u001b[92mTest accuracy: 7268/10000 =  72.68 % ||| loss 0.7334277033805847\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.7293457531929016\n",
            "Batch #200 Loss: 0.7002247363328934\n",
            "Batch #300 Loss: 0.6955663299560547\n",
            "\u001b[92mTrain accuracy: 34114/48000 =  71.07 % ||| loss 0.7451823949813843\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8548/12000 =  71.23 % ||| loss 0.7322022318840027\u001b[0m\n",
            "\u001b[92mTest accuracy: 7043/10000 =  70.43 % ||| loss 0.7673490643501282\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.6835473111271858\n",
            "Batch #200 Loss: 0.6709096163511277\n",
            "Batch #300 Loss: 0.6711099427938462\n",
            "\u001b[92mTrain accuracy: 35401/48000 =  73.75 % ||| loss 0.6853693723678589\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8860/12000 =  73.83 % ||| loss 0.681934654712677\u001b[0m\n",
            "\u001b[92mTest accuracy: 7286/10000 =  72.86 % ||| loss 0.7074609398841858\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.6497166082262993\n",
            "Batch #200 Loss: 0.6533775204420089\n",
            "Batch #300 Loss: 0.6424608466029167\n",
            "\u001b[92mTrain accuracy: 35771/48000 =  74.52 % ||| loss 0.6537426114082336\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9019/12000 =  75.16 % ||| loss 0.6456575393676758\u001b[0m\n",
            "\u001b[92mTest accuracy: 7392/10000 =  73.92 % ||| loss 0.68156498670578\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.6237850135564804\n",
            "Batch #200 Loss: 0.6372017151117325\n",
            "Batch #300 Loss: 0.6241243681311608\n",
            "\u001b[92mTrain accuracy: 37394/48000 =  77.9 % ||| loss 0.594681978225708\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9400/12000 =  78.33 % ||| loss 0.5872740745544434\u001b[0m\n",
            "\u001b[92mTest accuracy: 7697/10000 =  76.97 % ||| loss 0.6221205592155457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6017108008265495\n",
            "Batch #200 Loss: 0.6147532066702843\n",
            "Batch #300 Loss: 0.6146820065379143\n",
            "\u001b[92mTrain accuracy: 37554/48000 =  78.24 % ||| loss 0.5857909917831421\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9399/12000 =  78.33 % ||| loss 0.5805287957191467\u001b[0m\n",
            "\u001b[92mTest accuracy: 7698/10000 =  76.98 % ||| loss 0.620130717754364\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.6002377504110337\n",
            "Batch #200 Loss: 0.5884686005115509\n",
            "Batch #300 Loss: 0.5894928288459778\n",
            "\u001b[92mTrain accuracy: 37846/48000 =  78.85 % ||| loss 0.5725662112236023\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9521/12000 =  79.34 % ||| loss 0.5687378644943237\u001b[0m\n",
            "\u001b[92mTest accuracy: 7767/10000 =  77.67 % ||| loss 0.6029196381568909\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5798794507980347\n",
            "Batch #200 Loss: 0.5665200954675674\n",
            "Batch #300 Loss: 0.5723120322823525\n",
            "\u001b[92mTrain accuracy: 38249/48000 =  79.69 % ||| loss 0.5553682446479797\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9587/12000 =  79.89 % ||| loss 0.5559124946594238\u001b[0m\n",
            "\u001b[92mTest accuracy: 7847/10000 =  78.47 % ||| loss 0.5844151973724365\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.5597245770692826\n",
            "Batch #200 Loss: 0.5525109574198723\n",
            "Batch #300 Loss: 0.564298790693283\n",
            "\u001b[92mTrain accuracy: 38651/48000 =  80.52 % ||| loss 0.5379699468612671\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9658/12000 =  80.48 % ||| loss 0.5365192890167236\u001b[0m\n",
            "\u001b[92mTest accuracy: 7940/10000 =  79.4 % ||| loss 0.5654712319374084\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5474977222084999\n",
            "Batch #200 Loss: 0.5468736079335212\n",
            "Batch #300 Loss: 0.5488537925481797\n",
            "\u001b[92mTrain accuracy: 38723/48000 =  80.67 % ||| loss 0.5259885787963867\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9706/12000 =  80.88 % ||| loss 0.5249723196029663\u001b[0m\n",
            "\u001b[92mTest accuracy: 7920/10000 =  79.2 % ||| loss 0.5645751357078552\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.537373397052288\n",
            "Batch #200 Loss: 0.5229584538936615\n",
            "Batch #300 Loss: 0.5461446759104729\n",
            "\u001b[92mTrain accuracy: 38827/48000 =  80.89 % ||| loss 0.5215420722961426\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9722/12000 =  81.02 % ||| loss 0.5212783813476562\u001b[0m\n",
            "\u001b[92mTest accuracy: 7990/10000 =  79.9 % ||| loss 0.5569169521331787\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5228300508856774\n",
            "Batch #200 Loss: 0.52673096626997\n",
            "Batch #300 Loss: 0.5228617164492607\n",
            "\u001b[92mTrain accuracy: 39095/48000 =  81.45 % ||| loss 0.5026233196258545\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9812/12000 =  81.77 % ||| loss 0.5008476972579956\u001b[0m\n",
            "\u001b[92mTest accuracy: 8037/10000 =  80.37 % ||| loss 0.5441569685935974\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5134307664632797\n",
            "Batch #200 Loss: 0.517466082572937\n",
            "Batch #300 Loss: 0.5110321837663651\n",
            "\u001b[92mTrain accuracy: 39516/48000 =  82.33 % ||| loss 0.49149787425994873\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9892/12000 =  82.43 % ||| loss 0.49363449215888977\u001b[0m\n",
            "\u001b[92mTest accuracy: 8119/10000 =  81.19 % ||| loss 0.5244960784912109\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5010908108949661\n",
            "Batch #200 Loss: 0.5013501733541489\n",
            "Batch #300 Loss: 0.4944776406884193\n",
            "\u001b[92mTrain accuracy: 39377/48000 =  82.04 % ||| loss 0.49409741163253784\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9880/12000 =  82.33 % ||| loss 0.490438312292099\u001b[0m\n",
            "\u001b[92mTest accuracy: 8087/10000 =  80.87 % ||| loss 0.5332619547843933\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.48615282386541364\n",
            "Batch #200 Loss: 0.4973188188672066\n",
            "Batch #300 Loss: 0.4988859936594963\n",
            "\u001b[92mTrain accuracy: 39706/48000 =  82.72 % ||| loss 0.4780459403991699\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9924/12000 =  82.7 % ||| loss 0.4782702624797821\u001b[0m\n",
            "\u001b[92mTest accuracy: 8143/10000 =  81.43 % ||| loss 0.5154784917831421\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.48880483537912367\n",
            "Batch #200 Loss: 0.4804569289088249\n",
            "Batch #300 Loss: 0.4776693853735924\n",
            "\u001b[92mTrain accuracy: 39920/48000 =  83.17 % ||| loss 0.4695686101913452\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10000/12000 =  83.33 % ||| loss 0.4705319404602051\u001b[0m\n",
            "\u001b[92mTest accuracy: 8202/10000 =  82.02 % ||| loss 0.5029168128967285\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.4834380409121513\n",
            "Batch #200 Loss: 0.48102525115013123\n",
            "Batch #300 Loss: 0.4630566772818565\n",
            "\u001b[92mTrain accuracy: 39842/48000 =  83.0 % ||| loss 0.46426302194595337\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9940/12000 =  82.83 % ||| loss 0.4694541096687317\u001b[0m\n",
            "\u001b[92mTest accuracy: 8187/10000 =  81.87 % ||| loss 0.5022026300430298\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.4623685464262962\n",
            "Batch #200 Loss: 0.46869360983371733\n",
            "Batch #300 Loss: 0.4707918855547905\n",
            "\u001b[92mTrain accuracy: 40099/48000 =  83.54 % ||| loss 0.45406314730644226\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9985/12000 =  83.21 % ||| loss 0.4598475396633148\u001b[0m\n",
            "\u001b[92mTest accuracy: 8225/10000 =  82.25 % ||| loss 0.4914025366306305\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.46637297749519346\n",
            "Batch #200 Loss: 0.4691229936480522\n",
            "Batch #300 Loss: 0.457161018550396\n",
            "\u001b[92mTrain accuracy: 39876/48000 =  83.08 % ||| loss 0.45835617184638977\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9975/12000 =  83.12 % ||| loss 0.4606066644191742\u001b[0m\n",
            "\u001b[92mTest accuracy: 8176/10000 =  81.76 % ||| loss 0.4966174364089966\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.4515375289320946\n",
            "Batch #200 Loss: 0.46261815071105955\n",
            "Batch #300 Loss: 0.44440253376960753\n",
            "\u001b[92mTrain accuracy: 40315/48000 =  83.99 % ||| loss 0.44123637676239014\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10033/12000 =  83.61 % ||| loss 0.44797301292419434\u001b[0m\n",
            "\u001b[92mTest accuracy: 8286/10000 =  82.86 % ||| loss 0.4777991473674774\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_11</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_11' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_11</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_202927-Lenet5Decay_1726099655.2927098_12</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_12' target=\"_blank\">Lenet5Decay_1726099655.2927098_12</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_12' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_12</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302259931564331\n",
            "Batch #200 Loss: 2.3026157331466677\n",
            "Batch #300 Loss: 2.302902731895447\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3026444911956787\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026835918426514\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026881217956543\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.302785584926605\n",
            "Batch #200 Loss: 2.302642900943756\n",
            "Batch #300 Loss: 2.3027595114707946\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302619218826294\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3026654720306396\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026645183563232\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.302588691711426\n",
            "Batch #200 Loss: 2.3028423500061037\n",
            "Batch #300 Loss: 2.302712848186493\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.302600383758545\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.3026156425476074\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025944232940674\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3027329516410826\n",
            "Batch #200 Loss: 2.30276775598526\n",
            "Batch #300 Loss: 2.3027047538757324\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3026719093322754\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3026223182678223\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025965690612793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3028109288215637\n",
            "Batch #200 Loss: 2.302795844078064\n",
            "Batch #300 Loss: 2.3027642011642455\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.3026115894317627\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.3026788234710693\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025834560394287\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3026458024978638\n",
            "Batch #200 Loss: 2.3027670907974245\n",
            "Batch #300 Loss: 2.3027456331253053\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3026232719421387\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302684783935547\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026297092437744\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.302636649608612\n",
            "Batch #200 Loss: 2.302748680114746\n",
            "Batch #300 Loss: 2.302721803188324\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302595376968384\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302767515182495\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302618980407715\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.3026925683021546\n",
            "Batch #200 Loss: 2.3027807807922365\n",
            "Batch #300 Loss: 2.3026632404327394\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302633285522461\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3025999069213867\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302656650543213\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.302688789367676\n",
            "Batch #200 Loss: 2.3027267336845396\n",
            "Batch #300 Loss: 2.3027591490745545\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3026316165924072\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302657127380371\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026256561279297\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.3028300833702087\n",
            "Batch #200 Loss: 2.3026965832710267\n",
            "Batch #300 Loss: 2.30270779132843\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3026275634765625\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302717447280884\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026845455169678\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.302764778137207\n",
            "Batch #200 Loss: 2.302744493484497\n",
            "Batch #300 Loss: 2.3027943110466005\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302665948867798\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302638530731201\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302595376968384\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.3027013659477236\n",
            "Batch #200 Loss: 2.3025087118148804\n",
            "Batch #300 Loss: 2.3028280568122863\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.3026366233825684\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.302647590637207\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302605628967285\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.302638156414032\n",
            "Batch #200 Loss: 2.3027030777931214\n",
            "Batch #300 Loss: 2.3026945996284485\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.3026115894317627\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.3026368618011475\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025946617126465\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.3027796053886416\n",
            "Batch #200 Loss: 2.302811396121979\n",
            "Batch #300 Loss: 2.3028128004074095\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.3026232719421387\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.302741765975952\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026397228240967\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.3026846766471865\n",
            "Batch #200 Loss: 2.302777843475342\n",
            "Batch #300 Loss: 2.3027043175697326\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.302612066268921\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3026938438415527\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302656650543213\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.3027305459976195\n",
            "Batch #200 Loss: 2.3027201390266416\n",
            "Batch #300 Loss: 2.3027489018440246\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.302659034729004\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.3027901649475098\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027279376983643\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.3025572419166567\n",
            "Batch #200 Loss: 2.3026888275146487\n",
            "Batch #300 Loss: 2.302828450202942\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302600860595703\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.30265212059021\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302579641342163\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.3025742983818054\n",
            "Batch #200 Loss: 2.3026423478126525\n",
            "Batch #300 Loss: 2.302739007472992\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3026723861694336\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302746534347534\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302651882171631\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.302695155143738\n",
            "Batch #200 Loss: 2.3026506066322328\n",
            "Batch #300 Loss: 2.302761871814728\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.30259108543396\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3025991916656494\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302600383758545\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.302639734745026\n",
            "Batch #200 Loss: 2.3027215361595155\n",
            "Batch #300 Loss: 2.302828948497772\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302624225616455\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3027055263519287\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026719093322754\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.302685363292694\n",
            "Batch #200 Loss: 2.302700204849243\n",
            "Batch #300 Loss: 2.3027288484573365\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3026301860809326\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302663564682007\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026199340820312\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.302632074356079\n",
            "Batch #200 Loss: 2.3027874636650085\n",
            "Batch #300 Loss: 2.3026557302474977\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.302621841430664\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.302729845046997\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026347160339355\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.302720868587494\n",
            "Batch #200 Loss: 2.3027286529541016\n",
            "Batch #300 Loss: 2.3026078963279724\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302616834640503\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302738904953003\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026480674743652\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.3026941776275636\n",
            "Batch #200 Loss: 2.302633674144745\n",
            "Batch #300 Loss: 2.3027693915367125\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302628517150879\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3026325702667236\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026371002197266\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3027344942092896\n",
            "Batch #200 Loss: 2.3027571177482606\n",
            "Batch #300 Loss: 2.3027810406684877\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302615165710449\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.30271053314209\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302614212036133\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_12</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_12' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_12</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_203116-Lenet5Decay_1726099655.2927098_13</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_13' target=\"_blank\">Lenet5Decay_1726099655.2927098_13</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_13' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_13</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2966174387931826\n",
            "Batch #200 Loss: 1.7784050601720809\n",
            "Batch #300 Loss: 0.8759332007169723\n",
            "\u001b[92mTrain accuracy: 35512/48000 =  73.98 % ||| loss 0.7092249989509583\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8883/12000 =  74.02 % ||| loss 0.7032731175422668\u001b[0m\n",
            "\u001b[92mTest accuracy: 7283/10000 =  72.83 % ||| loss 0.7333725094795227\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.694485944211483\n",
            "Batch #200 Loss: 0.6574450004100799\n",
            "Batch #300 Loss: 0.623335340321064\n",
            "\u001b[92mTrain accuracy: 37530/48000 =  78.19 % ||| loss 0.567875325679779\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9385/12000 =  78.21 % ||| loss 0.5700944662094116\u001b[0m\n",
            "\u001b[92mTest accuracy: 7720/10000 =  77.2 % ||| loss 0.5919479727745056\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5691351234912873\n",
            "Batch #200 Loss: 0.5703487366437912\n",
            "Batch #300 Loss: 0.54814188092947\n",
            "\u001b[92mTrain accuracy: 37621/48000 =  78.38 % ||| loss 0.5645041465759277\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9372/12000 =  78.1 % ||| loss 0.5662523508071899\u001b[0m\n",
            "\u001b[92mTest accuracy: 7763/10000 =  77.63 % ||| loss 0.5835720896720886\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5469297981262207\n",
            "Batch #200 Loss: 0.5207953667640686\n",
            "Batch #300 Loss: 0.5121317657828331\n",
            "\u001b[92mTrain accuracy: 38854/48000 =  80.95 % ||| loss 0.5077934861183167\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9709/12000 =  80.91 % ||| loss 0.5125731825828552\u001b[0m\n",
            "\u001b[92mTest accuracy: 8010/10000 =  80.1 % ||| loss 0.5328279137611389\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5054323861002922\n",
            "Batch #200 Loss: 0.5039881572127343\n",
            "Batch #300 Loss: 0.5110386633872985\n",
            "\u001b[92mTrain accuracy: 38507/48000 =  80.22 % ||| loss 0.5286642909049988\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9675/12000 =  80.62 % ||| loss 0.5305209755897522\u001b[0m\n",
            "\u001b[92mTest accuracy: 7934/10000 =  79.34 % ||| loss 0.5555506944656372\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5016571128368378\n",
            "Batch #200 Loss: 0.4922817760705948\n",
            "Batch #300 Loss: 0.47059064120054245\n",
            "\u001b[92mTrain accuracy: 39790/48000 =  82.9 % ||| loss 0.46957942843437195\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9960/12000 =  83.0 % ||| loss 0.47240883111953735\u001b[0m\n",
            "\u001b[92mTest accuracy: 8223/10000 =  82.23 % ||| loss 0.4928581118583679\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.46902589678764345\n",
            "Batch #200 Loss: 0.46908629179000855\n",
            "Batch #300 Loss: 0.47060741633176806\n",
            "\u001b[92mTrain accuracy: 40121/48000 =  83.59 % ||| loss 0.4511634409427643\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10025/12000 =  83.54 % ||| loss 0.4566226303577423\u001b[0m\n",
            "\u001b[92mTest accuracy: 8298/10000 =  82.98 % ||| loss 0.4718726575374603\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.4672752970457077\n",
            "Batch #200 Loss: 0.4602893194556236\n",
            "Batch #300 Loss: 0.4539888396859169\n",
            "\u001b[92mTrain accuracy: 40060/48000 =  83.46 % ||| loss 0.4515925347805023\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10009/12000 =  83.41 % ||| loss 0.45892131328582764\u001b[0m\n",
            "\u001b[92mTest accuracy: 8242/10000 =  82.42 % ||| loss 0.48048362135887146\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.4572709321975708\n",
            "Batch #200 Loss: 0.4524398052692413\n",
            "Batch #300 Loss: 0.440959712266922\n",
            "\u001b[92mTrain accuracy: 39677/48000 =  82.66 % ||| loss 0.46936139464378357\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9964/12000 =  83.03 % ||| loss 0.4748654067516327\u001b[0m\n",
            "\u001b[92mTest accuracy: 8208/10000 =  82.08 % ||| loss 0.5004678964614868\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.45175976246595384\n",
            "Batch #200 Loss: 0.45226036161184313\n",
            "Batch #300 Loss: 0.43856691688299176\n",
            "\u001b[92mTrain accuracy: 40165/48000 =  83.68 % ||| loss 0.4443006217479706\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10042/12000 =  83.68 % ||| loss 0.45316192507743835\u001b[0m\n",
            "\u001b[92mTest accuracy: 8260/10000 =  82.6 % ||| loss 0.470087468624115\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.4324567463994026\n",
            "Batch #200 Loss: 0.4465625795722008\n",
            "Batch #300 Loss: 0.4422580698132515\n",
            "\u001b[92mTrain accuracy: 40256/48000 =  83.87 % ||| loss 0.44293713569641113\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10007/12000 =  83.39 % ||| loss 0.4542806148529053\u001b[0m\n",
            "\u001b[92mTest accuracy: 8318/10000 =  83.18 % ||| loss 0.46635934710502625\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.4294166532158852\n",
            "Batch #200 Loss: 0.4374303963780403\n",
            "Batch #300 Loss: 0.43445283979177474\n",
            "\u001b[92mTrain accuracy: 40971/48000 =  85.36 % ||| loss 0.41665026545524597\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10234/12000 =  85.28 % ||| loss 0.42421555519104004\u001b[0m\n",
            "\u001b[92mTest accuracy: 8451/10000 =  84.51 % ||| loss 0.44088736176490784\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.40846114963293073\n",
            "Batch #200 Loss: 0.4343626633286476\n",
            "Batch #300 Loss: 0.4356313467025757\n",
            "\u001b[92mTrain accuracy: 40244/48000 =  83.84 % ||| loss 0.4444384276866913\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9985/12000 =  83.21 % ||| loss 0.4561457633972168\u001b[0m\n",
            "\u001b[92mTest accuracy: 8306/10000 =  83.06 % ||| loss 0.46696582436561584\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.42668422996997835\n",
            "Batch #200 Loss: 0.42570159256458284\n",
            "Batch #300 Loss: 0.4139274999499321\n",
            "\u001b[92mTrain accuracy: 40883/48000 =  85.17 % ||| loss 0.41637691855430603\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10131/12000 =  84.42 % ||| loss 0.42837464809417725\u001b[0m\n",
            "\u001b[92mTest accuracy: 8375/10000 =  83.75 % ||| loss 0.44361236691474915\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.41360696524381635\n",
            "Batch #200 Loss: 0.43834824591875077\n",
            "Batch #300 Loss: 0.42303748220205306\n",
            "\u001b[92mTrain accuracy: 41080/48000 =  85.58 % ||| loss 0.41086554527282715\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10196/12000 =  84.97 % ||| loss 0.42253628373146057\u001b[0m\n",
            "\u001b[92mTest accuracy: 8491/10000 =  84.91 % ||| loss 0.43471232056617737\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.42339884489774704\n",
            "Batch #200 Loss: 0.41715945094823836\n",
            "Batch #300 Loss: 0.4179055821895599\n",
            "\u001b[92mTrain accuracy: 40911/48000 =  85.23 % ||| loss 0.4171712100505829\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10240/12000 =  85.33 % ||| loss 0.4256884753704071\u001b[0m\n",
            "\u001b[92mTest accuracy: 8466/10000 =  84.66 % ||| loss 0.4485759437084198\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.42972424924373626\n",
            "Batch #200 Loss: 0.4196580019593239\n",
            "Batch #300 Loss: 0.4089166775345802\n",
            "\u001b[92mTrain accuracy: 40930/48000 =  85.27 % ||| loss 0.413577139377594\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10213/12000 =  85.11 % ||| loss 0.4232361912727356\u001b[0m\n",
            "\u001b[92mTest accuracy: 8456/10000 =  84.56 % ||| loss 0.4392642676830292\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.4146707594394684\n",
            "Batch #200 Loss: 0.40892343133687975\n",
            "Batch #300 Loss: 0.4176588243246078\n",
            "\u001b[92mTrain accuracy: 40847/48000 =  85.1 % ||| loss 0.42186275124549866\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10158/12000 =  84.65 % ||| loss 0.4291531443595886\u001b[0m\n",
            "\u001b[92mTest accuracy: 8398/10000 =  83.98 % ||| loss 0.45070546865463257\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.4067688384652138\n",
            "Batch #200 Loss: 0.4061886605620384\n",
            "Batch #300 Loss: 0.41504998445510866\n",
            "\u001b[92mTrain accuracy: 41598/48000 =  86.66 % ||| loss 0.3851959705352783\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10346/12000 =  86.22 % ||| loss 0.39704635739326477\u001b[0m\n",
            "\u001b[92mTest accuracy: 8596/10000 =  85.96 % ||| loss 0.4102602005004883\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.405480270087719\n",
            "Batch #200 Loss: 0.41249632835388184\n",
            "Batch #300 Loss: 0.40951417982578275\n",
            "\u001b[92mTrain accuracy: 40994/48000 =  85.4 % ||| loss 0.4111766815185547\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10242/12000 =  85.35 % ||| loss 0.42215496301651\u001b[0m\n",
            "\u001b[92mTest accuracy: 8460/10000 =  84.6 % ||| loss 0.4380852282047272\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.4056704872846603\n",
            "Batch #200 Loss: 0.39807843387126923\n",
            "Batch #300 Loss: 0.415607273876667\n",
            "\u001b[92mTrain accuracy: 41165/48000 =  85.76 % ||| loss 0.4043181836605072\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10216/12000 =  85.13 % ||| loss 0.41289541125297546\u001b[0m\n",
            "\u001b[92mTest accuracy: 8434/10000 =  84.34 % ||| loss 0.4370323717594147\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.4020235401391983\n",
            "Batch #200 Loss: 0.4079670590162277\n",
            "Batch #300 Loss: 0.4131804430484772\n",
            "\u001b[92mTrain accuracy: 41064/48000 =  85.55 % ||| loss 0.3958909809589386\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10215/12000 =  85.12 % ||| loss 0.41004055738449097\u001b[0m\n",
            "\u001b[92mTest accuracy: 8475/10000 =  84.75 % ||| loss 0.4259246587753296\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.3997430197894573\n",
            "Batch #200 Loss: 0.41620335310697554\n",
            "Batch #300 Loss: 0.40205894619226457\n",
            "\u001b[92mTrain accuracy: 40836/48000 =  85.08 % ||| loss 0.4118365943431854\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10195/12000 =  84.96 % ||| loss 0.4195198714733124\u001b[0m\n",
            "\u001b[92mTest accuracy: 8431/10000 =  84.31 % ||| loss 0.4411543309688568\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.40163074642419816\n",
            "Batch #200 Loss: 0.39485607758164404\n",
            "Batch #300 Loss: 0.3904327034950256\n",
            "\u001b[92mTrain accuracy: 41683/48000 =  86.84 % ||| loss 0.37447771430015564\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10365/12000 =  86.38 % ||| loss 0.3848646581172943\u001b[0m\n",
            "\u001b[92mTest accuracy: 8579/10000 =  85.79 % ||| loss 0.40277042984962463\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.40289705589413644\n",
            "Batch #200 Loss: 0.3907370090484619\n",
            "Batch #300 Loss: 0.40174972400069237\n",
            "\u001b[92mTrain accuracy: 41344/48000 =  86.13 % ||| loss 0.38704654574394226\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10255/12000 =  85.46 % ||| loss 0.4012720584869385\u001b[0m\n",
            "\u001b[92mTest accuracy: 8516/10000 =  85.16 % ||| loss 0.4159359037876129\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_13</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_13' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_13</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_203304-Lenet5Decay_1726099655.2927098_14</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_14' target=\"_blank\">Lenet5Decay_1726099655.2927098_14</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_14' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_14</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2995368885993956\n",
            "Batch #200 Loss: 1.9626433503627778\n",
            "Batch #300 Loss: 0.8961515122652054\n",
            "\u001b[92mTrain accuracy: 35572/48000 =  74.11 % ||| loss 0.6888537406921387\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8884/12000 =  74.03 % ||| loss 0.6863142848014832\u001b[0m\n",
            "\u001b[92mTest accuracy: 7329/10000 =  73.29 % ||| loss 0.7124200463294983\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6677416241168976\n",
            "Batch #200 Loss: 0.617595970928669\n",
            "Batch #300 Loss: 0.5768308869004249\n",
            "\u001b[92mTrain accuracy: 38271/48000 =  79.73 % ||| loss 0.5302950143814087\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9595/12000 =  79.96 % ||| loss 0.5301538109779358\u001b[0m\n",
            "\u001b[92mTest accuracy: 7913/10000 =  79.13 % ||| loss 0.5560903549194336\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5343796795606613\n",
            "Batch #200 Loss: 0.5112782150506974\n",
            "Batch #300 Loss: 0.4954243609309196\n",
            "\u001b[92mTrain accuracy: 39465/48000 =  82.22 % ||| loss 0.478789746761322\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9871/12000 =  82.26 % ||| loss 0.48398059606552124\u001b[0m\n",
            "\u001b[92mTest accuracy: 8137/10000 =  81.37 % ||| loss 0.5012752413749695\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.48254950135946273\n",
            "Batch #200 Loss: 0.45863602876663206\n",
            "Batch #300 Loss: 0.45591669201850893\n",
            "\u001b[92mTrain accuracy: 40306/48000 =  83.97 % ||| loss 0.4320945143699646\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10058/12000 =  83.82 % ||| loss 0.44329237937927246\u001b[0m\n",
            "\u001b[92mTest accuracy: 8285/10000 =  82.85 % ||| loss 0.4631592929363251\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4385510918498039\n",
            "Batch #200 Loss: 0.42639449179172517\n",
            "Batch #300 Loss: 0.40923095703125\n",
            "\u001b[92mTrain accuracy: 40986/48000 =  85.39 % ||| loss 0.40559500455856323\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10153/12000 =  84.61 % ||| loss 0.4165343940258026\u001b[0m\n",
            "\u001b[92mTest accuracy: 8408/10000 =  84.08 % ||| loss 0.4367055892944336\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.4029090252518654\n",
            "Batch #200 Loss: 0.396382482200861\n",
            "Batch #300 Loss: 0.3971823082864285\n",
            "\u001b[92mTrain accuracy: 41586/48000 =  86.64 % ||| loss 0.3654497563838959\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10303/12000 =  85.86 % ||| loss 0.38657715916633606\u001b[0m\n",
            "\u001b[92mTest accuracy: 8555/10000 =  85.55 % ||| loss 0.4063032269477844\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3870689496397972\n",
            "Batch #200 Loss: 0.3792041888833046\n",
            "Batch #300 Loss: 0.3790689639747143\n",
            "\u001b[92mTrain accuracy: 41583/48000 =  86.63 % ||| loss 0.36429208517074585\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10327/12000 =  86.06 % ||| loss 0.38303521275520325\u001b[0m\n",
            "\u001b[92mTest accuracy: 8528/10000 =  85.28 % ||| loss 0.40093114972114563\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3644413268566132\n",
            "Batch #200 Loss: 0.366525337100029\n",
            "Batch #300 Loss: 0.36864761844277383\n",
            "\u001b[92mTrain accuracy: 41803/48000 =  87.09 % ||| loss 0.34873226284980774\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10374/12000 =  86.45 % ||| loss 0.37472423911094666\u001b[0m\n",
            "\u001b[92mTest accuracy: 8591/10000 =  85.91 % ||| loss 0.3905486464500427\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3447515584528446\n",
            "Batch #200 Loss: 0.3536957260966301\n",
            "Batch #300 Loss: 0.34860442861914637\n",
            "\u001b[92mTrain accuracy: 42212/48000 =  87.94 % ||| loss 0.3272918462753296\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10463/12000 =  87.19 % ||| loss 0.3508473336696625\u001b[0m\n",
            "\u001b[92mTest accuracy: 8649/10000 =  86.49 % ||| loss 0.3762448728084564\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3474131780862808\n",
            "Batch #200 Loss: 0.34967810958623885\n",
            "Batch #300 Loss: 0.3357901068031788\n",
            "\u001b[92mTrain accuracy: 42124/48000 =  87.76 % ||| loss 0.33241772651672363\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10402/12000 =  86.68 % ||| loss 0.3600260019302368\u001b[0m\n",
            "\u001b[92mTest accuracy: 8626/10000 =  86.26 % ||| loss 0.38501253724098206\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.3334262101352215\n",
            "Batch #200 Loss: 0.33055304750800135\n",
            "Batch #300 Loss: 0.33631877690553663\n",
            "\u001b[92mTrain accuracy: 42129/48000 =  87.77 % ||| loss 0.33241066336631775\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10438/12000 =  86.98 % ||| loss 0.35819000005722046\u001b[0m\n",
            "\u001b[92mTest accuracy: 8616/10000 =  86.16 % ||| loss 0.38542652130126953\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.31256691202521325\n",
            "Batch #200 Loss: 0.32796385273337364\n",
            "Batch #300 Loss: 0.3330617706477642\n",
            "\u001b[92mTrain accuracy: 42676/48000 =  88.91 % ||| loss 0.3095608353614807\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10497/12000 =  87.48 % ||| loss 0.3369462788105011\u001b[0m\n",
            "\u001b[92mTest accuracy: 8676/10000 =  86.76 % ||| loss 0.3581068217754364\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.30660031735897064\n",
            "Batch #200 Loss: 0.313301123380661\n",
            "Batch #300 Loss: 0.32580048888921737\n",
            "\u001b[92mTrain accuracy: 42656/48000 =  88.87 % ||| loss 0.30608996748924255\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10534/12000 =  87.78 % ||| loss 0.3345886468887329\u001b[0m\n",
            "\u001b[92mTest accuracy: 8674/10000 =  86.74 % ||| loss 0.35499823093414307\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.30888047128915785\n",
            "Batch #200 Loss: 0.32657543003559114\n",
            "Batch #300 Loss: 0.3104070842266083\n",
            "\u001b[92mTrain accuracy: 42782/48000 =  89.13 % ||| loss 0.2991735339164734\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10555/12000 =  87.96 % ||| loss 0.3255978524684906\u001b[0m\n",
            "\u001b[92mTest accuracy: 8730/10000 =  87.3 % ||| loss 0.35062769055366516\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3065715444087982\n",
            "Batch #200 Loss: 0.31562003031373026\n",
            "Batch #300 Loss: 0.3021125227212906\n",
            "\u001b[92mTrain accuracy: 42930/48000 =  89.44 % ||| loss 0.28852665424346924\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10574/12000 =  88.12 % ||| loss 0.3246815502643585\u001b[0m\n",
            "\u001b[92mTest accuracy: 8724/10000 =  87.24 % ||| loss 0.3509940207004547\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3000007216632366\n",
            "Batch #200 Loss: 0.2939158707857132\n",
            "Batch #300 Loss: 0.2998578004539013\n",
            "\u001b[92mTrain accuracy: 42972/48000 =  89.53 % ||| loss 0.28576019406318665\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10588/12000 =  88.23 % ||| loss 0.3215687870979309\u001b[0m\n",
            "\u001b[92mTest accuracy: 8716/10000 =  87.16 % ||| loss 0.35063761472702026\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.29533812940120696\n",
            "Batch #200 Loss: 0.2979380339384079\n",
            "Batch #300 Loss: 0.299671805202961\n",
            "\u001b[92mTrain accuracy: 43022/48000 =  89.63 % ||| loss 0.28158918023109436\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10626/12000 =  88.55 % ||| loss 0.31856590509414673\u001b[0m\n",
            "\u001b[92mTest accuracy: 8761/10000 =  87.61 % ||| loss 0.3410031497478485\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.28777159586548806\n",
            "Batch #200 Loss: 0.2976858687400818\n",
            "Batch #300 Loss: 0.29900950968265533\n",
            "\u001b[92mTrain accuracy: 42982/48000 =  89.55 % ||| loss 0.2845228612422943\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10574/12000 =  88.12 % ||| loss 0.324653297662735\u001b[0m\n",
            "\u001b[92mTest accuracy: 8745/10000 =  87.45 % ||| loss 0.3467518091201782\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.3013911756873131\n",
            "Batch #200 Loss: 0.27964470282196996\n",
            "Batch #300 Loss: 0.29706219509243964\n",
            "\u001b[92mTrain accuracy: 43260/48000 =  90.12 % ||| loss 0.27155017852783203\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10630/12000 =  88.58 % ||| loss 0.3116554021835327\u001b[0m\n",
            "\u001b[92mTest accuracy: 8799/10000 =  87.99 % ||| loss 0.33312517404556274\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.28164309009909627\n",
            "Batch #200 Loss: 0.2855679904669523\n",
            "Batch #300 Loss: 0.2783316044509411\n",
            "\u001b[92mTrain accuracy: 43055/48000 =  89.7 % ||| loss 0.27842581272125244\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10586/12000 =  88.22 % ||| loss 0.3201901316642761\u001b[0m\n",
            "\u001b[92mTest accuracy: 8737/10000 =  87.37 % ||| loss 0.3399704098701477\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.2772523784637451\n",
            "Batch #200 Loss: 0.28973729074001314\n",
            "Batch #300 Loss: 0.2811326617002487\n",
            "\u001b[92mTrain accuracy: 43395/48000 =  90.41 % ||| loss 0.26184239983558655\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10654/12000 =  88.78 % ||| loss 0.30811601877212524\u001b[0m\n",
            "\u001b[92mTest accuracy: 8801/10000 =  88.01 % ||| loss 0.3288238048553467\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.2734478560835123\n",
            "Batch #200 Loss: 0.283042728304863\n",
            "Batch #300 Loss: 0.2738041286170483\n",
            "\u001b[92mTrain accuracy: 43590/48000 =  90.81 % ||| loss 0.2575175166130066\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10687/12000 =  89.06 % ||| loss 0.30445200204849243\u001b[0m\n",
            "\u001b[92mTest accuracy: 8831/10000 =  88.31 % ||| loss 0.3264547884464264\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.27647880852222445\n",
            "Batch #200 Loss: 0.28012617737054823\n",
            "Batch #300 Loss: 0.27479845300316813\n",
            "\u001b[92mTrain accuracy: 43095/48000 =  89.78 % ||| loss 0.2714167833328247\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10594/12000 =  88.28 % ||| loss 0.32005947828292847\u001b[0m\n",
            "\u001b[92mTest accuracy: 8757/10000 =  87.57 % ||| loss 0.3475470542907715\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.2768188562989235\n",
            "Batch #200 Loss: 0.2626070973277092\n",
            "Batch #300 Loss: 0.2743492646515369\n",
            "\u001b[92mTrain accuracy: 43228/48000 =  90.06 % ||| loss 0.26833757758140564\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10596/12000 =  88.3 % ||| loss 0.3235241770744324\u001b[0m\n",
            "\u001b[92mTest accuracy: 8729/10000 =  87.29 % ||| loss 0.343756765127182\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.260491259098053\n",
            "Batch #200 Loss: 0.2684906654059887\n",
            "Batch #300 Loss: 0.2691595934331417\n",
            "\u001b[92mTrain accuracy: 43566/48000 =  90.76 % ||| loss 0.2507987320423126\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10652/12000 =  88.77 % ||| loss 0.3051781952381134\u001b[0m\n",
            "\u001b[92mTest accuracy: 8790/10000 =  87.9 % ||| loss 0.3310236930847168\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_14</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_14' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_14</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_203453-Lenet5Decay_1726099655.2927098_15</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_15' target=\"_blank\">Lenet5Decay_1726099655.2927098_15</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_15' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_15</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3007365965843203\n",
            "Batch #200 Loss: 2.300835783481598\n",
            "Batch #300 Loss: 2.3021324396133425\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3024861812591553\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3025243282318115\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025124073028564\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.302490746974945\n",
            "Batch #200 Loss: 2.3026555800437927\n",
            "Batch #300 Loss: 2.30272164106369\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3025896549224854\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3026468753814697\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302593469619751\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.302641546726227\n",
            "Batch #200 Loss: 2.302646915912628\n",
            "Batch #300 Loss: 2.302637894153595\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302582025527954\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3026492595672607\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026154041290283\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.302631886005402\n",
            "Batch #200 Loss: 2.3026817345619204\n",
            "Batch #300 Loss: 2.3026615619659423\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025858402252197\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3027665615081787\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025805950164795\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3026355004310606\n",
            "Batch #200 Loss: 2.3027446055412293\n",
            "Batch #300 Loss: 2.3025370407104493\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302577257156372\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026959896087646\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026199340820312\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.302624228000641\n",
            "Batch #200 Loss: 2.3025323009490966\n",
            "Batch #300 Loss: 2.3027496433258055\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025848865509033\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302628993988037\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026020526885986\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.3026536440849306\n",
            "Batch #200 Loss: 2.3026127886772154\n",
            "Batch #300 Loss: 2.3026075673103334\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025848865509033\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302673101425171\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026113510131836\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.302639951705933\n",
            "Batch #200 Loss: 2.3026185178756715\n",
            "Batch #300 Loss: 2.302712652683258\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.3025827407836914\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.302654504776001\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302583932876587\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.302619423866272\n",
            "Batch #200 Loss: 2.3026686906814575\n",
            "Batch #300 Loss: 2.3026386070251466\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025832176208496\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302652359008789\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025662899017334\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.3026258969306945\n",
            "Batch #200 Loss: 2.3026806712150574\n",
            "Batch #300 Loss: 2.3025878143310545\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.30258846282959\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026256561279297\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026106357574463\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.302549376487732\n",
            "Batch #200 Loss: 2.3027012586593627\n",
            "Batch #300 Loss: 2.302671892642975\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025898933410645\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3027234077453613\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302626132965088\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.302591347694397\n",
            "Batch #200 Loss: 2.3026452898979186\n",
            "Batch #300 Loss: 2.302696626186371\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.3025894165039062\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.3026390075683594\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026247024536133\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.3025835514068604\n",
            "Batch #200 Loss: 2.302544527053833\n",
            "Batch #300 Loss: 2.3027275919914247\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302593946456909\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.30265474319458\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025643825531006\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.302617332935333\n",
            "Batch #200 Loss: 2.3026444363594054\n",
            "Batch #300 Loss: 2.3027106523513794\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302584648132324\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.30267333984375\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026328086853027\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.3025730061531067\n",
            "Batch #200 Loss: 2.3026394128799437\n",
            "Batch #300 Loss: 2.3027214574813843\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302586078643799\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302654266357422\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025968074798584\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.302686824798584\n",
            "Batch #200 Loss: 2.3026767015457152\n",
            "Batch #300 Loss: 2.302646105289459\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302574872970581\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302696943283081\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026123046875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.3026397371292116\n",
            "Batch #200 Loss: 2.3026184487342833\n",
            "Batch #300 Loss: 2.302662360668182\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3025825023651123\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302658796310425\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302593469619751\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.3025096678733825\n",
            "Batch #200 Loss: 2.302709722518921\n",
            "Batch #300 Loss: 2.302651286125183\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3025786876678467\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3026680946350098\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025882244110107\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.3025998139381407\n",
            "Batch #200 Loss: 2.3027228045463564\n",
            "Batch #300 Loss: 2.302629783153534\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025782108306885\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026318550109863\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025834560394287\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.3026162910461427\n",
            "Batch #200 Loss: 2.3026437830924986\n",
            "Batch #300 Loss: 2.3026341366767884\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302581787109375\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026506900787354\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025903701782227\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.3026159739494325\n",
            "Batch #200 Loss: 2.302615237236023\n",
            "Batch #300 Loss: 2.302659440040588\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3025877475738525\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302676200866699\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302576780319214\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.302494206428528\n",
            "Batch #200 Loss: 2.3027365231513977\n",
            "Batch #300 Loss: 2.3026137590408324\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025786876678467\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026418685913086\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302595853805542\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.3025962495803833\n",
            "Batch #200 Loss: 2.3026752734184264\n",
            "Batch #300 Loss: 2.302676885128021\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025858402252197\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026087284088135\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026044368743896\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.3025996851921082\n",
            "Batch #200 Loss: 2.302704918384552\n",
            "Batch #300 Loss: 2.3026671481132506\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3025848865509033\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.302643299102783\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302600622177124\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3025774645805357\n",
            "Batch #200 Loss: 2.302671282291412\n",
            "Batch #300 Loss: 2.302651627063751\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025810718536377\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026771545410156\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302582025527954\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_15</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_15' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_15</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_203642-Lenet5Decay_1726099655.2927098_16</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_16' target=\"_blank\">Lenet5Decay_1726099655.2927098_16</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_16' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_16</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.299342839717865\n",
            "Batch #200 Loss: 2.2772849702835085\n",
            "Batch #300 Loss: 1.7477062714099885\n",
            "\u001b[92mTrain accuracy: 30553/48000 =  63.65 % ||| loss 0.9694582223892212\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7665/12000 =  63.88 % ||| loss 0.9612032175064087\u001b[0m\n",
            "\u001b[92mTest accuracy: 6334/10000 =  63.34 % ||| loss 0.9879734516143799\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.944523469209671\n",
            "Batch #200 Loss: 0.8762185609340668\n",
            "Batch #300 Loss: 0.8369526422023773\n",
            "\u001b[92mTrain accuracy: 34034/48000 =  70.9 % ||| loss 0.7885783910751343\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8566/12000 =  71.38 % ||| loss 0.7787668108940125\u001b[0m\n",
            "\u001b[92mTest accuracy: 7020/10000 =  70.2 % ||| loss 0.8099290132522583\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.7747401654720306\n",
            "Batch #200 Loss: 0.7449047768115997\n",
            "Batch #300 Loss: 0.7220705622434616\n",
            "\u001b[92mTrain accuracy: 34891/48000 =  72.69 % ||| loss 0.7115339636802673\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8721/12000 =  72.67 % ||| loss 0.7061929106712341\u001b[0m\n",
            "\u001b[92mTest accuracy: 7149/10000 =  71.49 % ||| loss 0.7346835136413574\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.6906632959842682\n",
            "Batch #200 Loss: 0.6900494739413261\n",
            "Batch #300 Loss: 0.6565825966000557\n",
            "\u001b[92mTrain accuracy: 35964/48000 =  74.92 % ||| loss 0.6410958766937256\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9014/12000 =  75.12 % ||| loss 0.6318506002426147\u001b[0m\n",
            "\u001b[92mTest accuracy: 7390/10000 =  73.9 % ||| loss 0.6620987057685852\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.6466252118349075\n",
            "Batch #200 Loss: 0.6217670080065727\n",
            "Batch #300 Loss: 0.6192523509263992\n",
            "\u001b[92mTrain accuracy: 36917/48000 =  76.91 % ||| loss 0.605866551399231\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9228/12000 =  76.9 % ||| loss 0.6028613448143005\u001b[0m\n",
            "\u001b[92mTest accuracy: 7568/10000 =  75.68 % ||| loss 0.6335288286209106\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.600750367641449\n",
            "Batch #200 Loss: 0.5991914677619934\n",
            "Batch #300 Loss: 0.5895496186614037\n",
            "\u001b[92mTrain accuracy: 37486/48000 =  78.1 % ||| loss 0.5722687840461731\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9405/12000 =  78.38 % ||| loss 0.5650438666343689\u001b[0m\n",
            "\u001b[92mTest accuracy: 7722/10000 =  77.22 % ||| loss 0.5940161943435669\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5807309478521347\n",
            "Batch #200 Loss: 0.5642571407556534\n",
            "Batch #300 Loss: 0.5671478593349457\n",
            "\u001b[92mTrain accuracy: 37703/48000 =  78.55 % ||| loss 0.5685715079307556\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9443/12000 =  78.69 % ||| loss 0.5688449740409851\u001b[0m\n",
            "\u001b[92mTest accuracy: 7753/10000 =  77.53 % ||| loss 0.5968116521835327\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.5523266443610191\n",
            "Batch #200 Loss: 0.5443940350413322\n",
            "Batch #300 Loss: 0.5536878362298012\n",
            "\u001b[92mTrain accuracy: 38258/48000 =  79.7 % ||| loss 0.5336684584617615\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9576/12000 =  79.8 % ||| loss 0.5321863293647766\u001b[0m\n",
            "\u001b[92mTest accuracy: 7900/10000 =  79.0 % ||| loss 0.5588990449905396\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.5404535189270974\n",
            "Batch #200 Loss: 0.5300946092605591\n",
            "Batch #300 Loss: 0.5285220763087273\n",
            "\u001b[92mTrain accuracy: 38717/48000 =  80.66 % ||| loss 0.5210678577423096\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9720/12000 =  81.0 % ||| loss 0.5172626972198486\u001b[0m\n",
            "\u001b[92mTest accuracy: 7962/10000 =  79.62 % ||| loss 0.5505959987640381\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.5167483556270599\n",
            "Batch #200 Loss: 0.5159481298923493\n",
            "Batch #300 Loss: 0.5149969658255578\n",
            "\u001b[92mTrain accuracy: 39295/48000 =  81.86 % ||| loss 0.5100967288017273\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9837/12000 =  81.97 % ||| loss 0.5126228928565979\u001b[0m\n",
            "\u001b[92mTest accuracy: 8069/10000 =  80.69 % ||| loss 0.5375643968582153\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.511140349805355\n",
            "Batch #200 Loss: 0.5060014581680298\n",
            "Batch #300 Loss: 0.5089810964465141\n",
            "\u001b[92mTrain accuracy: 39553/48000 =  82.4 % ||| loss 0.4854777753353119\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9890/12000 =  82.42 % ||| loss 0.4862693250179291\u001b[0m\n",
            "\u001b[92mTest accuracy: 8139/10000 =  81.39 % ||| loss 0.5128162503242493\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.4999129790067673\n",
            "Batch #200 Loss: 0.4960750749707222\n",
            "Batch #300 Loss: 0.49625695586204527\n",
            "\u001b[92mTrain accuracy: 39219/48000 =  81.71 % ||| loss 0.492847740650177\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9853/12000 =  82.11 % ||| loss 0.4919590950012207\u001b[0m\n",
            "\u001b[92mTest accuracy: 8079/10000 =  80.79 % ||| loss 0.5164923667907715\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.49564324378967284\n",
            "Batch #200 Loss: 0.4856926557421684\n",
            "Batch #300 Loss: 0.4813232246041298\n",
            "\u001b[92mTrain accuracy: 39847/48000 =  83.01 % ||| loss 0.4732454717159271\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9972/12000 =  83.1 % ||| loss 0.476235568523407\u001b[0m\n",
            "\u001b[92mTest accuracy: 8222/10000 =  82.22 % ||| loss 0.5061061978340149\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.47936561912298203\n",
            "Batch #200 Loss: 0.47835763841867446\n",
            "Batch #300 Loss: 0.48157406836748123\n",
            "\u001b[92mTrain accuracy: 39721/48000 =  82.75 % ||| loss 0.4691348969936371\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9902/12000 =  82.52 % ||| loss 0.4741225242614746\u001b[0m\n",
            "\u001b[92mTest accuracy: 8179/10000 =  81.79 % ||| loss 0.50111323595047\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.47710695773363115\n",
            "Batch #200 Loss: 0.4743460014462471\n",
            "Batch #300 Loss: 0.4721782985329628\n",
            "\u001b[92mTrain accuracy: 39681/48000 =  82.67 % ||| loss 0.48037421703338623\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9922/12000 =  82.68 % ||| loss 0.48146072030067444\u001b[0m\n",
            "\u001b[92mTest accuracy: 8183/10000 =  81.83 % ||| loss 0.5075143575668335\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.48322639495134356\n",
            "Batch #200 Loss: 0.4677648159861565\n",
            "Batch #300 Loss: 0.46616260826587674\n",
            "\u001b[92mTrain accuracy: 40241/48000 =  83.84 % ||| loss 0.4522230327129364\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10017/12000 =  83.47 % ||| loss 0.45826926827430725\u001b[0m\n",
            "\u001b[92mTest accuracy: 8309/10000 =  83.09 % ||| loss 0.48382318019866943\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.4594222566485405\n",
            "Batch #200 Loss: 0.46645879209041596\n",
            "Batch #300 Loss: 0.4621903043985367\n",
            "\u001b[92mTrain accuracy: 40344/48000 =  84.05 % ||| loss 0.4436146318912506\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10099/12000 =  84.16 % ||| loss 0.4469113349914551\u001b[0m\n",
            "\u001b[92mTest accuracy: 8339/10000 =  83.39 % ||| loss 0.4800197184085846\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.45432949632406233\n",
            "Batch #200 Loss: 0.45448887795209886\n",
            "Batch #300 Loss: 0.45634216696023944\n",
            "\u001b[92mTrain accuracy: 40452/48000 =  84.28 % ||| loss 0.44150713086128235\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10131/12000 =  84.42 % ||| loss 0.44364187121391296\u001b[0m\n",
            "\u001b[92mTest accuracy: 8372/10000 =  83.72 % ||| loss 0.4697491526603699\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.44580052852630614\n",
            "Batch #200 Loss: 0.46193589955568315\n",
            "Batch #300 Loss: 0.4490861791372299\n",
            "\u001b[92mTrain accuracy: 40070/48000 =  83.48 % ||| loss 0.45873257517814636\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10001/12000 =  83.34 % ||| loss 0.4616265296936035\u001b[0m\n",
            "\u001b[92mTest accuracy: 8274/10000 =  82.74 % ||| loss 0.4853847324848175\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.44921901732683184\n",
            "Batch #200 Loss: 0.4400726388394833\n",
            "Batch #300 Loss: 0.45618276596069335\n",
            "\u001b[92mTrain accuracy: 40395/48000 =  84.16 % ||| loss 0.44168925285339355\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10065/12000 =  83.88 % ||| loss 0.44897109270095825\u001b[0m\n",
            "\u001b[92mTest accuracy: 8318/10000 =  83.18 % ||| loss 0.46578249335289\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.4556789582967758\n",
            "Batch #200 Loss: 0.43835504472255704\n",
            "Batch #300 Loss: 0.439892606139183\n",
            "\u001b[92mTrain accuracy: 40498/48000 =  84.37 % ||| loss 0.430123895406723\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10096/12000 =  84.13 % ||| loss 0.4368610680103302\u001b[0m\n",
            "\u001b[92mTest accuracy: 8355/10000 =  83.55 % ||| loss 0.45918241143226624\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.44468448728322985\n",
            "Batch #200 Loss: 0.432535697221756\n",
            "Batch #300 Loss: 0.4383798488974571\n",
            "\u001b[92mTrain accuracy: 40300/48000 =  83.96 % ||| loss 0.43649834394454956\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10072/12000 =  83.93 % ||| loss 0.43998000025749207\u001b[0m\n",
            "\u001b[92mTest accuracy: 8264/10000 =  82.64 % ||| loss 0.463994562625885\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.43234698593616483\n",
            "Batch #200 Loss: 0.44457961022853854\n",
            "Batch #300 Loss: 0.4445522916316986\n",
            "\u001b[92mTrain accuracy: 40172/48000 =  83.69 % ||| loss 0.45039185881614685\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10029/12000 =  83.58 % ||| loss 0.45923447608947754\u001b[0m\n",
            "\u001b[92mTest accuracy: 8302/10000 =  83.02 % ||| loss 0.4802091717720032\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.42538752526044843\n",
            "Batch #200 Loss: 0.4459120944142342\n",
            "Batch #300 Loss: 0.42203323274850846\n",
            "\u001b[92mTrain accuracy: 40828/48000 =  85.06 % ||| loss 0.4202353060245514\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10197/12000 =  84.97 % ||| loss 0.42666101455688477\u001b[0m\n",
            "\u001b[92mTest accuracy: 8459/10000 =  84.59 % ||| loss 0.4502902328968048\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.4326573023200035\n",
            "Batch #200 Loss: 0.43281894743442534\n",
            "Batch #300 Loss: 0.4286814087629318\n",
            "\u001b[92mTrain accuracy: 40397/48000 =  84.16 % ||| loss 0.4287112355232239\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10088/12000 =  84.07 % ||| loss 0.43241560459136963\u001b[0m\n",
            "\u001b[92mTest accuracy: 8338/10000 =  83.38 % ||| loss 0.45933860540390015\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_16</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_16' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_16</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_203831-Lenet5Decay_1726099655.2927098_17</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_17' target=\"_blank\">Lenet5Decay_1726099655.2927098_17</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_17' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_17</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.297656455039978\n",
            "Batch #200 Loss: 2.2527981305122378\n",
            "Batch #300 Loss: 1.4090486890077591\n",
            "\u001b[92mTrain accuracy: 30688/48000 =  63.93 % ||| loss 0.930075466632843\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7712/12000 =  64.27 % ||| loss 0.921520471572876\u001b[0m\n",
            "\u001b[92mTest accuracy: 6307/10000 =  63.07 % ||| loss 0.9487720727920532\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.9019613313674927\n",
            "Batch #200 Loss: 0.8066965973377228\n",
            "Batch #300 Loss: 0.7667313933372497\n",
            "\u001b[92mTrain accuracy: 34887/48000 =  72.68 % ||| loss 0.7024754881858826\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8771/12000 =  73.09 % ||| loss 0.6918072700500488\u001b[0m\n",
            "\u001b[92mTest accuracy: 7209/10000 =  72.09 % ||| loss 0.7217388153076172\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.7095577001571656\n",
            "Batch #200 Loss: 0.6820918607711792\n",
            "Batch #300 Loss: 0.6651256555318832\n",
            "\u001b[92mTrain accuracy: 36042/48000 =  75.09 % ||| loss 0.6599144339561462\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9073/12000 =  75.61 % ||| loss 0.6515756845474243\u001b[0m\n",
            "\u001b[92mTest accuracy: 7435/10000 =  74.35 % ||| loss 0.6850587725639343\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.6243033790588379\n",
            "Batch #200 Loss: 0.6068536758422851\n",
            "Batch #300 Loss: 0.5984526136517525\n",
            "\u001b[92mTrain accuracy: 37765/48000 =  78.68 % ||| loss 0.5623703598976135\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9441/12000 =  78.67 % ||| loss 0.5629885196685791\u001b[0m\n",
            "\u001b[92mTest accuracy: 7772/10000 =  77.72 % ||| loss 0.5932519435882568\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5609855595231056\n",
            "Batch #200 Loss: 0.5610622835159301\n",
            "Batch #300 Loss: 0.5521344557404518\n",
            "\u001b[92mTrain accuracy: 39153/48000 =  81.57 % ||| loss 0.5128116607666016\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9781/12000 =  81.51 % ||| loss 0.5152362585067749\u001b[0m\n",
            "\u001b[92mTest accuracy: 8084/10000 =  80.84 % ||| loss 0.5453240275382996\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5276201429963112\n",
            "Batch #200 Loss: 0.5155257558822632\n",
            "Batch #300 Loss: 0.5101933711767197\n",
            "\u001b[92mTrain accuracy: 38970/48000 =  81.19 % ||| loss 0.5169726610183716\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9776/12000 =  81.47 % ||| loss 0.5227166414260864\u001b[0m\n",
            "\u001b[92mTest accuracy: 8048/10000 =  80.48 % ||| loss 0.541054904460907\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.48860537439584734\n",
            "Batch #200 Loss: 0.4936593544483185\n",
            "Batch #300 Loss: 0.4802755281329155\n",
            "\u001b[92mTrain accuracy: 39500/48000 =  82.29 % ||| loss 0.489977091550827\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9893/12000 =  82.44 % ||| loss 0.49215036630630493\u001b[0m\n",
            "\u001b[92mTest accuracy: 8105/10000 =  81.05 % ||| loss 0.5263482332229614\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.47976140171289444\n",
            "Batch #200 Loss: 0.46714992105960845\n",
            "Batch #300 Loss: 0.4707886058092117\n",
            "\u001b[92mTrain accuracy: 39756/48000 =  82.83 % ||| loss 0.4799337685108185\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9961/12000 =  83.01 % ||| loss 0.4844977557659149\u001b[0m\n",
            "\u001b[92mTest accuracy: 8166/10000 =  81.66 % ||| loss 0.5189749598503113\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.44871040910482407\n",
            "Batch #200 Loss: 0.4457231098413467\n",
            "Batch #300 Loss: 0.4460760620236397\n",
            "\u001b[92mTrain accuracy: 40282/48000 =  83.92 % ||| loss 0.440335214138031\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10027/12000 =  83.56 % ||| loss 0.4503214359283447\u001b[0m\n",
            "\u001b[92mTest accuracy: 8284/10000 =  82.84 % ||| loss 0.4735856354236603\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.4262622192502022\n",
            "Batch #200 Loss: 0.4403364038467407\n",
            "Batch #300 Loss: 0.4275115904211998\n",
            "\u001b[92mTrain accuracy: 40644/48000 =  84.67 % ||| loss 0.42109569907188416\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10127/12000 =  84.39 % ||| loss 0.43402764201164246\u001b[0m\n",
            "\u001b[92mTest accuracy: 8340/10000 =  83.4 % ||| loss 0.4613381326198578\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.41725579887628556\n",
            "Batch #200 Loss: 0.4283436706662178\n",
            "Batch #300 Loss: 0.4136944648623466\n",
            "\u001b[92mTrain accuracy: 41273/48000 =  85.99 % ||| loss 0.3920374810695648\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10265/12000 =  85.54 % ||| loss 0.4045228362083435\u001b[0m\n",
            "\u001b[92mTest accuracy: 8477/10000 =  84.77 % ||| loss 0.43029195070266724\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3985295586287975\n",
            "Batch #200 Loss: 0.43535305082798004\n",
            "Batch #300 Loss: 0.3950474342703819\n",
            "\u001b[92mTrain accuracy: 41043/48000 =  85.51 % ||| loss 0.4019962251186371\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10271/12000 =  85.59 % ||| loss 0.41729116439819336\u001b[0m\n",
            "\u001b[92mTest accuracy: 8468/10000 =  84.68 % ||| loss 0.4323262572288513\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.39568076938390734\n",
            "Batch #200 Loss: 0.3882798507809639\n",
            "Batch #300 Loss: 0.39690631061792375\n",
            "\u001b[92mTrain accuracy: 41362/48000 =  86.17 % ||| loss 0.3784889876842499\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10263/12000 =  85.52 % ||| loss 0.3978959321975708\u001b[0m\n",
            "\u001b[92mTest accuracy: 8477/10000 =  84.77 % ||| loss 0.42057713866233826\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3852307446300983\n",
            "Batch #200 Loss: 0.37818520560860636\n",
            "Batch #300 Loss: 0.3759040467441082\n",
            "\u001b[92mTrain accuracy: 41674/48000 =  86.82 % ||| loss 0.3666897416114807\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10342/12000 =  86.18 % ||| loss 0.38513410091400146\u001b[0m\n",
            "\u001b[92mTest accuracy: 8548/10000 =  85.48 % ||| loss 0.40689346194267273\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.36971253007650373\n",
            "Batch #200 Loss: 0.37100097835063933\n",
            "Batch #300 Loss: 0.37780262231826783\n",
            "\u001b[92mTrain accuracy: 41901/48000 =  87.29 % ||| loss 0.3524080812931061\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10422/12000 =  86.85 % ||| loss 0.36877599358558655\u001b[0m\n",
            "\u001b[92mTest accuracy: 8594/10000 =  85.94 % ||| loss 0.3909286856651306\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3675368586182594\n",
            "Batch #200 Loss: 0.35764697939157486\n",
            "Batch #300 Loss: 0.36972247168421746\n",
            "\u001b[92mTrain accuracy: 42168/48000 =  87.85 % ||| loss 0.3387133479118347\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10465/12000 =  87.21 % ||| loss 0.35918954014778137\u001b[0m\n",
            "\u001b[92mTest accuracy: 8658/10000 =  86.58 % ||| loss 0.37615859508514404\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3598194471001625\n",
            "Batch #200 Loss: 0.35914937615394593\n",
            "Batch #300 Loss: 0.34951546251773835\n",
            "\u001b[92mTrain accuracy: 42046/48000 =  87.6 % ||| loss 0.3432714343070984\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10430/12000 =  86.92 % ||| loss 0.3618415296077728\u001b[0m\n",
            "\u001b[92mTest accuracy: 8634/10000 =  86.34 % ||| loss 0.38566702604293823\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3558270311355591\n",
            "Batch #200 Loss: 0.347114120721817\n",
            "Batch #300 Loss: 0.344336004704237\n",
            "\u001b[92mTrain accuracy: 41949/48000 =  87.39 % ||| loss 0.3441949784755707\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10405/12000 =  86.71 % ||| loss 0.3652372658252716\u001b[0m\n",
            "\u001b[92mTest accuracy: 8591/10000 =  85.91 % ||| loss 0.3895237445831299\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.3441546258330345\n",
            "Batch #200 Loss: 0.338852426558733\n",
            "Batch #300 Loss: 0.3446785314381123\n",
            "\u001b[92mTrain accuracy: 42091/48000 =  87.69 % ||| loss 0.33715811371803284\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10426/12000 =  86.88 % ||| loss 0.3597889244556427\u001b[0m\n",
            "\u001b[92mTest accuracy: 8637/10000 =  86.37 % ||| loss 0.39025211334228516\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.34879093125462535\n",
            "Batch #200 Loss: 0.3425358256697655\n",
            "Batch #300 Loss: 0.32914857104420664\n",
            "\u001b[92mTrain accuracy: 42521/48000 =  88.59 % ||| loss 0.3186817467212677\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10528/12000 =  87.73 % ||| loss 0.3429184854030609\u001b[0m\n",
            "\u001b[92mTest accuracy: 8711/10000 =  87.11 % ||| loss 0.3624705970287323\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.33464972048997876\n",
            "Batch #200 Loss: 0.327098481208086\n",
            "Batch #300 Loss: 0.3318585017323494\n",
            "\u001b[92mTrain accuracy: 42435/48000 =  88.41 % ||| loss 0.3176038861274719\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10529/12000 =  87.74 % ||| loss 0.3445661962032318\u001b[0m\n",
            "\u001b[92mTest accuracy: 8687/10000 =  86.87 % ||| loss 0.3673498034477234\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.32795203998684885\n",
            "Batch #200 Loss: 0.3259908249974251\n",
            "Batch #300 Loss: 0.333435515165329\n",
            "\u001b[92mTrain accuracy: 42529/48000 =  88.6 % ||| loss 0.3140870928764343\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10554/12000 =  87.95 % ||| loss 0.33830907940864563\u001b[0m\n",
            "\u001b[92mTest accuracy: 8716/10000 =  87.16 % ||| loss 0.36088868975639343\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.32531434774398804\n",
            "Batch #200 Loss: 0.322324508279562\n",
            "Batch #300 Loss: 0.32475557789206505\n",
            "\u001b[92mTrain accuracy: 42618/48000 =  88.79 % ||| loss 0.30856359004974365\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10517/12000 =  87.64 % ||| loss 0.33563318848609924\u001b[0m\n",
            "\u001b[92mTest accuracy: 8687/10000 =  86.87 % ||| loss 0.35899171233177185\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.32045477598905564\n",
            "Batch #200 Loss: 0.3152037204802036\n",
            "Batch #300 Loss: 0.32181978315114973\n",
            "\u001b[92mTrain accuracy: 42442/48000 =  88.42 % ||| loss 0.31959712505340576\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10475/12000 =  87.29 % ||| loss 0.34984520077705383\u001b[0m\n",
            "\u001b[92mTest accuracy: 8673/10000 =  86.73 % ||| loss 0.3707544505596161\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3137903746962547\n",
            "Batch #200 Loss: 0.3058274754881859\n",
            "Batch #300 Loss: 0.3145425413548946\n",
            "\u001b[92mTrain accuracy: 42813/48000 =  89.19 % ||| loss 0.2981342673301697\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10589/12000 =  88.24 % ||| loss 0.3275546133518219\u001b[0m\n",
            "\u001b[92mTest accuracy: 8745/10000 =  87.45 % ||| loss 0.34913182258605957\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_17</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_17' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_17</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_204023-Lenet5Decay_1726099655.2927098_18</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_18' target=\"_blank\">Lenet5Decay_1726099655.2927098_18</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_18' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_18</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.30343754529953\n",
            "Batch #200 Loss: 2.3036925435066222\n",
            "Batch #300 Loss: 2.3028286600112917\n",
            "\u001b[92mTrain accuracy: 5498/48000 =  11.45 % ||| loss 2.3028602600097656\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1395/12000 =  11.62 % ||| loss 2.3031063079833984\u001b[0m\n",
            "\u001b[92mTest accuracy: 1194/10000 =  11.94 % ||| loss 2.3029446601867676\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3019616055488585\n",
            "Batch #200 Loss: 2.3030172324180604\n",
            "Batch #300 Loss: 2.302742247581482\n",
            "\u001b[92mTrain accuracy: 6048/48000 =  12.6 % ||| loss 2.3024051189422607\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1525/12000 =  12.71 % ||| loss 2.302626132965088\u001b[0m\n",
            "\u001b[92mTest accuracy: 1298/10000 =  12.98 % ||| loss 2.302302598953247\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3024021339416505\n",
            "Batch #200 Loss: 2.3022289657592774\n",
            "Batch #300 Loss: 2.3022041177749633\n",
            "\u001b[92mTrain accuracy: 6420/48000 =  13.38 % ||| loss 2.3021535873413086\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1621/12000 =  13.51 % ||| loss 2.3023643493652344\u001b[0m\n",
            "\u001b[92mTest accuracy: 1397/10000 =  13.97 % ||| loss 2.3022093772888184\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.302407066822052\n",
            "Batch #200 Loss: 2.3021312594413756\n",
            "Batch #300 Loss: 2.302371881008148\n",
            "\u001b[92mTrain accuracy: 6530/48000 =  13.6 % ||| loss 2.302034378051758\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1653/12000 =  13.78 % ||| loss 2.3022043704986572\u001b[0m\n",
            "\u001b[92mTest accuracy: 1415/10000 =  14.15 % ||| loss 2.302180290222168\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3024684500694277\n",
            "Batch #200 Loss: 2.3012778401374816\n",
            "Batch #300 Loss: 2.3016184163093567\n",
            "\u001b[92mTrain accuracy: 6420/48000 =  13.38 % ||| loss 2.3019890785217285\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1616/12000 =  13.47 % ||| loss 2.302158832550049\u001b[0m\n",
            "\u001b[92mTest accuracy: 1391/10000 =  13.91 % ||| loss 2.30185866355896\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3020681071281435\n",
            "Batch #200 Loss: 2.3023417520523073\n",
            "Batch #300 Loss: 2.3018572187423705\n",
            "\u001b[92mTrain accuracy: 6134/48000 =  12.78 % ||| loss 2.3019843101501465\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1545/12000 =  12.88 % ||| loss 2.3021421432495117\u001b[0m\n",
            "\u001b[92mTest accuracy: 1296/10000 =  12.96 % ||| loss 2.3021113872528076\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.3018754386901854\n",
            "Batch #200 Loss: 2.3021554231643675\n",
            "Batch #300 Loss: 2.301852309703827\n",
            "\u001b[92mTrain accuracy: 5693/48000 =  11.86 % ||| loss 2.302004814147949\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1422/12000 =  11.85 % ||| loss 2.3021695613861084\u001b[0m\n",
            "\u001b[92mTest accuracy: 1193/10000 =  11.93 % ||| loss 2.3019869327545166\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.301848704814911\n",
            "Batch #200 Loss: 2.301770796775818\n",
            "Batch #300 Loss: 2.3024333763122558\n",
            "\u001b[92mTrain accuracy: 5241/48000 =  10.92 % ||| loss 2.3020412921905518\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1294/12000 =  10.78 % ||| loss 2.3021955490112305\u001b[0m\n",
            "\u001b[92mTest accuracy: 1104/10000 =  11.04 % ||| loss 2.3020122051239014\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.3019342732429506\n",
            "Batch #200 Loss: 2.302255971431732\n",
            "Batch #300 Loss: 2.3020992279052734\n",
            "\u001b[92mTrain accuracy: 4945/48000 =  10.3 % ||| loss 2.302086591720581\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1226/12000 =  10.22 % ||| loss 2.3022258281707764\u001b[0m\n",
            "\u001b[92mTest accuracy: 1036/10000 =  10.36 % ||| loss 2.3021833896636963\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.3022005987167358\n",
            "Batch #200 Loss: 2.3021236157417295\n",
            "Batch #300 Loss: 2.30201495885849\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302137851715088\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1198/12000 =  9.983 % ||| loss 2.3022778034210205\u001b[0m\n",
            "\u001b[92mTest accuracy: 1005/10000 =  10.05 % ||| loss 2.3021745681762695\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.302136263847351\n",
            "Batch #200 Loss: 2.3023214745521545\n",
            "Batch #300 Loss: 2.302232813835144\n",
            "\u001b[92mTrain accuracy: 4810/48000 =  10.02 % ||| loss 2.302189350128174\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3023171424865723\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302234172821045\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.3020558285713197\n",
            "Batch #200 Loss: 2.302256817817688\n",
            "Batch #300 Loss: 2.3021599292755126\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302238941192627\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3023674488067627\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302157402038574\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.3023590517044066\n",
            "Batch #200 Loss: 2.302291147708893\n",
            "Batch #300 Loss: 2.301851077079773\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.30228590965271\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3024110794067383\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302318811416626\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.3022238945961\n",
            "Batch #200 Loss: 2.3026113057136537\n",
            "Batch #300 Loss: 2.3022308349609375\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3023288249969482\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3024511337280273\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302408456802368\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.302519931793213\n",
            "Batch #200 Loss: 2.302242817878723\n",
            "Batch #300 Loss: 2.3023147869110105\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302366256713867\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3024826049804688\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3024163246154785\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.302165896892548\n",
            "Batch #200 Loss: 2.3025116109848023\n",
            "Batch #300 Loss: 2.302421705722809\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3024020195007324\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302516222000122\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3023781776428223\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.302325599193573\n",
            "Batch #200 Loss: 2.3025426268577576\n",
            "Batch #300 Loss: 2.30243665933609\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3024306297302246\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302539348602295\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3024442195892334\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.30244323015213\n",
            "Batch #200 Loss: 2.302533073425293\n",
            "Batch #300 Loss: 2.3023493123054504\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3024561405181885\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302567481994629\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302558660507202\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.302422647476196\n",
            "Batch #200 Loss: 2.30252375125885\n",
            "Batch #300 Loss: 2.3024931144714356\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3024771213531494\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302581787109375\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302506685256958\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.302512547969818\n",
            "Batch #200 Loss: 2.302553837299347\n",
            "Batch #300 Loss: 2.3024896955490113\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3024961948394775\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302598237991333\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025286197662354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.30252968788147\n",
            "Batch #200 Loss: 2.3025505685806276\n",
            "Batch #300 Loss: 2.3025127768516542\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3025100231170654\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3026061058044434\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302502155303955\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.30249046087265\n",
            "Batch #200 Loss: 2.302483115196228\n",
            "Batch #300 Loss: 2.3025809264183046\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302522897720337\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3026158809661865\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025193214416504\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.30255841255188\n",
            "Batch #200 Loss: 2.302532298564911\n",
            "Batch #300 Loss: 2.3024567413330077\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302532196044922\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302631378173828\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302570104598999\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.302610282897949\n",
            "Batch #200 Loss: 2.3026067543029787\n",
            "Batch #300 Loss: 2.302438786029816\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3025424480438232\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.30263352394104\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026037216186523\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3025048923492433\n",
            "Batch #200 Loss: 2.3024224495887755\n",
            "Batch #300 Loss: 2.3026297974586485\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302548885345459\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302635908126831\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025922775268555\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_18</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_18' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_18</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_204214-Lenet5Decay_1726099655.2927098_19</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_19' target=\"_blank\">Lenet5Decay_1726099655.2927098_19</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_19' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_19</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3039837956428526\n",
            "Batch #200 Loss: 2.3036431741714476\n",
            "Batch #300 Loss: 2.3037900519371033\n",
            "\u001b[92mTrain accuracy: 4056/48000 =  8.45 % ||| loss 2.303394317626953\u001b[0m\n",
            "\u001b[92mValidation accuracy: 983/12000 =  8.192 % ||| loss 2.30389666557312\u001b[0m\n",
            "\u001b[92mTest accuracy: 852/10000 =  8.52 % ||| loss 2.303642988204956\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3033076882362367\n",
            "Batch #200 Loss: 2.3038375639915465\n",
            "Batch #300 Loss: 2.3026212239265442\n",
            "\u001b[92mTrain accuracy: 4360/48000 =  9.083 % ||| loss 2.302867889404297\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1049/12000 =  8.742 % ||| loss 2.3033626079559326\u001b[0m\n",
            "\u001b[92mTest accuracy: 918/10000 =  9.18 % ||| loss 2.302950382232666\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.302983500957489\n",
            "Batch #200 Loss: 2.3028168845176697\n",
            "Batch #300 Loss: 2.3022724962234498\n",
            "\u001b[92mTrain accuracy: 4776/48000 =  9.95 % ||| loss 2.3023619651794434\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1152/12000 =  9.6 % ||| loss 2.302830696105957\u001b[0m\n",
            "\u001b[92mTest accuracy: 971/10000 =  9.71 % ||| loss 2.3024940490722656\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3026712226867674\n",
            "Batch #200 Loss: 2.3019711136817933\n",
            "Batch #300 Loss: 2.302314519882202\n",
            "\u001b[92mTrain accuracy: 5309/48000 =  11.06 % ||| loss 2.301875591278076\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1272/12000 =  10.6 % ||| loss 2.3023288249969482\u001b[0m\n",
            "\u001b[92mTest accuracy: 1082/10000 =  10.82 % ||| loss 2.30173659324646\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3016951441764832\n",
            "Batch #200 Loss: 2.3015059566497804\n",
            "Batch #300 Loss: 2.3019720268249513\n",
            "\u001b[92mTrain accuracy: 5898/48000 =  12.29 % ||| loss 2.301408529281616\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1421/12000 =  11.84 % ||| loss 2.301854133605957\u001b[0m\n",
            "\u001b[92mTest accuracy: 1206/10000 =  12.06 % ||| loss 2.301396369934082\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3006621742248536\n",
            "Batch #200 Loss: 2.3013931941986083\n",
            "Batch #300 Loss: 2.301346974372864\n",
            "\u001b[92mTrain accuracy: 6533/48000 =  13.61 % ||| loss 2.300950765609741\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1574/12000 =  13.12 % ||| loss 2.3013904094696045\u001b[0m\n",
            "\u001b[92mTest accuracy: 1333/10000 =  13.33 % ||| loss 2.3010809421539307\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.301233687400818\n",
            "Batch #200 Loss: 2.3005069994926455\n",
            "Batch #300 Loss: 2.300706570148468\n",
            "\u001b[92mTrain accuracy: 7101/48000 =  14.79 % ||| loss 2.300496816635132\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1743/12000 =  14.52 % ||| loss 2.3009002208709717\u001b[0m\n",
            "\u001b[92mTest accuracy: 1472/10000 =  14.72 % ||| loss 2.300565004348755\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.300492067337036\n",
            "Batch #200 Loss: 2.300139284133911\n",
            "Batch #300 Loss: 2.300117528438568\n",
            "\u001b[92mTrain accuracy: 7505/48000 =  15.64 % ||| loss 2.300037384033203\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1849/12000 =  15.41 % ||| loss 2.3004355430603027\u001b[0m\n",
            "\u001b[92mTest accuracy: 1561/10000 =  15.61 % ||| loss 2.3002402782440186\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.3000289463996886\n",
            "Batch #200 Loss: 2.3000437688827513\n",
            "Batch #300 Loss: 2.2992184925079346\n",
            "\u001b[92mTrain accuracy: 7831/48000 =  16.31 % ||| loss 2.2995636463165283\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1925/12000 =  16.04 % ||| loss 2.299929141998291\u001b[0m\n",
            "\u001b[92mTest accuracy: 1641/10000 =  16.41 % ||| loss 2.299703359603882\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.2999383449554442\n",
            "Batch #200 Loss: 2.2995661163330077\n",
            "Batch #300 Loss: 2.2990078926086426\n",
            "\u001b[92mTrain accuracy: 8063/48000 =  16.8 % ||| loss 2.299067258834839\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1985/12000 =  16.54 % ||| loss 2.299433708190918\u001b[0m\n",
            "\u001b[92mTest accuracy: 1689/10000 =  16.89 % ||| loss 2.299191951751709\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.2993093848228456\n",
            "Batch #200 Loss: 2.2989454007148744\n",
            "Batch #300 Loss: 2.2982477951049805\n",
            "\u001b[92mTrain accuracy: 8292/48000 =  17.27 % ||| loss 2.298539876937866\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2050/12000 =  17.08 % ||| loss 2.2988929748535156\u001b[0m\n",
            "\u001b[92mTest accuracy: 1725/10000 =  17.25 % ||| loss 2.2985825538635254\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.2982204246520994\n",
            "Batch #200 Loss: 2.298330595493317\n",
            "Batch #300 Loss: 2.297970175743103\n",
            "\u001b[92mTrain accuracy: 8421/48000 =  17.54 % ||| loss 2.2979869842529297\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2082/12000 =  17.35 % ||| loss 2.2983226776123047\u001b[0m\n",
            "\u001b[92mTest accuracy: 1750/10000 =  17.5 % ||| loss 2.2979629039764404\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.2980739092826843\n",
            "Batch #200 Loss: 2.297553458213806\n",
            "Batch #300 Loss: 2.297469501495361\n",
            "\u001b[92mTrain accuracy: 8549/48000 =  17.81 % ||| loss 2.2974092960357666\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2111/12000 =  17.59 % ||| loss 2.2977375984191895\u001b[0m\n",
            "\u001b[92mTest accuracy: 1777/10000 =  17.77 % ||| loss 2.297576904296875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.297106704711914\n",
            "Batch #200 Loss: 2.2970746541023255\n",
            "Batch #300 Loss: 2.2974050354957583\n",
            "\u001b[92mTrain accuracy: 8651/48000 =  18.02 % ||| loss 2.296801805496216\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2137/12000 =  17.81 % ||| loss 2.297121524810791\u001b[0m\n",
            "\u001b[92mTest accuracy: 1792/10000 =  17.92 % ||| loss 2.2968642711639404\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.296840908527374\n",
            "Batch #200 Loss: 2.2965085315704346\n",
            "Batch #300 Loss: 2.2958593821525572\n",
            "\u001b[92mTrain accuracy: 8728/48000 =  18.18 % ||| loss 2.296147346496582\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2154/12000 =  17.95 % ||| loss 2.2964510917663574\u001b[0m\n",
            "\u001b[92mTest accuracy: 1809/10000 =  18.09 % ||| loss 2.2962276935577393\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.295932569503784\n",
            "Batch #200 Loss: 2.2959697127342222\n",
            "Batch #300 Loss: 2.2956645035743715\n",
            "\u001b[92mTrain accuracy: 8803/48000 =  18.34 % ||| loss 2.29543137550354\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2174/12000 =  18.12 % ||| loss 2.2957303524017334\u001b[0m\n",
            "\u001b[92mTest accuracy: 1821/10000 =  18.21 % ||| loss 2.295380115509033\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.295538647174835\n",
            "Batch #200 Loss: 2.2948902368545534\n",
            "Batch #300 Loss: 2.2948291087150574\n",
            "\u001b[92mTrain accuracy: 8862/48000 =  18.46 % ||| loss 2.2946364879608154\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2186/12000 =  18.22 % ||| loss 2.2949130535125732\u001b[0m\n",
            "\u001b[92mTest accuracy: 1830/10000 =  18.3 % ||| loss 2.2946889400482178\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.29447624206543\n",
            "Batch #200 Loss: 2.2941174840927125\n",
            "Batch #300 Loss: 2.29412305355072\n",
            "\u001b[92mTrain accuracy: 8900/48000 =  18.54 % ||| loss 2.2937281131744385\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2196/12000 =  18.3 % ||| loss 2.293998956680298\u001b[0m\n",
            "\u001b[92mTest accuracy: 1835/10000 =  18.35 % ||| loss 2.2937686443328857\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.293433129787445\n",
            "Batch #200 Loss: 2.2931223249435426\n",
            "Batch #300 Loss: 2.293297326564789\n",
            "\u001b[92mTrain accuracy: 8984/48000 =  18.72 % ||| loss 2.2926785945892334\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2216/12000 =  18.47 % ||| loss 2.292931318283081\u001b[0m\n",
            "\u001b[92mTest accuracy: 1858/10000 =  18.58 % ||| loss 2.2927262783050537\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.2927275490760803\n",
            "Batch #200 Loss: 2.292239046096802\n",
            "Batch #300 Loss: 2.29164733171463\n",
            "\u001b[92mTrain accuracy: 9520/48000 =  19.83 % ||| loss 2.291456460952759\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2348/12000 =  19.57 % ||| loss 2.2916877269744873\u001b[0m\n",
            "\u001b[92mTest accuracy: 1947/10000 =  19.47 % ||| loss 2.2913968563079834\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.291210644245148\n",
            "Batch #200 Loss: 2.290912230014801\n",
            "Batch #300 Loss: 2.290639805793762\n",
            "\u001b[92mTrain accuracy: 10442/48000 =  21.75 % ||| loss 2.2900097370147705\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2577/12000 =  21.48 % ||| loss 2.290222406387329\u001b[0m\n",
            "\u001b[92mTest accuracy: 2143/10000 =  21.43 % ||| loss 2.2899999618530273\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.2895004606246947\n",
            "Batch #200 Loss: 2.289203760623932\n",
            "Batch #300 Loss: 2.2891427874565125\n",
            "\u001b[92mTrain accuracy: 11324/48000 =  23.59 % ||| loss 2.2882769107818604\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2794/12000 =  23.28 % ||| loss 2.2884743213653564\u001b[0m\n",
            "\u001b[92mTest accuracy: 2326/10000 =  23.26 % ||| loss 2.2883286476135254\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.28803760766983\n",
            "Batch #200 Loss: 2.287615375518799\n",
            "Batch #300 Loss: 2.286935358047485\n",
            "\u001b[92mTrain accuracy: 11920/48000 =  24.83 % ||| loss 2.286170721054077\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2953/12000 =  24.61 % ||| loss 2.286355495452881\u001b[0m\n",
            "\u001b[92mTest accuracy: 2432/10000 =  24.32 % ||| loss 2.286302089691162\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.2860594630241393\n",
            "Batch #200 Loss: 2.2849688124656677\n",
            "Batch #300 Loss: 2.284569728374481\n",
            "\u001b[92mTrain accuracy: 12334/48000 =  25.7 % ||| loss 2.283616065979004\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3055/12000 =  25.46 % ||| loss 2.283778429031372\u001b[0m\n",
            "\u001b[92mTest accuracy: 2535/10000 =  25.35 % ||| loss 2.2836408615112305\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.283058009147644\n",
            "Batch #200 Loss: 2.2825572967529295\n",
            "Batch #300 Loss: 2.2816412353515627\n",
            "\u001b[92mTrain accuracy: 12897/48000 =  26.87 % ||| loss 2.2804746627807617\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3198/12000 =  26.65 % ||| loss 2.2806038856506348\u001b[0m\n",
            "\u001b[92mTest accuracy: 2647/10000 =  26.47 % ||| loss 2.2804768085479736\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_19</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_19' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_19</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_204404-Lenet5Decay_1726099655.2927098_20</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_20' target=\"_blank\">Lenet5Decay_1726099655.2927098_20</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_20' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_20</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3025998640060426\n",
            "Batch #200 Loss: 2.3028043866157533\n",
            "Batch #300 Loss: 2.302311534881592\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3016774654388428\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3022375106811523\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3017635345458984\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.301970853805542\n",
            "Batch #200 Loss: 2.3015373373031616\n",
            "Batch #300 Loss: 2.300195801258087\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.300427198410034\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3009464740753174\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3005833625793457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.2993561482429503\n",
            "Batch #200 Loss: 2.300087776184082\n",
            "Batch #300 Loss: 2.2999069190025327\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.299146890640259\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.299642562866211\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.299389123916626\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.2994391536712646\n",
            "Batch #200 Loss: 2.2983280873298644\n",
            "Batch #300 Loss: 2.298208940029144\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.2977209091186523\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.298143148422241\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.297966718673706\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.2974622511863707\n",
            "Batch #200 Loss: 2.2970216917991637\n",
            "Batch #300 Loss: 2.296649594306946\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.295935869216919\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.296326160430908\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2958908081054688\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.295891263484955\n",
            "Batch #200 Loss: 2.294985353946686\n",
            "Batch #300 Loss: 2.2944599890708925\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.293794631958008\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.29414439201355\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.294001579284668\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.2939079356193544\n",
            "Batch #200 Loss: 2.292278461456299\n",
            "Batch #300 Loss: 2.2919972348213196\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.29133677482605\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.2916293144226074\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.291639804840088\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.290762417316437\n",
            "Batch #200 Loss: 2.290551419258118\n",
            "Batch #300 Loss: 2.2892158436775207\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.288240909576416\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.2884914875030518\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.28838849067688\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.2877130031585695\n",
            "Batch #200 Loss: 2.286789779663086\n",
            "Batch #300 Loss: 2.286038374900818\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.284341335296631\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.284508228302002\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.284527540206909\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.2833746790885927\n",
            "Batch #200 Loss: 2.2826453471183776\n",
            "Batch #300 Loss: 2.2811496114730834\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.279156446456909\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1194/12000 =  9.95 % ||| loss 2.279261589050293\u001b[0m\n",
            "\u001b[92mTest accuracy: 1001/10000 =  10.01 % ||| loss 2.27931809425354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.278523211479187\n",
            "Batch #200 Loss: 2.276095499992371\n",
            "Batch #300 Loss: 2.2744524312019347\n",
            "\u001b[92mTrain accuracy: 5381/48000 =  11.21 % ||| loss 2.271638870239258\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1343/12000 =  11.19 % ||| loss 2.2716455459594727\u001b[0m\n",
            "\u001b[92mTest accuracy: 1135/10000 =  11.35 % ||| loss 2.271841287612915\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.270471432209015\n",
            "Batch #200 Loss: 2.2675667905807497\n",
            "Batch #300 Loss: 2.2651238560676576\n",
            "\u001b[92mTrain accuracy: 8748/48000 =  18.22 % ||| loss 2.260627031326294\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2174/12000 =  18.12 % ||| loss 2.260502815246582\u001b[0m\n",
            "\u001b[92mTest accuracy: 1826/10000 =  18.26 % ||| loss 2.2609329223632812\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.2583580374717713\n",
            "Batch #200 Loss: 2.2549369955062866\n",
            "Batch #300 Loss: 2.249854805469513\n",
            "\u001b[92mTrain accuracy: 13138/48000 =  27.37 % ||| loss 2.2437644004821777\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3272/12000 =  27.27 % ||| loss 2.243427276611328\u001b[0m\n",
            "\u001b[92mTest accuracy: 2764/10000 =  27.64 % ||| loss 2.2442097663879395\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.2409226799011233\n",
            "Batch #200 Loss: 2.235174651145935\n",
            "Batch #300 Loss: 2.22687881231308\n",
            "\u001b[92mTrain accuracy: 14507/48000 =  30.22 % ||| loss 2.2159080505371094\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3613/12000 =  30.11 % ||| loss 2.215334892272949\u001b[0m\n",
            "\u001b[92mTest accuracy: 3013/10000 =  30.13 % ||| loss 2.2166199684143066\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.211086723804474\n",
            "Batch #200 Loss: 2.2000532007217406\n",
            "Batch #300 Loss: 2.1864829087257385\n",
            "\u001b[92mTrain accuracy: 17762/48000 =  37.0 % ||| loss 2.16580867767334\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4408/12000 =  36.73 % ||| loss 2.164921998977661\u001b[0m\n",
            "\u001b[92mTest accuracy: 3702/10000 =  37.02 % ||| loss 2.167212963104248\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.1552794575691223\n",
            "Batch #200 Loss: 2.132628676891327\n",
            "Batch #300 Loss: 2.1065091061592103\n",
            "\u001b[92mTrain accuracy: 20175/48000 =  42.03 % ||| loss 2.0615553855895996\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4981/12000 =  41.51 % ||| loss 2.060276985168457\u001b[0m\n",
            "\u001b[92mTest accuracy: 4213/10000 =  42.13 % ||| loss 2.0626585483551025\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.0396523213386537\n",
            "Batch #200 Loss: 1.9922434854507447\n",
            "Batch #300 Loss: 1.9295611333847047\n",
            "\u001b[92mTrain accuracy: 26294/48000 =  54.78 % ||| loss 1.8386836051940918\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6562/12000 =  54.68 % ||| loss 1.8372564315795898\u001b[0m\n",
            "\u001b[92mTest accuracy: 5431/10000 =  54.31 % ||| loss 1.8411871194839478\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 1.8031387650966644\n",
            "Batch #200 Loss: 1.7068357145786286\n",
            "Batch #300 Loss: 1.6197654759883882\n",
            "\u001b[92mTrain accuracy: 27435/48000 =  57.16 % ||| loss 1.5119478702545166\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6865/12000 =  57.21 % ||| loss 1.5105884075164795\u001b[0m\n",
            "\u001b[92mTest accuracy: 5669/10000 =  56.69 % ||| loss 1.5157749652862549\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 1.473399510383606\n",
            "Batch #200 Loss: 1.4015182209014894\n",
            "Batch #300 Loss: 1.3284626662731172\n",
            "\u001b[92mTrain accuracy: 27635/48000 =  57.57 % ||| loss 1.2619633674621582\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6935/12000 =  57.79 % ||| loss 1.2590230703353882\u001b[0m\n",
            "\u001b[92mTest accuracy: 5700/10000 =  57.0 % ||| loss 1.2682063579559326\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 1.2283558535575867\n",
            "Batch #200 Loss: 1.1959260714054107\n",
            "Batch #300 Loss: 1.1727049434185028\n",
            "\u001b[92mTrain accuracy: 28565/48000 =  59.51 % ||| loss 1.1209052801132202\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7119/12000 =  59.33 % ||| loss 1.117040753364563\u001b[0m\n",
            "\u001b[92mTest accuracy: 5885/10000 =  58.85 % ||| loss 1.1308881044387817\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 1.109119012951851\n",
            "Batch #200 Loss: 1.0893935525417329\n",
            "Batch #300 Loss: 1.0614379841089248\n",
            "\u001b[92mTrain accuracy: 29319/48000 =  61.08 % ||| loss 1.0450246334075928\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7341/12000 =  61.18 % ||| loss 1.0397573709487915\u001b[0m\n",
            "\u001b[92mTest accuracy: 6034/10000 =  60.34 % ||| loss 1.0585044622421265\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 1.034752584695816\n",
            "Batch #200 Loss: 1.0169047331809997\n",
            "Batch #300 Loss: 1.0180205184221267\n",
            "\u001b[92mTrain accuracy: 29686/48000 =  61.85 % ||| loss 1.0002676248550415\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7438/12000 =  61.98 % ||| loss 0.9942970275878906\u001b[0m\n",
            "\u001b[92mTest accuracy: 6115/10000 =  61.15 % ||| loss 1.0149509906768799\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.996224769949913\n",
            "Batch #200 Loss: 0.9884359276294709\n",
            "Batch #300 Loss: 0.9763101035356522\n",
            "\u001b[92mTrain accuracy: 30442/48000 =  63.42 % ||| loss 0.9701176881790161\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7611/12000 =  63.42 % ||| loss 0.9626246690750122\u001b[0m\n",
            "\u001b[92mTest accuracy: 6246/10000 =  62.46 % ||| loss 0.9866840243339539\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.9735624623298645\n",
            "Batch #200 Loss: 0.9496694761514664\n",
            "Batch #300 Loss: 0.9426223695278168\n",
            "\u001b[92mTrain accuracy: 30961/48000 =  64.5 % ||| loss 0.9469131827354431\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7782/12000 =  64.85 % ||| loss 0.939253032207489\u001b[0m\n",
            "\u001b[92mTest accuracy: 6376/10000 =  63.76 % ||| loss 0.9618407487869263\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.948143162727356\n",
            "Batch #200 Loss: 0.9403300154209137\n",
            "Batch #300 Loss: 0.9342597180604935\n",
            "\u001b[92mTrain accuracy: 31242/48000 =  65.09 % ||| loss 0.9266431331634521\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7840/12000 =  65.33 % ||| loss 0.9188699722290039\u001b[0m\n",
            "\u001b[92mTest accuracy: 6429/10000 =  64.29 % ||| loss 0.9406073093414307\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_20</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_20' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_20</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_204555-Lenet5Decay_1726099655.2927098_21</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_21' target=\"_blank\">Lenet5Decay_1726099655.2927098_21</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_21' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_21</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3018429923057555\n",
            "Batch #200 Loss: 2.299313027858734\n",
            "Batch #300 Loss: 2.2997527551651\n",
            "\u001b[92mTrain accuracy: 9313/48000 =  19.4 % ||| loss 2.299668550491333\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2312/12000 =  19.27 % ||| loss 2.3000094890594482\u001b[0m\n",
            "\u001b[92mTest accuracy: 1896/10000 =  18.96 % ||| loss 2.299743413925171\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2999644446372987\n",
            "Batch #200 Loss: 2.300299730300903\n",
            "Batch #300 Loss: 2.3010628390312196\n",
            "\u001b[92mTrain accuracy: 4837/48000 =  10.08 % ||| loss 2.3015530109405518\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1183/12000 =  9.858 % ||| loss 2.3017072677612305\u001b[0m\n",
            "\u001b[92mTest accuracy: 1001/10000 =  10.01 % ||| loss 2.3015432357788086\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3017563915252683\n",
            "Batch #200 Loss: 2.302083475589752\n",
            "Batch #300 Loss: 2.302130765914917\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3023691177368164\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3024630546569824\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3023808002471924\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3024539089202882\n",
            "Batch #200 Loss: 2.3024582982063295\n",
            "Batch #300 Loss: 2.3025606203079225\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302546501159668\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026134967803955\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025598526000977\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3025879001617433\n",
            "Batch #200 Loss: 2.302615466117859\n",
            "Batch #300 Loss: 2.3025468611717224\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302572250366211\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026368618011475\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302605390548706\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3025616359710694\n",
            "Batch #200 Loss: 2.302554006576538\n",
            "Batch #300 Loss: 2.302678949832916\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302576780319214\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026325702667236\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026022911071777\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.3026106715202332\n",
            "Batch #200 Loss: 2.302550485134125\n",
            "Batch #300 Loss: 2.3026200461387636\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302577257156372\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026340007781982\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302600860595703\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.3026101994514465\n",
            "Batch #200 Loss: 2.3026167297363282\n",
            "Batch #300 Loss: 2.302543036937714\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025765419006348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.30263614654541\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302586555480957\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.3025716137886048\n",
            "Batch #200 Loss: 2.3026392006874086\n",
            "Batch #300 Loss: 2.3026300573349\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025741577148438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302645683288574\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302593469619751\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.302583818435669\n",
            "Batch #200 Loss: 2.3025907039642335\n",
            "Batch #300 Loss: 2.3026032400131227\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302638292312622\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025858402252197\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.3025730729103087\n",
            "Batch #200 Loss: 2.3025324368476867\n",
            "Batch #300 Loss: 2.3026350283622743\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025765419006348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026301860809326\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302589178085327\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.302604808807373\n",
            "Batch #200 Loss: 2.302588949203491\n",
            "Batch #300 Loss: 2.302617151737213\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025765419006348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302640438079834\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025920391082764\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.302574453353882\n",
            "Batch #200 Loss: 2.3025797057151793\n",
            "Batch #300 Loss: 2.3026481533050536\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025760650634766\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026416301727295\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302598237991333\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.302590024471283\n",
            "Batch #200 Loss: 2.3026309490203856\n",
            "Batch #300 Loss: 2.3025899863243104\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302642345428467\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302607297897339\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.302599983215332\n",
            "Batch #200 Loss: 2.302559959888458\n",
            "Batch #300 Loss: 2.302634856700897\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302640438079834\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025920391082764\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.3025713038444517\n",
            "Batch #200 Loss: 2.3026015305519105\n",
            "Batch #300 Loss: 2.302599766254425\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025758266448975\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302635431289673\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025951385498047\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.302599093914032\n",
            "Batch #200 Loss: 2.302587561607361\n",
            "Batch #300 Loss: 2.302620892524719\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025755882263184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026411533355713\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302586317062378\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.3025572299957275\n",
            "Batch #200 Loss: 2.3026347160339355\n",
            "Batch #300 Loss: 2.3025917434692382\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025755882263184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302642345428467\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025898933410645\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.3025730562210085\n",
            "Batch #200 Loss: 2.302605938911438\n",
            "Batch #300 Loss: 2.302618169784546\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025741577148438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026466369628906\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025991916656494\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.3025862193107605\n",
            "Batch #200 Loss: 2.3025826144218446\n",
            "Batch #300 Loss: 2.3026358485221863\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025755882263184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026459217071533\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302603006362915\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.3025966811180116\n",
            "Batch #200 Loss: 2.3026012873649595\n",
            "Batch #300 Loss: 2.3025996851921082\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025753498077393\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302647352218628\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302588939666748\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.302521014213562\n",
            "Batch #200 Loss: 2.3026677107810976\n",
            "Batch #300 Loss: 2.302602462768555\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302642822265625\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025755882263184\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.3026089215278627\n",
            "Batch #200 Loss: 2.3025945901870726\n",
            "Batch #300 Loss: 2.302593927383423\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025758266448975\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026411533355713\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025951385498047\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.30258939743042\n",
            "Batch #200 Loss: 2.3025962805747984\n",
            "Batch #300 Loss: 2.302604942321777\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026480674743652\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30258846282959\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3025368356704714\n",
            "Batch #200 Loss: 2.302622935771942\n",
            "Batch #300 Loss: 2.3026321125030518\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025755882263184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302638292312622\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302593469619751\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_21</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_21' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_21</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_204746-Lenet5Decay_1726099655.2927098_22</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_22' target=\"_blank\">Lenet5Decay_1726099655.2927098_22</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_22' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_22</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3017918825149537\n",
            "Batch #200 Loss: 2.298855338096619\n",
            "Batch #300 Loss: 2.2956708598136903\n",
            "\u001b[92mTrain accuracy: 4926/48000 =  10.26 % ||| loss 2.2898125648498535\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1246/12000 =  10.38 % ||| loss 2.28944993019104\u001b[0m\n",
            "\u001b[92mTest accuracy: 1038/10000 =  10.38 % ||| loss 2.289787769317627\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.286489930152893\n",
            "Batch #200 Loss: 2.273810029029846\n",
            "Batch #300 Loss: 2.248187556266785\n",
            "\u001b[92mTrain accuracy: 12409/48000 =  25.85 % ||| loss 2.1406168937683105\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3117/12000 =  25.97 % ||| loss 2.1398303508758545\u001b[0m\n",
            "\u001b[92mTest accuracy: 2595/10000 =  25.95 % ||| loss 2.140899181365967\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.9722369003295899\n",
            "Batch #200 Loss: 1.4072109687328338\n",
            "Batch #300 Loss: 1.0773801666498184\n",
            "\u001b[92mTrain accuracy: 29122/48000 =  60.67 % ||| loss 0.9985970854759216\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7323/12000 =  61.02 % ||| loss 0.9879027009010315\u001b[0m\n",
            "\u001b[92mTest accuracy: 6029/10000 =  60.29 % ||| loss 1.0092586278915405\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.967305491566658\n",
            "Batch #200 Loss: 0.9378069686889648\n",
            "Batch #300 Loss: 0.8914173316955566\n",
            "\u001b[92mTrain accuracy: 31864/48000 =  66.38 % ||| loss 0.8679794073104858\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7966/12000 =  66.38 % ||| loss 0.8579500317573547\u001b[0m\n",
            "\u001b[92mTest accuracy: 6567/10000 =  65.67 % ||| loss 0.884980320930481\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.8618213385343552\n",
            "Batch #200 Loss: 0.8529774248600006\n",
            "Batch #300 Loss: 0.8326042419672013\n",
            "\u001b[92mTrain accuracy: 33358/48000 =  69.5 % ||| loss 0.8085658550262451\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8439/12000 =  70.33 % ||| loss 0.7955464124679565\u001b[0m\n",
            "\u001b[92mTest accuracy: 6873/10000 =  68.73 % ||| loss 0.8234086632728577\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.7865957373380661\n",
            "Batch #200 Loss: 0.7835809594392776\n",
            "Batch #300 Loss: 0.7904324650764465\n",
            "\u001b[92mTrain accuracy: 32982/48000 =  68.71 % ||| loss 0.7847684025764465\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8270/12000 =  68.92 % ||| loss 0.774543046951294\u001b[0m\n",
            "\u001b[92mTest accuracy: 6753/10000 =  67.53 % ||| loss 0.8057056069374084\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.7715859317779541\n",
            "Batch #200 Loss: 0.7453360474109649\n",
            "Batch #300 Loss: 0.7440432864427566\n",
            "\u001b[92mTrain accuracy: 35024/48000 =  72.97 % ||| loss 0.7281945943832397\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8776/12000 =  73.13 % ||| loss 0.7181451916694641\u001b[0m\n",
            "\u001b[92mTest accuracy: 7244/10000 =  72.44 % ||| loss 0.7456015348434448\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.7359579855203628\n",
            "Batch #200 Loss: 0.7299784302711487\n",
            "Batch #300 Loss: 0.7128011989593506\n",
            "\u001b[92mTrain accuracy: 34140/48000 =  71.12 % ||| loss 0.7336501479148865\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8586/12000 =  71.55 % ||| loss 0.7252234816551208\u001b[0m\n",
            "\u001b[92mTest accuracy: 7036/10000 =  70.36 % ||| loss 0.7626754641532898\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.7098450118303299\n",
            "Batch #200 Loss: 0.6929820370674133\n",
            "Batch #300 Loss: 0.6835577815771103\n",
            "\u001b[92mTrain accuracy: 35956/48000 =  74.91 % ||| loss 0.674824595451355\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9074/12000 =  75.62 % ||| loss 0.6687718033790588\u001b[0m\n",
            "\u001b[92mTest accuracy: 7389/10000 =  73.89 % ||| loss 0.6978868246078491\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.676391272842884\n",
            "Batch #200 Loss: 0.6621009176969528\n",
            "Batch #300 Loss: 0.6723329916596412\n",
            "\u001b[92mTrain accuracy: 36322/48000 =  75.67 % ||| loss 0.6566230654716492\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9155/12000 =  76.29 % ||| loss 0.647578239440918\u001b[0m\n",
            "\u001b[92mTest accuracy: 7457/10000 =  74.57 % ||| loss 0.6801975965499878\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6554886895418167\n",
            "Batch #200 Loss: 0.6608384045958519\n",
            "Batch #300 Loss: 0.6461471727490425\n",
            "\u001b[92mTrain accuracy: 35992/48000 =  74.98 % ||| loss 0.6548650860786438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9027/12000 =  75.22 % ||| loss 0.65256267786026\u001b[0m\n",
            "\u001b[92mTest accuracy: 7411/10000 =  74.11 % ||| loss 0.6808061003684998\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.6424395912885665\n",
            "Batch #200 Loss: 0.6288004219532013\n",
            "Batch #300 Loss: 0.6505856001377106\n",
            "\u001b[92mTrain accuracy: 36279/48000 =  75.58 % ||| loss 0.6486572027206421\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9113/12000 =  75.94 % ||| loss 0.6407349705696106\u001b[0m\n",
            "\u001b[92mTest accuracy: 7488/10000 =  74.88 % ||| loss 0.6744280457496643\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.6403684410452842\n",
            "Batch #200 Loss: 0.6134663003683091\n",
            "Batch #300 Loss: 0.6191541358828545\n",
            "\u001b[92mTrain accuracy: 36843/48000 =  76.76 % ||| loss 0.6346320509910583\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9248/12000 =  77.07 % ||| loss 0.627020537853241\u001b[0m\n",
            "\u001b[92mTest accuracy: 7539/10000 =  75.39 % ||| loss 0.6660192012786865\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.626502323448658\n",
            "Batch #200 Loss: 0.6139762905240059\n",
            "Batch #300 Loss: 0.6264756590127945\n",
            "\u001b[92mTrain accuracy: 37091/48000 =  77.27 % ||| loss 0.6103853583335876\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9296/12000 =  77.47 % ||| loss 0.6040015816688538\u001b[0m\n",
            "\u001b[92mTest accuracy: 7663/10000 =  76.63 % ||| loss 0.6351328492164612\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.6021262270212173\n",
            "Batch #200 Loss: 0.6133809462189674\n",
            "Batch #300 Loss: 0.6071253323554993\n",
            "\u001b[92mTrain accuracy: 37124/48000 =  77.34 % ||| loss 0.5997717976570129\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9262/12000 =  77.18 % ||| loss 0.5965822339057922\u001b[0m\n",
            "\u001b[92mTest accuracy: 7683/10000 =  76.83 % ||| loss 0.6224846243858337\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.611522556245327\n",
            "Batch #200 Loss: 0.5938103580474854\n",
            "Batch #300 Loss: 0.5839449849724769\n",
            "\u001b[92mTrain accuracy: 36640/48000 =  76.33 % ||| loss 0.6075299978256226\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9161/12000 =  76.34 % ||| loss 0.6073889136314392\u001b[0m\n",
            "\u001b[92mTest accuracy: 7562/10000 =  75.62 % ||| loss 0.6318398118019104\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.59379145860672\n",
            "Batch #200 Loss: 0.597029354274273\n",
            "Batch #300 Loss: 0.5862573179602623\n",
            "\u001b[92mTrain accuracy: 37870/48000 =  78.9 % ||| loss 0.5780782699584961\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9489/12000 =  79.07 % ||| loss 0.5740692615509033\u001b[0m\n",
            "\u001b[92mTest accuracy: 7803/10000 =  78.03 % ||| loss 0.6091711521148682\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5825363352894783\n",
            "Batch #200 Loss: 0.590217447578907\n",
            "Batch #300 Loss: 0.5872495684027672\n",
            "\u001b[92mTrain accuracy: 37754/48000 =  78.65 % ||| loss 0.5820342302322388\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9434/12000 =  78.62 % ||| loss 0.5809953212738037\u001b[0m\n",
            "\u001b[92mTest accuracy: 7770/10000 =  77.7 % ||| loss 0.6106520891189575\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5779683968424797\n",
            "Batch #200 Loss: 0.5679767552018166\n",
            "Batch #300 Loss: 0.5760810551047325\n",
            "\u001b[92mTrain accuracy: 38348/48000 =  79.89 % ||| loss 0.5613225698471069\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9601/12000 =  80.01 % ||| loss 0.5594305992126465\u001b[0m\n",
            "\u001b[92mTest accuracy: 7922/10000 =  79.22 % ||| loss 0.5864635109901428\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.5689383488893509\n",
            "Batch #200 Loss: 0.5596416938304901\n",
            "Batch #300 Loss: 0.5672877556085587\n",
            "\u001b[92mTrain accuracy: 38034/48000 =  79.24 % ||| loss 0.5628442764282227\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9495/12000 =  79.12 % ||| loss 0.5602027177810669\u001b[0m\n",
            "\u001b[92mTest accuracy: 7814/10000 =  78.14 % ||| loss 0.5948498249053955\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.5649152639508247\n",
            "Batch #200 Loss: 0.5745985478162765\n",
            "Batch #300 Loss: 0.5524207651615143\n",
            "\u001b[92mTrain accuracy: 38427/48000 =  80.06 % ||| loss 0.5507500767707825\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9638/12000 =  80.32 % ||| loss 0.5484229326248169\u001b[0m\n",
            "\u001b[92mTest accuracy: 7885/10000 =  78.85 % ||| loss 0.5837177634239197\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.5480275475978851\n",
            "Batch #200 Loss: 0.5519583845138549\n",
            "Batch #300 Loss: 0.5669642266631126\n",
            "\u001b[92mTrain accuracy: 38702/48000 =  80.63 % ||| loss 0.5400633811950684\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9706/12000 =  80.88 % ||| loss 0.5373823046684265\u001b[0m\n",
            "\u001b[92mTest accuracy: 7981/10000 =  79.81 % ||| loss 0.5720429420471191\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5421117290854454\n",
            "Batch #200 Loss: 0.5535265880823136\n",
            "Batch #300 Loss: 0.5604798573255539\n",
            "\u001b[92mTrain accuracy: 38389/48000 =  79.98 % ||| loss 0.5386753678321838\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9609/12000 =  80.08 % ||| loss 0.5346167683601379\u001b[0m\n",
            "\u001b[92mTest accuracy: 7916/10000 =  79.16 % ||| loss 0.5718214511871338\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5370755258202553\n",
            "Batch #200 Loss: 0.5391782224178314\n",
            "Batch #300 Loss: 0.5403436315059662\n",
            "\u001b[92mTrain accuracy: 38229/48000 =  79.64 % ||| loss 0.5536013245582581\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9608/12000 =  80.07 % ||| loss 0.5497603416442871\u001b[0m\n",
            "\u001b[92mTest accuracy: 7857/10000 =  78.57 % ||| loss 0.5860247015953064\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5356314387917519\n",
            "Batch #200 Loss: 0.5466587269306182\n",
            "Batch #300 Loss: 0.5350105687975883\n",
            "\u001b[92mTrain accuracy: 38891/48000 =  81.02 % ||| loss 0.5287594199180603\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9790/12000 =  81.58 % ||| loss 0.5262543559074402\u001b[0m\n",
            "\u001b[92mTest accuracy: 8015/10000 =  80.15 % ||| loss 0.5607113838195801\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_22</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_22' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_22</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_204937-Lenet5Decay_1726099655.2927098_23</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_23' target=\"_blank\">Lenet5Decay_1726099655.2927098_23</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_23' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_23</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3053676176071165\n",
            "Batch #200 Loss: 2.3037994003295896\n",
            "Batch #300 Loss: 2.302124218940735\n",
            "\u001b[92mTrain accuracy: 8572/48000 =  17.86 % ||| loss 2.301220655441284\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2112/12000 =  17.6 % ||| loss 2.3016507625579834\u001b[0m\n",
            "\u001b[92mTest accuracy: 1782/10000 =  17.82 % ||| loss 2.3012523651123047\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3005732536315917\n",
            "Batch #200 Loss: 2.2992530059814453\n",
            "Batch #300 Loss: 2.2982293367385864\n",
            "\u001b[92mTrain accuracy: 10589/48000 =  22.06 % ||| loss 2.295741319656372\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2603/12000 =  21.69 % ||| loss 2.2960314750671387\u001b[0m\n",
            "\u001b[92mTest accuracy: 2180/10000 =  21.8 % ||| loss 2.295764923095703\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.2947284889221193\n",
            "Batch #200 Loss: 2.291555163860321\n",
            "Batch #300 Loss: 2.286749300956726\n",
            "\u001b[92mTrain accuracy: 20991/48000 =  43.73 % ||| loss 2.2764687538146973\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5278/12000 =  43.98 % ||| loss 2.276479959487915\u001b[0m\n",
            "\u001b[92mTest accuracy: 4356/10000 =  43.56 % ||| loss 2.2766849994659424\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.2689571714401247\n",
            "Batch #200 Loss: 2.237782518863678\n",
            "Batch #300 Loss: 2.1176483392715455\n",
            "\u001b[92mTrain accuracy: 27202/48000 =  56.67 % ||| loss 1.413084626197815\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6840/12000 =  57.0 % ||| loss 1.409181833267212\u001b[0m\n",
            "\u001b[92mTest accuracy: 5652/10000 =  56.52 % ||| loss 1.4158060550689697\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 1.1863411915302278\n",
            "Batch #200 Loss: 0.9949489653110504\n",
            "Batch #300 Loss: 0.9247866785526275\n",
            "\u001b[92mTrain accuracy: 32031/48000 =  66.73 % ||| loss 0.8714430332183838\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8066/12000 =  67.22 % ||| loss 0.8580501675605774\u001b[0m\n",
            "\u001b[92mTest accuracy: 6613/10000 =  66.13 % ||| loss 0.8850610256195068\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.8563242948055267\n",
            "Batch #200 Loss: 0.8319587993621826\n",
            "Batch #300 Loss: 0.8017887669801712\n",
            "\u001b[92mTrain accuracy: 33476/48000 =  69.74 % ||| loss 0.7961022853851318\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8354/12000 =  69.62 % ||| loss 0.7904931902885437\u001b[0m\n",
            "\u001b[92mTest accuracy: 6877/10000 =  68.77 % ||| loss 0.8137284517288208\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.7619925999641418\n",
            "Batch #200 Loss: 0.7310667610168458\n",
            "Batch #300 Loss: 0.7273350673913955\n",
            "\u001b[92mTrain accuracy: 35353/48000 =  73.65 % ||| loss 0.6942781805992126\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8836/12000 =  73.63 % ||| loss 0.6876943111419678\u001b[0m\n",
            "\u001b[92mTest accuracy: 7245/10000 =  72.45 % ||| loss 0.7146168351173401\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.7027677005529404\n",
            "Batch #200 Loss: 0.6771720400452614\n",
            "Batch #300 Loss: 0.6887424820661545\n",
            "\u001b[92mTrain accuracy: 36019/48000 =  75.04 % ||| loss 0.6586605310440063\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9039/12000 =  75.33 % ||| loss 0.650401771068573\u001b[0m\n",
            "\u001b[92mTest accuracy: 7454/10000 =  74.54 % ||| loss 0.6833256483078003\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.6772013962268829\n",
            "Batch #200 Loss: 0.6700512003898621\n",
            "Batch #300 Loss: 0.6511061882972717\n",
            "\u001b[92mTrain accuracy: 36561/48000 =  76.17 % ||| loss 0.6338809132575989\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9157/12000 =  76.31 % ||| loss 0.6279695630073547\u001b[0m\n",
            "\u001b[92mTest accuracy: 7530/10000 =  75.3 % ||| loss 0.6529199481010437\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.6341440552473068\n",
            "Batch #200 Loss: 0.6275805622339249\n",
            "Batch #300 Loss: 0.6364827048778534\n",
            "\u001b[92mTrain accuracy: 37140/48000 =  77.38 % ||| loss 0.6144165396690369\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9342/12000 =  77.85 % ||| loss 0.6069942116737366\u001b[0m\n",
            "\u001b[92mTest accuracy: 7624/10000 =  76.24 % ||| loss 0.6432651877403259\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6256339260935784\n",
            "Batch #200 Loss: 0.6163085359334946\n",
            "Batch #300 Loss: 0.6164036187529564\n",
            "\u001b[92mTrain accuracy: 37373/48000 =  77.86 % ||| loss 0.5955333113670349\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9411/12000 =  78.42 % ||| loss 0.5865536332130432\u001b[0m\n",
            "\u001b[92mTest accuracy: 7690/10000 =  76.9 % ||| loss 0.6225366592407227\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.6017774060368538\n",
            "Batch #200 Loss: 0.5889534512162209\n",
            "Batch #300 Loss: 0.585295350253582\n",
            "\u001b[92mTrain accuracy: 37537/48000 =  78.2 % ||| loss 0.5932422876358032\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9424/12000 =  78.53 % ||| loss 0.5840669870376587\u001b[0m\n",
            "\u001b[92mTest accuracy: 7705/10000 =  77.05 % ||| loss 0.6237239241600037\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5818209144473075\n",
            "Batch #200 Loss: 0.5704953598976136\n",
            "Batch #300 Loss: 0.5782549640536309\n",
            "\u001b[92mTrain accuracy: 37245/48000 =  77.59 % ||| loss 0.587722897529602\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9354/12000 =  77.95 % ||| loss 0.5811747908592224\u001b[0m\n",
            "\u001b[92mTest accuracy: 7675/10000 =  76.75 % ||| loss 0.6137367486953735\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.5763910216093063\n",
            "Batch #200 Loss: 0.5634370189905167\n",
            "Batch #300 Loss: 0.55140650421381\n",
            "\u001b[92mTrain accuracy: 38390/48000 =  79.98 % ||| loss 0.5456086993217468\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9631/12000 =  80.26 % ||| loss 0.5395511984825134\u001b[0m\n",
            "\u001b[92mTest accuracy: 7864/10000 =  78.64 % ||| loss 0.5755074620246887\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5522814473509788\n",
            "Batch #200 Loss: 0.5543323272466659\n",
            "Batch #300 Loss: 0.5346979948878289\n",
            "\u001b[92mTrain accuracy: 37936/48000 =  79.03 % ||| loss 0.5594885349273682\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9479/12000 =  78.99 % ||| loss 0.5628849267959595\u001b[0m\n",
            "\u001b[92mTest accuracy: 7844/10000 =  78.44 % ||| loss 0.5891526341438293\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.5405054357647896\n",
            "Batch #200 Loss: 0.534898253083229\n",
            "Batch #300 Loss: 0.5144601356983185\n",
            "\u001b[92mTrain accuracy: 38713/48000 =  80.65 % ||| loss 0.5252701044082642\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9686/12000 =  80.72 % ||| loss 0.5260946750640869\u001b[0m\n",
            "\u001b[92mTest accuracy: 7958/10000 =  79.58 % ||| loss 0.5538097620010376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5379638734459877\n",
            "Batch #200 Loss: 0.5186050873994827\n",
            "Batch #300 Loss: 0.5141673961281776\n",
            "\u001b[92mTrain accuracy: 39161/48000 =  81.59 % ||| loss 0.5029142498970032\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9809/12000 =  81.74 % ||| loss 0.5038473606109619\u001b[0m\n",
            "\u001b[92mTest accuracy: 8084/10000 =  80.84 % ||| loss 0.5360693335533142\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5067001962661744\n",
            "Batch #200 Loss: 0.492528680562973\n",
            "Batch #300 Loss: 0.5173427504301071\n",
            "\u001b[92mTrain accuracy: 39282/48000 =  81.84 % ||| loss 0.4935814142227173\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9847/12000 =  82.06 % ||| loss 0.4917896091938019\u001b[0m\n",
            "\u001b[92mTest accuracy: 8086/10000 =  80.86 % ||| loss 0.5257604718208313\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5114308735728263\n",
            "Batch #200 Loss: 0.4897386085987091\n",
            "Batch #300 Loss: 0.4838858214020729\n",
            "\u001b[92mTrain accuracy: 39808/48000 =  82.93 % ||| loss 0.4815198481082916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9965/12000 =  83.04 % ||| loss 0.4804820120334625\u001b[0m\n",
            "\u001b[92mTest accuracy: 8195/10000 =  81.95 % ||| loss 0.5158112645149231\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.4897484046220779\n",
            "Batch #200 Loss: 0.4881913858652115\n",
            "Batch #300 Loss: 0.4800953194499016\n",
            "\u001b[92mTrain accuracy: 39910/48000 =  83.15 % ||| loss 0.4666546583175659\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9985/12000 =  83.21 % ||| loss 0.4690597355365753\u001b[0m\n",
            "\u001b[92mTest accuracy: 8227/10000 =  82.27 % ||| loss 0.4973157048225403\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.47428963631391524\n",
            "Batch #200 Loss: 0.47860380113124845\n",
            "Batch #300 Loss: 0.47495698601007463\n",
            "\u001b[92mTrain accuracy: 40165/48000 =  83.68 % ||| loss 0.4613158106803894\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10037/12000 =  83.64 % ||| loss 0.4650176167488098\u001b[0m\n",
            "\u001b[92mTest accuracy: 8278/10000 =  82.78 % ||| loss 0.49862638115882874\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.47574410498142244\n",
            "Batch #200 Loss: 0.4621173366904259\n",
            "Batch #300 Loss: 0.468796429336071\n",
            "\u001b[92mTrain accuracy: 39942/48000 =  83.21 % ||| loss 0.4612703323364258\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9980/12000 =  83.17 % ||| loss 0.464338093996048\u001b[0m\n",
            "\u001b[92mTest accuracy: 8219/10000 =  82.19 % ||| loss 0.49902811646461487\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.4602830022573471\n",
            "Batch #200 Loss: 0.45156151831150054\n",
            "Batch #300 Loss: 0.4569083890318871\n",
            "\u001b[92mTrain accuracy: 40009/48000 =  83.35 % ||| loss 0.46097713708877563\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10001/12000 =  83.34 % ||| loss 0.4672866463661194\u001b[0m\n",
            "\u001b[92mTest accuracy: 8264/10000 =  82.64 % ||| loss 0.4895938038825989\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.44534380227327347\n",
            "Batch #200 Loss: 0.45379624515771866\n",
            "Batch #300 Loss: 0.45630489110946654\n",
            "\u001b[92mTrain accuracy: 40194/48000 =  83.74 % ||| loss 0.4443810284137726\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10016/12000 =  83.47 % ||| loss 0.44656193256378174\u001b[0m\n",
            "\u001b[92mTest accuracy: 8269/10000 =  82.69 % ||| loss 0.47932106256484985\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.447628156542778\n",
            "Batch #200 Loss: 0.43638173311948775\n",
            "Batch #300 Loss: 0.4479844120144844\n",
            "\u001b[92mTrain accuracy: 40592/48000 =  84.57 % ||| loss 0.4314800500869751\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10117/12000 =  84.31 % ||| loss 0.43586495518684387\u001b[0m\n",
            "\u001b[92mTest accuracy: 8363/10000 =  83.63 % ||| loss 0.468334436416626\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_23</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_23' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_23</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_205128-Lenet5Decay_1726099655.2927098_24</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_24' target=\"_blank\">Lenet5Decay_1726099655.2927098_24</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_24' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_24</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3065171957015993\n",
            "Batch #200 Loss: 2.3053143501281737\n",
            "Batch #300 Loss: 2.3044338917732237\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.304051160812378\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3038036823272705\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3043630123138428\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.304113552570343\n",
            "Batch #200 Loss: 2.3036108374595643\n",
            "Batch #300 Loss: 2.303409523963928\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3031818866729736\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.303011417388916\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3030691146850586\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3033233213424684\n",
            "Batch #200 Loss: 2.302769522666931\n",
            "Batch #300 Loss: 2.303096446990967\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3028533458709717\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3027291297912598\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027968406677246\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3025094079971313\n",
            "Batch #200 Loss: 2.3029424262046816\n",
            "Batch #300 Loss: 2.3027581453323362\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.302718162536621\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.302658796310425\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026134967803955\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3028636288642885\n",
            "Batch #200 Loss: 2.302636902332306\n",
            "Batch #300 Loss: 2.3027118587493898\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3026602268218994\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3026480674743652\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30264949798584\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3025196981430054\n",
            "Batch #200 Loss: 2.302614362239838\n",
            "Batch #300 Loss: 2.302692885398865\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3026304244995117\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302644729614258\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302600383758545\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.302619769573212\n",
            "Batch #200 Loss: 2.3026070308685305\n",
            "Batch #300 Loss: 2.302766590118408\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302612543106079\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3026363849639893\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025989532470703\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.30273197889328\n",
            "Batch #200 Loss: 2.302627234458923\n",
            "Batch #300 Loss: 2.302694923877716\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3026018142700195\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.30263614654541\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025920391082764\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.302548758983612\n",
            "Batch #200 Loss: 2.3025736570358277\n",
            "Batch #300 Loss: 2.3026010370254517\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3025925159454346\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302633762359619\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302607774734497\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.3026444053649904\n",
            "Batch #200 Loss: 2.3025675082206725\n",
            "Batch #300 Loss: 2.302571794986725\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302586317062378\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3026416301727295\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025925159454346\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.3025704216957092\n",
            "Batch #200 Loss: 2.302636110782623\n",
            "Batch #300 Loss: 2.3025601744651794\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3025832176208496\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3026394844055176\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025853633880615\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.3025928258895876\n",
            "Batch #200 Loss: 2.302555503845215\n",
            "Batch #300 Loss: 2.302607135772705\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302579402923584\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3026387691497803\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302607774734497\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.302608623504639\n",
            "Batch #200 Loss: 2.3025524973869325\n",
            "Batch #300 Loss: 2.3025804591178893\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302578926086426\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3026387691497803\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302593469619751\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.3026205945014953\n",
            "Batch #200 Loss: 2.302591633796692\n",
            "Batch #300 Loss: 2.3025731158256533\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025758266448975\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026413917541504\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302595615386963\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.3025367498397826\n",
            "Batch #200 Loss: 2.3025873279571534\n",
            "Batch #300 Loss: 2.302619926929474\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025763034820557\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026437759399414\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302583694458008\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.302560441493988\n",
            "Batch #200 Loss: 2.3025936222076417\n",
            "Batch #300 Loss: 2.302576582431793\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025753498077393\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302643060684204\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302586317062378\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.302546145915985\n",
            "Batch #200 Loss: 2.302625985145569\n",
            "Batch #300 Loss: 2.3026123905181883\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025765419006348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026390075683594\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025834560394287\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.302518401145935\n",
            "Batch #200 Loss: 2.3025493836402893\n",
            "Batch #300 Loss: 2.3026683592796324\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026387691497803\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025941848754883\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.3025673770904542\n",
            "Batch #200 Loss: 2.302635488510132\n",
            "Batch #300 Loss: 2.3025625777244567\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025763034820557\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302639961242676\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302593946456909\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.302584917545319\n",
            "Batch #200 Loss: 2.302580988407135\n",
            "Batch #300 Loss: 2.302580027580261\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026416301727295\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025827407836914\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.302552719116211\n",
            "Batch #200 Loss: 2.3025988960266113\n",
            "Batch #300 Loss: 2.302607319355011\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302574396133423\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302640438079834\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025996685028076\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.3026014447212217\n",
            "Batch #200 Loss: 2.302577772140503\n",
            "Batch #300 Loss: 2.302604818344116\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025765419006348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302640438079834\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302581310272217\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.302578983306885\n",
            "Batch #200 Loss: 2.302571909427643\n",
            "Batch #300 Loss: 2.3026229190826415\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026416301727295\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302586078643799\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.302585575580597\n",
            "Batch #200 Loss: 2.302594084739685\n",
            "Batch #300 Loss: 2.30259015083313\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025765419006348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026435375213623\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302586078643799\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3025988793373107\n",
            "Batch #200 Loss: 2.302560589313507\n",
            "Batch #300 Loss: 2.3026092743873594\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026435375213623\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025944232940674\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_24</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_24' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_24</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_205318-Lenet5Decay_1726099655.2927098_25</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_25' target=\"_blank\">Lenet5Decay_1726099655.2927098_25</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_25' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_25</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3035982179641725\n",
            "Batch #200 Loss: 2.303075728416443\n",
            "Batch #300 Loss: 2.303067219257355\n",
            "\u001b[92mTrain accuracy: 4739/48000 =  9.873 % ||| loss 2.3023226261138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1260/12000 =  10.5 % ||| loss 2.3013226985931396\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302030563354492\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3018946480751037\n",
            "Batch #200 Loss: 2.3017336750030517\n",
            "Batch #300 Loss: 2.301425886154175\n",
            "\u001b[92mTrain accuracy: 4735/48000 =  9.865 % ||| loss 2.300677537918091\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1261/12000 =  10.51 % ||| loss 2.299853563308716\u001b[0m\n",
            "\u001b[92mTest accuracy: 999/10000 =  9.99 % ||| loss 2.300621747970581\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.300478045940399\n",
            "Batch #200 Loss: 2.299976177215576\n",
            "Batch #300 Loss: 2.2997849011421203\n",
            "\u001b[92mTrain accuracy: 4604/48000 =  9.592 % ||| loss 2.2991068363189697\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1246/12000 =  10.38 % ||| loss 2.2984323501586914\u001b[0m\n",
            "\u001b[92mTest accuracy: 973/10000 =  9.73 % ||| loss 2.2991116046905518\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.299183123111725\n",
            "Batch #200 Loss: 2.298277530670166\n",
            "Batch #300 Loss: 2.2980444407463074\n",
            "\u001b[92mTrain accuracy: 5597/48000 =  11.66 % ||| loss 2.2973902225494385\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1459/12000 =  12.16 % ||| loss 2.296847343444824\u001b[0m\n",
            "\u001b[92mTest accuracy: 1200/10000 =  12.0 % ||| loss 2.2971949577331543\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.297324230670929\n",
            "Batch #200 Loss: 2.296414518356323\n",
            "Batch #300 Loss: 2.2959388208389284\n",
            "\u001b[92mTrain accuracy: 8280/48000 =  17.25 % ||| loss 2.2952206134796143\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2116/12000 =  17.63 % ||| loss 2.2947795391082764\u001b[0m\n",
            "\u001b[92mTest accuracy: 1718/10000 =  17.18 % ||| loss 2.2951431274414062\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.295037705898285\n",
            "Batch #200 Loss: 2.294116973876953\n",
            "Batch #300 Loss: 2.293135986328125\n",
            "\u001b[92mTrain accuracy: 9895/48000 =  20.61 % ||| loss 2.292145013809204\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2507/12000 =  20.89 % ||| loss 2.291783571243286\u001b[0m\n",
            "\u001b[92mTest accuracy: 2077/10000 =  20.77 % ||| loss 2.29211163520813\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.291797878742218\n",
            "Batch #200 Loss: 2.290545995235443\n",
            "Batch #300 Loss: 2.2890813755989075\n",
            "\u001b[92mTrain accuracy: 15201/48000 =  31.67 % ||| loss 2.28713059425354\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3764/12000 =  31.37 % ||| loss 2.2868192195892334\u001b[0m\n",
            "\u001b[92mTest accuracy: 3202/10000 =  32.02 % ||| loss 2.2870736122131348\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.2862453508377074\n",
            "Batch #200 Loss: 2.2841787362098693\n",
            "Batch #300 Loss: 2.281684718132019\n",
            "\u001b[92mTrain accuracy: 21092/48000 =  43.94 % ||| loss 2.2778055667877197\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5283/12000 =  44.02 % ||| loss 2.2775681018829346\u001b[0m\n",
            "\u001b[92mTest accuracy: 4412/10000 =  44.12 % ||| loss 2.2779364585876465\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.276086299419403\n",
            "Batch #200 Loss: 2.271546905040741\n",
            "Batch #300 Loss: 2.265838851928711\n",
            "\u001b[92mTrain accuracy: 19972/48000 =  41.61 % ||| loss 2.2554640769958496\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4980/12000 =  41.5 % ||| loss 2.2553505897521973\u001b[0m\n",
            "\u001b[92mTest accuracy: 4188/10000 =  41.88 % ||| loss 2.2555835247039795\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.2498152685165405\n",
            "Batch #200 Loss: 2.2345907282829285\n",
            "Batch #300 Loss: 2.211896221637726\n",
            "\u001b[92mTrain accuracy: 17328/48000 =  36.1 % ||| loss 2.162104845046997\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4328/12000 =  36.07 % ||| loss 2.162313938140869\u001b[0m\n",
            "\u001b[92mTest accuracy: 3600/10000 =  36.0 % ||| loss 2.1622374057769775\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.129549973011017\n",
            "Batch #200 Loss: 2.024600452184677\n",
            "Batch #300 Loss: 1.8368892300128936\n",
            "\u001b[92mTrain accuracy: 24211/48000 =  50.44 % ||| loss 1.4991201162338257\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6067/12000 =  50.56 % ||| loss 1.4961974620819092\u001b[0m\n",
            "\u001b[92mTest accuracy: 4997/10000 =  49.97 % ||| loss 1.502414584159851\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 1.3853354036808014\n",
            "Batch #200 Loss: 1.2123328709602357\n",
            "Batch #300 Loss: 1.1215564012527466\n",
            "\u001b[92mTrain accuracy: 29896/48000 =  62.28 % ||| loss 1.0518124103546143\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7520/12000 =  62.67 % ||| loss 1.0424301624298096\u001b[0m\n",
            "\u001b[92mTest accuracy: 6170/10000 =  61.7 % ||| loss 1.0632846355438232\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 1.0291318649053574\n",
            "Batch #200 Loss: 1.014916318655014\n",
            "Batch #300 Loss: 0.9823875242471695\n",
            "\u001b[92mTrain accuracy: 31169/48000 =  64.94 % ||| loss 0.9503524303436279\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7853/12000 =  65.44 % ||| loss 0.9391493201255798\u001b[0m\n",
            "\u001b[92mTest accuracy: 6396/10000 =  63.96 % ||| loss 0.9647018909454346\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.9476709723472595\n",
            "Batch #200 Loss: 0.9252407765388488\n",
            "Batch #300 Loss: 0.9234387820959091\n",
            "\u001b[92mTrain accuracy: 32089/48000 =  66.85 % ||| loss 0.8991126418113708\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8075/12000 =  67.29 % ||| loss 0.8875890970230103\u001b[0m\n",
            "\u001b[92mTest accuracy: 6578/10000 =  65.78 % ||| loss 0.9150420427322388\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.9067296195030212\n",
            "Batch #200 Loss: 0.8859227210283279\n",
            "Batch #300 Loss: 0.8717014133930207\n",
            "\u001b[92mTrain accuracy: 32864/48000 =  68.47 % ||| loss 0.8570864200592041\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8250/12000 =  68.75 % ||| loss 0.8460195064544678\u001b[0m\n",
            "\u001b[92mTest accuracy: 6749/10000 =  67.49 % ||| loss 0.8733898401260376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.8576421767473221\n",
            "Batch #200 Loss: 0.8588009417057038\n",
            "Batch #300 Loss: 0.8300491124391556\n",
            "\u001b[92mTrain accuracy: 32476/48000 =  67.66 % ||| loss 0.83644700050354\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8150/12000 =  67.92 % ||| loss 0.8231562376022339\u001b[0m\n",
            "\u001b[92mTest accuracy: 6718/10000 =  67.18 % ||| loss 0.8522316813468933\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.8144936448335648\n",
            "Batch #200 Loss: 0.8311894178390503\n",
            "Batch #300 Loss: 0.8088212215900421\n",
            "\u001b[92mTrain accuracy: 33947/48000 =  70.72 % ||| loss 0.8008471131324768\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8548/12000 =  71.23 % ||| loss 0.7901298999786377\u001b[0m\n",
            "\u001b[92mTest accuracy: 6984/10000 =  69.84 % ||| loss 0.8142201900482178\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.7954623174667358\n",
            "Batch #200 Loss: 0.7941633623838424\n",
            "Batch #300 Loss: 0.7949556833505631\n",
            "\u001b[92mTrain accuracy: 34353/48000 =  71.57 % ||| loss 0.7791028618812561\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8614/12000 =  71.78 % ||| loss 0.7693530321121216\u001b[0m\n",
            "\u001b[92mTest accuracy: 7046/10000 =  70.46 % ||| loss 0.7963561415672302\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.7800180870294571\n",
            "Batch #200 Loss: 0.7696214562654495\n",
            "Batch #300 Loss: 0.7694035214185715\n",
            "\u001b[92mTrain accuracy: 34101/48000 =  71.04 % ||| loss 0.7551256418228149\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8597/12000 =  71.64 % ||| loss 0.7443302869796753\u001b[0m\n",
            "\u001b[92mTest accuracy: 6999/10000 =  69.99 % ||| loss 0.7722071409225464\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.7683459669351578\n",
            "Batch #200 Loss: 0.7577028483152389\n",
            "Batch #300 Loss: 0.7487235194444657\n",
            "\u001b[92mTrain accuracy: 35002/48000 =  72.92 % ||| loss 0.741083025932312\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8825/12000 =  73.54 % ||| loss 0.7320235371589661\u001b[0m\n",
            "\u001b[92mTest accuracy: 7234/10000 =  72.34 % ||| loss 0.7573674321174622\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.7476662611961364\n",
            "Batch #200 Loss: 0.7397873592376709\n",
            "Batch #300 Loss: 0.735902208685875\n",
            "\u001b[92mTrain accuracy: 34623/48000 =  72.13 % ||| loss 0.7565458416938782\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8655/12000 =  72.12 % ||| loss 0.7515130639076233\u001b[0m\n",
            "\u001b[92mTest accuracy: 7155/10000 =  71.55 % ||| loss 0.7784135937690735\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.7285202276706696\n",
            "Batch #200 Loss: 0.7212592142820359\n",
            "Batch #300 Loss: 0.7365317177772522\n",
            "\u001b[92mTrain accuracy: 34796/48000 =  72.49 % ||| loss 0.7125351428985596\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8747/12000 =  72.89 % ||| loss 0.7046439051628113\u001b[0m\n",
            "\u001b[92mTest accuracy: 7171/10000 =  71.71 % ||| loss 0.7333133220672607\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.723514860868454\n",
            "Batch #200 Loss: 0.7024428308010101\n",
            "Batch #300 Loss: 0.7091777497529983\n",
            "\u001b[92mTrain accuracy: 35415/48000 =  73.78 % ||| loss 0.7045931816101074\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8845/12000 =  73.71 % ||| loss 0.6983010172843933\u001b[0m\n",
            "\u001b[92mTest accuracy: 7312/10000 =  73.12 % ||| loss 0.7284511923789978\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.7195188975334168\n",
            "Batch #200 Loss: 0.7031497049331665\n",
            "Batch #300 Loss: 0.688738420009613\n",
            "\u001b[92mTrain accuracy: 35161/48000 =  73.25 % ||| loss 0.6984996199607849\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8799/12000 =  73.32 % ||| loss 0.6936288475990295\u001b[0m\n",
            "\u001b[92mTest accuracy: 7264/10000 =  72.64 % ||| loss 0.7245342135429382\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.7001831358671189\n",
            "Batch #200 Loss: 0.6898964792490005\n",
            "Batch #300 Loss: 0.6847863000631332\n",
            "\u001b[92mTrain accuracy: 35972/48000 =  74.94 % ||| loss 0.6821355223655701\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9003/12000 =  75.02 % ||| loss 0.6766507625579834\u001b[0m\n",
            "\u001b[92mTest accuracy: 7436/10000 =  74.36 % ||| loss 0.7049587368965149\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_25</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_25' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_25</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240911_205509-Lenet5Decay_1726099655.2927098_26</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_26' target=\"_blank\">Lenet5Decay_1726099655.2927098_26</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_26' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_26</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.301480734348297\n",
            "Batch #200 Loss: 2.3013967657089234\n",
            "Batch #300 Loss: 2.299576222896576\n",
            "\u001b[92mTrain accuracy: 4860/48000 =  10.12 % ||| loss 2.2985053062438965\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.298767566680908\u001b[0m\n",
            "\u001b[92mTest accuracy: 1006/10000 =  10.06 % ||| loss 2.298574447631836\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.297719724178314\n",
            "Batch #200 Loss: 2.2965772795677184\n",
            "Batch #300 Loss: 2.2950818824768064\n",
            "\u001b[92mTrain accuracy: 9235/48000 =  19.24 % ||| loss 2.292980194091797\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2307/12000 =  19.23 % ||| loss 2.2931714057922363\u001b[0m\n",
            "\u001b[92mTest accuracy: 1911/10000 =  19.11 % ||| loss 2.29296875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.292139115333557\n",
            "Batch #200 Loss: 2.2900146007537843\n",
            "Batch #300 Loss: 2.288022904396057\n",
            "\u001b[92mTrain accuracy: 15143/48000 =  31.55 % ||| loss 2.284503936767578\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3778/12000 =  31.48 % ||| loss 2.2846274375915527\u001b[0m\n",
            "\u001b[92mTest accuracy: 3128/10000 =  31.28 % ||| loss 2.2847371101379395\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.2828690099716185\n",
            "Batch #200 Loss: 2.2792678236961366\n",
            "Batch #300 Loss: 2.2738246083259583\n",
            "\u001b[92mTrain accuracy: 17632/48000 =  36.73 % ||| loss 2.265312433242798\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4409/12000 =  36.74 % ||| loss 2.2651987075805664\u001b[0m\n",
            "\u001b[92mTest accuracy: 3690/10000 =  36.9 % ||| loss 2.2656502723693848\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.26071528673172\n",
            "Batch #200 Loss: 2.248930723667145\n",
            "Batch #300 Loss: 2.232928216457367\n",
            "\u001b[92mTrain accuracy: 20247/48000 =  42.18 % ||| loss 2.198664426803589\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5122/12000 =  42.68 % ||| loss 2.1980247497558594\u001b[0m\n",
            "\u001b[92mTest accuracy: 4222/10000 =  42.22 % ||| loss 2.1990771293640137\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.1767074632644654\n",
            "Batch #200 Loss: 2.1106890559196474\n",
            "Batch #300 Loss: 1.975753549337387\n",
            "\u001b[92mTrain accuracy: 21082/48000 =  43.92 % ||| loss 1.673632025718689\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5280/12000 =  44.0 % ||| loss 1.6717709302902222\u001b[0m\n",
            "\u001b[92mTest accuracy: 4372/10000 =  43.72 % ||| loss 1.6761711835861206\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 1.5334235537052154\n",
            "Batch #200 Loss: 1.313888258934021\n",
            "Batch #300 Loss: 1.1722384631633758\n",
            "\u001b[92mTrain accuracy: 28360/48000 =  59.08 % ||| loss 1.078151822090149\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7126/12000 =  59.38 % ||| loss 1.0715572834014893\u001b[0m\n",
            "\u001b[92mTest accuracy: 5898/10000 =  58.98 % ||| loss 1.089487075805664\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 1.0610790544748305\n",
            "Batch #200 Loss: 1.0214640825986863\n",
            "Batch #300 Loss: 0.9904906117916107\n",
            "\u001b[92mTrain accuracy: 30090/48000 =  62.69 % ||| loss 0.9684160351753235\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7547/12000 =  62.89 % ||| loss 0.959718644618988\u001b[0m\n",
            "\u001b[92mTest accuracy: 6207/10000 =  62.07 % ||| loss 0.984948456287384\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.9615937519073486\n",
            "Batch #200 Loss: 0.9556071299314499\n",
            "Batch #300 Loss: 0.9270430302619934\n",
            "\u001b[92mTrain accuracy: 31244/48000 =  65.09 % ||| loss 0.9087949395179749\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7879/12000 =  65.66 % ||| loss 0.8986435532569885\u001b[0m\n",
            "\u001b[92mTest accuracy: 6471/10000 =  64.71 % ||| loss 0.9235586524009705\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.9153307867050171\n",
            "Batch #200 Loss: 0.8757787674665451\n",
            "Batch #300 Loss: 0.8775042575597763\n",
            "\u001b[92mTrain accuracy: 32195/48000 =  67.07 % ||| loss 0.8588770627975464\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8050/12000 =  67.08 % ||| loss 0.8497033715248108\u001b[0m\n",
            "\u001b[92mTest accuracy: 6649/10000 =  66.49 % ||| loss 0.8790425062179565\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.8452102756500244\n",
            "Batch #200 Loss: 0.844919650554657\n",
            "Batch #300 Loss: 0.839778488278389\n",
            "\u001b[92mTrain accuracy: 33361/48000 =  69.5 % ||| loss 0.8158464431762695\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8345/12000 =  69.54 % ||| loss 0.8052449226379395\u001b[0m\n",
            "\u001b[92mTest accuracy: 6899/10000 =  68.99 % ||| loss 0.8358904719352722\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.8063348692655563\n",
            "Batch #200 Loss: 0.8030904179811478\n",
            "Batch #300 Loss: 0.809480249285698\n",
            "\u001b[92mTrain accuracy: 34022/48000 =  70.88 % ||| loss 0.7801114320755005\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8527/12000 =  71.06 % ||| loss 0.7712855935096741\u001b[0m\n",
            "\u001b[92mTest accuracy: 7033/10000 =  70.33 % ||| loss 0.7962856292724609\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.7785047173500061\n",
            "Batch #200 Loss: 0.770990573167801\n",
            "Batch #300 Loss: 0.753485934138298\n",
            "\u001b[92mTrain accuracy: 33808/48000 =  70.43 % ||| loss 0.751213550567627\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8450/12000 =  70.42 % ||| loss 0.7419218420982361\u001b[0m\n",
            "\u001b[92mTest accuracy: 7004/10000 =  70.04 % ||| loss 0.7695494890213013\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.7517729467153549\n",
            "Batch #200 Loss: 0.7373272264003754\n",
            "Batch #300 Loss: 0.7404202824831009\n",
            "\u001b[92mTrain accuracy: 34404/48000 =  71.67 % ||| loss 0.7331539988517761\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8618/12000 =  71.82 % ||| loss 0.7207010984420776\u001b[0m\n",
            "\u001b[92mTest accuracy: 7090/10000 =  70.9 % ||| loss 0.7458893060684204\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.7068734419345856\n",
            "Batch #200 Loss: 0.7270138841867447\n",
            "Batch #300 Loss: 0.718519629240036\n",
            "\u001b[92mTrain accuracy: 35225/48000 =  73.39 % ||| loss 0.7004330158233643\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8828/12000 =  73.57 % ||| loss 0.6920868754386902\u001b[0m\n",
            "\u001b[92mTest accuracy: 7268/10000 =  72.68 % ||| loss 0.7173613905906677\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.7020059472322464\n",
            "Batch #200 Loss: 0.7082633376121521\n",
            "Batch #300 Loss: 0.7022250145673752\n",
            "\u001b[92mTrain accuracy: 35220/48000 =  73.38 % ||| loss 0.6894516348838806\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8803/12000 =  73.36 % ||| loss 0.6837531328201294\u001b[0m\n",
            "\u001b[92mTest accuracy: 7267/10000 =  72.67 % ||| loss 0.7100635766983032\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.6988840693235397\n",
            "Batch #200 Loss: 0.6892185944318772\n",
            "Batch #300 Loss: 0.6740031689405441\n",
            "\u001b[92mTrain accuracy: 35408/48000 =  73.77 % ||| loss 0.6762994527816772\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8832/12000 =  73.6 % ||| loss 0.6689812541007996\u001b[0m\n",
            "\u001b[92mTest accuracy: 7292/10000 =  72.92 % ||| loss 0.6978455185890198\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.691096743941307\n",
            "Batch #200 Loss: 0.6681673470139503\n",
            "Batch #300 Loss: 0.6603867441415787\n",
            "\u001b[92mTrain accuracy: 35501/48000 =  73.96 % ||| loss 0.6895239353179932\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8903/12000 =  74.19 % ||| loss 0.681067943572998\u001b[0m\n",
            "\u001b[92mTest accuracy: 7357/10000 =  73.57 % ||| loss 0.709825873374939\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.6637788534164428\n",
            "Batch #200 Loss: 0.6622518038749695\n",
            "Batch #300 Loss: 0.6612717419862747\n",
            "\u001b[92mTrain accuracy: 36209/48000 =  75.44 % ||| loss 0.6507927775382996\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9062/12000 =  75.52 % ||| loss 0.6433515548706055\u001b[0m\n",
            "\u001b[92mTest accuracy: 7476/10000 =  74.76 % ||| loss 0.6736143231391907\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.6611311715841294\n",
            "Batch #200 Loss: 0.6489276349544525\n",
            "Batch #300 Loss: 0.6631072998046875\n",
            "\u001b[92mTrain accuracy: 36283/48000 =  75.59 % ||| loss 0.638515293598175\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9089/12000 =  75.74 % ||| loss 0.6313527822494507\u001b[0m\n",
            "\u001b[92mTest accuracy: 7486/10000 =  74.86 % ||| loss 0.6601312160491943\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.6502105379104615\n",
            "Batch #200 Loss: 0.6431127059459686\n",
            "Batch #300 Loss: 0.635463308095932\n",
            "\u001b[92mTrain accuracy: 36033/48000 =  75.07 % ||| loss 0.659971296787262\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9027/12000 =  75.22 % ||| loss 0.6509361863136292\u001b[0m\n",
            "\u001b[92mTest accuracy: 7420/10000 =  74.2 % ||| loss 0.6797803044319153\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.6420067045092582\n",
            "Batch #200 Loss: 0.6385842627286911\n",
            "Batch #300 Loss: 0.6200731128454209\n",
            "\u001b[92mTrain accuracy: 36765/48000 =  76.59 % ||| loss 0.6208480000495911\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9213/12000 =  76.78 % ||| loss 0.6152822971343994\u001b[0m\n",
            "\u001b[92mTest accuracy: 7551/10000 =  75.51 % ||| loss 0.6425278782844543\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.6235211279988289\n",
            "Batch #200 Loss: 0.6197586908936501\n",
            "Batch #300 Loss: 0.6349887236952781\n",
            "\u001b[92mTrain accuracy: 36530/48000 =  76.1 % ||| loss 0.6187973618507385\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9145/12000 =  76.21 % ||| loss 0.6121245622634888\u001b[0m\n",
            "\u001b[92mTest accuracy: 7536/10000 =  75.36 % ||| loss 0.6429617404937744\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.6237650385499001\n",
            "Batch #200 Loss: 0.6196470499038697\n",
            "Batch #300 Loss: 0.6116112750768662\n",
            "\u001b[92mTrain accuracy: 37022/48000 =  77.13 % ||| loss 0.6053169965744019\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9239/12000 =  76.99 % ||| loss 0.6016886830329895\u001b[0m\n",
            "\u001b[92mTest accuracy: 7610/10000 =  76.1 % ||| loss 0.630645751953125\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5975972604751587\n",
            "Batch #200 Loss: 0.6173778548836708\n",
            "Batch #300 Loss: 0.6286094135046005\n",
            "\u001b[92mTrain accuracy: 36542/48000 =  76.13 % ||| loss 0.6090060472488403\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9159/12000 =  76.33 % ||| loss 0.6030046343803406\u001b[0m\n",
            "\u001b[92mTest accuracy: 7515/10000 =  75.15 % ||| loss 0.6356220245361328\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726099655.2927098_26</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_26' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726099655.2927098_26</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[93m!!!!!!! Hyper Param Tuning Finished!!!!!!!!!!!\u001b[0m\n",
            "Best Model: Lenet5Decay(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "wandb name: Lenet5Decay_1726099655.2927098_2\n",
            "\n",
            "HyperParams: {'learning_rate': 0.001, 'momentum': 0.7, 'weight_decay': 0.001}\n",
            "\n",
            "Accuracies: {'train': 0.9135625, 'val': 0.89175, 'test': 0.8875}\n"
          ]
        }
      ],
      "source": [
        "class Lenet5Decay(Lenet5):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.decay = kwargs['weight_decay']\n",
        "    \n",
        "param_grid = {\n",
        "  'learning_rate':[0.1, 0.01,0.001],\n",
        "  'momentum':[0, 0.9, 0.7],\n",
        "  'weight_decay':[0.1, 0.01, 0.001]\n",
        "}\n",
        "\n",
        "best_weightdecay = hyperparameter_tuning(Lenet5Decay, dataloaders, device, 25, **param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChEh4AqnjmkK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOeqxEEwILJczy2HVXTs3LD",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1lAy_YT2kZxUVJvk7umNcXW3_Ded87R2i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
