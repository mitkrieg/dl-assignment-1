{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitkrieg/dl-assignment-1/blob/main/assignment1_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ56fD3aY8na"
      },
      "source": [
        "# Assignment 1: DL Basics\n",
        "\n",
        "### Goal\n",
        "Implement [LeNet5](https://arxiv.org/pdf/1502.03167v3) and compare various regularization techniques on the network using the FashionMNSIT dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-12 08:31:08--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 3.5.134.248, 52.219.170.32, 52.219.170.224, ...\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|3.5.134.248|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26421880 (25M) [binary/octet-stream]\n",
            "Saving to: ‘./data/train-images-idx3-ubyte.gz’\n",
            "\n",
            "./data/train-images 100%[===================>]  25.20M  1.36MB/s    in 19s     \n",
            "\n",
            "2024-09-12 08:31:27 (1.36 MB/s) - ‘./data/train-images-idx3-ubyte.gz’ saved [26421880/26421880]\n",
            "\n",
            "--2024-09-12 08:31:27--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.140.68, 52.219.168.28, 3.5.136.158, ...\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.140.68|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29515 (29K) [binary/octet-stream]\n",
            "Saving to: ‘./data/train-labels-idx1-ubyte.gz’\n",
            "\n",
            "./data/train-labels 100%[===================>]  28.82K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-09-12 08:31:27 (282 KB/s) - ‘./data/train-labels-idx1-ubyte.gz’ saved [29515/29515]\n",
            "\n",
            "--2024-09-12 08:31:27--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.46.6, 52.219.168.28, 3.5.136.158, ...\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.46.6|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4422102 (4.2M) [binary/octet-stream]\n",
            "Saving to: ‘./data/test-images-idx3-ubyte.gz’\n",
            "\n",
            "./data/test-images- 100%[===================>]   4.22M  5.13MB/s    in 0.8s    \n",
            "\n",
            "2024-09-12 08:31:28 (5.13 MB/s) - ‘./data/test-images-idx3-ubyte.gz’ saved [4422102/4422102]\n",
            "\n",
            "--2024-09-12 08:31:29--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 3.5.137.59, 52.219.168.28, 3.5.134.208, ...\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|3.5.137.59|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5148 (5.0K) [binary/octet-stream]\n",
            "Saving to: ‘./data/test-labels-idx1-ubyte.gz’\n",
            "\n",
            "./data/test-labels- 100%[===================>]   5.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-09-12 08:31:29 (79.2 MB/s) - ‘./data/test-labels-idx1-ubyte.gz’ saved [5148/5148]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz -O ./data/train-images-idx3-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz -O ./data/train-labels-idx1-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz -O ./data/test-images-idx3-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz -O ./data/test-labels-idx1-ubyte.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m912ceG9Z9X-"
      },
      "source": [
        "## Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "7roLS_owoJPv",
        "outputId": "84ede7b1-b21c-436a-e432-85168cddd11d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmitkrieger\u001b[0m (\u001b[33mmitkrieger-cornell-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from time import time\n",
        "import gzip\n",
        "import typing as T\n",
        "import wandb\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi805ZxpaFyO"
      },
      "source": [
        "## Check for GPU Access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwJwFQG4R8wy",
        "outputId": "c103cb24-ecbe-40ce-8ce5-58845cfda5dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------ ACCELERATION INFO -----\n",
            "CUDA GPU Available: False\n",
            "MPS GPU Available: True\n",
            "Using CPU\n"
          ]
        }
      ],
      "source": [
        "print(\"------ ACCELERATION INFO -----\")\n",
        "print('CUDA GPU Available:',torch.cuda.is_available())\n",
        "print('MPS GPU Available:', torch.backends.mps.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('GPU Name:',torch.cuda.get_device_name(0))\n",
        "  print('GPU Count:',torch.cuda.device_count())\n",
        "  print('GPU Memory Allocated:',torch.cuda.memory_allocated(0))\n",
        "  print('GPU Memory Cached:',torch.cuda.memory_reserved(0))\n",
        "# elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "#   device = torch.device('mps')\n",
        "#   print('Pytorch GPU Build:',torch.backends.mps.is_built())\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print('Using CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWsWErVcSRIv"
      },
      "source": [
        "## Load Data into Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kGjJiU0lDHzb"
      },
      "outputs": [],
      "source": [
        "PATH = './data'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ji077ZjaQNK"
      },
      "source": [
        "### Define FashionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1gxeQbXyRrqN"
      },
      "outputs": [],
      "source": [
        "class FasionMNISTDataset(Dataset):\n",
        "  def __init__(self, path: str, kind: str, transform=None, target_transform=None, device=None) -> None:\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.device = device\n",
        "    self.labels, self.images = self._load_data(path, kind)\n",
        "\n",
        "  def _load_data(self, path: str, kind: str) -> T.Tuple[np.ndarray, np.ndarray]:\n",
        "    with gzip.open(path + f'/{kind}-labels-idx1-ubyte.gz', 'rb') as lable_file:\n",
        "      lbls = np.frombuffer(lable_file.read(), dtype=np.int8, offset=8)\n",
        "      lbls = np.copy(lbls)\n",
        "    with gzip.open(path + f'/{kind}-images-idx3-ubyte.gz', 'rb') as lable_file:\n",
        "      imgs = np.frombuffer(lable_file.read(), dtype=np.uint8, offset=16).reshape(len(lbls), 1, 28, 28)\n",
        "      imgs = (np.copy(imgs) / 255).astype(np.float32)\n",
        "    return lbls, imgs\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return self.labels.size\n",
        "\n",
        "  def __getitem__(self, index: int) -> T.Tuple[torch.tensor, torch.tensor]:\n",
        "    label = torch.tensor(self.labels[index], dtype=torch.long)\n",
        "    img = torch.tensor(self.images[index])\n",
        "    if self.device:\n",
        "      img = img.to(self.device)\n",
        "      label = label.to(self.device)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "\n",
        "    return img, label\n",
        "\n",
        "def show_img(dataset: Dataset, index: int) -> None:\n",
        "  img, label = dataset[index]\n",
        "  labels_map = {\n",
        "            0: \"T-Shirt\",\n",
        "            1: \"Trouser\",\n",
        "            2: \"Pullover\",\n",
        "            3: \"Dress\",\n",
        "            4: \"Coat\",\n",
        "            5: \"Sandal\",\n",
        "            6: \"Shirt\",\n",
        "            7: \"Sneaker\",\n",
        "            8: \"Bag\",\n",
        "            9: \"Ankle Boot\",\n",
        "        }\n",
        "  plt.imshow(img.cpu().reshape(28,28), cmap='gray')\n",
        "  plt.title(labels_map[label.cpu().item()])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHx4kcRPabF5"
      },
      "source": [
        "### Create Train, Validation and Test sets with loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JXMp90QEcxwP"
      },
      "outputs": [],
      "source": [
        "gen = torch.Generator().manual_seed(123)\n",
        "\n",
        "train = FasionMNISTDataset(PATH, 'train', device=device)\n",
        "train, val = torch.utils.data.random_split(train, [0.8, 0.2], generator=gen)\n",
        "test = FasionMNISTDataset(PATH, 'test', device=device)\n",
        "\n",
        "batch = 128\n",
        "trainloader = DataLoader(train, batch, shuffle=True, generator=gen)\n",
        "valloader = DataLoader(val, batch, shuffle=True, generator=gen)\n",
        "testloader = DataLoader(test, batch, shuffle=True, generator=gen)\n",
        "\n",
        "dataloaders = {\n",
        "    'train': trainloader,\n",
        "    'val': valloader,\n",
        "    'test': testloader\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mktyFGDEaxyp"
      },
      "source": [
        "#### Example Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "SfV38Sn6PC89",
        "outputId": "12971f96-e85e-462c-a00a-c9768480fda5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_img(train, 0)\n",
        "show_img(train,101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u_NANyOa31H"
      },
      "source": [
        "## LeNet5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p0PflE0mV3Q7"
      },
      "outputs": [],
      "source": [
        "class Lenet5(nn.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
        "    self.max_pool1 = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "    self.max_pool2 = nn.MaxPool2d(2, 2)\n",
        "    self.fc1 = nn.Linear(16*5*5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.max_pool1(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.max_pool2(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI94H3E8a_Ph"
      },
      "source": [
        "### Define Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train_epoch(network, dataloader, loss_fn, optimizer, device, epoch, verbosity: int):\n",
        "  \"\"\"Train one epoch of a Lenet5 network\"\"\"\n",
        "  \n",
        "  network.train()\n",
        "  batch_loss = 0\n",
        "  total_loss = 0\n",
        "\n",
        "  # iterate over all batches\n",
        "  for i, data in enumerate(dataloader):\n",
        "    inputs, labels = data\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = network(inputs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    batch_loss += loss.item()\n",
        "    if i % verbosity == verbosity - 1:\n",
        "      print(f'Batch #{i + 1} Loss: {batch_loss / verbosity}')\n",
        "      batch_loss = 0\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "def eval_network(title, network, dataloader, loss_fn, epoch):\n",
        "  \"\"\"Evaluate model and log metrics to wandb\"\"\"\n",
        "\n",
        "  network.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data in dataloader:\n",
        "          images, labels = data\n",
        "          outputs = network(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "          loss += loss_fn(outputs, labels)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "      accuracy = correct / total\n",
        "      wandb.log({\n",
        "          f'{title}-loss': loss / len(dataloader),\n",
        "          f'{title}-accuracy': accuracy\n",
        "      }, step=epoch)\n",
        "  \n",
        "  print(f'\\033[92m{title} accuracy: {correct}/{total} = {100 * accuracy : .4} % ||| loss {loss / len(dataloader)}\\033[0m')\n",
        "  return accuracy\n",
        "\n",
        "def train_network(network, dataloaders, loss_fn, optimizer, device, epochs: int, verbosity: int):\n",
        "  \"\"\"Train network for given number of epochs using optimizer and loss_fn\"\"\"\n",
        "\n",
        "  train_accuracy, val_accuracy, test_accuracy = 0, 0, 0\n",
        "  for epoch in range(epochs):\n",
        "    print(f'----------- Epoch #{epoch + 1} ------------')\n",
        "    train_epoch(network, dataloaders['train'], loss_fn, optimizer, device, epoch, verbosity)\n",
        "    train_accuracy = eval_network('Train', network, dataloaders['train'], loss_fn, epoch)\n",
        "    val_accuracy = eval_network('Validation', network, dataloaders['val'], loss_fn, epoch)    \n",
        "    test_accuracy = eval_network('Test', network, dataloaders['test'], loss_fn, epoch)\n",
        "    print('------------------------------------\\n')\n",
        "  print('----------- Train Complete! ------------')\n",
        "  return {\n",
        "    \"train\":train_accuracy,\n",
        "    \"val\":val_accuracy,\n",
        "    \"test\":test_accuracy,\n",
        "  }\n",
        "\n",
        "\n",
        "def hyperparameter_tuning(network_cls, dataloaders, device, epochs: int, **kwargs):\n",
        "  \"\"\"Train multiple networks and print out hyperparameters & metrics for the highest performing model based on validation accuracy\"\"\"\n",
        "\n",
        "  ts = time()\n",
        "  keys = kwargs.keys()\n",
        "  best_model = None\n",
        "  for i, v in enumerate(itertools.product(*kwargs.values())):\n",
        "    hyperparams = dict(zip(keys, v))\n",
        "    network = network_cls(**hyperparams)\n",
        "    network.to(device)\n",
        "    name = network._get_name() + '_' + str(ts) + '_' + str(i)\n",
        "    run = wandb.init(\n",
        "      id=name,\n",
        "      config={\n",
        "        **hyperparams\n",
        "      }\n",
        "    )\n",
        "\n",
        "    print(f'XXXXXXXX Tuning Network {network._get_name()} XXXXXXXXX')\n",
        "    print('Hyperparameter Config:',hyperparams)\n",
        "\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "    sgd = optim.SGD(\n",
        "      network.parameters(), \n",
        "      lr=hyperparams['learning_rate'], \n",
        "      momentum=hyperparams['momentum'],\n",
        "      weight_decay=hyperparams.get('weight_decay',0)\n",
        "    )\n",
        "      \n",
        "    accuracies = train_network(network, dataloaders, cross_entropy, sgd, device, epochs, 100)\n",
        "    run.finish(quiet=True)\n",
        "    if best_model is None:\n",
        "      best_model = {'net':network, \"accuracy\":accuracies, \"name\":name}\n",
        "    elif best_model['accuracy']['val'] < accuracies['val']:\n",
        "      best_model = {'net':network, \"accuracy\":accuracies, \"name\":name}\n",
        "  print('\\033[93m!!!!!!! Hyper Param Tuning Finished!!!!!!!!!!!\\033[0m')\n",
        "  print(f'Best Model: {best_model['net']}\\nwandb name: {best_model['name']}\\n\\nHyperParams: {hyperparams}\\n\\nAccuracies: {best_model['accuracy']}')\n",
        "  return best_model\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Lenet5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_083142-Lenet5_1726144302.4821298_0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_0' target=\"_blank\">Lenet5_1726144302.4821298_0</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.043761570453644\n",
            "Batch #200 Loss: 1.088820230960846\n",
            "Batch #300 Loss: 0.8169421148300171\n",
            "\u001b[92mTrain accuracy: 35065/48000 =  73.05 % ||| loss 0.6850781440734863\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8778/12000 =  73.15 % ||| loss 0.6822770833969116\u001b[0m\n",
            "\u001b[92mTest accuracy: 7279/10000 =  72.79 % ||| loss 0.7035361528396606\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6466957980394363\n",
            "Batch #200 Loss: 0.6092464476823807\n",
            "Batch #300 Loss: 0.5557059881091118\n",
            "\u001b[92mTrain accuracy: 38450/48000 =  80.1 % ||| loss 0.5110880136489868\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9634/12000 =  80.28 % ||| loss 0.5074743032455444\u001b[0m\n",
            "\u001b[92mTest accuracy: 7942/10000 =  79.42 % ||| loss 0.5315250754356384\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5106605905294418\n",
            "Batch #200 Loss: 0.48817911714315415\n",
            "Batch #300 Loss: 0.4580409023165703\n",
            "\u001b[92mTrain accuracy: 40248/48000 =  83.85 % ||| loss 0.43818867206573486\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10034/12000 =  83.62 % ||| loss 0.4463564157485962\u001b[0m\n",
            "\u001b[92mTest accuracy: 8304/10000 =  83.04 % ||| loss 0.46596744656562805\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.4366384190320969\n",
            "Batch #200 Loss: 0.43097294598817826\n",
            "Batch #300 Loss: 0.41084124967455865\n",
            "\u001b[92mTrain accuracy: 39769/48000 =  82.85 % ||| loss 0.44974425435066223\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9954/12000 =  82.95 % ||| loss 0.4563385546207428\u001b[0m\n",
            "\u001b[92mTest accuracy: 8174/10000 =  81.74 % ||| loss 0.47849008440971375\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.39076541051268576\n",
            "Batch #200 Loss: 0.38229540944099427\n",
            "Batch #300 Loss: 0.38076343953609465\n",
            "\u001b[92mTrain accuracy: 41612/48000 =  86.69 % ||| loss 0.35933178663253784\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10385/12000 =  86.54 % ||| loss 0.3724421262741089\u001b[0m\n",
            "\u001b[92mTest accuracy: 8582/10000 =  85.82 % ||| loss 0.3937329649925232\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.35792891681194305\n",
            "Batch #200 Loss: 0.3656280305981636\n",
            "Batch #300 Loss: 0.3542800889909267\n",
            "\u001b[92mTrain accuracy: 41682/48000 =  86.84 % ||| loss 0.35165226459503174\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10394/12000 =  86.62 % ||| loss 0.36599236726760864\u001b[0m\n",
            "\u001b[92mTest accuracy: 8544/10000 =  85.44 % ||| loss 0.38946908712387085\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.34547964289784433\n",
            "Batch #200 Loss: 0.3380848748981953\n",
            "Batch #300 Loss: 0.32692332297563553\n",
            "\u001b[92mTrain accuracy: 42421/48000 =  88.38 % ||| loss 0.31333595514297485\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10520/12000 =  87.67 % ||| loss 0.333239883184433\u001b[0m\n",
            "\u001b[92mTest accuracy: 8687/10000 =  86.87 % ||| loss 0.35689792037010193\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.31700542226433753\n",
            "Batch #200 Loss: 0.3194122862815857\n",
            "Batch #300 Loss: 0.3232123500108719\n",
            "\u001b[92mTrain accuracy: 42772/48000 =  89.11 % ||| loss 0.29423314332962036\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10593/12000 =  88.28 % ||| loss 0.3192272484302521\u001b[0m\n",
            "\u001b[92mTest accuracy: 8770/10000 =  87.7 % ||| loss 0.34478163719177246\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.31063070192933084\n",
            "Batch #200 Loss: 0.3012015767395496\n",
            "Batch #300 Loss: 0.30983148112893105\n",
            "\u001b[92mTrain accuracy: 42464/48000 =  88.47 % ||| loss 0.3085559904575348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10488/12000 =  87.4 % ||| loss 0.33310964703559875\u001b[0m\n",
            "\u001b[92mTest accuracy: 8677/10000 =  86.77 % ||| loss 0.3564487397670746\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.2929663227498531\n",
            "Batch #200 Loss: 0.2913277657330036\n",
            "Batch #300 Loss: 0.28742556408047676\n",
            "\u001b[92mTrain accuracy: 43312/48000 =  90.23 % ||| loss 0.264135479927063\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10659/12000 =  88.83 % ||| loss 0.2974437475204468\u001b[0m\n",
            "\u001b[92mTest accuracy: 8858/10000 =  88.58 % ||| loss 0.3212786614894867\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.27719582051038744\n",
            "Batch #200 Loss: 0.28384968504309654\n",
            "Batch #300 Loss: 0.28723197847604753\n",
            "\u001b[92mTrain accuracy: 43095/48000 =  89.78 % ||| loss 0.2749537527561188\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10622/12000 =  88.52 % ||| loss 0.3077087700366974\u001b[0m\n",
            "\u001b[92mTest accuracy: 8787/10000 =  87.87 % ||| loss 0.3333900272846222\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.265094800144434\n",
            "Batch #200 Loss: 0.2704334570467472\n",
            "Batch #300 Loss: 0.2694646991789341\n",
            "\u001b[92mTrain accuracy: 43594/48000 =  90.82 % ||| loss 0.25056910514831543\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10705/12000 =  89.21 % ||| loss 0.2885177433490753\u001b[0m\n",
            "\u001b[92mTest accuracy: 8886/10000 =  88.86 % ||| loss 0.3100834786891937\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.26015480995178225\n",
            "Batch #200 Loss: 0.26481465697288514\n",
            "Batch #300 Loss: 0.2672987025976181\n",
            "\u001b[92mTrain accuracy: 43698/48000 =  91.04 % ||| loss 0.24172596633434296\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10710/12000 =  89.25 % ||| loss 0.28788119554519653\u001b[0m\n",
            "\u001b[92mTest accuracy: 8911/10000 =  89.11 % ||| loss 0.3140476942062378\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.2481665024161339\n",
            "Batch #200 Loss: 0.25658310666680334\n",
            "Batch #300 Loss: 0.2605443125218153\n",
            "\u001b[92mTrain accuracy: 43793/48000 =  91.24 % ||| loss 0.23698972165584564\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10741/12000 =  89.51 % ||| loss 0.28477761149406433\u001b[0m\n",
            "\u001b[92mTest accuracy: 8908/10000 =  89.08 % ||| loss 0.30949485301971436\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.2542724271118641\n",
            "Batch #200 Loss: 0.24345666997134685\n",
            "Batch #300 Loss: 0.24576567694544793\n",
            "\u001b[92mTrain accuracy: 43788/48000 =  91.22 % ||| loss 0.23216207325458527\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10735/12000 =  89.46 % ||| loss 0.28471875190734863\u001b[0m\n",
            "\u001b[92mTest accuracy: 8893/10000 =  88.93 % ||| loss 0.3065265119075775\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.23563354343175888\n",
            "Batch #200 Loss: 0.24077801942825316\n",
            "Batch #300 Loss: 0.24134467631578446\n",
            "\u001b[92mTrain accuracy: 44019/48000 =  91.71 % ||| loss 0.2184639424085617\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10756/12000 =  89.63 % ||| loss 0.2774462401866913\u001b[0m\n",
            "\u001b[92mTest accuracy: 8930/10000 =  89.3 % ||| loss 0.2985800802707672\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.2331576754152775\n",
            "Batch #200 Loss: 0.23523545138537882\n",
            "Batch #300 Loss: 0.2281245145201683\n",
            "\u001b[92mTrain accuracy: 43953/48000 =  91.57 % ||| loss 0.21823333203792572\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10745/12000 =  89.54 % ||| loss 0.2778153419494629\u001b[0m\n",
            "\u001b[92mTest accuracy: 8907/10000 =  89.07 % ||| loss 0.30432620644569397\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.22918671235442162\n",
            "Batch #200 Loss: 0.2198009869456291\n",
            "Batch #300 Loss: 0.22544219449162484\n",
            "\u001b[92mTrain accuracy: 44166/48000 =  92.01 % ||| loss 0.2154950499534607\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10748/12000 =  89.57 % ||| loss 0.282696932554245\u001b[0m\n",
            "\u001b[92mTest accuracy: 8921/10000 =  89.21 % ||| loss 0.30631402134895325\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.22329742431640626\n",
            "Batch #200 Loss: 0.2256633199006319\n",
            "Batch #300 Loss: 0.21819561898708342\n",
            "\u001b[92mTrain accuracy: 44076/48000 =  91.83 % ||| loss 0.21646112203598022\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10767/12000 =  89.72 % ||| loss 0.28770506381988525\u001b[0m\n",
            "\u001b[92mTest accuracy: 8885/10000 =  88.85 % ||| loss 0.31455594301223755\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.21765194863080978\n",
            "Batch #200 Loss: 0.21532657764852048\n",
            "Batch #300 Loss: 0.20395427547395228\n",
            "\u001b[92mTrain accuracy: 44346/48000 =  92.39 % ||| loss 0.20053763687610626\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10781/12000 =  89.84 % ||| loss 0.27693599462509155\u001b[0m\n",
            "\u001b[92mTest accuracy: 8931/10000 =  89.31 % ||| loss 0.3040978014469147\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.20603932619094847\n",
            "Batch #200 Loss: 0.2114536053687334\n",
            "Batch #300 Loss: 0.20135458432137965\n",
            "\u001b[92mTrain accuracy: 44537/48000 =  92.79 % ||| loss 0.19286447763442993\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10759/12000 =  89.66 % ||| loss 0.2733563482761383\u001b[0m\n",
            "\u001b[92mTest accuracy: 8935/10000 =  89.35 % ||| loss 0.3026003837585449\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.1990081699192524\n",
            "Batch #200 Loss: 0.2001749747991562\n",
            "Batch #300 Loss: 0.20792952813208104\n",
            "\u001b[92mTrain accuracy: 44176/48000 =  92.03 % ||| loss 0.2086607813835144\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10714/12000 =  89.28 % ||| loss 0.2941906452178955\u001b[0m\n",
            "\u001b[92mTest accuracy: 8884/10000 =  88.84 % ||| loss 0.31867724657058716\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.19637309215962886\n",
            "Batch #200 Loss: 0.19987750358879566\n",
            "Batch #300 Loss: 0.19664503425359725\n",
            "\u001b[92mTrain accuracy: 44688/48000 =  93.1 % ||| loss 0.18401440978050232\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10795/12000 =  89.96 % ||| loss 0.28152182698249817\u001b[0m\n",
            "\u001b[92mTest accuracy: 8937/10000 =  89.37 % ||| loss 0.31597667932510376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.18346384175121785\n",
            "Batch #200 Loss: 0.19032577522099017\n",
            "Batch #300 Loss: 0.19395508386194707\n",
            "\u001b[92mTrain accuracy: 44549/48000 =  92.81 % ||| loss 0.18706443905830383\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10742/12000 =  89.52 % ||| loss 0.29304662346839905\u001b[0m\n",
            "\u001b[92mTest accuracy: 8917/10000 =  89.17 % ||| loss 0.3203170895576477\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.18738435961306096\n",
            "Batch #200 Loss: 0.18397512093186377\n",
            "Batch #300 Loss: 0.18639866895973684\n",
            "\u001b[92mTrain accuracy: 45143/48000 =  94.05 % ||| loss 0.16108141839504242\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10845/12000 =  90.38 % ||| loss 0.26679715514183044\u001b[0m\n",
            "\u001b[92mTest accuracy: 8988/10000 =  89.88 % ||| loss 0.29512879252433777\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726144302.4821298_0</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_0</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_083414-Lenet5_1726144302.4821298_1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_1' target=\"_blank\">Lenet5_1726144302.4821298_1</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.7671215283870696\n",
            "Batch #200 Loss: 0.724150057733059\n",
            "Batch #300 Loss: 0.5928685265779495\n",
            "\u001b[92mTrain accuracy: 38874/48000 =  80.99 % ||| loss 0.501507580280304\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9711/12000 =  80.92 % ||| loss 0.5021095871925354\u001b[0m\n",
            "\u001b[92mTest accuracy: 7968/10000 =  79.68 % ||| loss 0.5316558480262756\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.5147992706298828\n",
            "Batch #200 Loss: 0.4771538418531418\n",
            "Batch #300 Loss: 0.46569743037223815\n",
            "\u001b[92mTrain accuracy: 38274/48000 =  79.74 % ||| loss 0.4991472065448761\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9577/12000 =  79.81 % ||| loss 0.5048019289970398\u001b[0m\n",
            "\u001b[92mTest accuracy: 7859/10000 =  78.59 % ||| loss 0.5305438041687012\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.43969909369945526\n",
            "Batch #200 Loss: 0.4198186382651329\n",
            "Batch #300 Loss: 0.4201322193443775\n",
            "\u001b[92mTrain accuracy: 40595/48000 =  84.57 % ||| loss 0.40889063477516174\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10134/12000 =  84.45 % ||| loss 0.4264432191848755\u001b[0m\n",
            "\u001b[92mTest accuracy: 8313/10000 =  83.13 % ||| loss 0.45331940054893494\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.3800644472241402\n",
            "Batch #200 Loss: 0.3957859718799591\n",
            "Batch #300 Loss: 0.38983701467514037\n",
            "\u001b[92mTrain accuracy: 41245/48000 =  85.93 % ||| loss 0.3761761784553528\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10259/12000 =  85.49 % ||| loss 0.3932041823863983\u001b[0m\n",
            "\u001b[92mTest accuracy: 8404/10000 =  84.04 % ||| loss 0.43450450897216797\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.37486188650131225\n",
            "Batch #200 Loss: 0.36653931975364684\n",
            "Batch #300 Loss: 0.37056096822023393\n",
            "\u001b[92mTrain accuracy: 41285/48000 =  86.01 % ||| loss 0.36024606227874756\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10248/12000 =  85.4 % ||| loss 0.3817392587661743\u001b[0m\n",
            "\u001b[92mTest accuracy: 8433/10000 =  84.33 % ||| loss 0.41293832659721375\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.3518320748209953\n",
            "Batch #200 Loss: 0.37240623697638514\n",
            "Batch #300 Loss: 0.35369436383247377\n",
            "\u001b[92mTrain accuracy: 41721/48000 =  86.92 % ||| loss 0.34782207012176514\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10305/12000 =  85.88 % ||| loss 0.3860086500644684\u001b[0m\n",
            "\u001b[92mTest accuracy: 8516/10000 =  85.16 % ||| loss 0.40373116731643677\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3441226162016392\n",
            "Batch #200 Loss: 0.3491343668103218\n",
            "Batch #300 Loss: 0.34579252913594244\n",
            "\u001b[92mTrain accuracy: 42170/48000 =  87.85 % ||| loss 0.3197695016860962\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10408/12000 =  86.73 % ||| loss 0.3592260777950287\u001b[0m\n",
            "\u001b[92mTest accuracy: 8561/10000 =  85.61 % ||| loss 0.38781219720840454\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3340383119881153\n",
            "Batch #200 Loss: 0.34015731140971184\n",
            "Batch #300 Loss: 0.33996272578835485\n",
            "\u001b[92mTrain accuracy: 41846/48000 =  87.18 % ||| loss 0.3283507823944092\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10354/12000 =  86.28 % ||| loss 0.35312607884407043\u001b[0m\n",
            "\u001b[92mTest accuracy: 8506/10000 =  85.06 % ||| loss 0.3866184949874878\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3313179145753384\n",
            "Batch #200 Loss: 0.33209150567650797\n",
            "Batch #300 Loss: 0.3352194491028786\n",
            "\u001b[92mTrain accuracy: 41995/48000 =  87.49 % ||| loss 0.32792890071868896\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10317/12000 =  85.97 % ||| loss 0.3797552287578583\u001b[0m\n",
            "\u001b[92mTest accuracy: 8546/10000 =  85.46 % ||| loss 0.4037463068962097\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3473024667799473\n",
            "Batch #200 Loss: 0.31955218240618705\n",
            "Batch #300 Loss: 0.3275398935377598\n",
            "\u001b[92mTrain accuracy: 42014/48000 =  87.53 % ||| loss 0.31783029437065125\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10356/12000 =  86.3 % ||| loss 0.35896241664886475\u001b[0m\n",
            "\u001b[92mTest accuracy: 8555/10000 =  85.55 % ||| loss 0.3913474977016449\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.3190282866358757\n",
            "Batch #200 Loss: 0.30973296269774436\n",
            "Batch #300 Loss: 0.3252993434667587\n",
            "\u001b[92mTrain accuracy: 42254/48000 =  88.03 % ||| loss 0.31077855825424194\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10384/12000 =  86.53 % ||| loss 0.3718997836112976\u001b[0m\n",
            "\u001b[92mTest accuracy: 8588/10000 =  85.88 % ||| loss 0.39682161808013916\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3071019184589386\n",
            "Batch #200 Loss: 0.3121385721862316\n",
            "Batch #300 Loss: 0.326456768065691\n",
            "\u001b[92mTrain accuracy: 42593/48000 =  88.74 % ||| loss 0.2949244976043701\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10409/12000 =  86.74 % ||| loss 0.35464149713516235\u001b[0m\n",
            "\u001b[92mTest accuracy: 8599/10000 =  85.99 % ||| loss 0.3770286440849304\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.2946480232477188\n",
            "Batch #200 Loss: 0.3156426233053207\n",
            "Batch #300 Loss: 0.31799350053071973\n",
            "\u001b[92mTrain accuracy: 42486/48000 =  88.51 % ||| loss 0.2965147793292999\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10420/12000 =  86.83 % ||| loss 0.3519928753376007\u001b[0m\n",
            "\u001b[92mTest accuracy: 8598/10000 =  85.98 % ||| loss 0.3801118731498718\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.2973217694461346\n",
            "Batch #200 Loss: 0.31467644453048704\n",
            "Batch #300 Loss: 0.31810967400670054\n",
            "\u001b[92mTrain accuracy: 42606/48000 =  88.76 % ||| loss 0.29484233260154724\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10390/12000 =  86.58 % ||| loss 0.38125061988830566\u001b[0m\n",
            "\u001b[92mTest accuracy: 8568/10000 =  85.68 % ||| loss 0.4094189405441284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3020308691263199\n",
            "Batch #200 Loss: 0.2963571095466614\n",
            "Batch #300 Loss: 0.30484838992357255\n",
            "\u001b[92mTrain accuracy: 42372/48000 =  88.28 % ||| loss 0.3015705347061157\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10370/12000 =  86.42 % ||| loss 0.37991026043891907\u001b[0m\n",
            "\u001b[92mTest accuracy: 8590/10000 =  85.9 % ||| loss 0.401080459356308\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.28972297951579096\n",
            "Batch #200 Loss: 0.29614668056368826\n",
            "Batch #300 Loss: 0.3069029642641544\n",
            "\u001b[92mTrain accuracy: 42706/48000 =  88.97 % ||| loss 0.2900735139846802\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10430/12000 =  86.92 % ||| loss 0.3767436742782593\u001b[0m\n",
            "\u001b[92mTest accuracy: 8576/10000 =  85.76 % ||| loss 0.40720415115356445\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.2899484631419182\n",
            "Batch #200 Loss: 0.3011729897558689\n",
            "Batch #300 Loss: 0.3034704650938511\n",
            "\u001b[92mTrain accuracy: 43045/48000 =  89.68 % ||| loss 0.26952865719795227\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10491/12000 =  87.42 % ||| loss 0.34363579750061035\u001b[0m\n",
            "\u001b[92mTest accuracy: 8657/10000 =  86.57 % ||| loss 0.37360286712646484\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.29537356436252593\n",
            "Batch #200 Loss: 0.28866503536701205\n",
            "Batch #300 Loss: 0.2881924730539322\n",
            "\u001b[92mTrain accuracy: 42947/48000 =  89.47 % ||| loss 0.2769533693790436\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10402/12000 =  86.68 % ||| loss 0.3681826591491699\u001b[0m\n",
            "\u001b[92mTest accuracy: 8585/10000 =  85.85 % ||| loss 0.3842844069004059\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.27960142120718956\n",
            "Batch #200 Loss: 0.28731523469090464\n",
            "Batch #300 Loss: 0.29661446794867513\n",
            "\u001b[92mTrain accuracy: 42879/48000 =  89.33 % ||| loss 0.2750270664691925\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10404/12000 =  86.7 % ||| loss 0.38704022765159607\u001b[0m\n",
            "\u001b[92mTest accuracy: 8577/10000 =  85.77 % ||| loss 0.4024634063243866\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.27543736904859545\n",
            "Batch #200 Loss: 0.2773374916613102\n",
            "Batch #300 Loss: 0.29296933218836785\n",
            "\u001b[92mTrain accuracy: 42889/48000 =  89.35 % ||| loss 0.27570340037345886\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10421/12000 =  86.84 % ||| loss 0.3685496747493744\u001b[0m\n",
            "\u001b[92mTest accuracy: 8653/10000 =  86.53 % ||| loss 0.3885263204574585\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.2767421795427799\n",
            "Batch #200 Loss: 0.27930104941129685\n",
            "Batch #300 Loss: 0.3109843884408474\n",
            "\u001b[92mTrain accuracy: 42941/48000 =  89.46 % ||| loss 0.27690666913986206\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10434/12000 =  86.95 % ||| loss 0.37021493911743164\u001b[0m\n",
            "\u001b[92mTest accuracy: 8647/10000 =  86.47 % ||| loss 0.3953168988227844\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.27193585991859437\n",
            "Batch #200 Loss: 0.2865388540923595\n",
            "Batch #300 Loss: 0.29553468197584154\n",
            "\u001b[92mTrain accuracy: 42672/48000 =  88.9 % ||| loss 0.2883008122444153\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10378/12000 =  86.48 % ||| loss 0.3840782344341278\u001b[0m\n",
            "\u001b[92mTest accuracy: 8576/10000 =  85.76 % ||| loss 0.4193096458911896\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.27593317314982413\n",
            "Batch #200 Loss: 0.28186234563589097\n",
            "Batch #300 Loss: 0.29938493177294734\n",
            "\u001b[92mTrain accuracy: 43405/48000 =  90.43 % ||| loss 0.24857710301876068\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10483/12000 =  87.36 % ||| loss 0.36723342537879944\u001b[0m\n",
            "\u001b[92mTest accuracy: 8709/10000 =  87.09 % ||| loss 0.38028833270072937\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.2578737823665142\n",
            "Batch #200 Loss: 0.2911914487183094\n",
            "Batch #300 Loss: 0.28183972418308256\n",
            "\u001b[92mTrain accuracy: 42981/48000 =  89.54 % ||| loss 0.2694302499294281\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10448/12000 =  87.07 % ||| loss 0.35955092310905457\u001b[0m\n",
            "\u001b[92mTest accuracy: 8620/10000 =  86.2 % ||| loss 0.3868315517902374\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.2603770519793034\n",
            "Batch #200 Loss: 0.27809848234057427\n",
            "Batch #300 Loss: 0.2773039501905441\n",
            "\u001b[92mTrain accuracy: 43179/48000 =  89.96 % ||| loss 0.27556928992271423\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10403/12000 =  86.69 % ||| loss 0.41889145970344543\u001b[0m\n",
            "\u001b[92mTest accuracy: 8611/10000 =  86.11 % ||| loss 0.45669904351234436\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726144302.4821298_1</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_1</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_083651-Lenet5_1726144302.4821298_2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_2' target=\"_blank\">Lenet5_1726144302.4821298_2</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.139003130197525\n",
            "Batch #200 Loss: 0.7907817295193672\n",
            "Batch #300 Loss: 0.5870957428216934\n",
            "\u001b[92mTrain accuracy: 39800/48000 =  82.92 % ||| loss 0.4635690450668335\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9948/12000 =  82.9 % ||| loss 0.4642657935619354\u001b[0m\n",
            "\u001b[92mTest accuracy: 8249/10000 =  82.49 % ||| loss 0.4822894334793091\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.462113262116909\n",
            "Batch #200 Loss: 0.43351308882236483\n",
            "Batch #300 Loss: 0.41307995408773424\n",
            "\u001b[92mTrain accuracy: 41532/48000 =  86.52 % ||| loss 0.35977619886398315\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10366/12000 =  86.38 % ||| loss 0.3685227930545807\u001b[0m\n",
            "\u001b[92mTest accuracy: 8551/10000 =  85.51 % ||| loss 0.38956785202026367\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.371801472902298\n",
            "Batch #200 Loss: 0.3499149803817272\n",
            "Batch #300 Loss: 0.35503616750240324\n",
            "\u001b[92mTrain accuracy: 42234/48000 =  87.99 % ||| loss 0.31851932406425476\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10447/12000 =  87.06 % ||| loss 0.3390870690345764\u001b[0m\n",
            "\u001b[92mTest accuracy: 8659/10000 =  86.59 % ||| loss 0.36417368054389954\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.31935915306210516\n",
            "Batch #200 Loss: 0.32621765151619914\n",
            "Batch #300 Loss: 0.32316996723413466\n",
            "\u001b[92mTrain accuracy: 43035/48000 =  89.66 % ||| loss 0.28248926997184753\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10622/12000 =  88.52 % ||| loss 0.30452200770378113\u001b[0m\n",
            "\u001b[92mTest accuracy: 8785/10000 =  87.85 % ||| loss 0.3374756872653961\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.3021026100218296\n",
            "Batch #200 Loss: 0.2928426876664162\n",
            "Batch #300 Loss: 0.3039282487332821\n",
            "\u001b[92mTrain accuracy: 42907/48000 =  89.39 % ||| loss 0.2861804962158203\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10619/12000 =  88.49 % ||| loss 0.3097441792488098\u001b[0m\n",
            "\u001b[92mTest accuracy: 8751/10000 =  87.51 % ||| loss 0.33938899636268616\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.2886636333167553\n",
            "Batch #200 Loss: 0.28428572803735735\n",
            "Batch #300 Loss: 0.2742320483922958\n",
            "\u001b[92mTrain accuracy: 43235/48000 =  90.07 % ||| loss 0.2677392363548279\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10658/12000 =  88.82 % ||| loss 0.30425938963890076\u001b[0m\n",
            "\u001b[92mTest accuracy: 8825/10000 =  88.25 % ||| loss 0.3322035074234009\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.2745433467626572\n",
            "Batch #200 Loss: 0.26637887448072434\n",
            "Batch #300 Loss: 0.2647131636738777\n",
            "\u001b[92mTrain accuracy: 42913/48000 =  89.4 % ||| loss 0.2851400673389435\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10608/12000 =  88.4 % ||| loss 0.3187384605407715\u001b[0m\n",
            "\u001b[92mTest accuracy: 8770/10000 =  87.7 % ||| loss 0.3418140709400177\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.2504777233302593\n",
            "Batch #200 Loss: 0.2548424877226353\n",
            "Batch #300 Loss: 0.259412227421999\n",
            "\u001b[92mTrain accuracy: 43654/48000 =  90.95 % ||| loss 0.24244846403598785\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10717/12000 =  89.31 % ||| loss 0.2863195836544037\u001b[0m\n",
            "\u001b[92mTest accuracy: 8893/10000 =  88.93 % ||| loss 0.3094586730003357\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.24574793234467507\n",
            "Batch #200 Loss: 0.24106424778699875\n",
            "Batch #300 Loss: 0.2488310830295086\n",
            "\u001b[92mTrain accuracy: 44031/48000 =  91.73 % ||| loss 0.22234593331813812\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10769/12000 =  89.74 % ||| loss 0.2709275186061859\u001b[0m\n",
            "\u001b[92mTest accuracy: 8932/10000 =  89.32 % ||| loss 0.2944335639476776\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.2256905435025692\n",
            "Batch #200 Loss: 0.2370800191909075\n",
            "Batch #300 Loss: 0.25679585680365563\n",
            "\u001b[92mTrain accuracy: 44123/48000 =  91.92 % ||| loss 0.22118684649467468\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10764/12000 =  89.7 % ||| loss 0.28175756335258484\u001b[0m\n",
            "\u001b[92mTest accuracy: 8913/10000 =  89.13 % ||| loss 0.3101261556148529\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.21897470876574515\n",
            "Batch #200 Loss: 0.2426948281377554\n",
            "Batch #300 Loss: 0.2216711413115263\n",
            "\u001b[92mTrain accuracy: 44212/48000 =  92.11 % ||| loss 0.20981565117835999\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10762/12000 =  89.68 % ||| loss 0.2761017084121704\u001b[0m\n",
            "\u001b[92mTest accuracy: 8914/10000 =  89.14 % ||| loss 0.30178841948509216\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.2226306325942278\n",
            "Batch #200 Loss: 0.21914575412869453\n",
            "Batch #300 Loss: 0.22607552729547023\n",
            "\u001b[92mTrain accuracy: 44341/48000 =  92.38 % ||| loss 0.20498861372470856\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10783/12000 =  89.86 % ||| loss 0.2757362425327301\u001b[0m\n",
            "\u001b[92mTest accuracy: 8933/10000 =  89.33 % ||| loss 0.2978161573410034\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.20443240202963353\n",
            "Batch #200 Loss: 0.21706064119935037\n",
            "Batch #300 Loss: 0.22393300250172615\n",
            "\u001b[92mTrain accuracy: 44543/48000 =  92.8 % ||| loss 0.1943698674440384\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10813/12000 =  90.11 % ||| loss 0.2709546983242035\u001b[0m\n",
            "\u001b[92mTest accuracy: 8974/10000 =  89.74 % ||| loss 0.29459795355796814\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.20905956938862802\n",
            "Batch #200 Loss: 0.21176591977477074\n",
            "Batch #300 Loss: 0.2078699616342783\n",
            "\u001b[92mTrain accuracy: 44544/48000 =  92.8 % ||| loss 0.19116318225860596\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10808/12000 =  90.07 % ||| loss 0.2806383967399597\u001b[0m\n",
            "\u001b[92mTest accuracy: 8950/10000 =  89.5 % ||| loss 0.3081205487251282\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.1882719746977091\n",
            "Batch #200 Loss: 0.19930568099021911\n",
            "Batch #300 Loss: 0.20514805048704146\n",
            "\u001b[92mTrain accuracy: 44220/48000 =  92.12 % ||| loss 0.20344853401184082\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10702/12000 =  89.18 % ||| loss 0.28683385252952576\u001b[0m\n",
            "\u001b[92mTest accuracy: 8866/10000 =  88.66 % ||| loss 0.31279778480529785\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.19559124790132046\n",
            "Batch #200 Loss: 0.19608262442052365\n",
            "Batch #300 Loss: 0.19761127404868603\n",
            "\u001b[92mTrain accuracy: 44453/48000 =  92.61 % ||| loss 0.19391325116157532\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10732/12000 =  89.43 % ||| loss 0.29283732175827026\u001b[0m\n",
            "\u001b[92mTest accuracy: 8927/10000 =  89.27 % ||| loss 0.3113035261631012\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.18701667085289955\n",
            "Batch #200 Loss: 0.19088471584022046\n",
            "Batch #300 Loss: 0.18168543674051763\n",
            "\u001b[92mTrain accuracy: 45018/48000 =  93.79 % ||| loss 0.16772373020648956\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10836/12000 =  90.3 % ||| loss 0.2755942940711975\u001b[0m\n",
            "\u001b[92mTest accuracy: 8963/10000 =  89.63 % ||| loss 0.2995454668998718\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.17172500871121885\n",
            "Batch #200 Loss: 0.18866549544036387\n",
            "Batch #300 Loss: 0.182297108694911\n",
            "\u001b[92mTrain accuracy: 44955/48000 =  93.66 % ||| loss 0.1714410036802292\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10792/12000 =  89.93 % ||| loss 0.2923978567123413\u001b[0m\n",
            "\u001b[92mTest accuracy: 8919/10000 =  89.19 % ||| loss 0.31328749656677246\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.16940550141036512\n",
            "Batch #200 Loss: 0.1709228990972042\n",
            "Batch #300 Loss: 0.19318403139710427\n",
            "\u001b[92mTrain accuracy: 45104/48000 =  93.97 % ||| loss 0.16022615134716034\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10767/12000 =  89.72 % ||| loss 0.302686482667923\u001b[0m\n",
            "\u001b[92mTest accuracy: 8940/10000 =  89.4 % ||| loss 0.33532777428627014\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.1698550935089588\n",
            "Batch #200 Loss: 0.19462258975952865\n",
            "Batch #300 Loss: 0.1784775859862566\n",
            "\u001b[92mTrain accuracy: 45156/48000 =  94.08 % ||| loss 0.15598946809768677\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10774/12000 =  89.78 % ||| loss 0.3052867650985718\u001b[0m\n",
            "\u001b[92mTest accuracy: 8933/10000 =  89.33 % ||| loss 0.32694047689437866\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.15929740950465202\n",
            "Batch #200 Loss: 0.1755678217858076\n",
            "Batch #300 Loss: 0.18796760581433772\n",
            "\u001b[92mTrain accuracy: 45243/48000 =  94.26 % ||| loss 0.15341773629188538\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10803/12000 =  90.03 % ||| loss 0.2996465265750885\u001b[0m\n",
            "\u001b[92mTest accuracy: 8978/10000 =  89.78 % ||| loss 0.33069226145744324\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.1530577975511551\n",
            "Batch #200 Loss: 0.18080670326948167\n",
            "Batch #300 Loss: 0.16348732884973288\n",
            "\u001b[92mTrain accuracy: 45202/48000 =  94.17 % ||| loss 0.1530846655368805\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10743/12000 =  89.53 % ||| loss 0.33231598138809204\u001b[0m\n",
            "\u001b[92mTest accuracy: 8905/10000 =  89.05 % ||| loss 0.3671221435070038\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.15100689250975846\n",
            "Batch #200 Loss: 0.16275924682617188\n",
            "Batch #300 Loss: 0.16064491726458072\n",
            "\u001b[92mTrain accuracy: 45530/48000 =  94.85 % ||| loss 0.1382564753293991\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10811/12000 =  90.09 % ||| loss 0.30328747630119324\u001b[0m\n",
            "\u001b[92mTest accuracy: 8932/10000 =  89.32 % ||| loss 0.3420373797416687\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.1453199175745249\n",
            "Batch #200 Loss: 0.15460287027060984\n",
            "Batch #300 Loss: 0.16415850538760424\n",
            "\u001b[92mTrain accuracy: 45688/48000 =  95.18 % ||| loss 0.13046430051326752\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10803/12000 =  90.03 % ||| loss 0.3156472146511078\u001b[0m\n",
            "\u001b[92mTest accuracy: 8971/10000 =  89.71 % ||| loss 0.3540138900279999\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.14637841783463956\n",
            "Batch #200 Loss: 0.14656502477824687\n",
            "Batch #300 Loss: 0.14872648507356645\n",
            "\u001b[92mTrain accuracy: 45292/48000 =  94.36 % ||| loss 0.14541348814964294\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10695/12000 =  89.12 % ||| loss 0.3486112952232361\u001b[0m\n",
            "\u001b[92mTest accuracy: 8874/10000 =  88.74 % ||| loss 0.3781408667564392\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726144302.4821298_2</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_2</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_083924-Lenet5_1726144302.4821298_3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_3' target=\"_blank\">Lenet5_1726144302.4821298_3</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302996973991394\n",
            "Batch #200 Loss: 2.3020989990234373\n",
            "Batch #300 Loss: 2.300459723472595\n",
            "\u001b[92mTrain accuracy: 9045/48000 =  18.84 % ||| loss 2.2985422611236572\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2255/12000 =  18.79 % ||| loss 2.298779249191284\u001b[0m\n",
            "\u001b[92mTest accuracy: 1879/10000 =  18.79 % ||| loss 2.2987043857574463\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.297253427505493\n",
            "Batch #200 Loss: 2.2947173953056335\n",
            "Batch #300 Loss: 2.2887642002105713\n",
            "\u001b[92mTrain accuracy: 16926/48000 =  35.26 % ||| loss 2.2739078998565674\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4243/12000 =  35.36 % ||| loss 2.2740230560302734\u001b[0m\n",
            "\u001b[92mTest accuracy: 3542/10000 =  35.42 % ||| loss 2.2740442752838135\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.2585627841949463\n",
            "Batch #200 Loss: 2.151235772371292\n",
            "Batch #300 Loss: 1.5694635677337647\n",
            "\u001b[92mTrain accuracy: 28282/48000 =  58.92 % ||| loss 1.052807092666626\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7108/12000 =  59.23 % ||| loss 1.0443592071533203\u001b[0m\n",
            "\u001b[92mTest accuracy: 5863/10000 =  58.63 % ||| loss 1.0611950159072876\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.0223396921157837\n",
            "Batch #200 Loss: 0.9651198238134384\n",
            "Batch #300 Loss: 0.9181885707378388\n",
            "\u001b[92mTrain accuracy: 33264/48000 =  69.3 % ||| loss 0.8268477916717529\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8351/12000 =  69.59 % ||| loss 0.8168417811393738\u001b[0m\n",
            "\u001b[92mTest accuracy: 6837/10000 =  68.37 % ||| loss 0.8448830246925354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.846333013176918\n",
            "Batch #200 Loss: 0.846874977350235\n",
            "Batch #300 Loss: 0.7877821856737137\n",
            "\u001b[92mTrain accuracy: 33689/48000 =  70.19 % ||| loss 0.7600595951080322\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8440/12000 =  70.33 % ||| loss 0.7480958104133606\u001b[0m\n",
            "\u001b[92mTest accuracy: 6915/10000 =  69.15 % ||| loss 0.7789228558540344\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.7660465830564499\n",
            "Batch #200 Loss: 0.7532234841585159\n",
            "Batch #300 Loss: 0.7405009472370148\n",
            "\u001b[92mTrain accuracy: 33500/48000 =  69.79 % ||| loss 0.7623718976974487\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8470/12000 =  70.58 % ||| loss 0.7526550889015198\u001b[0m\n",
            "\u001b[92mTest accuracy: 6904/10000 =  69.04 % ||| loss 0.7917026281356812\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.7150605338811874\n",
            "Batch #200 Loss: 0.6968562525510787\n",
            "Batch #300 Loss: 0.7004863294959068\n",
            "\u001b[92mTrain accuracy: 36421/48000 =  75.88 % ||| loss 0.6548033356666565\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9177/12000 =  76.48 % ||| loss 0.6469574570655823\u001b[0m\n",
            "\u001b[92mTest accuracy: 7534/10000 =  75.34 % ||| loss 0.6759612560272217\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.668756234049797\n",
            "Batch #200 Loss: 0.6704280045628548\n",
            "Batch #300 Loss: 0.6558939230442047\n",
            "\u001b[92mTrain accuracy: 36991/48000 =  77.06 % ||| loss 0.6180161237716675\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9252/12000 =  77.1 % ||| loss 0.6103620529174805\u001b[0m\n",
            "\u001b[92mTest accuracy: 7657/10000 =  76.57 % ||| loss 0.6363106966018677\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.648302056491375\n",
            "Batch #200 Loss: 0.6269730958342552\n",
            "Batch #300 Loss: 0.6325900679826737\n",
            "\u001b[92mTrain accuracy: 37415/48000 =  77.95 % ||| loss 0.6056222915649414\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9375/12000 =  78.12 % ||| loss 0.5990329384803772\u001b[0m\n",
            "\u001b[92mTest accuracy: 7656/10000 =  76.56 % ||| loss 0.6322076916694641\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.613588131070137\n",
            "Batch #200 Loss: 0.6129076933860779\n",
            "Batch #300 Loss: 0.597280743420124\n",
            "\u001b[92mTrain accuracy: 37600/48000 =  78.33 % ||| loss 0.5896378755569458\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9442/12000 =  78.68 % ||| loss 0.5851362347602844\u001b[0m\n",
            "\u001b[92mTest accuracy: 7712/10000 =  77.12 % ||| loss 0.6099398732185364\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.5870186012983322\n",
            "Batch #200 Loss: 0.5783939197659492\n",
            "Batch #300 Loss: 0.566675021648407\n",
            "\u001b[92mTrain accuracy: 38019/48000 =  79.21 % ||| loss 0.5638712048530579\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9546/12000 =  79.55 % ||| loss 0.5580711960792542\u001b[0m\n",
            "\u001b[92mTest accuracy: 7849/10000 =  78.49 % ||| loss 0.5894766449928284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5655528882145882\n",
            "Batch #200 Loss: 0.5641759431362152\n",
            "Batch #300 Loss: 0.5452549651265144\n",
            "\u001b[92mTrain accuracy: 38350/48000 =  79.9 % ||| loss 0.5507704615592957\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9616/12000 =  80.13 % ||| loss 0.5493778586387634\u001b[0m\n",
            "\u001b[92mTest accuracy: 7885/10000 =  78.85 % ||| loss 0.5773486495018005\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5448812025785447\n",
            "Batch #200 Loss: 0.5419815135002136\n",
            "Batch #300 Loss: 0.5404825913906097\n",
            "\u001b[92mTrain accuracy: 38196/48000 =  79.57 % ||| loss 0.5491352081298828\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9561/12000 =  79.67 % ||| loss 0.5426056385040283\u001b[0m\n",
            "\u001b[92mTest accuracy: 7829/10000 =  78.29 % ||| loss 0.5766915678977966\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.504450736939907\n",
            "Batch #200 Loss: 0.5349371859431267\n",
            "Batch #300 Loss: 0.5219361335039139\n",
            "\u001b[92mTrain accuracy: 38714/48000 =  80.65 % ||| loss 0.5113451480865479\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9674/12000 =  80.62 % ||| loss 0.5079231858253479\u001b[0m\n",
            "\u001b[92mTest accuracy: 7986/10000 =  79.86 % ||| loss 0.5352483987808228\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5062487182021141\n",
            "Batch #200 Loss: 0.5129305481910705\n",
            "Batch #300 Loss: 0.4978246399760246\n",
            "\u001b[92mTrain accuracy: 39638/48000 =  82.58 % ||| loss 0.48074010014533997\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9906/12000 =  82.55 % ||| loss 0.47914743423461914\u001b[0m\n",
            "\u001b[92mTest accuracy: 8133/10000 =  81.33 % ||| loss 0.5116597414016724\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.49011979341506956\n",
            "Batch #200 Loss: 0.48669875383377076\n",
            "Batch #300 Loss: 0.49066218972206116\n",
            "\u001b[92mTrain accuracy: 39622/48000 =  82.55 % ||| loss 0.4753015637397766\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9939/12000 =  82.83 % ||| loss 0.4724533259868622\u001b[0m\n",
            "\u001b[92mTest accuracy: 8151/10000 =  81.51 % ||| loss 0.5111788511276245\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.4841499671339989\n",
            "Batch #200 Loss: 0.4766878345608711\n",
            "Batch #300 Loss: 0.4798480448126793\n",
            "\u001b[92mTrain accuracy: 40181/48000 =  83.71 % ||| loss 0.45598873496055603\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10091/12000 =  84.09 % ||| loss 0.45446163415908813\u001b[0m\n",
            "\u001b[92mTest accuracy: 8264/10000 =  82.64 % ||| loss 0.48924580216407776\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.4713268327713013\n",
            "Batch #200 Loss: 0.4669479176402092\n",
            "Batch #300 Loss: 0.47575142711400986\n",
            "\u001b[92mTrain accuracy: 39860/48000 =  83.04 % ||| loss 0.4677993059158325\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9984/12000 =  83.2 % ||| loss 0.46952998638153076\u001b[0m\n",
            "\u001b[92mTest accuracy: 8199/10000 =  81.99 % ||| loss 0.4955196678638458\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.4570610934495926\n",
            "Batch #200 Loss: 0.46648751527071\n",
            "Batch #300 Loss: 0.45875461012125013\n",
            "\u001b[92mTrain accuracy: 39725/48000 =  82.76 % ||| loss 0.461081326007843\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9968/12000 =  83.07 % ||| loss 0.46009448170661926\u001b[0m\n",
            "\u001b[92mTest accuracy: 8164/10000 =  81.64 % ||| loss 0.49383193254470825\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.4513636291027069\n",
            "Batch #200 Loss: 0.4422387087345123\n",
            "Batch #300 Loss: 0.4569384902715683\n",
            "\u001b[92mTrain accuracy: 40068/48000 =  83.47 % ||| loss 0.44984138011932373\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10030/12000 =  83.58 % ||| loss 0.45056286454200745\u001b[0m\n",
            "\u001b[92mTest accuracy: 8238/10000 =  82.38 % ||| loss 0.48026859760284424\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.44808798730373384\n",
            "Batch #200 Loss: 0.4495417582988739\n",
            "Batch #300 Loss: 0.42532497853040696\n",
            "\u001b[92mTrain accuracy: 40692/48000 =  84.78 % ||| loss 0.42292895913124084\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10178/12000 =  84.82 % ||| loss 0.4278906285762787\u001b[0m\n",
            "\u001b[92mTest accuracy: 8396/10000 =  83.96 % ||| loss 0.4546619951725006\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.4358825287222862\n",
            "Batch #200 Loss: 0.43170529648661615\n",
            "Batch #300 Loss: 0.43019927620887755\n",
            "\u001b[92mTrain accuracy: 40729/48000 =  84.85 % ||| loss 0.4176371991634369\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10183/12000 =  84.86 % ||| loss 0.42441192269325256\u001b[0m\n",
            "\u001b[92mTest accuracy: 8384/10000 =  83.84 % ||| loss 0.4482790231704712\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.42016154631972313\n",
            "Batch #200 Loss: 0.4200593772530556\n",
            "Batch #300 Loss: 0.43113604485988616\n",
            "\u001b[92mTrain accuracy: 40791/48000 =  84.98 % ||| loss 0.4167947471141815\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10199/12000 =  84.99 % ||| loss 0.42447155714035034\u001b[0m\n",
            "\u001b[92mTest accuracy: 8382/10000 =  83.82 % ||| loss 0.4517113268375397\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.4168775996565819\n",
            "Batch #200 Loss: 0.4196638226509094\n",
            "Batch #300 Loss: 0.4207008561491966\n",
            "\u001b[92mTrain accuracy: 40770/48000 =  84.94 % ||| loss 0.412665456533432\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10186/12000 =  84.88 % ||| loss 0.4177015721797943\u001b[0m\n",
            "\u001b[92mTest accuracy: 8389/10000 =  83.89 % ||| loss 0.4485863447189331\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.40951106667518616\n",
            "Batch #200 Loss: 0.4167936795949936\n",
            "Batch #300 Loss: 0.4151982852816582\n",
            "\u001b[92mTrain accuracy: 40582/48000 =  84.55 % ||| loss 0.42081400752067566\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10129/12000 =  84.41 % ||| loss 0.4259679615497589\u001b[0m\n",
            "\u001b[92mTest accuracy: 8341/10000 =  83.41 % ||| loss 0.45212477445602417\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726144302.4821298_3</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_3</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_084203-Lenet5_1726144302.4821298_4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_4' target=\"_blank\">Lenet5_1726144302.4821298_4</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2384054040908814\n",
            "Batch #200 Loss: 1.0309255254268646\n",
            "Batch #300 Loss: 0.7662594252824784\n",
            "\u001b[92mTrain accuracy: 36319/48000 =  75.66 % ||| loss 0.6568754315376282\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9114/12000 =  75.95 % ||| loss 0.6491438150405884\u001b[0m\n",
            "\u001b[92mTest accuracy: 7479/10000 =  74.79 % ||| loss 0.6776103973388672\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6410941278934479\n",
            "Batch #200 Loss: 0.5894901832938194\n",
            "Batch #300 Loss: 0.5468637388944626\n",
            "\u001b[92mTrain accuracy: 39940/48000 =  83.21 % ||| loss 0.4660729467868805\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9987/12000 =  83.23 % ||| loss 0.465700626373291\u001b[0m\n",
            "\u001b[92mTest accuracy: 8199/10000 =  81.99 % ||| loss 0.4992970824241638\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5061547011137009\n",
            "Batch #200 Loss: 0.45895871132612226\n",
            "Batch #300 Loss: 0.44696549862623214\n",
            "\u001b[92mTrain accuracy: 40631/48000 =  84.65 % ||| loss 0.421658456325531\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10092/12000 =  84.1 % ||| loss 0.426880806684494\u001b[0m\n",
            "\u001b[92mTest accuracy: 8322/10000 =  83.22 % ||| loss 0.4557330012321472\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.42449414789676665\n",
            "Batch #200 Loss: 0.4068859615921974\n",
            "Batch #300 Loss: 0.39431230843067167\n",
            "\u001b[92mTrain accuracy: 40979/48000 =  85.37 % ||| loss 0.39663955569267273\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10205/12000 =  85.04 % ||| loss 0.4057197570800781\u001b[0m\n",
            "\u001b[92mTest accuracy: 8402/10000 =  84.02 % ||| loss 0.4363005459308624\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.38289112865924835\n",
            "Batch #200 Loss: 0.3690846765041351\n",
            "Batch #300 Loss: 0.37398056373000144\n",
            "\u001b[92mTrain accuracy: 41972/48000 =  87.44 % ||| loss 0.34377962350845337\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10463/12000 =  87.19 % ||| loss 0.35942479968070984\u001b[0m\n",
            "\u001b[92mTest accuracy: 8600/10000 =  86.0 % ||| loss 0.3841814696788788\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.35414375841617585\n",
            "Batch #200 Loss: 0.3493954201042652\n",
            "Batch #300 Loss: 0.35194297015666964\n",
            "\u001b[92mTrain accuracy: 42296/48000 =  88.12 % ||| loss 0.3236181437969208\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10474/12000 =  87.28 % ||| loss 0.3443518579006195\u001b[0m\n",
            "\u001b[92mTest accuracy: 8650/10000 =  86.5 % ||| loss 0.36782950162887573\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.33594161227345465\n",
            "Batch #200 Loss: 0.3350858049094677\n",
            "Batch #300 Loss: 0.30969528496265414\n",
            "\u001b[92mTrain accuracy: 42474/48000 =  88.49 % ||| loss 0.3135451376438141\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10530/12000 =  87.75 % ||| loss 0.33303576707839966\u001b[0m\n",
            "\u001b[92mTest accuracy: 8669/10000 =  86.69 % ||| loss 0.3581225574016571\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3217145240306854\n",
            "Batch #200 Loss: 0.312819601893425\n",
            "Batch #300 Loss: 0.3115364396572113\n",
            "\u001b[92mTrain accuracy: 42608/48000 =  88.77 % ||| loss 0.3017515242099762\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10553/12000 =  87.94 % ||| loss 0.3258747160434723\u001b[0m\n",
            "\u001b[92mTest accuracy: 8693/10000 =  86.93 % ||| loss 0.35709136724472046\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3003366534411907\n",
            "Batch #200 Loss: 0.29759020403027536\n",
            "Batch #300 Loss: 0.31243937149643897\n",
            "\u001b[92mTrain accuracy: 42991/48000 =  89.56 % ||| loss 0.28783780336380005\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10597/12000 =  88.31 % ||| loss 0.3171539306640625\u001b[0m\n",
            "\u001b[92mTest accuracy: 8776/10000 =  87.76 % ||| loss 0.3420953154563904\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.27890271693468094\n",
            "Batch #200 Loss: 0.29185850359499454\n",
            "Batch #300 Loss: 0.29720539584755895\n",
            "\u001b[92mTrain accuracy: 43089/48000 =  89.77 % ||| loss 0.2753768563270569\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10613/12000 =  88.44 % ||| loss 0.3171972930431366\u001b[0m\n",
            "\u001b[92mTest accuracy: 8813/10000 =  88.13 % ||| loss 0.3346530795097351\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.26494369506835935\n",
            "Batch #200 Loss: 0.2809406326711178\n",
            "Batch #300 Loss: 0.27694868311285975\n",
            "\u001b[92mTrain accuracy: 43101/48000 =  89.79 % ||| loss 0.27169036865234375\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10586/12000 =  88.22 % ||| loss 0.3137476146221161\u001b[0m\n",
            "\u001b[92mTest accuracy: 8797/10000 =  87.97 % ||| loss 0.3288532793521881\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.26377545729279517\n",
            "Batch #200 Loss: 0.2688090567290783\n",
            "Batch #300 Loss: 0.27305242136120794\n",
            "\u001b[92mTrain accuracy: 43524/48000 =  90.67 % ||| loss 0.2516816556453705\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10682/12000 =  89.02 % ||| loss 0.29879000782966614\u001b[0m\n",
            "\u001b[92mTest accuracy: 8844/10000 =  88.44 % ||| loss 0.32311421632766724\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.2589964993298054\n",
            "Batch #200 Loss: 0.2654099227488041\n",
            "Batch #300 Loss: 0.26358652859926224\n",
            "\u001b[92mTrain accuracy: 43384/48000 =  90.38 % ||| loss 0.2580198347568512\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10676/12000 =  88.97 % ||| loss 0.30222636461257935\u001b[0m\n",
            "\u001b[92mTest accuracy: 8833/10000 =  88.33 % ||| loss 0.3260948956012726\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.25525727480649946\n",
            "Batch #200 Loss: 0.24467080280184747\n",
            "Batch #300 Loss: 0.25493902191519735\n",
            "\u001b[92mTrain accuracy: 43849/48000 =  91.35 % ||| loss 0.23732614517211914\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10721/12000 =  89.34 % ||| loss 0.28644752502441406\u001b[0m\n",
            "\u001b[92mTest accuracy: 8904/10000 =  89.04 % ||| loss 0.3083496391773224\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.24418113484978676\n",
            "Batch #200 Loss: 0.2522475154697895\n",
            "Batch #300 Loss: 0.24865073643624783\n",
            "\u001b[92mTrain accuracy: 43994/48000 =  91.65 % ||| loss 0.22782376408576965\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10721/12000 =  89.34 % ||| loss 0.28634101152420044\u001b[0m\n",
            "\u001b[92mTest accuracy: 8873/10000 =  88.73 % ||| loss 0.30525389313697815\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.23920477494597436\n",
            "Batch #200 Loss: 0.23192188173532485\n",
            "Batch #300 Loss: 0.2399675791710615\n",
            "\u001b[92mTrain accuracy: 43795/48000 =  91.24 % ||| loss 0.23023533821105957\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10660/12000 =  88.83 % ||| loss 0.28682392835617065\u001b[0m\n",
            "\u001b[92mTest accuracy: 8835/10000 =  88.35 % ||| loss 0.30965131521224976\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.2293857405334711\n",
            "Batch #200 Loss: 0.23491388581693173\n",
            "Batch #300 Loss: 0.23698045998811723\n",
            "\u001b[92mTrain accuracy: 44176/48000 =  92.03 % ||| loss 0.21603567898273468\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10724/12000 =  89.37 % ||| loss 0.28097155690193176\u001b[0m\n",
            "\u001b[92mTest accuracy: 8910/10000 =  89.1 % ||| loss 0.3033296465873718\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.22408833362162114\n",
            "Batch #200 Loss: 0.22481824286282062\n",
            "Batch #300 Loss: 0.2256043115258217\n",
            "\u001b[92mTrain accuracy: 43697/48000 =  91.04 % ||| loss 0.23677514493465424\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10652/12000 =  88.77 % ||| loss 0.3081478476524353\u001b[0m\n",
            "\u001b[92mTest accuracy: 8845/10000 =  88.45 % ||| loss 0.3326689600944519\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.21495800644159316\n",
            "Batch #200 Loss: 0.2180986201018095\n",
            "Batch #300 Loss: 0.22062101736664771\n",
            "\u001b[92mTrain accuracy: 44322/48000 =  92.34 % ||| loss 0.20764365792274475\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10736/12000 =  89.47 % ||| loss 0.286526620388031\u001b[0m\n",
            "\u001b[92mTest accuracy: 8951/10000 =  89.51 % ||| loss 0.3090512454509735\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.20356749869883062\n",
            "Batch #200 Loss: 0.21772186882793904\n",
            "Batch #300 Loss: 0.2138558430969715\n",
            "\u001b[92mTrain accuracy: 44616/48000 =  92.95 % ||| loss 0.19141614437103271\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10798/12000 =  89.98 % ||| loss 0.2733765244483948\u001b[0m\n",
            "\u001b[92mTest accuracy: 8963/10000 =  89.63 % ||| loss 0.304629385471344\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.1943307226896286\n",
            "Batch #200 Loss: 0.21230963438749315\n",
            "Batch #300 Loss: 0.2186242063343525\n",
            "\u001b[92mTrain accuracy: 44745/48000 =  93.22 % ||| loss 0.18703092634677887\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10835/12000 =  90.29 % ||| loss 0.2705998420715332\u001b[0m\n",
            "\u001b[92mTest accuracy: 8984/10000 =  89.84 % ||| loss 0.2987914979457855\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.19309719726443292\n",
            "Batch #200 Loss: 0.20394479401409626\n",
            "Batch #300 Loss: 0.20437803454697132\n",
            "\u001b[92mTrain accuracy: 44821/48000 =  93.38 % ||| loss 0.18114729225635529\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10814/12000 =  90.12 % ||| loss 0.27249735593795776\u001b[0m\n",
            "\u001b[92mTest accuracy: 8948/10000 =  89.48 % ||| loss 0.29423627257347107\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.1963379518687725\n",
            "Batch #200 Loss: 0.19362167038023473\n",
            "Batch #300 Loss: 0.20026845641434193\n",
            "\u001b[92mTrain accuracy: 44878/48000 =  93.5 % ||| loss 0.17868772149085999\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10799/12000 =  89.99 % ||| loss 0.27494102716445923\u001b[0m\n",
            "\u001b[92mTest accuracy: 8962/10000 =  89.62 % ||| loss 0.3013380765914917\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.1816044234484434\n",
            "Batch #200 Loss: 0.1916197333484888\n",
            "Batch #300 Loss: 0.1873818876594305\n",
            "\u001b[92mTrain accuracy: 44823/48000 =  93.38 % ||| loss 0.17634470760822296\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10838/12000 =  90.32 % ||| loss 0.27839669585227966\u001b[0m\n",
            "\u001b[92mTest accuracy: 8941/10000 =  89.41 % ||| loss 0.30933934450149536\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.17384236238896847\n",
            "Batch #200 Loss: 0.1819905261695385\n",
            "Batch #300 Loss: 0.19205918446183204\n",
            "\u001b[92mTrain accuracy: 44984/48000 =  93.72 % ||| loss 0.17147845029830933\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10806/12000 =  90.05 % ||| loss 0.28086739778518677\u001b[0m\n",
            "\u001b[92mTest accuracy: 8950/10000 =  89.5 % ||| loss 0.30219411849975586\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726144302.4821298_4</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_4</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_084438-Lenet5_1726144302.4821298_5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_5' target=\"_blank\">Lenet5_1726144302.4821298_5</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2885247039794923\n",
            "Batch #200 Loss: 1.6475281244516373\n",
            "Batch #300 Loss: 0.9500290781259537\n",
            "\u001b[92mTrain accuracy: 32399/48000 =  67.5 % ||| loss 0.8390051126480103\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8160/12000 =  68.0 % ||| loss 0.8220757246017456\u001b[0m\n",
            "\u001b[92mTest accuracy: 6639/10000 =  66.39 % ||| loss 0.8528657555580139\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7983213675022125\n",
            "Batch #200 Loss: 0.7614836740493774\n",
            "Batch #300 Loss: 0.7318703085184097\n",
            "\u001b[92mTrain accuracy: 35641/48000 =  74.25 % ||| loss 0.6978323459625244\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8937/12000 =  74.48 % ||| loss 0.6789461374282837\u001b[0m\n",
            "\u001b[92mTest accuracy: 7318/10000 =  73.18 % ||| loss 0.7207396030426025\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6557262188196182\n",
            "Batch #200 Loss: 0.6379587137699128\n",
            "Batch #300 Loss: 0.6198430782556534\n",
            "\u001b[92mTrain accuracy: 37787/48000 =  78.72 % ||| loss 0.5595079660415649\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9484/12000 =  79.03 % ||| loss 0.5521568655967712\u001b[0m\n",
            "\u001b[92mTest accuracy: 7761/10000 =  77.61 % ||| loss 0.5831233263015747\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5926426607370376\n",
            "Batch #200 Loss: 0.5546584084630013\n",
            "Batch #300 Loss: 0.5495475113391877\n",
            "\u001b[92mTrain accuracy: 38966/48000 =  81.18 % ||| loss 0.5132476687431335\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9778/12000 =  81.48 % ||| loss 0.5084614157676697\u001b[0m\n",
            "\u001b[92mTest accuracy: 8009/10000 =  80.09 % ||| loss 0.5420132279396057\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5181369104981423\n",
            "Batch #200 Loss: 0.5067387932538986\n",
            "Batch #300 Loss: 0.5045463272929192\n",
            "\u001b[92mTrain accuracy: 39544/48000 =  82.38 % ||| loss 0.4848366975784302\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9857/12000 =  82.14 % ||| loss 0.48284026980400085\u001b[0m\n",
            "\u001b[92mTest accuracy: 8120/10000 =  81.2 % ||| loss 0.5166911482810974\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.46735850542783736\n",
            "Batch #200 Loss: 0.4728905341029167\n",
            "Batch #300 Loss: 0.46288244366645814\n",
            "\u001b[92mTrain accuracy: 40066/48000 =  83.47 % ||| loss 0.44943615794181824\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9932/12000 =  82.77 % ||| loss 0.45628097653388977\u001b[0m\n",
            "\u001b[92mTest accuracy: 8253/10000 =  82.53 % ||| loss 0.4772733747959137\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.45779301404953004\n",
            "Batch #200 Loss: 0.4468351289629936\n",
            "Batch #300 Loss: 0.4285342484712601\n",
            "\u001b[92mTrain accuracy: 40706/48000 =  84.8 % ||| loss 0.4159619212150574\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10158/12000 =  84.65 % ||| loss 0.42148351669311523\u001b[0m\n",
            "\u001b[92mTest accuracy: 8393/10000 =  83.93 % ||| loss 0.446037620306015\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.41764472037553785\n",
            "Batch #200 Loss: 0.41986911118030545\n",
            "Batch #300 Loss: 0.40290709227323535\n",
            "\u001b[92mTrain accuracy: 41118/48000 =  85.66 % ||| loss 0.3946067690849304\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10261/12000 =  85.51 % ||| loss 0.40325161814689636\u001b[0m\n",
            "\u001b[92mTest accuracy: 8464/10000 =  84.64 % ||| loss 0.4277350902557373\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.4003572592139244\n",
            "Batch #200 Loss: 0.38965146273374557\n",
            "Batch #300 Loss: 0.3862679573893547\n",
            "\u001b[92mTrain accuracy: 41200/48000 =  85.83 % ||| loss 0.38238000869750977\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10277/12000 =  85.64 % ||| loss 0.3951181471347809\u001b[0m\n",
            "\u001b[92mTest accuracy: 8465/10000 =  84.65 % ||| loss 0.4181421101093292\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.38418744191527365\n",
            "Batch #200 Loss: 0.3812507672607899\n",
            "Batch #300 Loss: 0.3690531072020531\n",
            "\u001b[92mTrain accuracy: 41387/48000 =  86.22 % ||| loss 0.3786557912826538\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10274/12000 =  85.62 % ||| loss 0.3934858739376068\u001b[0m\n",
            "\u001b[92mTest accuracy: 8491/10000 =  84.91 % ||| loss 0.4131000339984894\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.35975433588027955\n",
            "Batch #200 Loss: 0.35553609028458594\n",
            "Batch #300 Loss: 0.37027196735143664\n",
            "\u001b[92mTrain accuracy: 41856/48000 =  87.2 % ||| loss 0.3564509153366089\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10389/12000 =  86.58 % ||| loss 0.373984158039093\u001b[0m\n",
            "\u001b[92mTest accuracy: 8579/10000 =  85.79 % ||| loss 0.39662495255470276\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.35380389213562013\n",
            "Batch #200 Loss: 0.36064390376210215\n",
            "Batch #300 Loss: 0.34136235386133196\n",
            "\u001b[92mTrain accuracy: 41961/48000 =  87.42 % ||| loss 0.344439297914505\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10355/12000 =  86.29 % ||| loss 0.36065182089805603\u001b[0m\n",
            "\u001b[92mTest accuracy: 8573/10000 =  85.73 % ||| loss 0.3876531720161438\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.34640511125326157\n",
            "Batch #200 Loss: 0.34451473772525787\n",
            "Batch #300 Loss: 0.3367900140583515\n",
            "\u001b[92mTrain accuracy: 42299/48000 =  88.12 % ||| loss 0.326692670583725\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10507/12000 =  87.56 % ||| loss 0.3439558446407318\u001b[0m\n",
            "\u001b[92mTest accuracy: 8663/10000 =  86.63 % ||| loss 0.3702099621295929\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3387679608166218\n",
            "Batch #200 Loss: 0.3348105998337269\n",
            "Batch #300 Loss: 0.3249884329736233\n",
            "\u001b[92mTrain accuracy: 42339/48000 =  88.21 % ||| loss 0.32147929072380066\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10498/12000 =  87.48 % ||| loss 0.342284232378006\u001b[0m\n",
            "\u001b[92mTest accuracy: 8677/10000 =  86.77 % ||| loss 0.3657553195953369\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3268375098705292\n",
            "Batch #200 Loss: 0.32340406864881516\n",
            "Batch #300 Loss: 0.3194564738869667\n",
            "\u001b[92mTrain accuracy: 42520/48000 =  88.58 % ||| loss 0.3148353695869446\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10552/12000 =  87.93 % ||| loss 0.33748555183410645\u001b[0m\n",
            "\u001b[92mTest accuracy: 8709/10000 =  87.09 % ||| loss 0.3635044991970062\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3226365818083286\n",
            "Batch #200 Loss: 0.3229238210618496\n",
            "Batch #300 Loss: 0.31294642880558965\n",
            "\u001b[92mTrain accuracy: 42942/48000 =  89.46 % ||| loss 0.29326871037483215\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10608/12000 =  88.4 % ||| loss 0.3195551633834839\u001b[0m\n",
            "\u001b[92mTest accuracy: 8792/10000 =  87.92 % ||| loss 0.3478446900844574\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.30300829365849496\n",
            "Batch #200 Loss: 0.3179185888171196\n",
            "Batch #300 Loss: 0.3110489208996296\n",
            "\u001b[92mTrain accuracy: 42834/48000 =  89.24 % ||| loss 0.2952462434768677\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10573/12000 =  88.11 % ||| loss 0.3228376805782318\u001b[0m\n",
            "\u001b[92mTest accuracy: 8776/10000 =  87.76 % ||| loss 0.3474140167236328\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3033673046529293\n",
            "Batch #200 Loss: 0.30546688452363013\n",
            "Batch #300 Loss: 0.30634398594498635\n",
            "\u001b[92mTrain accuracy: 42849/48000 =  89.27 % ||| loss 0.2954988479614258\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10553/12000 =  87.94 % ||| loss 0.32952681183815\u001b[0m\n",
            "\u001b[92mTest accuracy: 8741/10000 =  87.41 % ||| loss 0.3517287075519562\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.30062588676810265\n",
            "Batch #200 Loss: 0.30104806154966357\n",
            "Batch #300 Loss: 0.2913032640516758\n",
            "\u001b[92mTrain accuracy: 43085/48000 =  89.76 % ||| loss 0.2831031382083893\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10655/12000 =  88.79 % ||| loss 0.31374940276145935\u001b[0m\n",
            "\u001b[92mTest accuracy: 8801/10000 =  88.01 % ||| loss 0.3370482623577118\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.2953673738241196\n",
            "Batch #200 Loss: 0.28949373319745064\n",
            "Batch #300 Loss: 0.29615799263119696\n",
            "\u001b[92mTrain accuracy: 42900/48000 =  89.38 % ||| loss 0.2913990914821625\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10599/12000 =  88.33 % ||| loss 0.32340580224990845\u001b[0m\n",
            "\u001b[92mTest accuracy: 8786/10000 =  87.86 % ||| loss 0.3491172194480896\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.28208171427249906\n",
            "Batch #200 Loss: 0.28195987820625307\n",
            "Batch #300 Loss: 0.2869732168316841\n",
            "\u001b[92mTrain accuracy: 43352/48000 =  90.32 % ||| loss 0.26739799976348877\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10657/12000 =  88.81 % ||| loss 0.30408111214637756\u001b[0m\n",
            "\u001b[92mTest accuracy: 8838/10000 =  88.38 % ||| loss 0.3342757821083069\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.2759981536865234\n",
            "Batch #200 Loss: 0.27623868569731713\n",
            "Batch #300 Loss: 0.29147442236542703\n",
            "\u001b[92mTrain accuracy: 43209/48000 =  90.02 % ||| loss 0.2698448598384857\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10629/12000 =  88.58 % ||| loss 0.30883362889289856\u001b[0m\n",
            "\u001b[92mTest accuracy: 8784/10000 =  87.84 % ||| loss 0.3355623483657837\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.2638470211625099\n",
            "Batch #200 Loss: 0.2797249658405781\n",
            "Batch #300 Loss: 0.2833132436871529\n",
            "\u001b[92mTrain accuracy: 43175/48000 =  89.95 % ||| loss 0.2712673246860504\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10686/12000 =  89.05 % ||| loss 0.3068648874759674\u001b[0m\n",
            "\u001b[92mTest accuracy: 8796/10000 =  87.96 % ||| loss 0.3342091739177704\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.26707826778292654\n",
            "Batch #200 Loss: 0.27765779361128806\n",
            "Batch #300 Loss: 0.27387573271989823\n",
            "\u001b[92mTrain accuracy: 43503/48000 =  90.63 % ||| loss 0.2580724358558655\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10669/12000 =  88.91 % ||| loss 0.300640732049942\u001b[0m\n",
            "\u001b[92mTest accuracy: 8823/10000 =  88.23 % ||| loss 0.32659608125686646\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.26581354677677155\n",
            "Batch #200 Loss: 0.26475090682506563\n",
            "Batch #300 Loss: 0.2694983235001564\n",
            "\u001b[92mTrain accuracy: 43194/48000 =  89.99 % ||| loss 0.2697233557701111\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10617/12000 =  88.48 % ||| loss 0.31201305985450745\u001b[0m\n",
            "\u001b[92mTest accuracy: 8772/10000 =  87.72 % ||| loss 0.33652782440185547\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726144302.4821298_5</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_5</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_084710-Lenet5_1726144302.4821298_6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_6' target=\"_blank\">Lenet5_1726144302.4821298_6</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.304381251335144\n",
            "Batch #200 Loss: 2.303034288883209\n",
            "Batch #300 Loss: 2.302363917827606\n",
            "\u001b[92mTrain accuracy: 4871/48000 =  10.15 % ||| loss 2.302396535873413\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1186/12000 =  9.883 % ||| loss 2.302698850631714\u001b[0m\n",
            "\u001b[92mTest accuracy: 1010/10000 =  10.1 % ||| loss 2.3025519847869873\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.303188631534576\n",
            "Batch #200 Loss: 2.3020843386650087\n",
            "Batch #300 Loss: 2.30074688911438\n",
            "\u001b[92mTrain accuracy: 4841/48000 =  10.09 % ||| loss 2.3008041381835938\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1181/12000 =  9.842 % ||| loss 2.3011176586151123\u001b[0m\n",
            "\u001b[92mTest accuracy: 1003/10000 =  10.03 % ||| loss 2.3009033203125\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.301806664466858\n",
            "Batch #200 Loss: 2.299688138961792\n",
            "Batch #300 Loss: 2.2995978498458864\n",
            "\u001b[92mTrain accuracy: 4838/48000 =  10.08 % ||| loss 2.2990667819976807\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1179/12000 =  9.825 % ||| loss 2.299351930618286\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2992801666259766\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.298560206890106\n",
            "Batch #200 Loss: 2.2981429052352906\n",
            "Batch #300 Loss: 2.297584099769592\n",
            "\u001b[92mTrain accuracy: 4843/48000 =  10.09 % ||| loss 2.2970948219299316\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.297369956970215\u001b[0m\n",
            "\u001b[92mTest accuracy: 1003/10000 =  10.03 % ||| loss 2.297311305999756\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.296727523803711\n",
            "Batch #200 Loss: 2.2961398935317994\n",
            "Batch #300 Loss: 2.29525475025177\n",
            "\u001b[92mTrain accuracy: 4874/48000 =  10.15 % ||| loss 2.2948405742645264\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.2951338291168213\u001b[0m\n",
            "\u001b[92mTest accuracy: 1010/10000 =  10.1 % ||| loss 2.2946619987487793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.2945139312744143\n",
            "Batch #200 Loss: 2.294449107646942\n",
            "Batch #300 Loss: 2.2927746510505678\n",
            "\u001b[92mTrain accuracy: 4995/48000 =  10.41 % ||| loss 2.2922768592834473\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1230/12000 =  10.25 % ||| loss 2.2925570011138916\u001b[0m\n",
            "\u001b[92mTest accuracy: 1042/10000 =  10.42 % ||| loss 2.2923946380615234\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.2916137552261353\n",
            "Batch #200 Loss: 2.2911087679862976\n",
            "Batch #300 Loss: 2.2905130648612975\n",
            "\u001b[92mTrain accuracy: 5597/48000 =  11.66 % ||| loss 2.2890937328338623\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1374/12000 =  11.45 % ||| loss 2.2894127368927\u001b[0m\n",
            "\u001b[92mTest accuracy: 1161/10000 =  11.61 % ||| loss 2.289280652999878\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.288485515117645\n",
            "Batch #200 Loss: 2.2875752377510072\n",
            "Batch #300 Loss: 2.286383104324341\n",
            "\u001b[92mTrain accuracy: 7528/48000 =  15.68 % ||| loss 2.2849435806274414\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1840/12000 =  15.33 % ||| loss 2.285292148590088\u001b[0m\n",
            "\u001b[92mTest accuracy: 1562/10000 =  15.62 % ||| loss 2.28491473197937\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.284321970939636\n",
            "Batch #200 Loss: 2.2829812026023864\n",
            "Batch #300 Loss: 2.2821144676208496\n",
            "\u001b[92mTrain accuracy: 9090/48000 =  18.94 % ||| loss 2.279670476913452\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2248/12000 =  18.73 % ||| loss 2.280066728591919\u001b[0m\n",
            "\u001b[92mTest accuracy: 1891/10000 =  18.91 % ||| loss 2.2796974182128906\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.278670320510864\n",
            "Batch #200 Loss: 2.276929273605347\n",
            "Batch #300 Loss: 2.2744104409217836\n",
            "\u001b[92mTrain accuracy: 9860/48000 =  20.54 % ||| loss 2.2726550102233887\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2425/12000 =  20.21 % ||| loss 2.2731001377105713\u001b[0m\n",
            "\u001b[92mTest accuracy: 2052/10000 =  20.52 % ||| loss 2.273015022277832\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.2711748099327087\n",
            "Batch #200 Loss: 2.26917619228363\n",
            "Batch #300 Loss: 2.26643100976944\n",
            "\u001b[92mTrain accuracy: 10066/48000 =  20.97 % ||| loss 2.262516736984253\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2474/12000 =  20.62 % ||| loss 2.2630419731140137\u001b[0m\n",
            "\u001b[92mTest accuracy: 2107/10000 =  21.07 % ||| loss 2.262587785720825\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.261040904521942\n",
            "Batch #200 Loss: 2.2565001893043517\n",
            "Batch #300 Loss: 2.2529728031158447\n",
            "\u001b[92mTrain accuracy: 10252/48000 =  21.36 % ||| loss 2.2466938495635986\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2531/12000 =  21.09 % ||| loss 2.2474076747894287\u001b[0m\n",
            "\u001b[92mTest accuracy: 2121/10000 =  21.21 % ||| loss 2.246805191040039\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.2438499188423155\n",
            "Batch #200 Loss: 2.2376129388809205\n",
            "Batch #300 Loss: 2.2310951566696167\n",
            "\u001b[92mTrain accuracy: 10515/48000 =  21.91 % ||| loss 2.2204749584198\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2631/12000 =  21.93 % ||| loss 2.221461772918701\u001b[0m\n",
            "\u001b[92mTest accuracy: 2156/10000 =  21.56 % ||| loss 2.2209973335266113\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.2153520345687867\n",
            "Batch #200 Loss: 2.204616889953613\n",
            "Batch #300 Loss: 2.1913767194747926\n",
            "\u001b[92mTrain accuracy: 12201/48000 =  25.42 % ||| loss 2.1703953742980957\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3030/12000 =  25.25 % ||| loss 2.1718664169311523\u001b[0m\n",
            "\u001b[92mTest accuracy: 2519/10000 =  25.19 % ||| loss 2.17075252532959\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.1612227272987368\n",
            "Batch #200 Loss: 2.13704341173172\n",
            "Batch #300 Loss: 2.1102349972724914\n",
            "\u001b[92mTrain accuracy: 15390/48000 =  32.06 % ||| loss 2.063981056213379\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3845/12000 =  32.04 % ||| loss 2.066176176071167\u001b[0m\n",
            "\u001b[92mTest accuracy: 3168/10000 =  31.68 % ||| loss 2.0644843578338623\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.039655760526657\n",
            "Batch #200 Loss: 1.9873161387443543\n",
            "Batch #300 Loss: 1.9238312089443206\n",
            "\u001b[92mTrain accuracy: 19141/48000 =  39.88 % ||| loss 1.8180087804794312\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4748/12000 =  39.57 % ||| loss 1.8206716775894165\u001b[0m\n",
            "\u001b[92mTest accuracy: 3936/10000 =  39.36 % ||| loss 1.8182339668273926\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 1.77290314912796\n",
            "Batch #200 Loss: 1.6722148191928863\n",
            "Batch #300 Loss: 1.5659882140159607\n",
            "\u001b[92mTrain accuracy: 26237/48000 =  54.66 % ||| loss 1.4484202861785889\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6500/12000 =  54.17 % ||| loss 1.4487138986587524\u001b[0m\n",
            "\u001b[92mTest accuracy: 5469/10000 =  54.69 % ||| loss 1.447964072227478\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 1.4037910568714143\n",
            "Batch #200 Loss: 1.3285399556159974\n",
            "Batch #300 Loss: 1.2513527584075927\n",
            "\u001b[92mTrain accuracy: 28244/48000 =  58.84 % ||| loss 1.1838427782058716\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7053/12000 =  58.77 % ||| loss 1.1811611652374268\u001b[0m\n",
            "\u001b[92mTest accuracy: 5835/10000 =  58.35 % ||| loss 1.1876945495605469\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 1.1613929605484008\n",
            "Batch #200 Loss: 1.12005157828331\n",
            "Batch #300 Loss: 1.081974542737007\n",
            "\u001b[92mTrain accuracy: 29421/48000 =  61.29 % ||| loss 1.0558160543441772\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7351/12000 =  61.26 % ||| loss 1.0505534410476685\u001b[0m\n",
            "\u001b[92mTest accuracy: 6074/10000 =  60.74 % ||| loss 1.061693549156189\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 1.0553489702939987\n",
            "Batch #200 Loss: 1.0182953292131425\n",
            "Batch #300 Loss: 1.0105865097045899\n",
            "\u001b[92mTrain accuracy: 29786/48000 =  62.05 % ||| loss 0.9942113161087036\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7471/12000 =  62.26 % ||| loss 0.9872943758964539\u001b[0m\n",
            "\u001b[92mTest accuracy: 6141/10000 =  61.41 % ||| loss 1.0029888153076172\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.9723467272520065\n",
            "Batch #200 Loss: 0.986054276227951\n",
            "Batch #300 Loss: 0.9674057710170746\n",
            "\u001b[92mTrain accuracy: 30679/48000 =  63.91 % ||| loss 0.9555147290229797\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7673/12000 =  63.94 % ||| loss 0.9481135606765747\u001b[0m\n",
            "\u001b[92mTest accuracy: 6303/10000 =  63.03 % ||| loss 0.9682449102401733\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.959428015947342\n",
            "Batch #200 Loss: 0.9340086299180984\n",
            "Batch #300 Loss: 0.932506730556488\n",
            "\u001b[92mTrain accuracy: 31135/48000 =  64.86 % ||| loss 0.9298902750015259\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7790/12000 =  64.92 % ||| loss 0.9228075742721558\u001b[0m\n",
            "\u001b[92mTest accuracy: 6424/10000 =  64.24 % ||| loss 0.9420214295387268\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.9184723669290542\n",
            "Batch #200 Loss: 0.9170135295391083\n",
            "Batch #300 Loss: 0.9136949962377549\n",
            "\u001b[92mTrain accuracy: 31522/48000 =  65.67 % ||| loss 0.909591555595398\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7904/12000 =  65.87 % ||| loss 0.9023314118385315\u001b[0m\n",
            "\u001b[92mTest accuracy: 6476/10000 =  64.76 % ||| loss 0.9254262447357178\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.9014001429080963\n",
            "Batch #200 Loss: 0.8908372461795807\n",
            "Batch #300 Loss: 0.8995595341920852\n",
            "\u001b[92mTrain accuracy: 32038/48000 =  66.75 % ||| loss 0.8875044584274292\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8074/12000 =  67.28 % ||| loss 0.8789696097373962\u001b[0m\n",
            "\u001b[92mTest accuracy: 6591/10000 =  65.91 % ||| loss 0.9090983867645264\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.8980328452587127\n",
            "Batch #200 Loss: 0.8671914768218995\n",
            "Batch #300 Loss: 0.8782956981658936\n",
            "\u001b[92mTrain accuracy: 32223/48000 =  67.13 % ||| loss 0.8730067610740662\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8078/12000 =  67.32 % ||| loss 0.8648203611373901\u001b[0m\n",
            "\u001b[92mTest accuracy: 6637/10000 =  66.37 % ||| loss 0.8953649997711182\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726144302.4821298_6</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_6</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_084945-Lenet5_1726144302.4821298_7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_7' target=\"_blank\">Lenet5_1726144302.4821298_7</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3018883204460145\n",
            "Batch #200 Loss: 2.296651201248169\n",
            "Batch #300 Loss: 2.289111773967743\n",
            "\u001b[92mTrain accuracy: 5742/48000 =  11.96 % ||| loss 2.270095109939575\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1466/12000 =  12.22 % ||| loss 2.2703986167907715\u001b[0m\n",
            "\u001b[92mTest accuracy: 1210/10000 =  12.1 % ||| loss 2.2702040672302246\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.251362645626068\n",
            "Batch #200 Loss: 2.1438792181015014\n",
            "Batch #300 Loss: 1.6076992964744568\n",
            "\u001b[92mTrain accuracy: 28982/48000 =  60.38 % ||| loss 1.0274797677993774\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7248/12000 =  60.4 % ||| loss 1.0210497379302979\u001b[0m\n",
            "\u001b[92mTest accuracy: 5992/10000 =  59.92 % ||| loss 1.0353279113769531\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.9949083453416825\n",
            "Batch #200 Loss: 0.9306860435009002\n",
            "Batch #300 Loss: 0.8865194946527482\n",
            "\u001b[92mTrain accuracy: 33405/48000 =  69.59 % ||| loss 0.8290403485298157\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8387/12000 =  69.89 % ||| loss 0.817973792552948\u001b[0m\n",
            "\u001b[92mTest accuracy: 6912/10000 =  69.12 % ||| loss 0.8399306535720825\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.8156698656082153\n",
            "Batch #200 Loss: 0.7944409501552582\n",
            "Batch #300 Loss: 0.7793043804168701\n",
            "\u001b[92mTrain accuracy: 33884/48000 =  70.59 % ||| loss 0.7610910534858704\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8504/12000 =  70.87 % ||| loss 0.750468373298645\u001b[0m\n",
            "\u001b[92mTest accuracy: 6998/10000 =  69.98 % ||| loss 0.7868321537971497\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.7464780360460281\n",
            "Batch #200 Loss: 0.728356659412384\n",
            "Batch #300 Loss: 0.7026716095209121\n",
            "\u001b[92mTrain accuracy: 35778/48000 =  74.54 % ||| loss 0.6927260756492615\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8944/12000 =  74.53 % ||| loss 0.6842954754829407\u001b[0m\n",
            "\u001b[92mTest accuracy: 7338/10000 =  73.38 % ||| loss 0.7105496525764465\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.6939794275164605\n",
            "Batch #200 Loss: 0.6684738034009934\n",
            "Batch #300 Loss: 0.6771162956953048\n",
            "\u001b[92mTrain accuracy: 36337/48000 =  75.7 % ||| loss 0.6485903263092041\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9139/12000 =  76.16 % ||| loss 0.638380229473114\u001b[0m\n",
            "\u001b[92mTest accuracy: 7485/10000 =  74.85 % ||| loss 0.6677750945091248\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.6495054849982261\n",
            "Batch #200 Loss: 0.6627634954452515\n",
            "Batch #300 Loss: 0.6399908819794655\n",
            "\u001b[92mTrain accuracy: 36260/48000 =  75.54 % ||| loss 0.6326061487197876\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9113/12000 =  75.94 % ||| loss 0.622211217880249\u001b[0m\n",
            "\u001b[92mTest accuracy: 7460/10000 =  74.6 % ||| loss 0.656924307346344\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.6326315274834633\n",
            "Batch #200 Loss: 0.6304095405340194\n",
            "Batch #300 Loss: 0.6099298119544982\n",
            "\u001b[92mTrain accuracy: 36852/48000 =  76.78 % ||| loss 0.6143448352813721\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9254/12000 =  77.12 % ||| loss 0.6073328256607056\u001b[0m\n",
            "\u001b[92mTest accuracy: 7562/10000 =  75.62 % ||| loss 0.6422590613365173\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.6186988171935082\n",
            "Batch #200 Loss: 0.5988718745112419\n",
            "Batch #300 Loss: 0.6036041298508644\n",
            "\u001b[92mTrain accuracy: 36263/48000 =  75.55 % ||| loss 0.6199836134910583\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9067/12000 =  75.56 % ||| loss 0.6157664060592651\u001b[0m\n",
            "\u001b[92mTest accuracy: 7473/10000 =  74.73 % ||| loss 0.6469776034355164\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.588194337785244\n",
            "Batch #200 Loss: 0.5681982001662255\n",
            "Batch #300 Loss: 0.578064514696598\n",
            "\u001b[92mTrain accuracy: 38262/48000 =  79.71 % ||| loss 0.5613075494766235\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9582/12000 =  79.85 % ||| loss 0.5562517046928406\u001b[0m\n",
            "\u001b[92mTest accuracy: 7850/10000 =  78.5 % ||| loss 0.5900188684463501\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.5646415323019027\n",
            "Batch #200 Loss: 0.5561333042383194\n",
            "Batch #300 Loss: 0.5581765267252922\n",
            "\u001b[92mTrain accuracy: 38306/48000 =  79.8 % ||| loss 0.5480007529258728\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9638/12000 =  80.32 % ||| loss 0.5395129323005676\u001b[0m\n",
            "\u001b[92mTest accuracy: 7864/10000 =  78.64 % ||| loss 0.5716120600700378\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5479462376236915\n",
            "Batch #200 Loss: 0.5452866873145104\n",
            "Batch #300 Loss: 0.5347185614705086\n",
            "\u001b[92mTrain accuracy: 39059/48000 =  81.37 % ||| loss 0.5134787559509277\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9814/12000 =  81.78 % ||| loss 0.5094639658927917\u001b[0m\n",
            "\u001b[92mTest accuracy: 8018/10000 =  80.18 % ||| loss 0.5410616993904114\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5324363034963607\n",
            "Batch #200 Loss: 0.5274602463841438\n",
            "Batch #300 Loss: 0.5021518778800964\n",
            "\u001b[92mTrain accuracy: 39049/48000 =  81.35 % ||| loss 0.5066372752189636\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9821/12000 =  81.84 % ||| loss 0.5050952434539795\u001b[0m\n",
            "\u001b[92mTest accuracy: 8013/10000 =  80.13 % ||| loss 0.5340700149536133\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.5014808696508407\n",
            "Batch #200 Loss: 0.5052647069096565\n",
            "Batch #300 Loss: 0.4997166100144386\n",
            "\u001b[92mTrain accuracy: 39772/48000 =  82.86 % ||| loss 0.48278242349624634\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9985/12000 =  83.21 % ||| loss 0.4804048240184784\u001b[0m\n",
            "\u001b[92mTest accuracy: 8178/10000 =  81.78 % ||| loss 0.5095000267028809\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.48318723529577257\n",
            "Batch #200 Loss: 0.48676908284425735\n",
            "Batch #300 Loss: 0.4951143327355385\n",
            "\u001b[92mTrain accuracy: 39860/48000 =  83.04 % ||| loss 0.4783841073513031\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9982/12000 =  83.18 % ||| loss 0.479559987783432\u001b[0m\n",
            "\u001b[92mTest accuracy: 8187/10000 =  81.87 % ||| loss 0.5037862658500671\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.4829187625646591\n",
            "Batch #200 Loss: 0.4624571391940117\n",
            "Batch #300 Loss: 0.47741619050502776\n",
            "\u001b[92mTrain accuracy: 40279/48000 =  83.91 % ||| loss 0.4609401822090149\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10089/12000 =  84.08 % ||| loss 0.4627409279346466\u001b[0m\n",
            "\u001b[92mTest accuracy: 8283/10000 =  82.83 % ||| loss 0.4891045391559601\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.4815631502866745\n",
            "Batch #200 Loss: 0.4654413577914238\n",
            "Batch #300 Loss: 0.44596570312976835\n",
            "\u001b[92mTrain accuracy: 40112/48000 =  83.57 % ||| loss 0.45612862706184387\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10040/12000 =  83.67 % ||| loss 0.4557105004787445\u001b[0m\n",
            "\u001b[92mTest accuracy: 8285/10000 =  82.85 % ||| loss 0.49105361104011536\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.4589913079142571\n",
            "Batch #200 Loss: 0.4470215344429016\n",
            "Batch #300 Loss: 0.4571757104992866\n",
            "\u001b[92mTrain accuracy: 39650/48000 =  82.6 % ||| loss 0.4694382846355438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9870/12000 =  82.25 % ||| loss 0.4720061719417572\u001b[0m\n",
            "\u001b[92mTest accuracy: 8137/10000 =  81.37 % ||| loss 0.4966115951538086\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.44204646468162534\n",
            "Batch #200 Loss: 0.4401418226957321\n",
            "Batch #300 Loss: 0.43178544342517855\n",
            "\u001b[92mTrain accuracy: 40470/48000 =  84.31 % ||| loss 0.4461216628551483\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10087/12000 =  84.06 % ||| loss 0.4523290991783142\u001b[0m\n",
            "\u001b[92mTest accuracy: 8336/10000 =  83.36 % ||| loss 0.4799940288066864\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.42733278185129164\n",
            "Batch #200 Loss: 0.4418508327007294\n",
            "Batch #300 Loss: 0.43705367743968965\n",
            "\u001b[92mTrain accuracy: 40651/48000 =  84.69 % ||| loss 0.4306766390800476\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10153/12000 =  84.61 % ||| loss 0.43574997782707214\u001b[0m\n",
            "\u001b[92mTest accuracy: 8360/10000 =  83.6 % ||| loss 0.46328088641166687\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.4366521945595741\n",
            "Batch #200 Loss: 0.42935384303331375\n",
            "Batch #300 Loss: 0.4190177662670612\n",
            "\u001b[92mTrain accuracy: 41026/48000 =  85.47 % ||| loss 0.41254568099975586\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10217/12000 =  85.14 % ||| loss 0.4185297191143036\u001b[0m\n",
            "\u001b[92mTest accuracy: 8431/10000 =  84.31 % ||| loss 0.44724002480506897\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.42081281036138535\n",
            "Batch #200 Loss: 0.42188122883439066\n",
            "Batch #300 Loss: 0.41471712306141856\n",
            "\u001b[92mTrain accuracy: 41236/48000 =  85.91 % ||| loss 0.3981553614139557\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10273/12000 =  85.61 % ||| loss 0.40515729784965515\u001b[0m\n",
            "\u001b[92mTest accuracy: 8508/10000 =  85.08 % ||| loss 0.4264489710330963\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.4097919370234013\n",
            "Batch #200 Loss: 0.40658874690532687\n",
            "Batch #300 Loss: 0.40708994537591936\n",
            "\u001b[92mTrain accuracy: 40888/48000 =  85.18 % ||| loss 0.40897342562675476\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10180/12000 =  84.83 % ||| loss 0.41666537523269653\u001b[0m\n",
            "\u001b[92mTest accuracy: 8411/10000 =  84.11 % ||| loss 0.4377698302268982\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.40175571501255036\n",
            "Batch #200 Loss: 0.39942906990647314\n",
            "Batch #300 Loss: 0.40846928358078005\n",
            "\u001b[92mTrain accuracy: 41113/48000 =  85.65 % ||| loss 0.3990609645843506\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10280/12000 =  85.67 % ||| loss 0.4049820899963379\u001b[0m\n",
            "\u001b[92mTest accuracy: 8478/10000 =  84.78 % ||| loss 0.4294202923774719\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.39457799449563025\n",
            "Batch #200 Loss: 0.3972416159510612\n",
            "Batch #300 Loss: 0.3923852489888668\n",
            "\u001b[92mTrain accuracy: 41147/48000 =  85.72 % ||| loss 0.39928576350212097\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10291/12000 =  85.76 % ||| loss 0.4051012396812439\u001b[0m\n",
            "\u001b[92mTest accuracy: 8434/10000 =  84.34 % ||| loss 0.4336676299571991\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726144302.4821298_7</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_7</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_085220-Lenet5_1726144302.4821298_8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_8' target=\"_blank\">Lenet5_1726144302.4821298_8</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5 XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3043522810935975\n",
            "Batch #200 Loss: 2.3025494194030762\n",
            "Batch #300 Loss: 2.3005655193328858\n",
            "\u001b[92mTrain accuracy: 6515/48000 =  13.57 % ||| loss 2.2996275424957275\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1620/12000 =  13.5 % ||| loss 2.2990212440490723\u001b[0m\n",
            "\u001b[92mTest accuracy: 1348/10000 =  13.48 % ||| loss 2.2996184825897217\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.29836389541626\n",
            "Batch #200 Loss: 2.2973652720451354\n",
            "Batch #300 Loss: 2.297163038253784\n",
            "\u001b[92mTrain accuracy: 6667/48000 =  13.89 % ||| loss 2.2942347526550293\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1679/12000 =  13.99 % ||| loss 2.2937238216400146\u001b[0m\n",
            "\u001b[92mTest accuracy: 1359/10000 =  13.59 % ||| loss 2.294356107711792\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.2931468200683596\n",
            "Batch #200 Loss: 2.291328172683716\n",
            "Batch #300 Loss: 2.2891672348976133\n",
            "\u001b[92mTrain accuracy: 9851/48000 =  20.52 % ||| loss 2.2853987216949463\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2461/12000 =  20.51 % ||| loss 2.2850043773651123\u001b[0m\n",
            "\u001b[92mTest accuracy: 2068/10000 =  20.68 % ||| loss 2.2854347229003906\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.2835908007621764\n",
            "Batch #200 Loss: 2.279143068790436\n",
            "Batch #300 Loss: 2.27439204454422\n",
            "\u001b[92mTrain accuracy: 13828/48000 =  28.81 % ||| loss 2.264662742614746\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3477/12000 =  28.98 % ||| loss 2.2643485069274902\u001b[0m\n",
            "\u001b[92mTest accuracy: 2816/10000 =  28.16 % ||| loss 2.2652013301849365\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.260034058094025\n",
            "Batch #200 Loss: 2.246009068489075\n",
            "Batch #300 Loss: 2.2252884101867676\n",
            "\u001b[92mTrain accuracy: 18056/48000 =  37.62 % ||| loss 2.1794815063476562\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4493/12000 =  37.44 % ||| loss 2.179105281829834\u001b[0m\n",
            "\u001b[92mTest accuracy: 3717/10000 =  37.17 % ||| loss 2.180640459060669\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.146580309867859\n",
            "Batch #200 Loss: 2.033091299533844\n",
            "Batch #300 Loss: 1.8056160485744477\n",
            "\u001b[92mTrain accuracy: 26540/48000 =  55.29 % ||| loss 1.3848867416381836\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6604/12000 =  55.03 % ||| loss 1.382148265838623\u001b[0m\n",
            "\u001b[92mTest accuracy: 5486/10000 =  54.86 % ||| loss 1.3911750316619873\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 1.2590579879283905\n",
            "Batch #200 Loss: 1.114133682847023\n",
            "Batch #300 Loss: 1.0344351905584335\n",
            "\u001b[92mTrain accuracy: 31009/48000 =  64.6 % ||| loss 0.9564559459686279\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7771/12000 =  64.76 % ||| loss 0.9487338066101074\u001b[0m\n",
            "\u001b[92mTest accuracy: 6388/10000 =  63.88 % ||| loss 0.968874990940094\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.9542719024419785\n",
            "Batch #200 Loss: 0.9121602243185043\n",
            "Batch #300 Loss: 0.8803672623634339\n",
            "\u001b[92mTrain accuracy: 32635/48000 =  67.99 % ||| loss 0.8547968864440918\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8200/12000 =  68.33 % ||| loss 0.8463533520698547\u001b[0m\n",
            "\u001b[92mTest accuracy: 6706/10000 =  67.06 % ||| loss 0.8720865249633789\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.8412427085638047\n",
            "Batch #200 Loss: 0.8354938507080079\n",
            "Batch #300 Loss: 0.8174090641736984\n",
            "\u001b[92mTrain accuracy: 33836/48000 =  70.49 % ||| loss 0.7962027788162231\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8496/12000 =  70.8 % ||| loss 0.7866769433021545\u001b[0m\n",
            "\u001b[92mTest accuracy: 6898/10000 =  68.98 % ||| loss 0.8242668509483337\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.7876168709993362\n",
            "Batch #200 Loss: 0.7770791906118393\n",
            "Batch #300 Loss: 0.7679592573642731\n",
            "\u001b[92mTrain accuracy: 34769/48000 =  72.44 % ||| loss 0.7517040371894836\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8710/12000 =  72.58 % ||| loss 0.7431614398956299\u001b[0m\n",
            "\u001b[92mTest accuracy: 7161/10000 =  71.61 % ||| loss 0.7701789140701294\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.7489691996574401\n",
            "Batch #200 Loss: 0.7444099831581116\n",
            "Batch #300 Loss: 0.7394382989406586\n",
            "\u001b[92mTrain accuracy: 35153/48000 =  73.24 % ||| loss 0.7201400399208069\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8858/12000 =  73.82 % ||| loss 0.7103877067565918\u001b[0m\n",
            "\u001b[92mTest accuracy: 7282/10000 =  72.82 % ||| loss 0.7409240007400513\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.7317891126871109\n",
            "Batch #200 Loss: 0.7114759880304337\n",
            "Batch #300 Loss: 0.7046484565734863\n",
            "\u001b[92mTrain accuracy: 34624/48000 =  72.13 % ||| loss 0.7125751972198486\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8671/12000 =  72.26 % ||| loss 0.7063674926757812\u001b[0m\n",
            "\u001b[92mTest accuracy: 7162/10000 =  71.62 % ||| loss 0.7303448915481567\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.6971752241253852\n",
            "Batch #200 Loss: 0.7023572844266891\n",
            "Batch #300 Loss: 0.6907437565922737\n",
            "\u001b[92mTrain accuracy: 35821/48000 =  74.63 % ||| loss 0.681962788105011\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8988/12000 =  74.9 % ||| loss 0.6746921539306641\u001b[0m\n",
            "\u001b[92mTest accuracy: 7417/10000 =  74.17 % ||| loss 0.7058086395263672\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.6878390476107598\n",
            "Batch #200 Loss: 0.6960301038622856\n",
            "Batch #300 Loss: 0.6807749140262603\n",
            "\u001b[92mTrain accuracy: 36028/48000 =  75.06 % ||| loss 0.6650733351707458\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9024/12000 =  75.2 % ||| loss 0.6585714817047119\u001b[0m\n",
            "\u001b[92mTest accuracy: 7439/10000 =  74.39 % ||| loss 0.6869174242019653\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.672445564866066\n",
            "Batch #200 Loss: 0.6792887073755264\n",
            "Batch #300 Loss: 0.6850287181138992\n",
            "\u001b[92mTrain accuracy: 35919/48000 =  74.83 % ||| loss 0.6670113801956177\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9043/12000 =  75.36 % ||| loss 0.6573054194450378\u001b[0m\n",
            "\u001b[92mTest accuracy: 7423/10000 =  74.23 % ||| loss 0.6916986107826233\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.6721644139289856\n",
            "Batch #200 Loss: 0.6610960748791694\n",
            "Batch #300 Loss: 0.658466169834137\n",
            "\u001b[92mTrain accuracy: 35949/48000 =  74.89 % ||| loss 0.6534560918807983\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9014/12000 =  75.12 % ||| loss 0.6463913321495056\u001b[0m\n",
            "\u001b[92mTest accuracy: 7409/10000 =  74.09 % ||| loss 0.6755203604698181\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.6579924726486206\n",
            "Batch #200 Loss: 0.6468534484505654\n",
            "Batch #300 Loss: 0.6542556783556939\n",
            "\u001b[92mTrain accuracy: 36593/48000 =  76.24 % ||| loss 0.6395168304443359\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9165/12000 =  76.38 % ||| loss 0.6310661435127258\u001b[0m\n",
            "\u001b[92mTest accuracy: 7553/10000 =  75.53 % ||| loss 0.6621332168579102\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.6474316820502282\n",
            "Batch #200 Loss: 0.6325809818506241\n",
            "Batch #300 Loss: 0.6459113478660583\n",
            "\u001b[92mTrain accuracy: 35912/48000 =  74.82 % ||| loss 0.6456187963485718\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9018/12000 =  75.15 % ||| loss 0.6403374075889587\u001b[0m\n",
            "\u001b[92mTest accuracy: 7432/10000 =  74.32 % ||| loss 0.6742928624153137\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.6409709730744362\n",
            "Batch #200 Loss: 0.6362017658352852\n",
            "Batch #300 Loss: 0.6196485054492951\n",
            "\u001b[92mTrain accuracy: 36624/48000 =  76.3 % ||| loss 0.6230077147483826\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9185/12000 =  76.54 % ||| loss 0.6122040152549744\u001b[0m\n",
            "\u001b[92mTest accuracy: 7554/10000 =  75.54 % ||| loss 0.6465660333633423\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.6238798469305038\n",
            "Batch #200 Loss: 0.6305718719959259\n",
            "Batch #300 Loss: 0.6128742179274559\n",
            "\u001b[92mTrain accuracy: 37257/48000 =  77.62 % ||| loss 0.609709620475769\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9324/12000 =  77.7 % ||| loss 0.603715717792511\u001b[0m\n",
            "\u001b[92mTest accuracy: 7685/10000 =  76.85 % ||| loss 0.6289670467376709\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.6080367973446846\n",
            "Batch #200 Loss: 0.6174502298235893\n",
            "Batch #300 Loss: 0.6103131133317947\n",
            "\u001b[92mTrain accuracy: 37396/48000 =  77.91 % ||| loss 0.5955681800842285\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9343/12000 =  77.86 % ||| loss 0.5892766118049622\u001b[0m\n",
            "\u001b[92mTest accuracy: 7709/10000 =  77.09 % ||| loss 0.6210498809814453\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.6083388853073121\n",
            "Batch #200 Loss: 0.607271793782711\n",
            "Batch #300 Loss: 0.592370833158493\n",
            "\u001b[92mTrain accuracy: 37499/48000 =  78.12 % ||| loss 0.5863497853279114\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9391/12000 =  78.26 % ||| loss 0.5796789526939392\u001b[0m\n",
            "\u001b[92mTest accuracy: 7742/10000 =  77.42 % ||| loss 0.6147228479385376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5840516224503517\n",
            "Batch #200 Loss: 0.5955403256416321\n",
            "Batch #300 Loss: 0.5898125854134559\n",
            "\u001b[92mTrain accuracy: 37394/48000 =  77.9 % ||| loss 0.5886974334716797\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9372/12000 =  78.1 % ||| loss 0.5795227885246277\u001b[0m\n",
            "\u001b[92mTest accuracy: 7677/10000 =  76.77 % ||| loss 0.6160502433776855\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5897257137298584\n",
            "Batch #200 Loss: 0.5988592502474784\n",
            "Batch #300 Loss: 0.5696646165847778\n",
            "\u001b[92mTrain accuracy: 37872/48000 =  78.9 % ||| loss 0.5802292227745056\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9482/12000 =  79.02 % ||| loss 0.571403443813324\u001b[0m\n",
            "\u001b[92mTest accuracy: 7790/10000 =  77.9 % ||| loss 0.6057315468788147\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.569252273440361\n",
            "Batch #200 Loss: 0.5704611504077911\n",
            "Batch #300 Loss: 0.5760978606343269\n",
            "\u001b[92mTrain accuracy: 37865/48000 =  78.89 % ||| loss 0.5598455667495728\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9474/12000 =  78.95 % ||| loss 0.5521639585494995\u001b[0m\n",
            "\u001b[92mTest accuracy: 7845/10000 =  78.45 % ||| loss 0.5864765048027039\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5_1726144302.4821298_8</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5_1726144302.4821298_8</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[93m!!!!!!! Hyper Param Tuning Finished!!!!!!!!!!!\u001b[0m\n",
            "Best Model: Lenet5(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "wandb name: Lenet5_1726144302.4821298_0\n",
            "\n",
            "HyperParams: {'learning_rate': 0.001, 'momentum': 0.7}\n",
            "\n",
            "Accuracies: {'train': 0.9404791666666666, 'val': 0.90375, 'test': 0.8988}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "param_grid = {\n",
        "  'learning_rate':[0.1, 0.01,0.001],\n",
        "  'momentum':[0, 0.9, 0.7]\n",
        "}\n",
        "\n",
        "best_lenet = hyperparameter_tuning(Lenet5, dataloaders, device, 25, **param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4pr-yptbLJT"
      },
      "source": [
        "### Variations on LeNet5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG5Q03J9bZV_"
      },
      "source": [
        "#### Using Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3rtOUZCKYWG",
        "outputId": "d7e23175-cc12-4be7-aef1-1a6ea6a0ee8c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_094153-Lenet5BN_1726148513.1655092_0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_0' target=\"_blank\">Lenet5BN_1726148513.1655092_0</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.0446658504009247\n",
            "Batch #200 Loss: 0.5739246925711632\n",
            "Batch #300 Loss: 0.49486792653799055\n",
            "\u001b[92mTrain accuracy: 40819/48000 =  85.04 % ||| loss 0.4092806875705719\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10192/12000 =  84.93 % ||| loss 0.4160032570362091\u001b[0m\n",
            "\u001b[92mTest accuracy: 8401/10000 =  84.01 % ||| loss 0.4360859990119934\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.43182781964540484\n",
            "Batch #200 Loss: 0.39138297483325\n",
            "Batch #300 Loss: 0.37900727197527884\n",
            "\u001b[92mTrain accuracy: 42016/48000 =  87.53 % ||| loss 0.33688291907310486\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10414/12000 =  86.78 % ||| loss 0.3442322313785553\u001b[0m\n",
            "\u001b[92mTest accuracy: 8635/10000 =  86.35 % ||| loss 0.3711235821247101\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.35437857642769816\n",
            "Batch #200 Loss: 0.34832215517759324\n",
            "Batch #300 Loss: 0.3294182221591473\n",
            "\u001b[92mTrain accuracy: 42690/48000 =  88.94 % ||| loss 0.30077528953552246\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10545/12000 =  87.88 % ||| loss 0.3188779950141907\u001b[0m\n",
            "\u001b[92mTest accuracy: 8735/10000 =  87.35 % ||| loss 0.34384098649024963\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.32043424040079116\n",
            "Batch #200 Loss: 0.3034465822577477\n",
            "Batch #300 Loss: 0.29735309809446336\n",
            "\u001b[92mTrain accuracy: 42785/48000 =  89.14 % ||| loss 0.29145383834838867\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10569/12000 =  88.08 % ||| loss 0.3165990710258484\u001b[0m\n",
            "\u001b[92mTest accuracy: 8724/10000 =  87.24 % ||| loss 0.3441857099533081\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.2766829323768616\n",
            "Batch #200 Loss: 0.28824862614274027\n",
            "Batch #300 Loss: 0.28684731155633925\n",
            "\u001b[92mTrain accuracy: 42726/48000 =  89.01 % ||| loss 0.29624056816101074\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10565/12000 =  88.04 % ||| loss 0.31920430064201355\u001b[0m\n",
            "\u001b[92mTest accuracy: 8748/10000 =  87.48 % ||| loss 0.3435263931751251\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.2641701099276543\n",
            "Batch #200 Loss: 0.27574092864990235\n",
            "Batch #300 Loss: 0.27143423929810523\n",
            "\u001b[92mTrain accuracy: 43122/48000 =  89.84 % ||| loss 0.26944002509117126\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10604/12000 =  88.37 % ||| loss 0.31059959530830383\u001b[0m\n",
            "\u001b[92mTest accuracy: 8802/10000 =  88.02 % ||| loss 0.33024269342422485\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.2500320018827915\n",
            "Batch #200 Loss: 0.2545806784182787\n",
            "Batch #300 Loss: 0.2597449146211147\n",
            "\u001b[92mTrain accuracy: 43795/48000 =  91.24 % ||| loss 0.24176041781902313\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10697/12000 =  89.14 % ||| loss 0.28574177622795105\u001b[0m\n",
            "\u001b[92mTest accuracy: 8911/10000 =  89.11 % ||| loss 0.3097495436668396\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.24034212581813336\n",
            "Batch #200 Loss: 0.23663602769374847\n",
            "Batch #300 Loss: 0.23845500826835633\n",
            "\u001b[92mTrain accuracy: 43910/48000 =  91.48 % ||| loss 0.23132653534412384\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10757/12000 =  89.64 % ||| loss 0.27196961641311646\u001b[0m\n",
            "\u001b[92mTest accuracy: 8876/10000 =  88.76 % ||| loss 0.30147790908813477\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.22974125444889068\n",
            "Batch #200 Loss: 0.23240313410758973\n",
            "Batch #300 Loss: 0.2362682322412729\n",
            "\u001b[92mTrain accuracy: 44245/48000 =  92.18 % ||| loss 0.21349160373210907\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10838/12000 =  90.32 % ||| loss 0.26147153973579407\u001b[0m\n",
            "\u001b[92mTest accuracy: 8950/10000 =  89.5 % ||| loss 0.29069244861602783\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.21565778627991677\n",
            "Batch #200 Loss: 0.22779748789966106\n",
            "Batch #300 Loss: 0.22203404851257802\n",
            "\u001b[92mTrain accuracy: 44384/48000 =  92.47 % ||| loss 0.20670320093631744\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10823/12000 =  90.19 % ||| loss 0.2590433359146118\u001b[0m\n",
            "\u001b[92mTest accuracy: 9011/10000 =  90.11 % ||| loss 0.2913018763065338\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.2090707326680422\n",
            "Batch #200 Loss: 0.21461854986846446\n",
            "Batch #300 Loss: 0.2116569269448519\n",
            "\u001b[92mTrain accuracy: 44374/48000 =  92.45 % ||| loss 0.19884754717350006\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10802/12000 =  90.02 % ||| loss 0.2601751387119293\u001b[0m\n",
            "\u001b[92mTest accuracy: 8947/10000 =  89.47 % ||| loss 0.2861626148223877\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.21106057576835155\n",
            "Batch #200 Loss: 0.20545212544500827\n",
            "Batch #300 Loss: 0.19169887386262416\n",
            "\u001b[92mTrain accuracy: 43733/48000 =  91.11 % ||| loss 0.22366878390312195\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10669/12000 =  88.91 % ||| loss 0.29305240511894226\u001b[0m\n",
            "\u001b[92mTest accuracy: 8803/10000 =  88.03 % ||| loss 0.3169567883014679\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.19830760501325131\n",
            "Batch #200 Loss: 0.18992853805422782\n",
            "Batch #300 Loss: 0.2032115323096514\n",
            "\u001b[92mTrain accuracy: 44900/48000 =  93.54 % ||| loss 0.17744244635105133\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10870/12000 =  90.58 % ||| loss 0.2528453767299652\u001b[0m\n",
            "\u001b[92mTest accuracy: 9047/10000 =  90.47 % ||| loss 0.27029427886009216\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.18322336465120315\n",
            "Batch #200 Loss: 0.19375636585056782\n",
            "Batch #300 Loss: 0.18348516121506692\n",
            "\u001b[92mTrain accuracy: 41692/48000 =  86.86 % ||| loss 0.3237914741039276\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10174/12000 =  84.78 % ||| loss 0.4037589728832245\u001b[0m\n",
            "\u001b[92mTest accuracy: 8411/10000 =  84.11 % ||| loss 0.43327173590660095\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.18818369016051292\n",
            "Batch #200 Loss: 0.17940978400409222\n",
            "Batch #300 Loss: 0.18588439330458642\n",
            "\u001b[92mTrain accuracy: 44400/48000 =  92.5 % ||| loss 0.1947641223669052\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10734/12000 =  89.45 % ||| loss 0.285549134016037\u001b[0m\n",
            "\u001b[92mTest accuracy: 8859/10000 =  88.59 % ||| loss 0.30930814146995544\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.16661526717245578\n",
            "Batch #200 Loss: 0.17381220184266566\n",
            "Batch #300 Loss: 0.18042864963412286\n",
            "\u001b[92mTrain accuracy: 45317/48000 =  94.41 % ||| loss 0.15582406520843506\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10860/12000 =  90.5 % ||| loss 0.2618138790130615\u001b[0m\n",
            "\u001b[92mTest accuracy: 9024/10000 =  90.24 % ||| loss 0.27844637632369995\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.15768583066761493\n",
            "Batch #200 Loss: 0.16460219241678714\n",
            "Batch #300 Loss: 0.18045066639781\n",
            "\u001b[92mTrain accuracy: 44648/48000 =  93.02 % ||| loss 0.18158306181430817\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10744/12000 =  89.53 % ||| loss 0.2885843813419342\u001b[0m\n",
            "\u001b[92mTest accuracy: 8890/10000 =  88.9 % ||| loss 0.3119083046913147\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.15642468236386775\n",
            "Batch #200 Loss: 0.1602141920477152\n",
            "Batch #300 Loss: 0.1769347757101059\n",
            "\u001b[92mTrain accuracy: 44006/48000 =  91.68 % ||| loss 0.21936658024787903\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10598/12000 =  88.32 % ||| loss 0.3360048532485962\u001b[0m\n",
            "\u001b[92mTest accuracy: 8775/10000 =  87.75 % ||| loss 0.36835554242134094\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.15216842167079447\n",
            "Batch #200 Loss: 0.1540156152099371\n",
            "Batch #300 Loss: 0.15606817536056042\n",
            "\u001b[92mTrain accuracy: 44718/48000 =  93.16 % ||| loss 0.1834079623222351\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10735/12000 =  89.46 % ||| loss 0.31067928671836853\u001b[0m\n",
            "\u001b[92mTest accuracy: 8875/10000 =  88.75 % ||| loss 0.3306659758090973\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.15483442895114422\n",
            "Batch #200 Loss: 0.14626091003417968\n",
            "Batch #300 Loss: 0.1527187564224005\n",
            "\u001b[92mTrain accuracy: 45216/48000 =  94.2 % ||| loss 0.15115131437778473\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10799/12000 =  89.99 % ||| loss 0.2892865538597107\u001b[0m\n",
            "\u001b[92mTest accuracy: 8981/10000 =  89.81 % ||| loss 0.3093467652797699\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.14077886767685413\n",
            "Batch #200 Loss: 0.1386290978640318\n",
            "Batch #300 Loss: 0.15468278724700213\n",
            "\u001b[92mTrain accuracy: 45505/48000 =  94.8 % ||| loss 0.13873086869716644\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10874/12000 =  90.62 % ||| loss 0.26583483815193176\u001b[0m\n",
            "\u001b[92mTest accuracy: 9000/10000 =  90.0 % ||| loss 0.2964603900909424\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.1309705127030611\n",
            "Batch #200 Loss: 0.13705498356372117\n",
            "Batch #300 Loss: 0.14884961806237698\n",
            "\u001b[92mTrain accuracy: 45365/48000 =  94.51 % ||| loss 0.14199353754520416\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10809/12000 =  90.08 % ||| loss 0.29405319690704346\u001b[0m\n",
            "\u001b[92mTest accuracy: 8970/10000 =  89.7 % ||| loss 0.3176548182964325\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.13011384665966033\n",
            "Batch #200 Loss: 0.13956929285079242\n",
            "Batch #300 Loss: 0.1372788853943348\n",
            "\u001b[92mTrain accuracy: 45811/48000 =  95.44 % ||| loss 0.12330438196659088\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10858/12000 =  90.48 % ||| loss 0.28610625863075256\u001b[0m\n",
            "\u001b[92mTest accuracy: 9008/10000 =  90.08 % ||| loss 0.3079993724822998\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.13065428514033556\n",
            "Batch #200 Loss: 0.12616847556084396\n",
            "Batch #300 Loss: 0.13517970409244298\n",
            "\u001b[92mTrain accuracy: 46045/48000 =  95.93 % ||| loss 0.10813934355974197\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10906/12000 =  90.88 % ||| loss 0.2800847291946411\u001b[0m\n",
            "\u001b[92mTest accuracy: 9023/10000 =  90.23 % ||| loss 0.30594944953918457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.11767004113644361\n",
            "Batch #200 Loss: 0.12215843699872493\n",
            "Batch #300 Loss: 0.12698796190321446\n",
            "\u001b[92mTrain accuracy: 45546/48000 =  94.89 % ||| loss 0.13190241158008575\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10785/12000 =  89.88 % ||| loss 0.3161813020706177\u001b[0m\n",
            "\u001b[92mTest accuracy: 8973/10000 =  89.73 % ||| loss 0.33312854170799255\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726148513.1655092_0</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_0</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_094443-Lenet5BN_1726148513.1655092_1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_1' target=\"_blank\">Lenet5BN_1726148513.1655092_1</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 0.8451352313160896\n",
            "Batch #200 Loss: 0.46304437577724455\n",
            "Batch #300 Loss: 0.40868780881166455\n",
            "\u001b[92mTrain accuracy: 41907/48000 =  87.31 % ||| loss 0.3383839428424835\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10422/12000 =  86.85 % ||| loss 0.35293886065483093\u001b[0m\n",
            "\u001b[92mTest accuracy: 8592/10000 =  85.92 % ||| loss 0.3677535355091095\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.3511903329193592\n",
            "Batch #200 Loss: 0.34771961227059367\n",
            "Batch #300 Loss: 0.31179230123758317\n",
            "\u001b[92mTrain accuracy: 42404/48000 =  88.34 % ||| loss 0.3180089592933655\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10526/12000 =  87.72 % ||| loss 0.33621835708618164\u001b[0m\n",
            "\u001b[92mTest accuracy: 8694/10000 =  86.94 % ||| loss 0.3626209795475006\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.2950274342298508\n",
            "Batch #200 Loss: 0.29506304770708086\n",
            "Batch #300 Loss: 0.2895484559237957\n",
            "\u001b[92mTrain accuracy: 43493/48000 =  90.61 % ||| loss 0.25390854477882385\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10739/12000 =  89.49 % ||| loss 0.2845628559589386\u001b[0m\n",
            "\u001b[92mTest accuracy: 8914/10000 =  89.14 % ||| loss 0.29827621579170227\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.2577541169524193\n",
            "Batch #200 Loss: 0.2769245430827141\n",
            "Batch #300 Loss: 0.2749777993559837\n",
            "\u001b[92mTrain accuracy: 42983/48000 =  89.55 % ||| loss 0.2820417881011963\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10604/12000 =  88.37 % ||| loss 0.3232261836528778\u001b[0m\n",
            "\u001b[92mTest accuracy: 8772/10000 =  87.72 % ||| loss 0.3396018445491791\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.2513100674748421\n",
            "Batch #200 Loss: 0.26358609050512316\n",
            "Batch #300 Loss: 0.25875221341848376\n",
            "\u001b[92mTrain accuracy: 42635/48000 =  88.82 % ||| loss 0.2787705957889557\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10490/12000 =  87.42 % ||| loss 0.329120934009552\u001b[0m\n",
            "\u001b[92mTest accuracy: 8650/10000 =  86.5 % ||| loss 0.3483104109764099\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.22857876770198346\n",
            "Batch #200 Loss: 0.24079484537243842\n",
            "Batch #300 Loss: 0.23747763462364674\n",
            "\u001b[92mTrain accuracy: 43405/48000 =  90.43 % ||| loss 0.24847379326820374\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10643/12000 =  88.69 % ||| loss 0.2979658246040344\u001b[0m\n",
            "\u001b[92mTest accuracy: 8850/10000 =  88.5 % ||| loss 0.32292360067367554\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.21937003426253796\n",
            "Batch #200 Loss: 0.2223826365172863\n",
            "Batch #300 Loss: 0.23170813247561456\n",
            "\u001b[92mTrain accuracy: 44430/48000 =  92.56 % ||| loss 0.19733475148677826\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10791/12000 =  89.92 % ||| loss 0.27642539143562317\u001b[0m\n",
            "\u001b[92mTest accuracy: 8985/10000 =  89.85 % ||| loss 0.29704609513282776\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.19808689922094344\n",
            "Batch #200 Loss: 0.21641404256224633\n",
            "Batch #300 Loss: 0.2251842852681875\n",
            "\u001b[92mTrain accuracy: 44596/48000 =  92.91 % ||| loss 0.18680055439472198\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10835/12000 =  90.29 % ||| loss 0.2640363276004791\u001b[0m\n",
            "\u001b[92mTest accuracy: 8994/10000 =  89.94 % ||| loss 0.2863503098487854\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.18286477535963058\n",
            "Batch #200 Loss: 0.19239169538021086\n",
            "Batch #300 Loss: 0.21084312617778778\n",
            "\u001b[92mTrain accuracy: 44767/48000 =  93.26 % ||| loss 0.17585140466690063\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10792/12000 =  89.93 % ||| loss 0.2714834213256836\u001b[0m\n",
            "\u001b[92mTest accuracy: 8959/10000 =  89.59 % ||| loss 0.28988316655158997\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.1843059555441141\n",
            "Batch #200 Loss: 0.18862403377890588\n",
            "Batch #300 Loss: 0.19201751701533795\n",
            "\u001b[92mTrain accuracy: 44443/48000 =  92.59 % ||| loss 0.19263753294944763\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10726/12000 =  89.38 % ||| loss 0.2972292900085449\u001b[0m\n",
            "\u001b[92mTest accuracy: 8907/10000 =  89.07 % ||| loss 0.3265322744846344\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.18333381496369838\n",
            "Batch #200 Loss: 0.18881948791444303\n",
            "Batch #300 Loss: 0.1816203824430704\n",
            "\u001b[92mTrain accuracy: 44985/48000 =  93.72 % ||| loss 0.16745631396770477\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10837/12000 =  90.31 % ||| loss 0.2858434319496155\u001b[0m\n",
            "\u001b[92mTest accuracy: 8984/10000 =  89.84 % ||| loss 0.32012543082237244\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.15822696357965468\n",
            "Batch #200 Loss: 0.17160040110349656\n",
            "Batch #300 Loss: 0.1787654995173216\n",
            "\u001b[92mTrain accuracy: 44637/48000 =  92.99 % ||| loss 0.18704406917095184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10758/12000 =  89.65 % ||| loss 0.3174486458301544\u001b[0m\n",
            "\u001b[92mTest accuracy: 8965/10000 =  89.65 % ||| loss 0.34570232033729553\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.1644395298138261\n",
            "Batch #200 Loss: 0.1650690434128046\n",
            "Batch #300 Loss: 0.1793749026209116\n",
            "\u001b[92mTrain accuracy: 44531/48000 =  92.77 % ||| loss 0.18627005815505981\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10720/12000 =  89.33 % ||| loss 0.3095625340938568\u001b[0m\n",
            "\u001b[92mTest accuracy: 8873/10000 =  88.73 % ||| loss 0.33991092443466187\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.14637170672416688\n",
            "Batch #200 Loss: 0.15214617643505335\n",
            "Batch #300 Loss: 0.1764817975834012\n",
            "\u001b[92mTrain accuracy: 45305/48000 =  94.39 % ||| loss 0.14540588855743408\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10830/12000 =  90.25 % ||| loss 0.2873399257659912\u001b[0m\n",
            "\u001b[92mTest accuracy: 8968/10000 =  89.68 % ||| loss 0.3085758686065674\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.13970872439444065\n",
            "Batch #200 Loss: 0.15381021104753018\n",
            "Batch #300 Loss: 0.14959939647465945\n",
            "\u001b[92mTrain accuracy: 45420/48000 =  94.62 % ||| loss 0.1431780755519867\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10786/12000 =  89.88 % ||| loss 0.3157859146595001\u001b[0m\n",
            "\u001b[92mTest accuracy: 8990/10000 =  89.9 % ||| loss 0.3283131420612335\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.13998383212834598\n",
            "Batch #200 Loss: 0.14706350296735762\n",
            "Batch #300 Loss: 0.14986707419157028\n",
            "\u001b[92mTrain accuracy: 45684/48000 =  95.17 % ||| loss 0.12967929244041443\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10840/12000 =  90.33 % ||| loss 0.3087020814418793\u001b[0m\n",
            "\u001b[92mTest accuracy: 8998/10000 =  89.98 % ||| loss 0.34192150831222534\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.14132359065115452\n",
            "Batch #200 Loss: 0.13719428285956384\n",
            "Batch #300 Loss: 0.1500446716323495\n",
            "\u001b[92mTrain accuracy: 45446/48000 =  94.68 % ||| loss 0.1393483728170395\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10774/12000 =  89.78 % ||| loss 0.329748272895813\u001b[0m\n",
            "\u001b[92mTest accuracy: 8966/10000 =  89.66 % ||| loss 0.35476207733154297\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.12474393859505653\n",
            "Batch #200 Loss: 0.14175459776073696\n",
            "Batch #300 Loss: 0.1448282477259636\n",
            "\u001b[92mTrain accuracy: 45819/48000 =  95.46 % ||| loss 0.12002933770418167\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10868/12000 =  90.57 % ||| loss 0.30224162340164185\u001b[0m\n",
            "\u001b[92mTest accuracy: 8982/10000 =  89.82 % ||| loss 0.3227256238460541\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.1311260575428605\n",
            "Batch #200 Loss: 0.1264393711835146\n",
            "Batch #300 Loss: 0.1291999826580286\n",
            "\u001b[92mTrain accuracy: 45649/48000 =  95.1 % ||| loss 0.1304958462715149\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10783/12000 =  89.86 % ||| loss 0.35803142189979553\u001b[0m\n",
            "\u001b[92mTest accuracy: 8991/10000 =  89.91 % ||| loss 0.39046284556388855\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.11691243961453437\n",
            "Batch #200 Loss: 0.12201716683804989\n",
            "Batch #300 Loss: 0.12205764822661877\n",
            "\u001b[92mTrain accuracy: 45693/48000 =  95.19 % ||| loss 0.12512066960334778\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10822/12000 =  90.18 % ||| loss 0.34922754764556885\u001b[0m\n",
            "\u001b[92mTest accuracy: 8960/10000 =  89.6 % ||| loss 0.3726147413253784\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.12509382175281644\n",
            "Batch #200 Loss: 0.12487181888893246\n",
            "Batch #300 Loss: 0.13306665182113647\n",
            "\u001b[92mTrain accuracy: 45785/48000 =  95.39 % ||| loss 0.12070045620203018\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10776/12000 =  89.8 % ||| loss 0.33867907524108887\u001b[0m\n",
            "\u001b[92mTest accuracy: 8975/10000 =  89.75 % ||| loss 0.36491671204566956\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.1068437037244439\n",
            "Batch #200 Loss: 0.12458305057138204\n",
            "Batch #300 Loss: 0.11776836216449738\n",
            "\u001b[92mTrain accuracy: 45891/48000 =  95.61 % ||| loss 0.11347775161266327\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10797/12000 =  89.98 % ||| loss 0.3563458025455475\u001b[0m\n",
            "\u001b[92mTest accuracy: 8990/10000 =  89.9 % ||| loss 0.37048643827438354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.10704981030896306\n",
            "Batch #200 Loss: 0.10807668227702379\n",
            "Batch #300 Loss: 0.11265766471624375\n",
            "\u001b[92mTrain accuracy: 45351/48000 =  94.48 % ||| loss 0.14053544402122498\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10691/12000 =  89.09 % ||| loss 0.3871608078479767\u001b[0m\n",
            "\u001b[92mTest accuracy: 8879/10000 =  88.79 % ||| loss 0.4123307168483734\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.11173293970525265\n",
            "Batch #200 Loss: 0.10440806530416012\n",
            "Batch #300 Loss: 0.11727856878191233\n",
            "\u001b[92mTrain accuracy: 46180/48000 =  96.21 % ||| loss 0.09931634366512299\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10811/12000 =  90.09 % ||| loss 0.39290305972099304\u001b[0m\n",
            "\u001b[92mTest accuracy: 9012/10000 =  90.12 % ||| loss 0.4258626699447632\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.10217437215149403\n",
            "Batch #200 Loss: 0.1049979867041111\n",
            "Batch #300 Loss: 0.12423868488520384\n",
            "\u001b[92mTrain accuracy: 46040/48000 =  95.92 % ||| loss 0.10543185472488403\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10792/12000 =  89.93 % ||| loss 0.35157644748687744\u001b[0m\n",
            "\u001b[92mTest accuracy: 8965/10000 =  89.65 % ||| loss 0.39719563722610474\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726148513.1655092_1</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_1</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_094728-Lenet5BN_1726148513.1655092_2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_2' target=\"_blank\">Lenet5BN_1726148513.1655092_2</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 0.8174184626340866\n",
            "Batch #200 Loss: 0.46230275571346285\n",
            "Batch #300 Loss: 0.40234781220555305\n",
            "\u001b[92mTrain accuracy: 41930/48000 =  87.35 % ||| loss 0.3482136130332947\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10455/12000 =  87.12 % ||| loss 0.3622535467147827\u001b[0m\n",
            "\u001b[92mTest accuracy: 8631/10000 =  86.31 % ||| loss 0.3791547119617462\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.3485809129476547\n",
            "Batch #200 Loss: 0.31860011488199236\n",
            "Batch #300 Loss: 0.33081537708640096\n",
            "\u001b[92mTrain accuracy: 42788/48000 =  89.14 % ||| loss 0.292752742767334\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10589/12000 =  88.24 % ||| loss 0.31671324372291565\u001b[0m\n",
            "\u001b[92mTest accuracy: 8784/10000 =  87.84 % ||| loss 0.3326539993286133\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.29031343445181845\n",
            "Batch #200 Loss: 0.30298608347773553\n",
            "Batch #300 Loss: 0.291920221298933\n",
            "\u001b[92mTrain accuracy: 43049/48000 =  89.69 % ||| loss 0.27065518498420715\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10687/12000 =  89.06 % ||| loss 0.29259952902793884\u001b[0m\n",
            "\u001b[92mTest accuracy: 8810/10000 =  88.1 % ||| loss 0.313789039850235\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.25602600410580634\n",
            "Batch #200 Loss: 0.2734582369029522\n",
            "Batch #300 Loss: 0.25475363433361053\n",
            "\u001b[92mTrain accuracy: 42523/48000 =  88.59 % ||| loss 0.2986956536769867\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10482/12000 =  87.35 % ||| loss 0.33631086349487305\u001b[0m\n",
            "\u001b[92mTest accuracy: 8674/10000 =  86.74 % ||| loss 0.35363706946372986\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.24604312919080257\n",
            "Batch #200 Loss: 0.2441551876068115\n",
            "Batch #300 Loss: 0.23299655705690384\n",
            "\u001b[92mTrain accuracy: 44198/48000 =  92.08 % ||| loss 0.21313926577568054\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10864/12000 =  90.53 % ||| loss 0.25632694363594055\u001b[0m\n",
            "\u001b[92mTest accuracy: 8966/10000 =  89.66 % ||| loss 0.27874431014060974\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.23299760818481446\n",
            "Batch #200 Loss: 0.23214361287653446\n",
            "Batch #300 Loss: 0.23137399703264236\n",
            "\u001b[92mTrain accuracy: 44033/48000 =  91.74 % ||| loss 0.21902358531951904\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10796/12000 =  89.97 % ||| loss 0.2693321108818054\u001b[0m\n",
            "\u001b[92mTest accuracy: 8888/10000 =  88.88 % ||| loss 0.29776284098625183\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.20979887887835502\n",
            "Batch #200 Loss: 0.22140299297869206\n",
            "Batch #300 Loss: 0.22530683472752572\n",
            "\u001b[92mTrain accuracy: 43811/48000 =  91.27 % ||| loss 0.22981223464012146\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10683/12000 =  89.03 % ||| loss 0.29488128423690796\u001b[0m\n",
            "\u001b[92mTest accuracy: 8863/10000 =  88.63 % ||| loss 0.3175840377807617\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.19268031507730485\n",
            "Batch #200 Loss: 0.20197953648865222\n",
            "Batch #300 Loss: 0.21068732582032682\n",
            "\u001b[92mTrain accuracy: 44592/48000 =  92.9 % ||| loss 0.19003863632678986\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10839/12000 =  90.33 % ||| loss 0.26461827754974365\u001b[0m\n",
            "\u001b[92mTest accuracy: 8932/10000 =  89.32 % ||| loss 0.28890350461006165\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.1873066049069166\n",
            "Batch #200 Loss: 0.19427582308650015\n",
            "Batch #300 Loss: 0.1958000174164772\n",
            "\u001b[92mTrain accuracy: 44916/48000 =  93.58 % ||| loss 0.17458803951740265\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10876/12000 =  90.63 % ||| loss 0.2620161473751068\u001b[0m\n",
            "\u001b[92mTest accuracy: 8994/10000 =  89.94 % ||| loss 0.2831491529941559\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.17958368249237538\n",
            "Batch #200 Loss: 0.1813035501539707\n",
            "Batch #300 Loss: 0.18381721653044225\n",
            "\u001b[92mTrain accuracy: 44806/48000 =  93.35 % ||| loss 0.1747695654630661\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10825/12000 =  90.21 % ||| loss 0.2658936679363251\u001b[0m\n",
            "\u001b[92mTest accuracy: 8918/10000 =  89.18 % ||| loss 0.29215073585510254\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.16635431483387947\n",
            "Batch #200 Loss: 0.17049957472831012\n",
            "Batch #300 Loss: 0.18020784698426723\n",
            "\u001b[92mTrain accuracy: 44702/48000 =  93.13 % ||| loss 0.1746281534433365\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10804/12000 =  90.03 % ||| loss 0.27524811029434204\u001b[0m\n",
            "\u001b[92mTest accuracy: 8902/10000 =  89.02 % ||| loss 0.30550920963287354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.1601482243090868\n",
            "Batch #200 Loss: 0.16724860049784185\n",
            "Batch #300 Loss: 0.16983031012117863\n",
            "\u001b[92mTrain accuracy: 45353/48000 =  94.49 % ||| loss 0.14612500369548798\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10868/12000 =  90.57 % ||| loss 0.2647450268268585\u001b[0m\n",
            "\u001b[92mTest accuracy: 8999/10000 =  89.99 % ||| loss 0.2951204776763916\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.14368015009909868\n",
            "Batch #200 Loss: 0.15412533931434155\n",
            "Batch #300 Loss: 0.15360731340944767\n",
            "\u001b[92mTrain accuracy: 45350/48000 =  94.48 % ||| loss 0.1471506953239441\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10840/12000 =  90.33 % ||| loss 0.26898786425590515\u001b[0m\n",
            "\u001b[92mTest accuracy: 8993/10000 =  89.93 % ||| loss 0.28945192694664\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.13424539763480425\n",
            "Batch #200 Loss: 0.1447985977679491\n",
            "Batch #300 Loss: 0.1524648915231228\n",
            "\u001b[92mTrain accuracy: 45091/48000 =  93.94 % ||| loss 0.15986375510692596\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10763/12000 =  89.69 % ||| loss 0.3261682689189911\u001b[0m\n",
            "\u001b[92mTest accuracy: 8942/10000 =  89.42 % ||| loss 0.3521400988101959\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.1281366365775466\n",
            "Batch #200 Loss: 0.13789788007736206\n",
            "Batch #300 Loss: 0.14214836437255143\n",
            "\u001b[92mTrain accuracy: 45773/48000 =  95.36 % ||| loss 0.12351037561893463\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10865/12000 =  90.54 % ||| loss 0.2920873165130615\u001b[0m\n",
            "\u001b[92mTest accuracy: 9009/10000 =  90.09 % ||| loss 0.31818702816963196\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.1227501580864191\n",
            "Batch #200 Loss: 0.12353497672826051\n",
            "Batch #300 Loss: 0.13435893382877112\n",
            "\u001b[92mTrain accuracy: 46086/48000 =  96.01 % ||| loss 0.10633232444524765\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10890/12000 =  90.75 % ||| loss 0.28838232159614563\u001b[0m\n",
            "\u001b[92mTest accuracy: 9036/10000 =  90.36 % ||| loss 0.3253331184387207\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.1139853834733367\n",
            "Batch #200 Loss: 0.1278904840350151\n",
            "Batch #300 Loss: 0.12976437605917454\n",
            "\u001b[92mTrain accuracy: 45619/48000 =  95.04 % ||| loss 0.1303047239780426\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10822/12000 =  90.18 % ||| loss 0.31846120953559875\u001b[0m\n",
            "\u001b[92mTest accuracy: 8961/10000 =  89.61 % ||| loss 0.34733426570892334\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.11318711094558238\n",
            "Batch #200 Loss: 0.11964507125318051\n",
            "Batch #300 Loss: 0.12282760221511126\n",
            "\u001b[92mTrain accuracy: 46067/48000 =  95.97 % ||| loss 0.10942567884922028\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10885/12000 =  90.71 % ||| loss 0.296593576669693\u001b[0m\n",
            "\u001b[92mTest accuracy: 8968/10000 =  89.68 % ||| loss 0.33712735772132874\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.10365928186103701\n",
            "Batch #200 Loss: 0.11835142504423857\n",
            "Batch #300 Loss: 0.11198359474539757\n",
            "\u001b[92mTrain accuracy: 45278/48000 =  94.33 % ||| loss 0.1417989283800125\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10687/12000 =  89.06 % ||| loss 0.367931604385376\u001b[0m\n",
            "\u001b[92mTest accuracy: 8891/10000 =  88.91 % ||| loss 0.3947463631629944\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.0948379716090858\n",
            "Batch #200 Loss: 0.1090473933517933\n",
            "Batch #300 Loss: 0.11285230312496423\n",
            "\u001b[92mTrain accuracy: 45231/48000 =  94.23 % ||| loss 0.15620902180671692\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10682/12000 =  89.02 % ||| loss 0.38431400060653687\u001b[0m\n",
            "\u001b[92mTest accuracy: 8871/10000 =  88.71 % ||| loss 0.42821696400642395\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.09944463247433305\n",
            "Batch #200 Loss: 0.10592731691896916\n",
            "Batch #300 Loss: 0.09944970870390535\n",
            "\u001b[92mTrain accuracy: 45775/48000 =  95.36 % ||| loss 0.11805234849452972\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10729/12000 =  89.41 % ||| loss 0.35872316360473633\u001b[0m\n",
            "\u001b[92mTest accuracy: 8906/10000 =  89.06 % ||| loss 0.39047083258628845\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.08634504516609014\n",
            "Batch #200 Loss: 0.09010633716359734\n",
            "Batch #300 Loss: 0.09359038216993212\n",
            "\u001b[92mTrain accuracy: 46281/48000 =  96.42 % ||| loss 0.09367336332798004\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10811/12000 =  90.09 % ||| loss 0.3549799919128418\u001b[0m\n",
            "\u001b[92mTest accuracy: 8945/10000 =  89.45 % ||| loss 0.4060194790363312\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.0815039793588221\n",
            "Batch #200 Loss: 0.08120149793103337\n",
            "Batch #300 Loss: 0.09443592144176365\n",
            "\u001b[92mTrain accuracy: 46509/48000 =  96.89 % ||| loss 0.08284885436296463\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10848/12000 =  90.4 % ||| loss 0.36488285660743713\u001b[0m\n",
            "\u001b[92mTest accuracy: 8986/10000 =  89.86 % ||| loss 0.4125162959098816\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.07226141206920147\n",
            "Batch #200 Loss: 0.0804474189132452\n",
            "Batch #300 Loss: 0.08768570329993963\n",
            "\u001b[92mTrain accuracy: 46593/48000 =  97.07 % ||| loss 0.07680309563875198\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10816/12000 =  90.13 % ||| loss 0.37102943658828735\u001b[0m\n",
            "\u001b[92mTest accuracy: 8957/10000 =  89.57 % ||| loss 0.40956419706344604\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.07000843159854413\n",
            "Batch #200 Loss: 0.0819651685655117\n",
            "Batch #300 Loss: 0.08586567141115665\n",
            "\u001b[92mTrain accuracy: 46915/48000 =  97.74 % ||| loss 0.061217814683914185\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10852/12000 =  90.43 % ||| loss 0.3817150294780731\u001b[0m\n",
            "\u001b[92mTest accuracy: 8991/10000 =  89.91 % ||| loss 0.42447197437286377\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726148513.1655092_2</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_2</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_095017-Lenet5BN_1726148513.1655092_3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_3' target=\"_blank\">Lenet5BN_1726148513.1655092_3</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.0958289885520935\n",
            "Batch #200 Loss: 1.4412029993534088\n",
            "Batch #300 Loss: 0.9797769403457641\n",
            "\u001b[92mTrain accuracy: 35725/48000 =  74.43 % ||| loss 0.7440170645713806\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8929/12000 =  74.41 % ||| loss 0.7364634871482849\u001b[0m\n",
            "\u001b[92mTest accuracy: 7383/10000 =  73.83 % ||| loss 0.7493080496788025\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7026269069314003\n",
            "Batch #200 Loss: 0.6286708348989487\n",
            "Batch #300 Loss: 0.5842629545927047\n",
            "\u001b[92mTrain accuracy: 38188/48000 =  79.56 % ||| loss 0.547257661819458\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9541/12000 =  79.51 % ||| loss 0.5439222455024719\u001b[0m\n",
            "\u001b[92mTest accuracy: 7910/10000 =  79.1 % ||| loss 0.5605707764625549\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5445692947506905\n",
            "Batch #200 Loss: 0.5022343984246254\n",
            "Batch #300 Loss: 0.49898526817560196\n",
            "\u001b[92mTrain accuracy: 39709/48000 =  82.73 % ||| loss 0.4748277962207794\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9939/12000 =  82.83 % ||| loss 0.4736751317977905\u001b[0m\n",
            "\u001b[92mTest accuracy: 8169/10000 =  81.69 % ||| loss 0.49773284792900085\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.4659208154678345\n",
            "Batch #200 Loss: 0.4514263188838959\n",
            "Batch #300 Loss: 0.44430395781993864\n",
            "\u001b[92mTrain accuracy: 40585/48000 =  84.55 % ||| loss 0.4286641776561737\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10178/12000 =  84.82 % ||| loss 0.42817214131355286\u001b[0m\n",
            "\u001b[92mTest accuracy: 8348/10000 =  83.48 % ||| loss 0.451532244682312\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.42134584933519365\n",
            "Batch #200 Loss: 0.4257669275999069\n",
            "Batch #300 Loss: 0.4105354642868042\n",
            "\u001b[92mTrain accuracy: 40682/48000 =  84.75 % ||| loss 0.4191550016403198\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10168/12000 =  84.73 % ||| loss 0.42234286665916443\u001b[0m\n",
            "\u001b[92mTest accuracy: 8344/10000 =  83.44 % ||| loss 0.4504432678222656\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.39838856637477876\n",
            "Batch #200 Loss: 0.37620021104812623\n",
            "Batch #300 Loss: 0.3968922573328018\n",
            "\u001b[92mTrain accuracy: 41485/48000 =  86.43 % ||| loss 0.3790298402309418\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10367/12000 =  86.39 % ||| loss 0.38469216227531433\u001b[0m\n",
            "\u001b[92mTest accuracy: 8519/10000 =  85.19 % ||| loss 0.41061222553253174\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.37560158550739287\n",
            "Batch #200 Loss: 0.367456411421299\n",
            "Batch #300 Loss: 0.3655155172944069\n",
            "\u001b[92mTrain accuracy: 41679/48000 =  86.83 % ||| loss 0.3639233410358429\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10367/12000 =  86.39 % ||| loss 0.36884352564811707\u001b[0m\n",
            "\u001b[92mTest accuracy: 8545/10000 =  85.45 % ||| loss 0.4009125232696533\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.35317189067602156\n",
            "Batch #200 Loss: 0.3531388060748577\n",
            "Batch #300 Loss: 0.3546511958539486\n",
            "\u001b[92mTrain accuracy: 42053/48000 =  87.61 % ||| loss 0.3417538106441498\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10470/12000 =  87.25 % ||| loss 0.353244811296463\u001b[0m\n",
            "\u001b[92mTest accuracy: 8625/10000 =  86.25 % ||| loss 0.37812167406082153\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3553104580938816\n",
            "Batch #200 Loss: 0.3283598978817463\n",
            "Batch #300 Loss: 0.32911744505167007\n",
            "\u001b[92mTrain accuracy: 42491/48000 =  88.52 % ||| loss 0.32019004225730896\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10558/12000 =  87.98 % ||| loss 0.33347880840301514\u001b[0m\n",
            "\u001b[92mTest accuracy: 8683/10000 =  86.83 % ||| loss 0.3636002540588379\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.33204080164432526\n",
            "Batch #200 Loss: 0.32373751416802404\n",
            "Batch #300 Loss: 0.3348208382725716\n",
            "\u001b[92mTrain accuracy: 42389/48000 =  88.31 % ||| loss 0.32326170802116394\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10514/12000 =  87.62 % ||| loss 0.3397465944290161\u001b[0m\n",
            "\u001b[92mTest accuracy: 8665/10000 =  86.65 % ||| loss 0.3694532513618469\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.3274595110118389\n",
            "Batch #200 Loss: 0.32578101933002473\n",
            "Batch #300 Loss: 0.3076468986272812\n",
            "\u001b[92mTrain accuracy: 42503/48000 =  88.55 % ||| loss 0.31402552127838135\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10527/12000 =  87.72 % ||| loss 0.33025413751602173\u001b[0m\n",
            "\u001b[92mTest accuracy: 8670/10000 =  86.7 % ||| loss 0.3585750460624695\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.302856605052948\n",
            "Batch #200 Loss: 0.31475737795233727\n",
            "Batch #300 Loss: 0.307775602042675\n",
            "\u001b[92mTrain accuracy: 42680/48000 =  88.92 % ||| loss 0.3083855211734772\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10596/12000 =  88.3 % ||| loss 0.3233857750892639\u001b[0m\n",
            "\u001b[92mTest accuracy: 8700/10000 =  87.0 % ||| loss 0.35522517561912537\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.31147009909152984\n",
            "Batch #200 Loss: 0.3089148931205273\n",
            "Batch #300 Loss: 0.30143950670957564\n",
            "\u001b[92mTrain accuracy: 42837/48000 =  89.24 % ||| loss 0.29995039105415344\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10622/12000 =  88.52 % ||| loss 0.3192688226699829\u001b[0m\n",
            "\u001b[92mTest accuracy: 8744/10000 =  87.44 % ||| loss 0.35041162371635437\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3099698443710804\n",
            "Batch #200 Loss: 0.2916690464317799\n",
            "Batch #300 Loss: 0.30445043176412584\n",
            "\u001b[92mTrain accuracy: 42766/48000 =  89.1 % ||| loss 0.306776762008667\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10597/12000 =  88.31 % ||| loss 0.32452157139778137\u001b[0m\n",
            "\u001b[92mTest accuracy: 8719/10000 =  87.19 % ||| loss 0.3600803315639496\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.2907446512579918\n",
            "Batch #200 Loss: 0.2961334770917892\n",
            "Batch #300 Loss: 0.28918132320046425\n",
            "\u001b[92mTrain accuracy: 43244/48000 =  90.09 % ||| loss 0.27736735343933105\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10660/12000 =  88.83 % ||| loss 0.3017570674419403\u001b[0m\n",
            "\u001b[92mTest accuracy: 8800/10000 =  88.0 % ||| loss 0.3293974995613098\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.29146048828959464\n",
            "Batch #200 Loss: 0.2937215961515903\n",
            "Batch #300 Loss: 0.3038587440550327\n",
            "\u001b[92mTrain accuracy: 43266/48000 =  90.14 % ||| loss 0.27511048316955566\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10667/12000 =  88.89 % ||| loss 0.3012855350971222\u001b[0m\n",
            "\u001b[92mTest accuracy: 8809/10000 =  88.09 % ||| loss 0.3321089744567871\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.28707782983779906\n",
            "Batch #200 Loss: 0.28354167520999907\n",
            "Batch #300 Loss: 0.28794039398431776\n",
            "\u001b[92mTrain accuracy: 43172/48000 =  89.94 % ||| loss 0.28061774373054504\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10641/12000 =  88.67 % ||| loss 0.30957117676734924\u001b[0m\n",
            "\u001b[92mTest accuracy: 8774/10000 =  87.74 % ||| loss 0.3405151963233948\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.27811359718441964\n",
            "Batch #200 Loss: 0.281328849196434\n",
            "Batch #300 Loss: 0.27662632808089255\n",
            "\u001b[92mTrain accuracy: 43126/48000 =  89.85 % ||| loss 0.28035396337509155\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10631/12000 =  88.59 % ||| loss 0.31234219670295715\u001b[0m\n",
            "\u001b[92mTest accuracy: 8770/10000 =  87.7 % ||| loss 0.34274640679359436\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.2737793010473251\n",
            "Batch #200 Loss: 0.2751014094054699\n",
            "Batch #300 Loss: 0.28153515920042993\n",
            "\u001b[92mTrain accuracy: 43376/48000 =  90.37 % ||| loss 0.2667151689529419\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10675/12000 =  88.96 % ||| loss 0.2996799945831299\u001b[0m\n",
            "\u001b[92mTest accuracy: 8815/10000 =  88.15 % ||| loss 0.32764261960983276\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.2682471153140068\n",
            "Batch #200 Loss: 0.2796703241765499\n",
            "Batch #300 Loss: 0.2655866138637066\n",
            "\u001b[92mTrain accuracy: 43477/48000 =  90.58 % ||| loss 0.2622268497943878\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10704/12000 =  89.2 % ||| loss 0.29892829060554504\u001b[0m\n",
            "\u001b[92mTest accuracy: 8848/10000 =  88.48 % ||| loss 0.3293260633945465\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.26724271327257154\n",
            "Batch #200 Loss: 0.28014935061335566\n",
            "Batch #300 Loss: 0.26795505538582803\n",
            "\u001b[92mTrain accuracy: 43601/48000 =  90.84 % ||| loss 0.2553355395793915\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10713/12000 =  89.28 % ||| loss 0.29146286845207214\u001b[0m\n",
            "\u001b[92mTest accuracy: 8838/10000 =  88.38 % ||| loss 0.32764896750450134\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.27219212874770166\n",
            "Batch #200 Loss: 0.267008808106184\n",
            "Batch #300 Loss: 0.2694801491498947\n",
            "\u001b[92mTrain accuracy: 43306/48000 =  90.22 % ||| loss 0.26666319370269775\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10658/12000 =  88.82 % ||| loss 0.3047870397567749\u001b[0m\n",
            "\u001b[92mTest accuracy: 8805/10000 =  88.05 % ||| loss 0.3335336744785309\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.2592697574198246\n",
            "Batch #200 Loss: 0.26132311061024666\n",
            "Batch #300 Loss: 0.2672081443667412\n",
            "\u001b[92mTrain accuracy: 43620/48000 =  90.88 % ||| loss 0.25193071365356445\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10713/12000 =  89.28 % ||| loss 0.29237183928489685\u001b[0m\n",
            "\u001b[92mTest accuracy: 8865/10000 =  88.65 % ||| loss 0.32239583134651184\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.2608557169139385\n",
            "Batch #200 Loss: 0.2560746070742607\n",
            "Batch #300 Loss: 0.2639238612353802\n",
            "\u001b[92mTrain accuracy: 43656/48000 =  90.95 % ||| loss 0.252140074968338\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10714/12000 =  89.28 % ||| loss 0.2956271469593048\u001b[0m\n",
            "\u001b[92mTest accuracy: 8847/10000 =  88.47 % ||| loss 0.33482980728149414\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.24058616936206817\n",
            "Batch #200 Loss: 0.26636397674679757\n",
            "Batch #300 Loss: 0.25654837012290954\n",
            "\u001b[92mTrain accuracy: 43510/48000 =  90.65 % ||| loss 0.2564047574996948\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10659/12000 =  88.83 % ||| loss 0.2986885607242584\u001b[0m\n",
            "\u001b[92mTest accuracy: 8794/10000 =  87.94 % ||| loss 0.3342214524745941\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726148513.1655092_3</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_3</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_095303-Lenet5BN_1726148513.1655092_4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_4' target=\"_blank\">Lenet5BN_1726148513.1655092_4</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.1261591506004334\n",
            "Batch #200 Loss: 0.5296496108174324\n",
            "Batch #300 Loss: 0.468885657787323\n",
            "\u001b[92mTrain accuracy: 40849/48000 =  85.1 % ||| loss 0.4049919545650482\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10159/12000 =  84.66 % ||| loss 0.41412660479545593\u001b[0m\n",
            "\u001b[92mTest accuracy: 8405/10000 =  84.05 % ||| loss 0.4352976679801941\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.3962798205018043\n",
            "Batch #200 Loss: 0.379927761554718\n",
            "Batch #300 Loss: 0.3533117647469044\n",
            "\u001b[92mTrain accuracy: 41607/48000 =  86.68 % ||| loss 0.3612612187862396\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10327/12000 =  86.06 % ||| loss 0.3748820722103119\u001b[0m\n",
            "\u001b[92mTest accuracy: 8545/10000 =  85.45 % ||| loss 0.4044921398162842\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.34764961823821067\n",
            "Batch #200 Loss: 0.3278602504730225\n",
            "Batch #300 Loss: 0.3280740827322006\n",
            "\u001b[92mTrain accuracy: 42699/48000 =  88.96 % ||| loss 0.2987964153289795\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10583/12000 =  88.19 % ||| loss 0.3189021646976471\u001b[0m\n",
            "\u001b[92mTest accuracy: 8770/10000 =  87.7 % ||| loss 0.3478676378726959\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.316584712266922\n",
            "Batch #200 Loss: 0.31117153555154803\n",
            "Batch #300 Loss: 0.2922983808815479\n",
            "\u001b[92mTrain accuracy: 42607/48000 =  88.76 % ||| loss 0.29589205980300903\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10595/12000 =  88.29 % ||| loss 0.31918343901634216\u001b[0m\n",
            "\u001b[92mTest accuracy: 8743/10000 =  87.43 % ||| loss 0.34667059779167175\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.27782132729887965\n",
            "Batch #200 Loss: 0.2769743847846985\n",
            "Batch #300 Loss: 0.27083142995834353\n",
            "\u001b[92mTrain accuracy: 42765/48000 =  89.09 % ||| loss 0.29022452235221863\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10576/12000 =  88.13 % ||| loss 0.32203593850135803\u001b[0m\n",
            "\u001b[92mTest accuracy: 8713/10000 =  87.13 % ||| loss 0.35305696725845337\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.2670358523726463\n",
            "Batch #200 Loss: 0.2767596051841974\n",
            "Batch #300 Loss: 0.2572043906152248\n",
            "\u001b[92mTrain accuracy: 43291/48000 =  90.19 % ||| loss 0.26565906405448914\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10660/12000 =  88.83 % ||| loss 0.29786479473114014\u001b[0m\n",
            "\u001b[92mTest accuracy: 8790/10000 =  87.9 % ||| loss 0.3249804973602295\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.23943405881524085\n",
            "Batch #200 Loss: 0.25168512523174286\n",
            "Batch #300 Loss: 0.23656056687235832\n",
            "\u001b[92mTrain accuracy: 44089/48000 =  91.85 % ||| loss 0.22193673253059387\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10816/12000 =  90.13 % ||| loss 0.2694770395755768\u001b[0m\n",
            "\u001b[92mTest accuracy: 8898/10000 =  88.98 % ||| loss 0.30181747674942017\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.23081754006445407\n",
            "Batch #200 Loss: 0.2342211763560772\n",
            "Batch #300 Loss: 0.23868720956146716\n",
            "\u001b[92mTrain accuracy: 44318/48000 =  92.33 % ||| loss 0.20823253691196442\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10831/12000 =  90.26 % ||| loss 0.25911927223205566\u001b[0m\n",
            "\u001b[92mTest accuracy: 8956/10000 =  89.56 % ||| loss 0.2804078161716461\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.22105555430054666\n",
            "Batch #200 Loss: 0.21685006082057953\n",
            "Batch #300 Loss: 0.22487459696829318\n",
            "\u001b[92mTrain accuracy: 44168/48000 =  92.02 % ||| loss 0.21255318820476532\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10760/12000 =  89.67 % ||| loss 0.2712384760379791\u001b[0m\n",
            "\u001b[92mTest accuracy: 8922/10000 =  89.22 % ||| loss 0.2965494394302368\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.21334822729229927\n",
            "Batch #200 Loss: 0.21497312180697917\n",
            "Batch #300 Loss: 0.21965595424175263\n",
            "\u001b[92mTrain accuracy: 44352/48000 =  92.4 % ||| loss 0.2095918506383896\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10759/12000 =  89.66 % ||| loss 0.272846519947052\u001b[0m\n",
            "\u001b[92mTest accuracy: 8916/10000 =  89.16 % ||| loss 0.2946445643901825\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.19917606823146344\n",
            "Batch #200 Loss: 0.20569732949137687\n",
            "Batch #300 Loss: 0.21502715289592744\n",
            "\u001b[92mTrain accuracy: 44231/48000 =  92.15 % ||| loss 0.20875194668769836\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10796/12000 =  89.97 % ||| loss 0.27622297406196594\u001b[0m\n",
            "\u001b[92mTest accuracy: 8902/10000 =  89.02 % ||| loss 0.3055766224861145\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.2060238241404295\n",
            "Batch #200 Loss: 0.19765680879354477\n",
            "Batch #300 Loss: 0.19767293743789197\n",
            "\u001b[92mTrain accuracy: 44891/48000 =  93.52 % ||| loss 0.1771676242351532\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10882/12000 =  90.68 % ||| loss 0.2553896903991699\u001b[0m\n",
            "\u001b[92mTest accuracy: 8983/10000 =  89.83 % ||| loss 0.2864319086074829\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.1862771613150835\n",
            "Batch #200 Loss: 0.19224364928901194\n",
            "Batch #300 Loss: 0.19259522832930087\n",
            "\u001b[92mTrain accuracy: 44963/48000 =  93.67 % ||| loss 0.17337465286254883\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10874/12000 =  90.62 % ||| loss 0.25543665885925293\u001b[0m\n",
            "\u001b[92mTest accuracy: 8970/10000 =  89.7 % ||| loss 0.2814467251300812\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.18317118745297192\n",
            "Batch #200 Loss: 0.195105362534523\n",
            "Batch #300 Loss: 0.17759233027696608\n",
            "\u001b[92mTrain accuracy: 45149/48000 =  94.06 % ||| loss 0.16148316860198975\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10877/12000 =  90.64 % ||| loss 0.25793296098709106\u001b[0m\n",
            "\u001b[92mTest accuracy: 9000/10000 =  90.0 % ||| loss 0.28304246068000793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.17480461552739143\n",
            "Batch #200 Loss: 0.17364680014550685\n",
            "Batch #300 Loss: 0.18556010864675046\n",
            "\u001b[92mTrain accuracy: 45299/48000 =  94.37 % ||| loss 0.15339511632919312\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10877/12000 =  90.64 % ||| loss 0.2619401812553406\u001b[0m\n",
            "\u001b[92mTest accuracy: 9007/10000 =  90.07 % ||| loss 0.28502339124679565\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.16758057184517383\n",
            "Batch #200 Loss: 0.1787053956836462\n",
            "Batch #300 Loss: 0.17247852750122547\n",
            "\u001b[92mTrain accuracy: 45365/48000 =  94.51 % ||| loss 0.14817483723163605\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10894/12000 =  90.78 % ||| loss 0.2606638967990875\u001b[0m\n",
            "\u001b[92mTest accuracy: 9028/10000 =  90.28 % ||| loss 0.28549623489379883\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.15659898616373538\n",
            "Batch #200 Loss: 0.1629506979137659\n",
            "Batch #300 Loss: 0.17461555793881417\n",
            "\u001b[92mTrain accuracy: 45576/48000 =  94.95 % ||| loss 0.1405201554298401\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10885/12000 =  90.71 % ||| loss 0.25736120343208313\u001b[0m\n",
            "\u001b[92mTest accuracy: 9036/10000 =  90.36 % ||| loss 0.28435832262039185\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.160800002515316\n",
            "Batch #200 Loss: 0.1554615591466427\n",
            "Batch #300 Loss: 0.1633277303725481\n",
            "\u001b[92mTrain accuracy: 45083/48000 =  93.92 % ||| loss 0.1596352905035019\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10839/12000 =  90.33 % ||| loss 0.28032758831977844\u001b[0m\n",
            "\u001b[92mTest accuracy: 8950/10000 =  89.5 % ||| loss 0.31410732865333557\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.14575412966310977\n",
            "Batch #200 Loss: 0.15344746746122837\n",
            "Batch #300 Loss: 0.15323004491627215\n",
            "\u001b[92mTrain accuracy: 45808/48000 =  95.43 % ||| loss 0.12688620388507843\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10900/12000 =  90.83 % ||| loss 0.2596232295036316\u001b[0m\n",
            "\u001b[92mTest accuracy: 8998/10000 =  89.98 % ||| loss 0.28549644351005554\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.14529704697430135\n",
            "Batch #200 Loss: 0.14578489910811185\n",
            "Batch #300 Loss: 0.1529605034738779\n",
            "\u001b[92mTrain accuracy: 45761/48000 =  95.34 % ||| loss 0.12820033729076385\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10879/12000 =  90.66 % ||| loss 0.2691374123096466\u001b[0m\n",
            "\u001b[92mTest accuracy: 9025/10000 =  90.25 % ||| loss 0.29500290751457214\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.12821767546236515\n",
            "Batch #200 Loss: 0.13958498608320952\n",
            "Batch #300 Loss: 0.12970313414931298\n",
            "\u001b[92mTrain accuracy: 45640/48000 =  95.08 % ||| loss 0.1361125111579895\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10866/12000 =  90.55 % ||| loss 0.28216931223869324\u001b[0m\n",
            "\u001b[92mTest accuracy: 8955/10000 =  89.55 % ||| loss 0.30888018012046814\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.12457052953541278\n",
            "Batch #200 Loss: 0.13921964537352324\n",
            "Batch #300 Loss: 0.13306360319256783\n",
            "\u001b[92mTrain accuracy: 45683/48000 =  95.17 % ||| loss 0.1253884732723236\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10861/12000 =  90.51 % ||| loss 0.28548675775527954\u001b[0m\n",
            "\u001b[92mTest accuracy: 8964/10000 =  89.64 % ||| loss 0.31708046793937683\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.11488288283348083\n",
            "Batch #200 Loss: 0.13449601281434298\n",
            "Batch #300 Loss: 0.1286406674608588\n",
            "\u001b[92mTrain accuracy: 45552/48000 =  94.9 % ||| loss 0.13465359807014465\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10794/12000 =  89.95 % ||| loss 0.2994450330734253\u001b[0m\n",
            "\u001b[92mTest accuracy: 8906/10000 =  89.06 % ||| loss 0.32934296131134033\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.12296985428780317\n",
            "Batch #200 Loss: 0.11320179961621761\n",
            "Batch #300 Loss: 0.12317945841699839\n",
            "\u001b[92mTrain accuracy: 45847/48000 =  95.51 % ||| loss 0.11658984422683716\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10828/12000 =  90.23 % ||| loss 0.30517578125\u001b[0m\n",
            "\u001b[92mTest accuracy: 8928/10000 =  89.28 % ||| loss 0.3291877806186676\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.11632976677268743\n",
            "Batch #200 Loss: 0.11955703601241112\n",
            "Batch #300 Loss: 0.12177897598594427\n",
            "\u001b[92mTrain accuracy: 46010/48000 =  95.85 % ||| loss 0.11108147352933884\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10858/12000 =  90.48 % ||| loss 0.306727796792984\u001b[0m\n",
            "\u001b[92mTest accuracy: 8964/10000 =  89.64 % ||| loss 0.3299751579761505\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726148513.1655092_4</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_4</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_095549-Lenet5BN_1726148513.1655092_5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_5' target=\"_blank\">Lenet5BN_1726148513.1655092_5</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.527071886062622\n",
            "Batch #200 Loss: 0.6834864082932472\n",
            "Batch #300 Loss: 0.5369028499722481\n",
            "\u001b[92mTrain accuracy: 39952/48000 =  83.23 % ||| loss 0.46105095744132996\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10028/12000 =  83.57 % ||| loss 0.45558100938796997\u001b[0m\n",
            "\u001b[92mTest accuracy: 8252/10000 =  82.52 % ||| loss 0.4802183210849762\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.4549357292056084\n",
            "Batch #200 Loss: 0.4163497525453568\n",
            "Batch #300 Loss: 0.406800676882267\n",
            "\u001b[92mTrain accuracy: 41589/48000 =  86.64 % ||| loss 0.37531787157058716\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10370/12000 =  86.42 % ||| loss 0.37658625841140747\u001b[0m\n",
            "\u001b[92mTest accuracy: 8557/10000 =  85.57 % ||| loss 0.3971366584300995\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.36266845852136614\n",
            "Batch #200 Loss: 0.36459933176636694\n",
            "Batch #300 Loss: 0.3568873246014118\n",
            "\u001b[92mTrain accuracy: 41733/48000 =  86.94 % ||| loss 0.3637423813343048\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10348/12000 =  86.23 % ||| loss 0.37544453144073486\u001b[0m\n",
            "\u001b[92mTest accuracy: 8599/10000 =  85.99 % ||| loss 0.3924906253814697\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.3359392860531807\n",
            "Batch #200 Loss: 0.32638498589396475\n",
            "Batch #300 Loss: 0.338340058773756\n",
            "\u001b[92mTrain accuracy: 42661/48000 =  88.88 % ||| loss 0.3084069490432739\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10582/12000 =  88.18 % ||| loss 0.3260975182056427\u001b[0m\n",
            "\u001b[92mTest accuracy: 8748/10000 =  87.48 % ||| loss 0.3512103259563446\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.32025581613183024\n",
            "Batch #200 Loss: 0.3034408999979496\n",
            "Batch #300 Loss: 0.30569050416350363\n",
            "\u001b[92mTrain accuracy: 41827/48000 =  87.14 % ||| loss 0.3453599810600281\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10332/12000 =  86.1 % ||| loss 0.3708531856536865\u001b[0m\n",
            "\u001b[92mTest accuracy: 8575/10000 =  85.75 % ||| loss 0.39534884691238403\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.29894660010933877\n",
            "Batch #200 Loss: 0.29211251825094225\n",
            "Batch #300 Loss: 0.30129784032702445\n",
            "\u001b[92mTrain accuracy: 43130/48000 =  89.85 % ||| loss 0.2839907705783844\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10666/12000 =  88.88 % ||| loss 0.31037795543670654\u001b[0m\n",
            "\u001b[92mTest accuracy: 8814/10000 =  88.14 % ||| loss 0.32930150628089905\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.2847683550417423\n",
            "Batch #200 Loss: 0.28337876722216604\n",
            "Batch #300 Loss: 0.28613887429237367\n",
            "\u001b[92mTrain accuracy: 43067/48000 =  89.72 % ||| loss 0.2828449010848999\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10633/12000 =  88.61 % ||| loss 0.31103378534317017\u001b[0m\n",
            "\u001b[92mTest accuracy: 8801/10000 =  88.01 % ||| loss 0.3356612026691437\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.27331987872719765\n",
            "Batch #200 Loss: 0.27056968182325364\n",
            "Batch #300 Loss: 0.27293312206864356\n",
            "\u001b[92mTrain accuracy: 43658/48000 =  90.95 % ||| loss 0.25301966071128845\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10734/12000 =  89.45 % ||| loss 0.28726717829704285\u001b[0m\n",
            "\u001b[92mTest accuracy: 8874/10000 =  88.74 % ||| loss 0.3082057237625122\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.26270706549286843\n",
            "Batch #200 Loss: 0.26642824679613114\n",
            "Batch #300 Loss: 0.2614621901512146\n",
            "\u001b[92mTrain accuracy: 43519/48000 =  90.66 % ||| loss 0.2614389657974243\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10674/12000 =  88.95 % ||| loss 0.3002307713031769\u001b[0m\n",
            "\u001b[92mTest accuracy: 8849/10000 =  88.49 % ||| loss 0.3224107623100281\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.2562253224849701\n",
            "Batch #200 Loss: 0.25651233941316604\n",
            "Batch #300 Loss: 0.25828575566411016\n",
            "\u001b[92mTrain accuracy: 43874/48000 =  91.4 % ||| loss 0.2388051301240921\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10759/12000 =  89.66 % ||| loss 0.2805595099925995\u001b[0m\n",
            "\u001b[92mTest accuracy: 8899/10000 =  88.99 % ||| loss 0.3039757311344147\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.2519991047680378\n",
            "Batch #200 Loss: 0.25588657990098\n",
            "Batch #300 Loss: 0.23947253167629243\n",
            "\u001b[92mTrain accuracy: 43997/48000 =  91.66 % ||| loss 0.23176035284996033\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10774/12000 =  89.78 % ||| loss 0.2787056267261505\u001b[0m\n",
            "\u001b[92mTest accuracy: 8897/10000 =  88.97 % ||| loss 0.3033646047115326\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.24097584173083306\n",
            "Batch #200 Loss: 0.2365282230079174\n",
            "Batch #300 Loss: 0.24603225335478782\n",
            "\u001b[92mTrain accuracy: 43800/48000 =  91.25 % ||| loss 0.2400120049715042\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10706/12000 =  89.22 % ||| loss 0.29233518242836\u001b[0m\n",
            "\u001b[92mTest accuracy: 8864/10000 =  88.64 % ||| loss 0.30867841839790344\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.2336745171248913\n",
            "Batch #200 Loss: 0.2427051344513893\n",
            "Batch #300 Loss: 0.23362065076828004\n",
            "\u001b[92mTrain accuracy: 43570/48000 =  90.77 % ||| loss 0.24993884563446045\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10704/12000 =  89.2 % ||| loss 0.2979176938533783\u001b[0m\n",
            "\u001b[92mTest accuracy: 8837/10000 =  88.37 % ||| loss 0.32365092635154724\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.2276646027714014\n",
            "Batch #200 Loss: 0.23723183959722519\n",
            "Batch #300 Loss: 0.23111155807971953\n",
            "\u001b[92mTrain accuracy: 44079/48000 =  91.83 % ||| loss 0.22164864838123322\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10794/12000 =  89.95 % ||| loss 0.27478042244911194\u001b[0m\n",
            "\u001b[92mTest accuracy: 8950/10000 =  89.5 % ||| loss 0.2969355583190918\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.22165463045239447\n",
            "Batch #200 Loss: 0.22448132589459419\n",
            "Batch #300 Loss: 0.2259613460302353\n",
            "\u001b[92mTrain accuracy: 44245/48000 =  92.18 % ||| loss 0.21485674381256104\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10781/12000 =  89.84 % ||| loss 0.27563658356666565\u001b[0m\n",
            "\u001b[92mTest accuracy: 8915/10000 =  89.15 % ||| loss 0.29680436849594116\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.21415008455514908\n",
            "Batch #200 Loss: 0.22806896038353444\n",
            "Batch #300 Loss: 0.22970998421311378\n",
            "\u001b[92mTrain accuracy: 43356/48000 =  90.33 % ||| loss 0.2584332823753357\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10562/12000 =  88.02 % ||| loss 0.3250712454319\u001b[0m\n",
            "\u001b[92mTest accuracy: 8774/10000 =  87.74 % ||| loss 0.3479427993297577\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.21175041161477565\n",
            "Batch #200 Loss: 0.21515957728028298\n",
            "Batch #300 Loss: 0.22844746544957162\n",
            "\u001b[92mTrain accuracy: 44509/48000 =  92.73 % ||| loss 0.20037657022476196\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10801/12000 =  90.01 % ||| loss 0.2685621678829193\u001b[0m\n",
            "\u001b[92mTest accuracy: 8934/10000 =  89.34 % ||| loss 0.29165783524513245\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.20548959255218505\n",
            "Batch #200 Loss: 0.22451977588236333\n",
            "Batch #300 Loss: 0.2064121500402689\n",
            "\u001b[92mTrain accuracy: 44537/48000 =  92.79 % ||| loss 0.19722285866737366\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10804/12000 =  90.03 % ||| loss 0.2718977928161621\u001b[0m\n",
            "\u001b[92mTest accuracy: 8968/10000 =  89.68 % ||| loss 0.2917250096797943\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.20608577579259874\n",
            "Batch #200 Loss: 0.20466898642480374\n",
            "Batch #300 Loss: 0.20874444723129273\n",
            "\u001b[92mTrain accuracy: 44505/48000 =  92.72 % ||| loss 0.19887888431549072\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10760/12000 =  89.67 % ||| loss 0.2769814431667328\u001b[0m\n",
            "\u001b[92mTest accuracy: 8933/10000 =  89.33 % ||| loss 0.29559949040412903\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.20382977165281774\n",
            "Batch #200 Loss: 0.2017989721894264\n",
            "Batch #300 Loss: 0.20742770232260227\n",
            "\u001b[92mTrain accuracy: 44487/48000 =  92.68 % ||| loss 0.19998949766159058\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10789/12000 =  89.91 % ||| loss 0.2831577956676483\u001b[0m\n",
            "\u001b[92mTest accuracy: 8925/10000 =  89.25 % ||| loss 0.2984202802181244\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.20144578620791434\n",
            "Batch #200 Loss: 0.19366843581199647\n",
            "Batch #300 Loss: 0.2050363202393055\n",
            "\u001b[92mTrain accuracy: 44292/48000 =  92.27 % ||| loss 0.2032453566789627\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10719/12000 =  89.33 % ||| loss 0.2879781424999237\u001b[0m\n",
            "\u001b[92mTest accuracy: 8867/10000 =  88.67 % ||| loss 0.30553579330444336\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.19022500567138195\n",
            "Batch #200 Loss: 0.20263052240014076\n",
            "Batch #300 Loss: 0.19580089285969735\n",
            "\u001b[92mTrain accuracy: 44228/48000 =  92.14 % ||| loss 0.21185943484306335\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10695/12000 =  89.12 % ||| loss 0.3035857379436493\u001b[0m\n",
            "\u001b[92mTest accuracy: 8835/10000 =  88.35 % ||| loss 0.32541292905807495\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.1839998073130846\n",
            "Batch #200 Loss: 0.19109369531273843\n",
            "Batch #300 Loss: 0.19943610712885856\n",
            "\u001b[92mTrain accuracy: 44910/48000 =  93.56 % ||| loss 0.1790296882390976\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10823/12000 =  90.19 % ||| loss 0.2778221070766449\u001b[0m\n",
            "\u001b[92mTest accuracy: 8946/10000 =  89.46 % ||| loss 0.2967914938926697\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.17487274572253228\n",
            "Batch #200 Loss: 0.18985529631376266\n",
            "Batch #300 Loss: 0.19410492315888406\n",
            "\u001b[92mTrain accuracy: 44954/48000 =  93.65 % ||| loss 0.17489385604858398\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10795/12000 =  89.96 % ||| loss 0.2746734917163849\u001b[0m\n",
            "\u001b[92mTest accuracy: 8962/10000 =  89.62 % ||| loss 0.2856127917766571\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.18818650424480438\n",
            "Batch #200 Loss: 0.17901258505880832\n",
            "Batch #300 Loss: 0.1838714262843132\n",
            "\u001b[92mTrain accuracy: 45005/48000 =  93.76 % ||| loss 0.17256906628608704\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10802/12000 =  90.02 % ||| loss 0.2759262025356293\u001b[0m\n",
            "\u001b[92mTest accuracy: 8975/10000 =  89.75 % ||| loss 0.2934008240699768\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726148513.1655092_5</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_5</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_095834-Lenet5BN_1726148513.1655092_6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_6' target=\"_blank\">Lenet5BN_1726148513.1655092_6</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.301483016014099\n",
            "Batch #200 Loss: 2.277115733623505\n",
            "Batch #300 Loss: 2.2518686294555663\n",
            "\u001b[92mTrain accuracy: 13025/48000 =  27.14 % ||| loss 2.2167434692382812\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3238/12000 =  26.98 % ||| loss 2.2165842056274414\u001b[0m\n",
            "\u001b[92mTest accuracy: 2737/10000 =  27.37 % ||| loss 2.2171883583068848\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2023776054382322\n",
            "Batch #200 Loss: 2.1709154200553895\n",
            "Batch #300 Loss: 2.1321024107933044\n",
            "\u001b[92mTrain accuracy: 20918/48000 =  43.58 % ||| loss 2.0808181762695312\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5185/12000 =  43.21 % ||| loss 2.081382989883423\u001b[0m\n",
            "\u001b[92mTest accuracy: 4400/10000 =  44.0 % ||| loss 2.0822434425354004\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.057843626737595\n",
            "Batch #200 Loss: 2.007995811700821\n",
            "Batch #300 Loss: 1.958933699131012\n",
            "\u001b[92mTrain accuracy: 23211/48000 =  48.36 % ||| loss 1.8837119340896606\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5833/12000 =  48.61 % ||| loss 1.8842604160308838\u001b[0m\n",
            "\u001b[92mTest accuracy: 4875/10000 =  48.75 % ||| loss 1.8841724395751953\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.852973439693451\n",
            "Batch #200 Loss: 1.7928605568408966\n",
            "Batch #300 Loss: 1.7381800413131714\n",
            "\u001b[92mTrain accuracy: 26882/48000 =  56.0 % ||| loss 1.6476190090179443\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6696/12000 =  55.8 % ||| loss 1.6474565267562866\u001b[0m\n",
            "\u001b[92mTest accuracy: 5643/10000 =  56.43 % ||| loss 1.6485979557037354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 1.6154969894886018\n",
            "Batch #200 Loss: 1.5473707056045531\n",
            "Batch #300 Loss: 1.4827260017395019\n",
            "\u001b[92mTrain accuracy: 30940/48000 =  64.46 % ||| loss 1.3974813222885132\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7749/12000 =  64.58 % ||| loss 1.3951869010925293\u001b[0m\n",
            "\u001b[92mTest accuracy: 6437/10000 =  64.37 % ||| loss 1.3988980054855347\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 1.3602448964118958\n",
            "Batch #200 Loss: 1.2998997271060944\n",
            "Batch #300 Loss: 1.249065259695053\n",
            "\u001b[92mTrain accuracy: 32960/48000 =  68.67 % ||| loss 1.1780714988708496\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8301/12000 =  69.17 % ||| loss 1.1740494966506958\u001b[0m\n",
            "\u001b[92mTest accuracy: 6893/10000 =  68.93 % ||| loss 1.1840126514434814\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 1.1486709690093995\n",
            "Batch #200 Loss: 1.1151504874229432\n",
            "Batch #300 Loss: 1.074546828866005\n",
            "\u001b[92mTrain accuracy: 33890/48000 =  70.6 % ||| loss 1.0174682140350342\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8502/12000 =  70.85 % ||| loss 1.0121885538101196\u001b[0m\n",
            "\u001b[92mTest accuracy: 7021/10000 =  70.21 % ||| loss 1.0197324752807617\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 1.0001434767246247\n",
            "Batch #200 Loss: 0.9717477893829346\n",
            "Batch #300 Loss: 0.9445983600616455\n",
            "\u001b[92mTrain accuracy: 34589/48000 =  72.06 % ||| loss 0.9081801772117615\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8681/12000 =  72.34 % ||| loss 0.9024137854576111\u001b[0m\n",
            "\u001b[92mTest accuracy: 7169/10000 =  71.69 % ||| loss 0.9149918556213379\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.894682297706604\n",
            "Batch #200 Loss: 0.8795692789554596\n",
            "Batch #300 Loss: 0.8543454802036285\n",
            "\u001b[92mTrain accuracy: 35010/48000 =  72.94 % ||| loss 0.8333560228347778\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8810/12000 =  73.42 % ||| loss 0.8275021314620972\u001b[0m\n",
            "\u001b[92mTest accuracy: 7248/10000 =  72.48 % ||| loss 0.8419567346572876\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.8219844830036164\n",
            "Batch #200 Loss: 0.8140087479352951\n",
            "Batch #300 Loss: 0.7966155833005906\n",
            "\u001b[92mTrain accuracy: 35308/48000 =  73.56 % ||| loss 0.7797143459320068\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8909/12000 =  74.24 % ||| loss 0.7735746502876282\u001b[0m\n",
            "\u001b[92mTest accuracy: 7315/10000 =  73.15 % ||| loss 0.7871562838554382\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.7832321983575821\n",
            "Batch #200 Loss: 0.7628802448511124\n",
            "Batch #300 Loss: 0.7446516489982605\n",
            "\u001b[92mTrain accuracy: 35594/48000 =  74.15 % ||| loss 0.735612690448761\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8972/12000 =  74.77 % ||| loss 0.7294694781303406\u001b[0m\n",
            "\u001b[92mTest accuracy: 7360/10000 =  73.6 % ||| loss 0.7428704500198364\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.7230971759557724\n",
            "Batch #200 Loss: 0.7376714533567429\n",
            "Batch #300 Loss: 0.715519341826439\n",
            "\u001b[92mTrain accuracy: 35910/48000 =  74.81 % ||| loss 0.7028170824050903\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9023/12000 =  75.19 % ||| loss 0.6972335577011108\u001b[0m\n",
            "\u001b[92mTest accuracy: 7435/10000 =  74.35 % ||| loss 0.7119743227958679\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.6931262570619583\n",
            "Batch #200 Loss: 0.6912670195102691\n",
            "Batch #300 Loss: 0.6925135213136673\n",
            "\u001b[92mTrain accuracy: 36187/48000 =  75.39 % ||| loss 0.6762086153030396\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9092/12000 =  75.77 % ||| loss 0.6710527539253235\u001b[0m\n",
            "\u001b[92mTest accuracy: 7482/10000 =  74.82 % ||| loss 0.6903416514396667\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.6724904656410218\n",
            "Batch #200 Loss: 0.6752561256289482\n",
            "Batch #300 Loss: 0.6596848940849305\n",
            "\u001b[92mTrain accuracy: 36419/48000 =  75.87 % ||| loss 0.6536420583724976\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9157/12000 =  76.31 % ||| loss 0.6491199135780334\u001b[0m\n",
            "\u001b[92mTest accuracy: 7526/10000 =  75.26 % ||| loss 0.6661215424537659\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.6500451028347015\n",
            "Batch #200 Loss: 0.6575060960650444\n",
            "Batch #300 Loss: 0.6341293880343437\n",
            "\u001b[92mTrain accuracy: 36670/48000 =  76.4 % ||| loss 0.6338216662406921\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9214/12000 =  76.78 % ||| loss 0.6295682787895203\u001b[0m\n",
            "\u001b[92mTest accuracy: 7591/10000 =  75.91 % ||| loss 0.6448042392730713\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.624067852795124\n",
            "Batch #200 Loss: 0.630731760263443\n",
            "Batch #300 Loss: 0.6219999134540558\n",
            "\u001b[92mTrain accuracy: 36883/48000 =  76.84 % ||| loss 0.6167745590209961\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9268/12000 =  77.23 % ||| loss 0.6127287745475769\u001b[0m\n",
            "\u001b[92mTest accuracy: 7653/10000 =  76.53 % ||| loss 0.6312193274497986\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.6111209446191788\n",
            "Batch #200 Loss: 0.6128369665145874\n",
            "Batch #300 Loss: 0.6107594826817513\n",
            "\u001b[92mTrain accuracy: 37211/48000 =  77.52 % ||| loss 0.599762499332428\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9341/12000 =  77.84 % ||| loss 0.5968629121780396\u001b[0m\n",
            "\u001b[92mTest accuracy: 7714/10000 =  77.14 % ||| loss 0.6154248714447021\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5910625851154327\n",
            "Batch #200 Loss: 0.5940182414650917\n",
            "Batch #300 Loss: 0.5892238837480545\n",
            "\u001b[92mTrain accuracy: 37434/48000 =  77.99 % ||| loss 0.5850571990013123\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9401/12000 =  78.34 % ||| loss 0.5823877453804016\u001b[0m\n",
            "\u001b[92mTest accuracy: 7756/10000 =  77.56 % ||| loss 0.6000012755393982\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5895669528841972\n",
            "Batch #200 Loss: 0.5887179300189018\n",
            "Batch #300 Loss: 0.5702336218953132\n",
            "\u001b[92mTrain accuracy: 37694/48000 =  78.53 % ||| loss 0.5725660920143127\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9456/12000 =  78.8 % ||| loss 0.5702839493751526\u001b[0m\n",
            "\u001b[92mTest accuracy: 7802/10000 =  78.02 % ||| loss 0.5927552580833435\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.5654093831777572\n",
            "Batch #200 Loss: 0.5611822494864463\n",
            "Batch #300 Loss: 0.5772184339165688\n",
            "\u001b[92mTrain accuracy: 37927/48000 =  79.01 % ||| loss 0.5603976249694824\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9522/12000 =  79.35 % ||| loss 0.5583950877189636\u001b[0m\n",
            "\u001b[92mTest accuracy: 7851/10000 =  78.51 % ||| loss 0.5779542326927185\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.5688394957780838\n",
            "Batch #200 Loss: 0.553200746178627\n",
            "Batch #300 Loss: 0.5501594930887223\n",
            "\u001b[92mTrain accuracy: 38188/48000 =  79.56 % ||| loss 0.5489234328269958\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9581/12000 =  79.84 % ||| loss 0.5474759936332703\u001b[0m\n",
            "\u001b[92mTest accuracy: 7886/10000 =  78.86 % ||| loss 0.5661534667015076\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.554649973809719\n",
            "Batch #200 Loss: 0.5360003435611724\n",
            "Batch #300 Loss: 0.547756173312664\n",
            "\u001b[92mTrain accuracy: 38358/48000 =  79.91 % ||| loss 0.5393521189689636\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9640/12000 =  80.33 % ||| loss 0.5383073687553406\u001b[0m\n",
            "\u001b[92mTest accuracy: 7913/10000 =  79.13 % ||| loss 0.556857168674469\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5451806318759919\n",
            "Batch #200 Loss: 0.5342708453536034\n",
            "Batch #300 Loss: 0.5341016042232514\n",
            "\u001b[92mTrain accuracy: 38592/48000 =  80.4 % ||| loss 0.5290167927742004\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9666/12000 =  80.55 % ||| loss 0.5285744071006775\u001b[0m\n",
            "\u001b[92mTest accuracy: 7961/10000 =  79.61 % ||| loss 0.549264132976532\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5286954954266548\n",
            "Batch #200 Loss: 0.5277334168553353\n",
            "Batch #300 Loss: 0.5226784044504166\n",
            "\u001b[92mTrain accuracy: 38806/48000 =  80.85 % ||| loss 0.5205457806587219\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9727/12000 =  81.06 % ||| loss 0.5200433731079102\u001b[0m\n",
            "\u001b[92mTest accuracy: 7994/10000 =  79.94 % ||| loss 0.5404584407806396\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5256963697075844\n",
            "Batch #200 Loss: 0.5142443901300431\n",
            "Batch #300 Loss: 0.5223078563809395\n",
            "\u001b[92mTrain accuracy: 38960/48000 =  81.17 % ||| loss 0.5120529532432556\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9781/12000 =  81.51 % ||| loss 0.511765718460083\u001b[0m\n",
            "\u001b[92mTest accuracy: 8029/10000 =  80.29 % ||| loss 0.5309829711914062\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726148513.1655092_6</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_6</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_100117-Lenet5BN_1726148513.1655092_7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_7' target=\"_blank\">Lenet5BN_1726148513.1655092_7</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.165496015548706\n",
            "Batch #200 Loss: 1.6281274342536927\n",
            "Batch #300 Loss: 1.0611012583971025\n",
            "\u001b[92mTrain accuracy: 35566/48000 =  74.1 % ||| loss 0.7791801691055298\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8907/12000 =  74.22 % ||| loss 0.771898627281189\u001b[0m\n",
            "\u001b[92mTest accuracy: 7355/10000 =  73.55 % ||| loss 0.785028874874115\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7424821317195892\n",
            "Batch #200 Loss: 0.6595764964818954\n",
            "Batch #300 Loss: 0.6031594467163086\n",
            "\u001b[92mTrain accuracy: 37742/48000 =  78.63 % ||| loss 0.5656781196594238\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9462/12000 =  78.85 % ||| loss 0.5662926435470581\u001b[0m\n",
            "\u001b[92mTest accuracy: 7798/10000 =  77.98 % ||| loss 0.5794824957847595\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5598759189248085\n",
            "Batch #200 Loss: 0.5239861053228378\n",
            "Batch #300 Loss: 0.4995621424913406\n",
            "\u001b[92mTrain accuracy: 39247/48000 =  81.76 % ||| loss 0.48631465435028076\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9800/12000 =  81.67 % ||| loss 0.4906359016895294\u001b[0m\n",
            "\u001b[92mTest accuracy: 8089/10000 =  80.89 % ||| loss 0.5061493515968323\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.46661848336458206\n",
            "Batch #200 Loss: 0.4609650456905365\n",
            "Batch #300 Loss: 0.4504018014669418\n",
            "\u001b[92mTrain accuracy: 40588/48000 =  84.56 % ||| loss 0.4304378628730774\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10097/12000 =  84.14 % ||| loss 0.4359939396381378\u001b[0m\n",
            "\u001b[92mTest accuracy: 8352/10000 =  83.52 % ||| loss 0.45167821645736694\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.41852630257606505\n",
            "Batch #200 Loss: 0.41421842157840727\n",
            "Batch #300 Loss: 0.41663579404354095\n",
            "\u001b[92mTrain accuracy: 41027/48000 =  85.47 % ||| loss 0.39631974697113037\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10221/12000 =  85.17 % ||| loss 0.40403613448143005\u001b[0m\n",
            "\u001b[92mTest accuracy: 8446/10000 =  84.46 % ||| loss 0.4192050099372864\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.38277293533086776\n",
            "Batch #200 Loss: 0.39689202785491945\n",
            "Batch #300 Loss: 0.3852821531891823\n",
            "\u001b[92mTrain accuracy: 41630/48000 =  86.73 % ||| loss 0.3701685667037964\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10396/12000 =  86.63 % ||| loss 0.3785902261734009\u001b[0m\n",
            "\u001b[92mTest accuracy: 8549/10000 =  85.49 % ||| loss 0.40314775705337524\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3708633936941624\n",
            "Batch #200 Loss: 0.35598170340061186\n",
            "Batch #300 Loss: 0.3740083435177803\n",
            "\u001b[92mTrain accuracy: 41795/48000 =  87.07 % ||| loss 0.3592675030231476\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10389/12000 =  86.58 % ||| loss 0.37299877405166626\u001b[0m\n",
            "\u001b[92mTest accuracy: 8578/10000 =  85.78 % ||| loss 0.38858821988105774\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3594697877764702\n",
            "Batch #200 Loss: 0.35635592505335806\n",
            "Batch #300 Loss: 0.3448436963558197\n",
            "\u001b[92mTrain accuracy: 42144/48000 =  87.8 % ||| loss 0.34064963459968567\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10491/12000 =  87.42 % ||| loss 0.3539249300956726\u001b[0m\n",
            "\u001b[92mTest accuracy: 8656/10000 =  86.56 % ||| loss 0.3732259273529053\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3355800850689411\n",
            "Batch #200 Loss: 0.34774270191788675\n",
            "Batch #300 Loss: 0.3414579658210278\n",
            "\u001b[92mTrain accuracy: 42278/48000 =  88.08 % ||| loss 0.3304968774318695\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10492/12000 =  87.43 % ||| loss 0.3465571999549866\u001b[0m\n",
            "\u001b[92mTest accuracy: 8660/10000 =  86.6 % ||| loss 0.36737605929374695\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.32026303470134737\n",
            "Batch #200 Loss: 0.32634789556264876\n",
            "Batch #300 Loss: 0.3230262492597103\n",
            "\u001b[92mTrain accuracy: 42471/48000 =  88.48 % ||| loss 0.31922265887260437\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10557/12000 =  87.98 % ||| loss 0.337072491645813\u001b[0m\n",
            "\u001b[92mTest accuracy: 8707/10000 =  87.07 % ||| loss 0.3587783873081207\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.32786212176084517\n",
            "Batch #200 Loss: 0.3051086227595806\n",
            "Batch #300 Loss: 0.32558883786201476\n",
            "\u001b[92mTrain accuracy: 41817/48000 =  87.12 % ||| loss 0.3494918942451477\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10380/12000 =  86.5 % ||| loss 0.36821597814559937\u001b[0m\n",
            "\u001b[92mTest accuracy: 8551/10000 =  85.51 % ||| loss 0.3945970833301544\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3073874858021736\n",
            "Batch #200 Loss: 0.316167788207531\n",
            "Batch #300 Loss: 0.3137902791798115\n",
            "\u001b[92mTrain accuracy: 42955/48000 =  89.49 % ||| loss 0.2940613329410553\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10644/12000 =  88.7 % ||| loss 0.3166963756084442\u001b[0m\n",
            "\u001b[92mTest accuracy: 8798/10000 =  87.98 % ||| loss 0.33732372522354126\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.3049247266352177\n",
            "Batch #200 Loss: 0.3056963612139225\n",
            "Batch #300 Loss: 0.2996198755502701\n",
            "\u001b[92mTrain accuracy: 42908/48000 =  89.39 % ||| loss 0.29871731996536255\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10584/12000 =  88.2 % ||| loss 0.3276771903038025\u001b[0m\n",
            "\u001b[92mTest accuracy: 8773/10000 =  87.73 % ||| loss 0.34835660457611084\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3079333961009979\n",
            "Batch #200 Loss: 0.28780112192034724\n",
            "Batch #300 Loss: 0.2865353085100651\n",
            "\u001b[92mTrain accuracy: 43040/48000 =  89.67 % ||| loss 0.28866663575172424\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10626/12000 =  88.55 % ||| loss 0.3155733644962311\u001b[0m\n",
            "\u001b[92mTest accuracy: 8813/10000 =  88.13 % ||| loss 0.33997511863708496\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.2923053032159805\n",
            "Batch #200 Loss: 0.2846329042315483\n",
            "Batch #300 Loss: 0.2920677974820137\n",
            "\u001b[92mTrain accuracy: 43043/48000 =  89.67 % ||| loss 0.2858630418777466\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10654/12000 =  88.78 % ||| loss 0.3148011267185211\u001b[0m\n",
            "\u001b[92mTest accuracy: 8795/10000 =  87.95 % ||| loss 0.3373771011829376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.28708293572068216\n",
            "Batch #200 Loss: 0.2822145949304104\n",
            "Batch #300 Loss: 0.2846322172880173\n",
            "\u001b[92mTrain accuracy: 42879/48000 =  89.33 % ||| loss 0.2961563169956207\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10596/12000 =  88.3 % ||| loss 0.3234280049800873\u001b[0m\n",
            "\u001b[92mTest accuracy: 8777/10000 =  87.77 % ||| loss 0.34644126892089844\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.28407268643379213\n",
            "Batch #200 Loss: 0.27720247641205786\n",
            "Batch #300 Loss: 0.27853216633200645\n",
            "\u001b[92mTrain accuracy: 43089/48000 =  89.77 % ||| loss 0.28308403491973877\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10636/12000 =  88.63 % ||| loss 0.3162080943584442\u001b[0m\n",
            "\u001b[92mTest accuracy: 8804/10000 =  88.04 % ||| loss 0.33780747652053833\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.2877584053575993\n",
            "Batch #200 Loss: 0.2676661777496338\n",
            "Batch #300 Loss: 0.27557878747582437\n",
            "\u001b[92mTrain accuracy: 43419/48000 =  90.46 % ||| loss 0.2681904435157776\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10673/12000 =  88.94 % ||| loss 0.3022799491882324\u001b[0m\n",
            "\u001b[92mTest accuracy: 8823/10000 =  88.23 % ||| loss 0.32837122678756714\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.26755211025476455\n",
            "Batch #200 Loss: 0.270355140119791\n",
            "Batch #300 Loss: 0.2787232613563538\n",
            "\u001b[92mTrain accuracy: 43546/48000 =  90.72 % ||| loss 0.26044759154319763\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10729/12000 =  89.41 % ||| loss 0.29395076632499695\u001b[0m\n",
            "\u001b[92mTest accuracy: 8855/10000 =  88.55 % ||| loss 0.31748440861701965\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.26409366250038147\n",
            "Batch #200 Loss: 0.26442885398864746\n",
            "Batch #300 Loss: 0.2672236306965351\n",
            "\u001b[92mTrain accuracy: 43562/48000 =  90.75 % ||| loss 0.25946342945098877\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10714/12000 =  89.28 % ||| loss 0.2968386709690094\u001b[0m\n",
            "\u001b[92mTest accuracy: 8849/10000 =  88.49 % ||| loss 0.32034823298454285\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.25408187180757524\n",
            "Batch #200 Loss: 0.2565312211215496\n",
            "Batch #300 Loss: 0.278775744587183\n",
            "\u001b[92mTrain accuracy: 42906/48000 =  89.39 % ||| loss 0.28481602668762207\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10587/12000 =  88.22 % ||| loss 0.31783974170684814\u001b[0m\n",
            "\u001b[92mTest accuracy: 8730/10000 =  87.3 % ||| loss 0.349998414516449\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.26320080943405627\n",
            "Batch #200 Loss: 0.2546847976744175\n",
            "Batch #300 Loss: 0.26415706872940065\n",
            "\u001b[92mTrain accuracy: 43605/48000 =  90.84 % ||| loss 0.25301527976989746\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10735/12000 =  89.46 % ||| loss 0.2915566861629486\u001b[0m\n",
            "\u001b[92mTest accuracy: 8875/10000 =  88.75 % ||| loss 0.3172357976436615\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.2576491526514292\n",
            "Batch #200 Loss: 0.25679325595498087\n",
            "Batch #300 Loss: 0.2510445475578308\n",
            "\u001b[92mTrain accuracy: 43780/48000 =  91.21 % ||| loss 0.24414022266864777\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10732/12000 =  89.43 % ||| loss 0.28783461451530457\u001b[0m\n",
            "\u001b[92mTest accuracy: 8892/10000 =  88.92 % ||| loss 0.3094269931316376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.24956148326396943\n",
            "Batch #200 Loss: 0.2596384910494089\n",
            "Batch #300 Loss: 0.2575462570786476\n",
            "\u001b[92mTrain accuracy: 43810/48000 =  91.27 % ||| loss 0.2461724579334259\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10760/12000 =  89.67 % ||| loss 0.28827401995658875\u001b[0m\n",
            "\u001b[92mTest accuracy: 8883/10000 =  88.83 % ||| loss 0.31631433963775635\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.23646336778998375\n",
            "Batch #200 Loss: 0.25609004475176333\n",
            "Batch #300 Loss: 0.2600415399670601\n",
            "\u001b[92mTrain accuracy: 43594/48000 =  90.82 % ||| loss 0.2559130787849426\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10675/12000 =  88.96 % ||| loss 0.30203983187675476\u001b[0m\n",
            "\u001b[92mTest accuracy: 8840/10000 =  88.4 % ||| loss 0.3251522183418274\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726148513.1655092_7</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_7</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_100401-Lenet5BN_1726148513.1655092_8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_8' target=\"_blank\">Lenet5BN_1726148513.1655092_8</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5BN XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.242199652194977\n",
            "Batch #200 Loss: 2.094474229812622\n",
            "Batch #300 Loss: 1.8766529607772826\n",
            "\u001b[92mTrain accuracy: 28260/48000 =  58.88 % ||| loss 1.555908203125\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7012/12000 =  58.43 % ||| loss 1.5551756620407104\u001b[0m\n",
            "\u001b[92mTest accuracy: 5859/10000 =  58.59 % ||| loss 1.5588762760162354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 1.4357602298259735\n",
            "Batch #200 Loss: 1.2307883644104003\n",
            "Batch #300 Loss: 1.068445537686348\n",
            "\u001b[92mTrain accuracy: 34561/48000 =  72.0 % ||| loss 0.9405660629272461\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8671/12000 =  72.26 % ||| loss 0.9356590509414673\u001b[0m\n",
            "\u001b[92mTest accuracy: 7185/10000 =  71.85 % ||| loss 0.9437160491943359\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.8934063148498536\n",
            "Batch #200 Loss: 0.8459268581867218\n",
            "Batch #300 Loss: 0.793720206618309\n",
            "\u001b[92mTrain accuracy: 35802/48000 =  74.59 % ||| loss 0.7429060935974121\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8977/12000 =  74.81 % ||| loss 0.738099217414856\u001b[0m\n",
            "\u001b[92mTest accuracy: 7425/10000 =  74.25 % ||| loss 0.7480243444442749\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.7275094312429428\n",
            "Batch #200 Loss: 0.7023683661222457\n",
            "Batch #300 Loss: 0.6738986706733704\n",
            "\u001b[92mTrain accuracy: 36752/48000 =  76.57 % ||| loss 0.650076687335968\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9243/12000 =  77.03 % ||| loss 0.645182728767395\u001b[0m\n",
            "\u001b[92mTest accuracy: 7619/10000 =  76.19 % ||| loss 0.6592205762863159\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.6318731904029846\n",
            "Batch #200 Loss: 0.6323638996481895\n",
            "Batch #300 Loss: 0.6019468331336975\n",
            "\u001b[92mTrain accuracy: 37468/48000 =  78.06 % ||| loss 0.5895676612854004\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9427/12000 =  78.56 % ||| loss 0.5858271718025208\u001b[0m\n",
            "\u001b[92mTest accuracy: 7759/10000 =  77.59 % ||| loss 0.5992471575737\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5839966452121734\n",
            "Batch #200 Loss: 0.5801411998271943\n",
            "Batch #300 Loss: 0.5535219967365265\n",
            "\u001b[92mTrain accuracy: 38211/48000 =  79.61 % ||| loss 0.54575514793396\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9617/12000 =  80.14 % ||| loss 0.5431646704673767\u001b[0m\n",
            "\u001b[92mTest accuracy: 7935/10000 =  79.35 % ||| loss 0.5574280619621277\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5522432997822762\n",
            "Batch #200 Loss: 0.5324391379952431\n",
            "Batch #300 Loss: 0.5219103834033012\n",
            "\u001b[92mTrain accuracy: 38888/48000 =  81.02 % ||| loss 0.5130540132522583\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9760/12000 =  81.33 % ||| loss 0.5117275714874268\u001b[0m\n",
            "\u001b[92mTest accuracy: 8063/10000 =  80.63 % ||| loss 0.5254889130592346\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.509424215555191\n",
            "Batch #200 Loss: 0.517998004257679\n",
            "Batch #300 Loss: 0.4915580406785011\n",
            "\u001b[92mTrain accuracy: 39357/48000 =  81.99 % ||| loss 0.48897022008895874\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9885/12000 =  82.38 % ||| loss 0.48973697423934937\u001b[0m\n",
            "\u001b[92mTest accuracy: 8154/10000 =  81.54 % ||| loss 0.5038870573043823\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.48226834744215014\n",
            "Batch #200 Loss: 0.47604219675064086\n",
            "Batch #300 Loss: 0.4840602689981461\n",
            "\u001b[92mTrain accuracy: 39879/48000 =  83.08 % ||| loss 0.46440547704696655\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9977/12000 =  83.14 % ||| loss 0.46710023283958435\u001b[0m\n",
            "\u001b[92mTest accuracy: 8242/10000 =  82.42 % ||| loss 0.4810650944709778\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.45942786008119585\n",
            "Batch #200 Loss: 0.46728425592184064\n",
            "Batch #300 Loss: 0.45411986351013184\n",
            "\u001b[92mTrain accuracy: 40275/48000 =  83.91 % ||| loss 0.44808638095855713\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10063/12000 =  83.86 % ||| loss 0.4517364799976349\u001b[0m\n",
            "\u001b[92mTest accuracy: 8296/10000 =  82.96 % ||| loss 0.46669715642929077\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.4371261182427406\n",
            "Batch #200 Loss: 0.44814616233110427\n",
            "Batch #300 Loss: 0.4438902705907822\n",
            "\u001b[92mTrain accuracy: 40554/48000 =  84.49 % ||| loss 0.4332255423069\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10114/12000 =  84.28 % ||| loss 0.438016802072525\u001b[0m\n",
            "\u001b[92mTest accuracy: 8361/10000 =  83.61 % ||| loss 0.4581790268421173\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.4433714792132378\n",
            "Batch #200 Loss: 0.42684614777565\n",
            "Batch #300 Loss: 0.42121838957071306\n",
            "\u001b[92mTrain accuracy: 40816/48000 =  85.03 % ||| loss 0.4190111458301544\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10146/12000 =  84.55 % ||| loss 0.42559632658958435\u001b[0m\n",
            "\u001b[92mTest accuracy: 8418/10000 =  84.18 % ||| loss 0.44035735726356506\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.42344104886054995\n",
            "Batch #200 Loss: 0.4200749018788338\n",
            "Batch #300 Loss: 0.4141184487938881\n",
            "\u001b[92mTrain accuracy: 40988/48000 =  85.39 % ||| loss 0.4074343740940094\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10239/12000 =  85.32 % ||| loss 0.41419947147369385\u001b[0m\n",
            "\u001b[92mTest accuracy: 8437/10000 =  84.37 % ||| loss 0.43447792530059814\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.40723819941282274\n",
            "Batch #200 Loss: 0.40973135143518447\n",
            "Batch #300 Loss: 0.4015563675761223\n",
            "\u001b[92mTrain accuracy: 41122/48000 =  85.67 % ||| loss 0.3976440727710724\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10256/12000 =  85.47 % ||| loss 0.40446987748146057\u001b[0m\n",
            "\u001b[92mTest accuracy: 8484/10000 =  84.84 % ||| loss 0.4227791428565979\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.39789517879486086\n",
            "Batch #200 Loss: 0.3943426303565502\n",
            "Batch #300 Loss: 0.3902239318192005\n",
            "\u001b[92mTrain accuracy: 41385/48000 =  86.22 % ||| loss 0.3860476016998291\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10299/12000 =  85.82 % ||| loss 0.3938608467578888\u001b[0m\n",
            "\u001b[92mTest accuracy: 8529/10000 =  85.29 % ||| loss 0.40881067514419556\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.38332541048526764\n",
            "Batch #200 Loss: 0.38237509191036223\n",
            "Batch #300 Loss: 0.3837778291106224\n",
            "\u001b[92mTrain accuracy: 41488/48000 =  86.43 % ||| loss 0.37761369347572327\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10326/12000 =  86.05 % ||| loss 0.38711264729499817\u001b[0m\n",
            "\u001b[92mTest accuracy: 8542/10000 =  85.42 % ||| loss 0.40944063663482666\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.37515427365899084\n",
            "Batch #200 Loss: 0.3737948574125767\n",
            "Batch #300 Loss: 0.37851773053407667\n",
            "\u001b[92mTrain accuracy: 41648/48000 =  86.77 % ||| loss 0.3690169155597687\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10374/12000 =  86.45 % ||| loss 0.37888020277023315\u001b[0m\n",
            "\u001b[92mTest accuracy: 8555/10000 =  85.55 % ||| loss 0.39702919125556946\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3726616157591343\n",
            "Batch #200 Loss: 0.3686837092041969\n",
            "Batch #300 Loss: 0.3621673944592476\n",
            "\u001b[92mTrain accuracy: 41754/48000 =  86.99 % ||| loss 0.3624178469181061\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10383/12000 =  86.52 % ||| loss 0.3727954626083374\u001b[0m\n",
            "\u001b[92mTest accuracy: 8585/10000 =  85.85 % ||| loss 0.3893076777458191\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.3634139959514141\n",
            "Batch #200 Loss: 0.36542993009090424\n",
            "Batch #300 Loss: 0.3617129011452198\n",
            "\u001b[92mTrain accuracy: 41808/48000 =  87.1 % ||| loss 0.3570801615715027\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10421/12000 =  86.84 % ||| loss 0.3679867088794708\u001b[0m\n",
            "\u001b[92mTest accuracy: 8605/10000 =  86.05 % ||| loss 0.3868066072463989\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.3597928485274315\n",
            "Batch #200 Loss: 0.3624751037359238\n",
            "Batch #300 Loss: 0.3472773517668247\n",
            "\u001b[92mTrain accuracy: 41957/48000 =  87.41 % ||| loss 0.3498285710811615\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10450/12000 =  87.08 % ||| loss 0.36113840341567993\u001b[0m\n",
            "\u001b[92mTest accuracy: 8650/10000 =  86.5 % ||| loss 0.3844783306121826\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.34389621689915656\n",
            "Batch #200 Loss: 0.3544100522994995\n",
            "Batch #300 Loss: 0.3459643669426441\n",
            "\u001b[92mTrain accuracy: 41883/48000 =  87.26 % ||| loss 0.3515815734863281\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10400/12000 =  86.67 % ||| loss 0.36416056752204895\u001b[0m\n",
            "\u001b[92mTest accuracy: 8589/10000 =  85.89 % ||| loss 0.38116365671157837\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.3562851616740227\n",
            "Batch #200 Loss: 0.3369420839846134\n",
            "Batch #300 Loss: 0.3411534394323826\n",
            "\u001b[92mTrain accuracy: 42211/48000 =  87.94 % ||| loss 0.3371961712837219\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10481/12000 =  87.34 % ||| loss 0.3507702052593231\u001b[0m\n",
            "\u001b[92mTest accuracy: 8659/10000 =  86.59 % ||| loss 0.3697999119758606\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.3461582592129707\n",
            "Batch #200 Loss: 0.3361000411212444\n",
            "Batch #300 Loss: 0.32707202672958374\n",
            "\u001b[92mTrain accuracy: 42260/48000 =  88.04 % ||| loss 0.33207768201828003\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10508/12000 =  87.57 % ||| loss 0.3455018103122711\u001b[0m\n",
            "\u001b[92mTest accuracy: 8683/10000 =  86.83 % ||| loss 0.3709769546985626\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.32202054142951964\n",
            "Batch #200 Loss: 0.3448669095337391\n",
            "Batch #300 Loss: 0.336538295596838\n",
            "\u001b[92mTrain accuracy: 42227/48000 =  87.97 % ||| loss 0.33332791924476624\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10489/12000 =  87.41 % ||| loss 0.3487367331981659\u001b[0m\n",
            "\u001b[92mTest accuracy: 8684/10000 =  86.84 % ||| loss 0.3744119107723236\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3322648076713085\n",
            "Batch #200 Loss: 0.3249686349928379\n",
            "Batch #300 Loss: 0.3311384303867817\n",
            "\u001b[92mTrain accuracy: 42444/48000 =  88.42 % ||| loss 0.3240678012371063\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10523/12000 =  87.69 % ||| loss 0.3397788405418396\u001b[0m\n",
            "\u001b[92mTest accuracy: 8699/10000 =  86.99 % ||| loss 0.3684220016002655\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5BN_1726148513.1655092_8</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5BN_1726148513.1655092_8</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[93m!!!!!!! Hyper Param Tuning Finished!!!!!!!!!!!\u001b[0m\n",
            "Best Model: Lenet5BN(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            "  (BN1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (BN2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "wandb name: Lenet5BN_1726148513.1655092_4\n",
            "\n",
            "HyperParams: {'learning_rate': 0.001, 'momentum': 0.7}\n",
            "\n",
            "Accuracies: {'train': 0.9585416666666666, 'val': 0.9048333333333334, 'test': 0.8964}\n"
          ]
        }
      ],
      "source": [
        "class Lenet5BN(Lenet5):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.BN1 = nn.BatchNorm2d(6)\n",
        "        self.BN2 = nn.BatchNorm2d(16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.BN1(x))\n",
        "        x = self.max_pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(self.BN2(x))\n",
        "        x = self.max_pool2(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "  'learning_rate':[0.1, 0.01,0.001],\n",
        "  'momentum':[0, 0.9, 0.7]\n",
        "}\n",
        "\n",
        "best_batchnorm = hyperparameter_tuning(Lenet5BN, dataloaders, device, 25, **param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krfDLKqLbbyu"
      },
      "source": [
        "#### Using Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vN9DTB7ILfqk"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_100750-Lenet5Dropout_1726150070.105228_0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_0' target=\"_blank\">Lenet5Dropout_1726150070.105228_0</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.073439586162567\n",
            "Batch #200 Loss: 1.1145613950490951\n",
            "Batch #300 Loss: 0.8360087215900421\n",
            "\u001b[92mTrain accuracy: 34685/48000 =  72.26 % ||| loss 0.6898988485336304\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8656/12000 =  72.13 % ||| loss 0.6835964322090149\u001b[0m\n",
            "\u001b[92mTest accuracy: 7160/10000 =  71.6 % ||| loss 0.716858983039856\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6896619722247124\n",
            "Batch #200 Loss: 0.6473941630125046\n",
            "Batch #300 Loss: 0.5799733337759971\n",
            "\u001b[92mTrain accuracy: 39256/48000 =  81.78 % ||| loss 0.4768204689025879\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9839/12000 =  81.99 % ||| loss 0.4726957380771637\u001b[0m\n",
            "\u001b[92mTest accuracy: 8088/10000 =  80.88 % ||| loss 0.4942706227302551\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5371308833360672\n",
            "Batch #200 Loss: 0.5234456199407578\n",
            "Batch #300 Loss: 0.5068196108937264\n",
            "\u001b[92mTrain accuracy: 40434/48000 =  84.24 % ||| loss 0.4212760329246521\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10107/12000 =  84.23 % ||| loss 0.42225703597068787\u001b[0m\n",
            "\u001b[92mTest accuracy: 8357/10000 =  83.57 % ||| loss 0.4477025866508484\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.4645337462425232\n",
            "Batch #200 Loss: 0.46214702636003496\n",
            "Batch #300 Loss: 0.4530916118621826\n",
            "\u001b[92mTrain accuracy: 41267/48000 =  85.97 % ||| loss 0.38116854429244995\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10302/12000 =  85.85 % ||| loss 0.38840538263320923\u001b[0m\n",
            "\u001b[92mTest accuracy: 8500/10000 =  85.0 % ||| loss 0.4085255265235901\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.42918140918016434\n",
            "Batch #200 Loss: 0.4140517543256283\n",
            "Batch #300 Loss: 0.42394406974315646\n",
            "\u001b[92mTrain accuracy: 41595/48000 =  86.66 % ||| loss 0.3571687340736389\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10361/12000 =  86.34 % ||| loss 0.3678179681301117\u001b[0m\n",
            "\u001b[92mTest accuracy: 8593/10000 =  85.93 % ||| loss 0.3858785629272461\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.40593947887420656\n",
            "Batch #200 Loss: 0.39673938423395155\n",
            "Batch #300 Loss: 0.3887400858104229\n",
            "\u001b[92mTrain accuracy: 41750/48000 =  86.98 % ||| loss 0.35521379113197327\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10347/12000 =  86.22 % ||| loss 0.3666017949581146\u001b[0m\n",
            "\u001b[92mTest accuracy: 8553/10000 =  85.53 % ||| loss 0.389760285615921\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.38102314203977583\n",
            "Batch #200 Loss: 0.37146002367138864\n",
            "Batch #300 Loss: 0.36902720630168917\n",
            "\u001b[92mTrain accuracy: 42277/48000 =  88.08 % ||| loss 0.3242606520652771\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10489/12000 =  87.41 % ||| loss 0.34260693192481995\u001b[0m\n",
            "\u001b[92mTest accuracy: 8679/10000 =  86.79 % ||| loss 0.35696882009506226\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3563410823047161\n",
            "Batch #200 Loss: 0.361922459602356\n",
            "Batch #300 Loss: 0.3516358554363251\n",
            "\u001b[92mTrain accuracy: 41751/48000 =  86.98 % ||| loss 0.3391406536102295\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10349/12000 =  86.24 % ||| loss 0.3557087481021881\u001b[0m\n",
            "\u001b[92mTest accuracy: 8539/10000 =  85.39 % ||| loss 0.37835344672203064\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3486095717549324\n",
            "Batch #200 Loss: 0.34668847411870957\n",
            "Batch #300 Loss: 0.34410019740462305\n",
            "\u001b[92mTrain accuracy: 42694/48000 =  88.95 % ||| loss 0.3010717034339905\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10563/12000 =  88.02 % ||| loss 0.3236032724380493\u001b[0m\n",
            "\u001b[92mTest accuracy: 8763/10000 =  87.63 % ||| loss 0.34204599261283875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.32927614733576777\n",
            "Batch #200 Loss: 0.33227270424365996\n",
            "Batch #300 Loss: 0.33507101953029633\n",
            "\u001b[92mTrain accuracy: 42962/48000 =  89.5 % ||| loss 0.28600171208381653\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10631/12000 =  88.59 % ||| loss 0.31015434861183167\u001b[0m\n",
            "\u001b[92mTest accuracy: 8795/10000 =  87.95 % ||| loss 0.32935160398483276\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.31577068015933035\n",
            "Batch #200 Loss: 0.32256456166505815\n",
            "Batch #300 Loss: 0.3163224141299725\n",
            "\u001b[92mTrain accuracy: 43000/48000 =  89.58 % ||| loss 0.27998554706573486\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10664/12000 =  88.87 % ||| loss 0.3075491189956665\u001b[0m\n",
            "\u001b[92mTest accuracy: 8805/10000 =  88.05 % ||| loss 0.33299121260643005\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3145433957874775\n",
            "Batch #200 Loss: 0.31639001235365866\n",
            "Batch #300 Loss: 0.30999084055423737\n",
            "\u001b[92mTrain accuracy: 43272/48000 =  90.15 % ||| loss 0.26741641759872437\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10690/12000 =  89.08 % ||| loss 0.2928948998451233\u001b[0m\n",
            "\u001b[92mTest accuracy: 8836/10000 =  88.36 % ||| loss 0.3174038529396057\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.29427778512239455\n",
            "Batch #200 Loss: 0.3011704045534134\n",
            "Batch #300 Loss: 0.3081767770648003\n",
            "\u001b[92mTrain accuracy: 43517/48000 =  90.66 % ||| loss 0.2547479569911957\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10736/12000 =  89.47 % ||| loss 0.28418606519699097\u001b[0m\n",
            "\u001b[92mTest accuracy: 8880/10000 =  88.8 % ||| loss 0.3054390847682953\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.28984039664268496\n",
            "Batch #200 Loss: 0.29124397709965705\n",
            "Batch #300 Loss: 0.3009889215230942\n",
            "\u001b[92mTrain accuracy: 43439/48000 =  90.5 % ||| loss 0.2579037547111511\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10681/12000 =  89.01 % ||| loss 0.2927440106868744\u001b[0m\n",
            "\u001b[92mTest accuracy: 8842/10000 =  88.42 % ||| loss 0.31669148802757263\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.28601663693785667\n",
            "Batch #200 Loss: 0.2908913987874985\n",
            "Batch #300 Loss: 0.29205889329314233\n",
            "\u001b[92mTrain accuracy: 43501/48000 =  90.63 % ||| loss 0.2522244155406952\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10742/12000 =  89.52 % ||| loss 0.28887301683425903\u001b[0m\n",
            "\u001b[92mTest accuracy: 8881/10000 =  88.81 % ||| loss 0.31207773089408875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.2864979496598244\n",
            "Batch #200 Loss: 0.2857760167121887\n",
            "Batch #300 Loss: 0.2748286534845829\n",
            "\u001b[92mTrain accuracy: 43853/48000 =  91.36 % ||| loss 0.23216381669044495\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10801/12000 =  90.01 % ||| loss 0.2748396098613739\u001b[0m\n",
            "\u001b[92mTest accuracy: 8957/10000 =  89.57 % ||| loss 0.28896045684814453\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.27302916705608365\n",
            "Batch #200 Loss: 0.2781610115617514\n",
            "Batch #300 Loss: 0.2676651345193386\n",
            "\u001b[92mTrain accuracy: 43860/48000 =  91.38 % ||| loss 0.232394739985466\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10785/12000 =  89.88 % ||| loss 0.27664080262184143\u001b[0m\n",
            "\u001b[92mTest accuracy: 8932/10000 =  89.32 % ||| loss 0.2981538772583008\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.2648021665215492\n",
            "Batch #200 Loss: 0.27433028668165205\n",
            "Batch #300 Loss: 0.2557897657155991\n",
            "\u001b[92mTrain accuracy: 44024/48000 =  91.72 % ||| loss 0.22793829441070557\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10782/12000 =  89.85 % ||| loss 0.2739741802215576\u001b[0m\n",
            "\u001b[92mTest accuracy: 8954/10000 =  89.54 % ||| loss 0.29132646322250366\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.2597867901623249\n",
            "Batch #200 Loss: 0.2618342314660549\n",
            "Batch #300 Loss: 0.26517245814204216\n",
            "\u001b[92mTrain accuracy: 44075/48000 =  91.82 % ||| loss 0.21902231872081757\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10826/12000 =  90.22 % ||| loss 0.2666141986846924\u001b[0m\n",
            "\u001b[92mTest accuracy: 8963/10000 =  89.63 % ||| loss 0.28401094675064087\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.2506892040371895\n",
            "Batch #200 Loss: 0.2693453554809093\n",
            "Batch #300 Loss: 0.2540616399049759\n",
            "\u001b[92mTrain accuracy: 44232/48000 =  92.15 % ||| loss 0.21192999184131622\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10822/12000 =  90.18 % ||| loss 0.26642632484436035\u001b[0m\n",
            "\u001b[92mTest accuracy: 8989/10000 =  89.89 % ||| loss 0.28791600465774536\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.24902091726660727\n",
            "Batch #200 Loss: 0.25783455193042754\n",
            "Batch #300 Loss: 0.25620308160781863\n",
            "\u001b[92mTrain accuracy: 44191/48000 =  92.06 % ||| loss 0.20995095372200012\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10837/12000 =  90.31 % ||| loss 0.26842838525772095\u001b[0m\n",
            "\u001b[92mTest accuracy: 8966/10000 =  89.66 % ||| loss 0.29691946506500244\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.24103685110807418\n",
            "Batch #200 Loss: 0.2457515712827444\n",
            "Batch #300 Loss: 0.2567926913499832\n",
            "\u001b[92mTrain accuracy: 44318/48000 =  92.33 % ||| loss 0.2041289061307907\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10825/12000 =  90.21 % ||| loss 0.26864755153656006\u001b[0m\n",
            "\u001b[92mTest accuracy: 8959/10000 =  89.59 % ||| loss 0.2886408865451813\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.22830810606479646\n",
            "Batch #200 Loss: 0.2497415979206562\n",
            "Batch #300 Loss: 0.24232336059212684\n",
            "\u001b[92mTrain accuracy: 44346/48000 =  92.39 % ||| loss 0.20957092940807343\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10868/12000 =  90.57 % ||| loss 0.26448461413383484\u001b[0m\n",
            "\u001b[92mTest accuracy: 8977/10000 =  89.77 % ||| loss 0.28917616605758667\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.23622436948120595\n",
            "Batch #200 Loss: 0.23594635784626006\n",
            "Batch #300 Loss: 0.2381540921330452\n",
            "\u001b[92mTrain accuracy: 44482/48000 =  92.67 % ||| loss 0.19512227177619934\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10854/12000 =  90.45 % ||| loss 0.26225078105926514\u001b[0m\n",
            "\u001b[92mTest accuracy: 8990/10000 =  89.9 % ||| loss 0.28498393297195435\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.2359978200495243\n",
            "Batch #200 Loss: 0.23760149851441384\n",
            "Batch #300 Loss: 0.23061106517910956\n",
            "\u001b[92mTrain accuracy: 44708/48000 =  93.14 % ||| loss 0.192829430103302\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10851/12000 =  90.42 % ||| loss 0.2617681920528412\u001b[0m\n",
            "\u001b[92mTest accuracy: 8996/10000 =  89.96 % ||| loss 0.2818288505077362\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_0</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_0</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_101023-Lenet5Dropout_1726150070.105228_1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_1' target=\"_blank\">Lenet5Dropout_1726150070.105228_1</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.290601394176483\n",
            "Batch #200 Loss: 1.5492647409439086\n",
            "Batch #300 Loss: 1.0987387645244597\n",
            "\u001b[92mTrain accuracy: 35503/48000 =  73.96 % ||| loss 0.70895916223526\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8907/12000 =  74.22 % ||| loss 0.6957642436027527\u001b[0m\n",
            "\u001b[92mTest accuracy: 7338/10000 =  73.38 % ||| loss 0.7273643612861633\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.801567223072052\n",
            "Batch #200 Loss: 0.7367712634801865\n",
            "Batch #300 Loss: 0.6861104968190194\n",
            "\u001b[92mTrain accuracy: 36672/48000 =  76.4 % ||| loss 0.590094804763794\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9154/12000 =  76.28 % ||| loss 0.5883119702339172\u001b[0m\n",
            "\u001b[92mTest accuracy: 7553/10000 =  75.53 % ||| loss 0.6138530373573303\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6268688833713532\n",
            "Batch #200 Loss: 0.585225734114647\n",
            "Batch #300 Loss: 0.5708734855055809\n",
            "\u001b[92mTrain accuracy: 39397/48000 =  82.08 % ||| loss 0.4970196485519409\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9856/12000 =  82.13 % ||| loss 0.4969791769981384\u001b[0m\n",
            "\u001b[92mTest accuracy: 8094/10000 =  80.94 % ||| loss 0.5267491936683655\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5420585283637047\n",
            "Batch #200 Loss: 0.5174636662006378\n",
            "Batch #300 Loss: 0.5259527319669723\n",
            "\u001b[92mTrain accuracy: 39970/48000 =  83.27 % ||| loss 0.46391910314559937\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9985/12000 =  83.21 % ||| loss 0.4600142538547516\u001b[0m\n",
            "\u001b[92mTest accuracy: 8240/10000 =  82.4 % ||| loss 0.49358034133911133\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.48903994500637055\n",
            "Batch #200 Loss: 0.4870714822411537\n",
            "Batch #300 Loss: 0.4835820093750954\n",
            "\u001b[92mTrain accuracy: 40806/48000 =  85.01 % ||| loss 0.39824357628822327\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10165/12000 =  84.71 % ||| loss 0.40113478899002075\u001b[0m\n",
            "\u001b[92mTest accuracy: 8426/10000 =  84.26 % ||| loss 0.4245266914367676\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.4513592180609703\n",
            "Batch #200 Loss: 0.46462601870298387\n",
            "Batch #300 Loss: 0.4518018102645874\n",
            "\u001b[92mTrain accuracy: 41382/48000 =  86.21 % ||| loss 0.3779873549938202\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10307/12000 =  85.89 % ||| loss 0.3840664029121399\u001b[0m\n",
            "\u001b[92mTest accuracy: 8530/10000 =  85.3 % ||| loss 0.40690556168556213\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.44112069517374036\n",
            "Batch #200 Loss: 0.43319774299860003\n",
            "Batch #300 Loss: 0.4204756435751915\n",
            "\u001b[92mTrain accuracy: 41577/48000 =  86.62 % ||| loss 0.3660965859889984\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10340/12000 =  86.17 % ||| loss 0.3747316300868988\u001b[0m\n",
            "\u001b[92mTest accuracy: 8565/10000 =  85.65 % ||| loss 0.399055540561676\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.39774680361151693\n",
            "Batch #200 Loss: 0.4193268868327141\n",
            "Batch #300 Loss: 0.4112603583931923\n",
            "\u001b[92mTrain accuracy: 42094/48000 =  87.7 % ||| loss 0.33404895663261414\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10451/12000 =  87.09 % ||| loss 0.3481072783470154\u001b[0m\n",
            "\u001b[92mTest accuracy: 8682/10000 =  86.82 % ||| loss 0.36566975712776184\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.38667214825749396\n",
            "Batch #200 Loss: 0.39913526773452757\n",
            "Batch #300 Loss: 0.3935174222290516\n",
            "\u001b[92mTrain accuracy: 42360/48000 =  88.25 % ||| loss 0.3185528814792633\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10508/12000 =  87.57 % ||| loss 0.3364710509777069\u001b[0m\n",
            "\u001b[92mTest accuracy: 8721/10000 =  87.21 % ||| loss 0.3547631800174713\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.37389207884669307\n",
            "Batch #200 Loss: 0.3849608573317528\n",
            "Batch #300 Loss: 0.3778142961859703\n",
            "\u001b[92mTrain accuracy: 42459/48000 =  88.46 % ||| loss 0.31048229336738586\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10527/12000 =  87.72 % ||| loss 0.33066046237945557\u001b[0m\n",
            "\u001b[92mTest accuracy: 8729/10000 =  87.29 % ||| loss 0.3512464761734009\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.37742292031645774\n",
            "Batch #200 Loss: 0.3543038406968117\n",
            "Batch #300 Loss: 0.36021132454276084\n",
            "\u001b[92mTrain accuracy: 42817/48000 =  89.2 % ||| loss 0.29337993264198303\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10618/12000 =  88.48 % ||| loss 0.312173068523407\u001b[0m\n",
            "\u001b[92mTest accuracy: 8797/10000 =  87.97 % ||| loss 0.3346802294254303\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3530807267129421\n",
            "Batch #200 Loss: 0.3388119529187679\n",
            "Batch #300 Loss: 0.3512376943230629\n",
            "\u001b[92mTrain accuracy: 42918/48000 =  89.41 % ||| loss 0.2877570688724518\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10620/12000 =  88.5 % ||| loss 0.3102659583091736\u001b[0m\n",
            "\u001b[92mTest accuracy: 8779/10000 =  87.79 % ||| loss 0.3324061632156372\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.3365068788826466\n",
            "Batch #200 Loss: 0.34364740520715714\n",
            "Batch #300 Loss: 0.3516353116929531\n",
            "\u001b[92mTrain accuracy: 42993/48000 =  89.57 % ||| loss 0.28107506036758423\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10630/12000 =  88.58 % ||| loss 0.30714747309684753\u001b[0m\n",
            "\u001b[92mTest accuracy: 8806/10000 =  88.06 % ||| loss 0.3272939920425415\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.31891763061285017\n",
            "Batch #200 Loss: 0.3341021908819675\n",
            "Batch #300 Loss: 0.34659882038831713\n",
            "\u001b[92mTrain accuracy: 43209/48000 =  90.02 % ||| loss 0.271584153175354\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10663/12000 =  88.86 % ||| loss 0.2990732491016388\u001b[0m\n",
            "\u001b[92mTest accuracy: 8840/10000 =  88.4 % ||| loss 0.3184940814971924\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.32257958114147184\n",
            "Batch #200 Loss: 0.3232125580310822\n",
            "Batch #300 Loss: 0.3289641910791397\n",
            "\u001b[92mTrain accuracy: 43176/48000 =  89.95 % ||| loss 0.27111223340034485\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10686/12000 =  89.05 % ||| loss 0.30287986993789673\u001b[0m\n",
            "\u001b[92mTest accuracy: 8844/10000 =  88.44 % ||| loss 0.3179895281791687\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3183527274429798\n",
            "Batch #200 Loss: 0.3229523964226246\n",
            "Batch #300 Loss: 0.32089707016944885\n",
            "\u001b[92mTrain accuracy: 43418/48000 =  90.45 % ||| loss 0.25827568769454956\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10708/12000 =  89.23 % ||| loss 0.29080188274383545\u001b[0m\n",
            "\u001b[92mTest accuracy: 8907/10000 =  89.07 % ||| loss 0.3085002303123474\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3148269672691822\n",
            "Batch #200 Loss: 0.3133869206905365\n",
            "Batch #300 Loss: 0.31226471915841103\n",
            "\u001b[92mTrain accuracy: 43525/48000 =  90.68 % ||| loss 0.2500467300415039\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10680/12000 =  89.0 % ||| loss 0.284951776266098\u001b[0m\n",
            "\u001b[92mTest accuracy: 8901/10000 =  89.01 % ||| loss 0.2994396984577179\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.30208037376403807\n",
            "Batch #200 Loss: 0.31048145905137065\n",
            "Batch #300 Loss: 0.295187031775713\n",
            "\u001b[92mTrain accuracy: 43485/48000 =  90.59 % ||| loss 0.2542880177497864\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10698/12000 =  89.15 % ||| loss 0.29294776916503906\u001b[0m\n",
            "\u001b[92mTest accuracy: 8886/10000 =  88.86 % ||| loss 0.3249953091144562\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.303014332652092\n",
            "Batch #200 Loss: 0.2986672078073025\n",
            "Batch #300 Loss: 0.2973488001525402\n",
            "\u001b[92mTrain accuracy: 43782/48000 =  91.21 % ||| loss 0.23909905552864075\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10762/12000 =  89.68 % ||| loss 0.27996835112571716\u001b[0m\n",
            "\u001b[92mTest accuracy: 8932/10000 =  89.32 % ||| loss 0.2984861433506012\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.2895244251191616\n",
            "Batch #200 Loss: 0.28935768216848373\n",
            "Batch #300 Loss: 0.2949127285182476\n",
            "\u001b[92mTrain accuracy: 43079/48000 =  89.75 % ||| loss 0.2713967561721802\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10592/12000 =  88.27 % ||| loss 0.3178991675376892\u001b[0m\n",
            "\u001b[92mTest accuracy: 8781/10000 =  87.81 % ||| loss 0.33504173159599304\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.2923412384092808\n",
            "Batch #200 Loss: 0.2896411493420601\n",
            "Batch #300 Loss: 0.27847957268357276\n",
            "\u001b[92mTrain accuracy: 43686/48000 =  91.01 % ||| loss 0.23840901255607605\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10720/12000 =  89.33 % ||| loss 0.2846497595310211\u001b[0m\n",
            "\u001b[92mTest accuracy: 8896/10000 =  88.96 % ||| loss 0.30784547328948975\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.291888959556818\n",
            "Batch #200 Loss: 0.2845621632039547\n",
            "Batch #300 Loss: 0.2855727250874043\n",
            "\u001b[92mTrain accuracy: 43901/48000 =  91.46 % ||| loss 0.22620871663093567\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10769/12000 =  89.74 % ||| loss 0.27650752663612366\u001b[0m\n",
            "\u001b[92mTest accuracy: 8953/10000 =  89.53 % ||| loss 0.29310110211372375\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.27317200154066085\n",
            "Batch #200 Loss: 0.28545421853661535\n",
            "Batch #300 Loss: 0.2780180607736111\n",
            "\u001b[92mTrain accuracy: 43912/48000 =  91.48 % ||| loss 0.22774189710617065\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10752/12000 =  89.6 % ||| loss 0.27771157026290894\u001b[0m\n",
            "\u001b[92mTest accuracy: 8920/10000 =  89.2 % ||| loss 0.29188913106918335\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.2607994359731674\n",
            "Batch #200 Loss: 0.27646044120192526\n",
            "Batch #300 Loss: 0.29107133984565736\n",
            "\u001b[92mTrain accuracy: 44123/48000 =  91.92 % ||| loss 0.21854911744594574\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10803/12000 =  90.03 % ||| loss 0.26861658692359924\u001b[0m\n",
            "\u001b[92mTest accuracy: 8971/10000 =  89.71 % ||| loss 0.28776347637176514\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.26609263360500335\n",
            "Batch #200 Loss: 0.27333186358213424\n",
            "Batch #300 Loss: 0.2667771692574024\n",
            "\u001b[92mTrain accuracy: 44031/48000 =  91.73 % ||| loss 0.22110119462013245\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10756/12000 =  89.63 % ||| loss 0.27375656366348267\u001b[0m\n",
            "\u001b[92mTest accuracy: 8958/10000 =  89.58 % ||| loss 0.2952156662940979\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_1</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_1</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_101305-Lenet5Dropout_1726150070.105228_2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_2' target=\"_blank\">Lenet5Dropout_1726150070.105228_2</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.263292090892792\n",
            "Batch #200 Loss: 1.3528934127092362\n",
            "Batch #300 Loss: 0.9212269479036331\n",
            "\u001b[92mTrain accuracy: 35197/48000 =  73.33 % ||| loss 0.6577841639518738\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8811/12000 =  73.42 % ||| loss 0.6477727293968201\u001b[0m\n",
            "\u001b[92mTest accuracy: 7276/10000 =  72.76 % ||| loss 0.6683856844902039\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7749114108085632\n",
            "Batch #200 Loss: 0.7275846296548844\n",
            "Batch #300 Loss: 0.6867255228757858\n",
            "\u001b[92mTrain accuracy: 37374/48000 =  77.86 % ||| loss 0.5483962297439575\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9365/12000 =  78.04 % ||| loss 0.5415701270103455\u001b[0m\n",
            "\u001b[92mTest accuracy: 7687/10000 =  76.87 % ||| loss 0.5641484260559082\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6488562613725662\n",
            "Batch #200 Loss: 0.6311677289009094\n",
            "Batch #300 Loss: 0.6244960385560989\n",
            "\u001b[92mTrain accuracy: 38355/48000 =  79.91 % ||| loss 0.49897101521492004\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9602/12000 =  80.02 % ||| loss 0.4975903630256653\u001b[0m\n",
            "\u001b[92mTest accuracy: 7905/10000 =  79.05 % ||| loss 0.5214444994926453\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5876439973711968\n",
            "Batch #200 Loss: 0.5860079723596573\n",
            "Batch #300 Loss: 0.5678575682640076\n",
            "\u001b[92mTrain accuracy: 38972/48000 =  81.19 % ||| loss 0.4616081118583679\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9726/12000 =  81.05 % ||| loss 0.4679937958717346\u001b[0m\n",
            "\u001b[92mTest accuracy: 8033/10000 =  80.33 % ||| loss 0.4913972318172455\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5396270799636841\n",
            "Batch #200 Loss: 0.545917357802391\n",
            "Batch #300 Loss: 0.5414747807383538\n",
            "\u001b[92mTrain accuracy: 40335/48000 =  84.03 % ||| loss 0.43002843856811523\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10095/12000 =  84.12 % ||| loss 0.43892818689346313\u001b[0m\n",
            "\u001b[92mTest accuracy: 8299/10000 =  82.99 % ||| loss 0.4625696539878845\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5187804862856865\n",
            "Batch #200 Loss: 0.51576180934906\n",
            "Batch #300 Loss: 0.49597825706005094\n",
            "\u001b[92mTrain accuracy: 40977/48000 =  85.37 % ||| loss 0.39522624015808105\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10246/12000 =  85.38 % ||| loss 0.4017813801765442\u001b[0m\n",
            "\u001b[92mTest accuracy: 8446/10000 =  84.46 % ||| loss 0.4257112443447113\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.4817331138253212\n",
            "Batch #200 Loss: 0.4779719999432564\n",
            "Batch #300 Loss: 0.48108628898859024\n",
            "\u001b[92mTrain accuracy: 41411/48000 =  86.27 % ||| loss 0.3755115270614624\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10294/12000 =  85.78 % ||| loss 0.38606080412864685\u001b[0m\n",
            "\u001b[92mTest accuracy: 8539/10000 =  85.39 % ||| loss 0.40742194652557373\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.4633667939901352\n",
            "Batch #200 Loss: 0.4497950854897499\n",
            "Batch #300 Loss: 0.4498364320397377\n",
            "\u001b[92mTrain accuracy: 41691/48000 =  86.86 % ||| loss 0.35821422934532166\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10382/12000 =  86.52 % ||| loss 0.3695612847805023\u001b[0m\n",
            "\u001b[92mTest accuracy: 8565/10000 =  85.65 % ||| loss 0.39130979776382446\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.4367754542827606\n",
            "Batch #200 Loss: 0.4316709700226784\n",
            "Batch #300 Loss: 0.4257394531369209\n",
            "\u001b[92mTrain accuracy: 41954/48000 =  87.4 % ||| loss 0.3382296860218048\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10419/12000 =  86.83 % ||| loss 0.35245639085769653\u001b[0m\n",
            "\u001b[92mTest accuracy: 8612/10000 =  86.12 % ||| loss 0.37579137086868286\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.4293923011422157\n",
            "Batch #200 Loss: 0.41071555837988855\n",
            "Batch #300 Loss: 0.42210657596588136\n",
            "\u001b[92mTrain accuracy: 42160/48000 =  87.83 % ||| loss 0.33087924122810364\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10504/12000 =  87.53 % ||| loss 0.3451269865036011\u001b[0m\n",
            "\u001b[92mTest accuracy: 8684/10000 =  86.84 % ||| loss 0.36776891350746155\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.4116431900858879\n",
            "Batch #200 Loss: 0.40532963901758196\n",
            "Batch #300 Loss: 0.3979825785756111\n",
            "\u001b[92mTrain accuracy: 42374/48000 =  88.28 % ||| loss 0.31451237201690674\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10544/12000 =  87.87 % ||| loss 0.3323665261268616\u001b[0m\n",
            "\u001b[92mTest accuracy: 8698/10000 =  86.98 % ||| loss 0.3541429936885834\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3838806948065758\n",
            "Batch #200 Loss: 0.39032022207975386\n",
            "Batch #300 Loss: 0.3956916734576225\n",
            "\u001b[92mTrain accuracy: 42549/48000 =  88.64 % ||| loss 0.3074149787425995\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10541/12000 =  87.84 % ||| loss 0.3287774324417114\u001b[0m\n",
            "\u001b[92mTest accuracy: 8733/10000 =  87.33 % ||| loss 0.3496895432472229\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.38251687854528427\n",
            "Batch #200 Loss: 0.3812174066901207\n",
            "Batch #300 Loss: 0.38562015146017076\n",
            "\u001b[92mTrain accuracy: 42713/48000 =  88.99 % ||| loss 0.2941446304321289\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10570/12000 =  88.08 % ||| loss 0.3161337971687317\u001b[0m\n",
            "\u001b[92mTest accuracy: 8773/10000 =  87.73 % ||| loss 0.33886173367500305\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.37484046071767807\n",
            "Batch #200 Loss: 0.3622110891342163\n",
            "Batch #300 Loss: 0.3611565387248993\n",
            "\u001b[92mTrain accuracy: 42769/48000 =  89.1 % ||| loss 0.29175323247909546\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10602/12000 =  88.35 % ||| loss 0.31282928586006165\u001b[0m\n",
            "\u001b[92mTest accuracy: 8791/10000 =  87.91 % ||| loss 0.33568450808525085\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3565418802201748\n",
            "Batch #200 Loss: 0.3680878794193268\n",
            "Batch #300 Loss: 0.3568595263361931\n",
            "\u001b[92mTrain accuracy: 42899/48000 =  89.37 % ||| loss 0.28146177530288696\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10637/12000 =  88.64 % ||| loss 0.30705195665359497\u001b[0m\n",
            "\u001b[92mTest accuracy: 8800/10000 =  88.0 % ||| loss 0.332902729511261\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3616657018661499\n",
            "Batch #200 Loss: 0.3471368508040905\n",
            "Batch #300 Loss: 0.3589676688611507\n",
            "\u001b[92mTrain accuracy: 43152/48000 =  89.9 % ||| loss 0.2727150022983551\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10660/12000 =  88.83 % ||| loss 0.30015528202056885\u001b[0m\n",
            "\u001b[92mTest accuracy: 8844/10000 =  88.44 % ||| loss 0.32267025113105774\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.34571315720677376\n",
            "Batch #200 Loss: 0.3403374966979027\n",
            "Batch #300 Loss: 0.35832642763853073\n",
            "\u001b[92mTrain accuracy: 43230/48000 =  90.06 % ||| loss 0.26814284920692444\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10680/12000 =  89.0 % ||| loss 0.2978108823299408\u001b[0m\n",
            "\u001b[92mTest accuracy: 8859/10000 =  88.59 % ||| loss 0.31970953941345215\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.32880519464612007\n",
            "Batch #200 Loss: 0.3441637107729912\n",
            "Batch #300 Loss: 0.35064761102199554\n",
            "\u001b[92mTrain accuracy: 43210/48000 =  90.02 % ||| loss 0.2691236436367035\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10643/12000 =  88.69 % ||| loss 0.2981756329536438\u001b[0m\n",
            "\u001b[92mTest accuracy: 8817/10000 =  88.17 % ||| loss 0.3234840929508209\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.34034631758928297\n",
            "Batch #200 Loss: 0.3342281024158001\n",
            "Batch #300 Loss: 0.3291670009493828\n",
            "\u001b[92mTrain accuracy: 43235/48000 =  90.07 % ||| loss 0.2616920471191406\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10675/12000 =  88.96 % ||| loss 0.2929705083370209\u001b[0m\n",
            "\u001b[92mTest accuracy: 8817/10000 =  88.17 % ||| loss 0.3185407817363739\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.333399807959795\n",
            "Batch #200 Loss: 0.33604822590947153\n",
            "Batch #300 Loss: 0.3208306400477886\n",
            "\u001b[92mTrain accuracy: 43517/48000 =  90.66 % ||| loss 0.24933388829231262\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10720/12000 =  89.33 % ||| loss 0.2801280617713928\u001b[0m\n",
            "\u001b[92mTest accuracy: 8881/10000 =  88.81 % ||| loss 0.3025910556316376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.3183682575821877\n",
            "Batch #200 Loss: 0.330690333545208\n",
            "Batch #300 Loss: 0.3101861023902893\n",
            "\u001b[92mTrain accuracy: 43558/48000 =  90.75 % ||| loss 0.24739770591259003\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10738/12000 =  89.48 % ||| loss 0.28054308891296387\u001b[0m\n",
            "\u001b[92mTest accuracy: 8878/10000 =  88.78 % ||| loss 0.3092746138572693\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.3220566540956497\n",
            "Batch #200 Loss: 0.3195648409426212\n",
            "Batch #300 Loss: 0.3171702256798744\n",
            "\u001b[92mTrain accuracy: 43704/48000 =  91.05 % ||| loss 0.24012471735477448\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10759/12000 =  89.66 % ||| loss 0.27691650390625\u001b[0m\n",
            "\u001b[92mTest accuracy: 8912/10000 =  89.12 % ||| loss 0.30690035223960876\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.3119529113173485\n",
            "Batch #200 Loss: 0.3150556442141533\n",
            "Batch #300 Loss: 0.3165761508047581\n",
            "\u001b[92mTrain accuracy: 43603/48000 =  90.84 % ||| loss 0.24627618491649628\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10720/12000 =  89.33 % ||| loss 0.28255924582481384\u001b[0m\n",
            "\u001b[92mTest accuracy: 8891/10000 =  88.91 % ||| loss 0.3096109926700592\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.30323228433728217\n",
            "Batch #200 Loss: 0.315270908921957\n",
            "Batch #300 Loss: 0.31401431024074555\n",
            "\u001b[92mTrain accuracy: 43723/48000 =  91.09 % ||| loss 0.23475578427314758\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10741/12000 =  89.51 % ||| loss 0.2731260359287262\u001b[0m\n",
            "\u001b[92mTest accuracy: 8910/10000 =  89.1 % ||| loss 0.3033376932144165\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3085260757803917\n",
            "Batch #200 Loss: 0.30961344435811045\n",
            "Batch #300 Loss: 0.2928453865647316\n",
            "\u001b[92mTrain accuracy: 43668/48000 =  90.97 % ||| loss 0.2404203861951828\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10729/12000 =  89.41 % ||| loss 0.2795976400375366\u001b[0m\n",
            "\u001b[92mTest accuracy: 8890/10000 =  88.9 % ||| loss 0.3127261698246002\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_2</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_2</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_101536-Lenet5Dropout_1726150070.105228_3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_3' target=\"_blank\">Lenet5Dropout_1726150070.105228_3</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.0512830090522765\n",
            "Batch #200 Loss: 1.0029096275568008\n",
            "Batch #300 Loss: 0.8762562870979309\n",
            "\u001b[92mTrain accuracy: 35642/48000 =  74.25 % ||| loss 0.679370641708374\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8992/12000 =  74.93 % ||| loss 0.6710630655288696\u001b[0m\n",
            "\u001b[92mTest accuracy: 7316/10000 =  73.16 % ||| loss 0.7053787112236023\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7158803993463516\n",
            "Batch #200 Loss: 0.6659798610210419\n",
            "Batch #300 Loss: 0.6731505760550499\n",
            "\u001b[92mTrain accuracy: 36945/48000 =  76.97 % ||| loss 0.6098640561103821\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9260/12000 =  77.17 % ||| loss 0.6111122965812683\u001b[0m\n",
            "\u001b[92mTest accuracy: 7609/10000 =  76.09 % ||| loss 0.631308376789093\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.645799058675766\n",
            "Batch #200 Loss: 0.6270603513717652\n",
            "Batch #300 Loss: 0.6410883367061615\n",
            "\u001b[92mTrain accuracy: 38525/48000 =  80.26 % ||| loss 0.5321557521820068\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9588/12000 =  79.9 % ||| loss 0.537222683429718\u001b[0m\n",
            "\u001b[92mTest accuracy: 7912/10000 =  79.12 % ||| loss 0.5622605681419373\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.57821576744318\n",
            "Batch #200 Loss: 0.5964200878143311\n",
            "Batch #300 Loss: 0.577468048632145\n",
            "\u001b[92mTrain accuracy: 39313/48000 =  81.9 % ||| loss 0.49131107330322266\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9782/12000 =  81.52 % ||| loss 0.4981914758682251\u001b[0m\n",
            "\u001b[92mTest accuracy: 8126/10000 =  81.26 % ||| loss 0.5150272250175476\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5915996870398521\n",
            "Batch #200 Loss: 0.5822495317459107\n",
            "Batch #300 Loss: 0.5896487283706665\n",
            "\u001b[92mTrain accuracy: 37994/48000 =  79.15 % ||| loss 0.5508773922920227\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9473/12000 =  78.94 % ||| loss 0.5577665567398071\u001b[0m\n",
            "\u001b[92mTest accuracy: 7823/10000 =  78.23 % ||| loss 0.5723159313201904\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5959008446335793\n",
            "Batch #200 Loss: 0.5567342427372932\n",
            "Batch #300 Loss: 0.5487862950563431\n",
            "\u001b[92mTrain accuracy: 38823/48000 =  80.88 % ||| loss 0.4977353513240814\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9679/12000 =  80.66 % ||| loss 0.508112907409668\u001b[0m\n",
            "\u001b[92mTest accuracy: 7961/10000 =  79.61 % ||| loss 0.5331552028656006\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5478337123990059\n",
            "Batch #200 Loss: 0.6258251005411148\n",
            "Batch #300 Loss: 0.5743299016356468\n",
            "\u001b[92mTrain accuracy: 38926/48000 =  81.1 % ||| loss 0.5173125863075256\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9687/12000 =  80.73 % ||| loss 0.5285328030586243\u001b[0m\n",
            "\u001b[92mTest accuracy: 7985/10000 =  79.85 % ||| loss 0.5564795136451721\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.5434147119522095\n",
            "Batch #200 Loss: 0.5752414429187774\n",
            "Batch #300 Loss: 0.5639321100711823\n",
            "\u001b[92mTrain accuracy: 38480/48000 =  80.17 % ||| loss 0.535129189491272\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9569/12000 =  79.74 % ||| loss 0.550486147403717\u001b[0m\n",
            "\u001b[92mTest accuracy: 7935/10000 =  79.35 % ||| loss 0.5743463635444641\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.5448668465018273\n",
            "Batch #200 Loss: 0.5421275594830512\n",
            "Batch #300 Loss: 0.5702635040879249\n",
            "\u001b[92mTrain accuracy: 40029/48000 =  83.39 % ||| loss 0.4525214433670044\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9910/12000 =  82.58 % ||| loss 0.4738815128803253\u001b[0m\n",
            "\u001b[92mTest accuracy: 8242/10000 =  82.42 % ||| loss 0.4886961281299591\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.534624635875225\n",
            "Batch #200 Loss: 0.5197705435752868\n",
            "Batch #300 Loss: 0.5239775827527047\n",
            "\u001b[92mTrain accuracy: 37723/48000 =  78.59 % ||| loss 0.5665186047554016\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9358/12000 =  77.98 % ||| loss 0.5720091462135315\u001b[0m\n",
            "\u001b[92mTest accuracy: 7757/10000 =  77.57 % ||| loss 0.6105011701583862\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.5340668764710427\n",
            "Batch #200 Loss: 0.5561852794885636\n",
            "Batch #300 Loss: 0.5149926415085793\n",
            "\u001b[92mTrain accuracy: 40001/48000 =  83.34 % ||| loss 0.44955742359161377\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9914/12000 =  82.62 % ||| loss 0.4673152565956116\u001b[0m\n",
            "\u001b[92mTest accuracy: 8163/10000 =  81.63 % ||| loss 0.49303722381591797\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5702202591300011\n",
            "Batch #200 Loss: 0.5608771029114723\n",
            "Batch #300 Loss: 0.5311198806762696\n",
            "\u001b[92mTrain accuracy: 39754/48000 =  82.82 % ||| loss 0.45098328590393066\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9869/12000 =  82.24 % ||| loss 0.46439462900161743\u001b[0m\n",
            "\u001b[92mTest accuracy: 8174/10000 =  81.74 % ||| loss 0.4860333800315857\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.50153035312891\n",
            "Batch #200 Loss: 0.5236069920659066\n",
            "Batch #300 Loss: 0.5097855055332183\n",
            "\u001b[92mTrain accuracy: 40115/48000 =  83.57 % ||| loss 0.43453824520111084\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9976/12000 =  83.13 % ||| loss 0.45652052760124207\u001b[0m\n",
            "\u001b[92mTest accuracy: 8230/10000 =  82.3 % ||| loss 0.47863247990608215\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.49277008801698685\n",
            "Batch #200 Loss: 0.5518455609679223\n",
            "Batch #300 Loss: 0.4890499582886696\n",
            "\u001b[92mTrain accuracy: 39886/48000 =  83.1 % ||| loss 0.450632244348526\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9878/12000 =  82.32 % ||| loss 0.4677942991256714\u001b[0m\n",
            "\u001b[92mTest accuracy: 8195/10000 =  81.95 % ||| loss 0.49818265438079834\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5059443446993828\n",
            "Batch #200 Loss: 0.500687680542469\n",
            "Batch #300 Loss: 0.5063748097419739\n",
            "\u001b[92mTrain accuracy: 40665/48000 =  84.72 % ||| loss 0.4160168766975403\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10091/12000 =  84.09 % ||| loss 0.4393351972103119\u001b[0m\n",
            "\u001b[92mTest accuracy: 8341/10000 =  83.41 % ||| loss 0.46533095836639404\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.502311150431633\n",
            "Batch #200 Loss: 0.4992358058691025\n",
            "Batch #300 Loss: 0.4940186500549316\n",
            "\u001b[92mTrain accuracy: 39806/48000 =  82.93 % ||| loss 0.44082581996917725\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9867/12000 =  82.23 % ||| loss 0.4638550877571106\u001b[0m\n",
            "\u001b[92mTest accuracy: 8136/10000 =  81.36 % ||| loss 0.48358994722366333\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5027671533823014\n",
            "Batch #200 Loss: 0.5003011909127235\n",
            "Batch #300 Loss: 0.5028374156355858\n",
            "\u001b[92mTrain accuracy: 40864/48000 =  85.13 % ||| loss 0.3977707028388977\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10130/12000 =  84.42 % ||| loss 0.4221419394016266\u001b[0m\n",
            "\u001b[92mTest accuracy: 8371/10000 =  83.71 % ||| loss 0.44738247990608215\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5117113754153252\n",
            "Batch #200 Loss: 0.5180053254961967\n",
            "Batch #300 Loss: 0.4983454918861389\n",
            "\u001b[92mTrain accuracy: 40676/48000 =  84.74 % ||| loss 0.4105655550956726\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10124/12000 =  84.37 % ||| loss 0.4352225363254547\u001b[0m\n",
            "\u001b[92mTest accuracy: 8342/10000 =  83.42 % ||| loss 0.45415806770324707\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5018738493323326\n",
            "Batch #200 Loss: 0.4954520079493523\n",
            "Batch #300 Loss: 0.4738468363881111\n",
            "\u001b[92mTrain accuracy: 40210/48000 =  83.77 % ||| loss 0.4199146032333374\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9977/12000 =  83.14 % ||| loss 0.4523872137069702\u001b[0m\n",
            "\u001b[92mTest accuracy: 8283/10000 =  82.83 % ||| loss 0.4690867066383362\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.498307803273201\n",
            "Batch #200 Loss: 0.5240229697525501\n",
            "Batch #300 Loss: 0.5094548144936561\n",
            "\u001b[92mTrain accuracy: 40524/48000 =  84.42 % ||| loss 0.4178377389907837\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10063/12000 =  83.86 % ||| loss 0.44124582409858704\u001b[0m\n",
            "\u001b[92mTest accuracy: 8261/10000 =  82.61 % ||| loss 0.46558070182800293\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.5046166884899139\n",
            "Batch #200 Loss: 0.5321287533640862\n",
            "Batch #300 Loss: 0.48574579775333404\n",
            "\u001b[92mTrain accuracy: 40116/48000 =  83.58 % ||| loss 0.44730451703071594\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9922/12000 =  82.68 % ||| loss 0.4744396209716797\u001b[0m\n",
            "\u001b[92mTest accuracy: 8178/10000 =  81.78 % ||| loss 0.5033854246139526\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.5043267276883125\n",
            "Batch #200 Loss: 0.4671265843510628\n",
            "Batch #300 Loss: 0.4984908851981163\n",
            "\u001b[92mTrain accuracy: 40234/48000 =  83.82 % ||| loss 0.43525707721710205\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9898/12000 =  82.48 % ||| loss 0.4629954397678375\u001b[0m\n",
            "\u001b[92mTest accuracy: 8243/10000 =  82.43 % ||| loss 0.4931437373161316\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.47850358337163923\n",
            "Batch #200 Loss: 0.5441460412740707\n",
            "Batch #300 Loss: 0.49042610704898837\n",
            "\u001b[92mTrain accuracy: 40378/48000 =  84.12 % ||| loss 0.4300214946269989\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10020/12000 =  83.5 % ||| loss 0.45682963728904724\u001b[0m\n",
            "\u001b[92mTest accuracy: 8241/10000 =  82.41 % ||| loss 0.48942434787750244\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.4558811223506927\n",
            "Batch #200 Loss: 0.4817849025130272\n",
            "Batch #300 Loss: 0.4852178040146828\n",
            "\u001b[92mTrain accuracy: 40588/48000 =  84.56 % ||| loss 0.3979075253009796\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10048/12000 =  83.73 % ||| loss 0.4345978796482086\u001b[0m\n",
            "\u001b[92mTest accuracy: 8310/10000 =  83.1 % ||| loss 0.45604604482650757\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.476808620095253\n",
            "Batch #200 Loss: 0.4569263444840908\n",
            "Batch #300 Loss: 0.4861870715022087\n",
            "\u001b[92mTrain accuracy: 41059/48000 =  85.54 % ||| loss 0.38442346453666687\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10174/12000 =  84.78 % ||| loss 0.41571345925331116\u001b[0m\n",
            "\u001b[92mTest accuracy: 8391/10000 =  83.91 % ||| loss 0.4411802589893341\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_3</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_3</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_101807-Lenet5Dropout_1726150070.105228_4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_4' target=\"_blank\">Lenet5Dropout_1726150070.105228_4</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.48872523188591\n",
            "Batch #200 Loss: 0.7269177830219269\n",
            "Batch #300 Loss: 0.5971058574318886\n",
            "\u001b[92mTrain accuracy: 39322/48000 =  81.92 % ||| loss 0.4676925539970398\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9812/12000 =  81.77 % ||| loss 0.47087880969047546\u001b[0m\n",
            "\u001b[92mTest accuracy: 8058/10000 =  80.58 % ||| loss 0.49350348114967346\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.5132968211174012\n",
            "Batch #200 Loss: 0.4965219795703888\n",
            "Batch #300 Loss: 0.46865652948617936\n",
            "\u001b[92mTrain accuracy: 41291/48000 =  86.02 % ||| loss 0.3776966631412506\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10316/12000 =  85.97 % ||| loss 0.38622280955314636\u001b[0m\n",
            "\u001b[92mTest accuracy: 8515/10000 =  85.15 % ||| loss 0.40568387508392334\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.45039369374513627\n",
            "Batch #200 Loss: 0.44422244489192964\n",
            "Batch #300 Loss: 0.4297747367620468\n",
            "\u001b[92mTrain accuracy: 41362/48000 =  86.17 % ||| loss 0.3745845556259155\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10270/12000 =  85.58 % ||| loss 0.38914620876312256\u001b[0m\n",
            "\u001b[92mTest accuracy: 8474/10000 =  84.74 % ||| loss 0.41009652614593506\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.4144993218779564\n",
            "Batch #200 Loss: 0.41438921421766284\n",
            "Batch #300 Loss: 0.4095948253571987\n",
            "\u001b[92mTrain accuracy: 41457/48000 =  86.37 % ||| loss 0.36921316385269165\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10285/12000 =  85.71 % ||| loss 0.38835883140563965\u001b[0m\n",
            "\u001b[92mTest accuracy: 8452/10000 =  84.52 % ||| loss 0.4108503460884094\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.41684516504406927\n",
            "Batch #200 Loss: 0.41242441460490226\n",
            "Batch #300 Loss: 0.38916840687394144\n",
            "\u001b[92mTrain accuracy: 42218/48000 =  87.95 % ||| loss 0.31833627820014954\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10516/12000 =  87.63 % ||| loss 0.3406093716621399\u001b[0m\n",
            "\u001b[92mTest accuracy: 8642/10000 =  86.42 % ||| loss 0.3702160120010376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.3851817670464516\n",
            "Batch #200 Loss: 0.39043200701475145\n",
            "Batch #300 Loss: 0.3863005748391151\n",
            "\u001b[92mTrain accuracy: 42237/48000 =  87.99 % ||| loss 0.3173414170742035\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10442/12000 =  87.02 % ||| loss 0.33825063705444336\u001b[0m\n",
            "\u001b[92mTest accuracy: 8631/10000 =  86.31 % ||| loss 0.3778762221336365\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3790172672271728\n",
            "Batch #200 Loss: 0.3822510436177254\n",
            "Batch #300 Loss: 0.3937960322201252\n",
            "\u001b[92mTrain accuracy: 42591/48000 =  88.73 % ||| loss 0.30558350682258606\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10497/12000 =  87.48 % ||| loss 0.33666789531707764\u001b[0m\n",
            "\u001b[92mTest accuracy: 8695/10000 =  86.95 % ||| loss 0.36029252409935\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3633104977011681\n",
            "Batch #200 Loss: 0.3670234590768814\n",
            "Batch #300 Loss: 0.37987733259797096\n",
            "\u001b[92mTrain accuracy: 42235/48000 =  87.99 % ||| loss 0.3145425617694855\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10403/12000 =  86.69 % ||| loss 0.3456937074661255\u001b[0m\n",
            "\u001b[92mTest accuracy: 8622/10000 =  86.22 % ||| loss 0.3691217601299286\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.36504810854792596\n",
            "Batch #200 Loss: 0.3646467159688473\n",
            "Batch #300 Loss: 0.37281424656510354\n",
            "\u001b[92mTrain accuracy: 42964/48000 =  89.51 % ||| loss 0.2883988618850708\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10589/12000 =  88.24 % ||| loss 0.3232273757457733\u001b[0m\n",
            "\u001b[92mTest accuracy: 8769/10000 =  87.69 % ||| loss 0.34238025546073914\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3658383348584175\n",
            "Batch #200 Loss: 0.35432195276021955\n",
            "Batch #300 Loss: 0.3741073115170002\n",
            "\u001b[92mTrain accuracy: 42300/48000 =  88.12 % ||| loss 0.3319208025932312\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10376/12000 =  86.47 % ||| loss 0.369802325963974\u001b[0m\n",
            "\u001b[92mTest accuracy: 8633/10000 =  86.33 % ||| loss 0.38636907935142517\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.3458980646729469\n",
            "Batch #200 Loss: 0.3700805716216564\n",
            "Batch #300 Loss: 0.3636464358866215\n",
            "\u001b[92mTrain accuracy: 42627/48000 =  88.81 % ||| loss 0.2978040874004364\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10475/12000 =  87.29 % ||| loss 0.33473268151283264\u001b[0m\n",
            "\u001b[92mTest accuracy: 8634/10000 =  86.34 % ||| loss 0.3616960346698761\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.34896831572055814\n",
            "Batch #200 Loss: 0.3534452222287655\n",
            "Batch #300 Loss: 0.34825400859117506\n",
            "\u001b[92mTrain accuracy: 42789/48000 =  89.14 % ||| loss 0.2914818823337555\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10490/12000 =  87.42 % ||| loss 0.3330923318862915\u001b[0m\n",
            "\u001b[92mTest accuracy: 8715/10000 =  87.15 % ||| loss 0.3550907075405121\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.36756729453802106\n",
            "Batch #200 Loss: 0.3555923077464104\n",
            "Batch #300 Loss: 0.3338681873679161\n",
            "\u001b[92mTrain accuracy: 42906/48000 =  89.39 % ||| loss 0.2822811007499695\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10576/12000 =  88.13 % ||| loss 0.31800955533981323\u001b[0m\n",
            "\u001b[92mTest accuracy: 8706/10000 =  87.06 % ||| loss 0.3527105748653412\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3516253699362278\n",
            "Batch #200 Loss: 0.3731263408064842\n",
            "Batch #300 Loss: 0.3609176418185234\n",
            "\u001b[92mTrain accuracy: 43302/48000 =  90.21 % ||| loss 0.26127541065216064\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10640/12000 =  88.67 % ||| loss 0.31171277165412903\u001b[0m\n",
            "\u001b[92mTest accuracy: 8818/10000 =  88.18 % ||| loss 0.3365808427333832\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3561653292179108\n",
            "Batch #200 Loss: 0.3560765366256237\n",
            "Batch #300 Loss: 0.3451899817585945\n",
            "\u001b[92mTrain accuracy: 42449/48000 =  88.44 % ||| loss 0.29230090975761414\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10405/12000 =  86.71 % ||| loss 0.3382020592689514\u001b[0m\n",
            "\u001b[92mTest accuracy: 8643/10000 =  86.43 % ||| loss 0.3578406572341919\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.33485745459794997\n",
            "Batch #200 Loss: 0.346871951520443\n",
            "Batch #300 Loss: 0.3483327707648277\n",
            "\u001b[92mTrain accuracy: 43099/48000 =  89.79 % ||| loss 0.2710053324699402\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10570/12000 =  88.08 % ||| loss 0.31984856724739075\u001b[0m\n",
            "\u001b[92mTest accuracy: 8782/10000 =  87.82 % ||| loss 0.35229915380477905\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3287688675522804\n",
            "Batch #200 Loss: 0.3357784867286682\n",
            "Batch #300 Loss: 0.3614715492725372\n",
            "\u001b[92mTrain accuracy: 42851/48000 =  89.27 % ||| loss 0.27798551321029663\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10526/12000 =  87.72 % ||| loss 0.31786224246025085\u001b[0m\n",
            "\u001b[92mTest accuracy: 8700/10000 =  87.0 % ||| loss 0.34226953983306885\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3315795964002609\n",
            "Batch #200 Loss: 0.3446057851612568\n",
            "Batch #300 Loss: 0.3543881477415562\n",
            "\u001b[92mTrain accuracy: 43380/48000 =  90.38 % ||| loss 0.25403156876564026\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10632/12000 =  88.6 % ||| loss 0.30274978280067444\u001b[0m\n",
            "\u001b[92mTest accuracy: 8823/10000 =  88.23 % ||| loss 0.3221485912799835\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.329324803352356\n",
            "Batch #200 Loss: 0.3310946261882782\n",
            "Batch #300 Loss: 0.3572158990800381\n",
            "\u001b[92mTrain accuracy: 42779/48000 =  89.12 % ||| loss 0.28223034739494324\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10509/12000 =  87.58 % ||| loss 0.3340206742286682\u001b[0m\n",
            "\u001b[92mTest accuracy: 8701/10000 =  87.01 % ||| loss 0.3571779429912567\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.34803183749318123\n",
            "Batch #200 Loss: 0.3588124953210354\n",
            "Batch #300 Loss: 0.35332704976201057\n",
            "\u001b[92mTrain accuracy: 42844/48000 =  89.26 % ||| loss 0.2838471233844757\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10533/12000 =  87.78 % ||| loss 0.3273601531982422\u001b[0m\n",
            "\u001b[92mTest accuracy: 8763/10000 =  87.63 % ||| loss 0.35402393341064453\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.32645764425396917\n",
            "Batch #200 Loss: 0.34298254296183583\n",
            "Batch #300 Loss: 0.35010999292135236\n",
            "\u001b[92mTrain accuracy: 43111/48000 =  89.81 % ||| loss 0.2631417512893677\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10563/12000 =  88.02 % ||| loss 0.31557339429855347\u001b[0m\n",
            "\u001b[92mTest accuracy: 8742/10000 =  87.42 % ||| loss 0.3538263142108917\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.34082819521427155\n",
            "Batch #200 Loss: 0.33992634654045106\n",
            "Batch #300 Loss: 0.33222746521234514\n",
            "\u001b[92mTrain accuracy: 42871/48000 =  89.31 % ||| loss 0.28209513425827026\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10505/12000 =  87.54 % ||| loss 0.3281519114971161\u001b[0m\n",
            "\u001b[92mTest accuracy: 8710/10000 =  87.1 % ||| loss 0.35520386695861816\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.33936473339796064\n",
            "Batch #200 Loss: 0.33745569229125977\n",
            "Batch #300 Loss: 0.33758048072457314\n",
            "\u001b[92mTrain accuracy: 43122/48000 =  89.84 % ||| loss 0.27063730359077454\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10572/12000 =  88.1 % ||| loss 0.32227209210395813\u001b[0m\n",
            "\u001b[92mTest accuracy: 8713/10000 =  87.13 % ||| loss 0.3546163737773895\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.32531473904848096\n",
            "Batch #200 Loss: 0.3340754836797714\n",
            "Batch #300 Loss: 0.3193295074999332\n",
            "\u001b[92mTrain accuracy: 43121/48000 =  89.84 % ||| loss 0.2708590626716614\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10594/12000 =  88.28 % ||| loss 0.3327639400959015\u001b[0m\n",
            "\u001b[92mTest accuracy: 8772/10000 =  87.72 % ||| loss 0.35903364419937134\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.32783071622252463\n",
            "Batch #200 Loss: 0.3394401428103447\n",
            "Batch #300 Loss: 0.3514570988714695\n",
            "\u001b[92mTrain accuracy: 42859/48000 =  89.29 % ||| loss 0.2740263342857361\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10472/12000 =  87.27 % ||| loss 0.3320190906524658\u001b[0m\n",
            "\u001b[92mTest accuracy: 8701/10000 =  87.01 % ||| loss 0.35817164182662964\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_4</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_4</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_102037-Lenet5Dropout_1726150070.105228_5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_5' target=\"_blank\">Lenet5Dropout_1726150070.105228_5</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.6337334990501404\n",
            "Batch #200 Loss: 0.8499245661497116\n",
            "Batch #300 Loss: 0.738835860490799\n",
            "\u001b[92mTrain accuracy: 38830/48000 =  80.9 % ||| loss 0.5274945497512817\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9707/12000 =  80.89 % ||| loss 0.5275982618331909\u001b[0m\n",
            "\u001b[92mTest accuracy: 8014/10000 =  80.14 % ||| loss 0.5480266809463501\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6349603992700577\n",
            "Batch #200 Loss: 0.6044207391142845\n",
            "Batch #300 Loss: 0.5610689741373062\n",
            "\u001b[92mTrain accuracy: 40184/48000 =  83.72 % ||| loss 0.45102009177207947\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10045/12000 =  83.71 % ||| loss 0.4507391154766083\u001b[0m\n",
            "\u001b[92mTest accuracy: 8237/10000 =  82.37 % ||| loss 0.48244839906692505\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5514840087294579\n",
            "Batch #200 Loss: 0.5622483560442925\n",
            "Batch #300 Loss: 0.5428181925415992\n",
            "\u001b[92mTrain accuracy: 40699/48000 =  84.79 % ||| loss 0.4058695435523987\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10218/12000 =  85.15 % ||| loss 0.4145580530166626\u001b[0m\n",
            "\u001b[92mTest accuracy: 8388/10000 =  83.88 % ||| loss 0.43681374192237854\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5185152068734169\n",
            "Batch #200 Loss: 0.5116198170185089\n",
            "Batch #300 Loss: 0.5172691354155541\n",
            "\u001b[92mTrain accuracy: 41056/48000 =  85.53 % ||| loss 0.38757389783859253\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10202/12000 =  85.02 % ||| loss 0.3997713029384613\u001b[0m\n",
            "\u001b[92mTest accuracy: 8471/10000 =  84.71 % ||| loss 0.42530182003974915\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4860399478673935\n",
            "Batch #200 Loss: 0.512537479698658\n",
            "Batch #300 Loss: 0.5113424864411354\n",
            "\u001b[92mTrain accuracy: 40356/48000 =  84.08 % ||| loss 0.40973469614982605\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10040/12000 =  83.67 % ||| loss 0.4244246482849121\u001b[0m\n",
            "\u001b[92mTest accuracy: 8339/10000 =  83.39 % ||| loss 0.44494035840034485\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.4880716773867607\n",
            "Batch #200 Loss: 0.4744233813881874\n",
            "Batch #300 Loss: 0.5117306712269784\n",
            "\u001b[92mTrain accuracy: 41682/48000 =  86.84 % ||| loss 0.35273322463035583\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10397/12000 =  86.64 % ||| loss 0.3692607879638672\u001b[0m\n",
            "\u001b[92mTest accuracy: 8562/10000 =  85.62 % ||| loss 0.3936256170272827\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.46681423753499984\n",
            "Batch #200 Loss: 0.4786416578292847\n",
            "Batch #300 Loss: 0.4817326268553734\n",
            "\u001b[92mTrain accuracy: 41011/48000 =  85.44 % ||| loss 0.3883463442325592\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10211/12000 =  85.09 % ||| loss 0.40516233444213867\u001b[0m\n",
            "\u001b[92mTest accuracy: 8446/10000 =  84.46 % ||| loss 0.41887298226356506\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.4669797566533089\n",
            "Batch #200 Loss: 0.4562515076994896\n",
            "Batch #300 Loss: 0.4804337938129902\n",
            "\u001b[92mTrain accuracy: 41405/48000 =  86.26 % ||| loss 0.3579801321029663\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10294/12000 =  85.78 % ||| loss 0.38189178705215454\u001b[0m\n",
            "\u001b[92mTest accuracy: 8475/10000 =  84.75 % ||| loss 0.4060075879096985\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.4686700928211212\n",
            "Batch #200 Loss: 0.47980525612831115\n",
            "Batch #300 Loss: 0.46176750972867014\n",
            "\u001b[92mTrain accuracy: 41667/48000 =  86.81 % ||| loss 0.3547823131084442\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10352/12000 =  86.27 % ||| loss 0.37369832396507263\u001b[0m\n",
            "\u001b[92mTest accuracy: 8537/10000 =  85.37 % ||| loss 0.4071636497974396\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.4623363408446312\n",
            "Batch #200 Loss: 0.4422585979104042\n",
            "Batch #300 Loss: 0.4776830556988716\n",
            "\u001b[92mTrain accuracy: 41400/48000 =  86.25 % ||| loss 0.36539462208747864\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10263/12000 =  85.52 % ||| loss 0.39784181118011475\u001b[0m\n",
            "\u001b[92mTest accuracy: 8521/10000 =  85.21 % ||| loss 0.4179416000843048\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.43853719741106034\n",
            "Batch #200 Loss: 0.4730773863196373\n",
            "Batch #300 Loss: 0.4667415153980255\n",
            "\u001b[92mTrain accuracy: 41921/48000 =  87.34 % ||| loss 0.34143945574760437\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10378/12000 =  86.48 % ||| loss 0.3682408034801483\u001b[0m\n",
            "\u001b[92mTest accuracy: 8576/10000 =  85.76 % ||| loss 0.39241087436676025\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.45095049977302554\n",
            "Batch #200 Loss: 0.4662900099158287\n",
            "Batch #300 Loss: 0.46540469899773595\n",
            "\u001b[92mTrain accuracy: 41886/48000 =  87.26 % ||| loss 0.3401550352573395\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10427/12000 =  86.89 % ||| loss 0.3668338656425476\u001b[0m\n",
            "\u001b[92mTest accuracy: 8610/10000 =  86.1 % ||| loss 0.39608871936798096\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.4365465293824673\n",
            "Batch #200 Loss: 0.4385843911767006\n",
            "Batch #300 Loss: 0.4571294067800045\n",
            "\u001b[92mTrain accuracy: 41749/48000 =  86.98 % ||| loss 0.3465327024459839\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10339/12000 =  86.16 % ||| loss 0.37315770983695984\u001b[0m\n",
            "\u001b[92mTest accuracy: 8535/10000 =  85.35 % ||| loss 0.39636024832725525\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.44100459694862365\n",
            "Batch #200 Loss: 0.45135158896446226\n",
            "Batch #300 Loss: 0.46485315799713134\n",
            "\u001b[92mTrain accuracy: 42046/48000 =  87.6 % ||| loss 0.3599158227443695\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10396/12000 =  86.63 % ||| loss 0.38808202743530273\u001b[0m\n",
            "\u001b[92mTest accuracy: 8576/10000 =  85.76 % ||| loss 0.4164029359817505\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.4454538008570671\n",
            "Batch #200 Loss: 0.45526600629091263\n",
            "Batch #300 Loss: 0.4576698651909828\n",
            "\u001b[92mTrain accuracy: 42191/48000 =  87.9 % ||| loss 0.3273111879825592\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10457/12000 =  87.14 % ||| loss 0.3566286563873291\u001b[0m\n",
            "\u001b[92mTest accuracy: 8658/10000 =  86.58 % ||| loss 0.3790680468082428\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.4319519302248955\n",
            "Batch #200 Loss: 0.47796973198652265\n",
            "Batch #300 Loss: 0.4774813577532768\n",
            "\u001b[92mTrain accuracy: 41508/48000 =  86.48 % ||| loss 0.3710324168205261\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10327/12000 =  86.06 % ||| loss 0.3902415931224823\u001b[0m\n",
            "\u001b[92mTest accuracy: 8486/10000 =  84.86 % ||| loss 0.41365861892700195\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.467365640103817\n",
            "Batch #200 Loss: 0.46450322031974794\n",
            "Batch #300 Loss: 0.4668216812610626\n",
            "\u001b[92mTrain accuracy: 41403/48000 =  86.26 % ||| loss 0.363873690366745\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10232/12000 =  85.27 % ||| loss 0.39231985807418823\u001b[0m\n",
            "\u001b[92mTest accuracy: 8492/10000 =  84.92 % ||| loss 0.4063933193683624\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.45618562191724776\n",
            "Batch #200 Loss: 0.453543880879879\n",
            "Batch #300 Loss: 0.4596581031382084\n",
            "\u001b[92mTrain accuracy: 41899/48000 =  87.29 % ||| loss 0.3382555842399597\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10401/12000 =  86.67 % ||| loss 0.36755862832069397\u001b[0m\n",
            "\u001b[92mTest accuracy: 8584/10000 =  85.84 % ||| loss 0.3882395327091217\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.4497631162405014\n",
            "Batch #200 Loss: 0.4489899241924286\n",
            "Batch #300 Loss: 0.4708460362255573\n",
            "\u001b[92mTrain accuracy: 41860/48000 =  87.21 % ||| loss 0.34295734763145447\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10436/12000 =  86.97 % ||| loss 0.36121535301208496\u001b[0m\n",
            "\u001b[92mTest accuracy: 8626/10000 =  86.26 % ||| loss 0.3919427990913391\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.4519436895847321\n",
            "Batch #200 Loss: 0.47859516978263855\n",
            "Batch #300 Loss: 0.45124229297041896\n",
            "\u001b[92mTrain accuracy: 41936/48000 =  87.37 % ||| loss 0.3367356061935425\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10421/12000 =  86.84 % ||| loss 0.36130544543266296\u001b[0m\n",
            "\u001b[92mTest accuracy: 8607/10000 =  86.07 % ||| loss 0.39235353469848633\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.45498051166534426\n",
            "Batch #200 Loss: 0.45983014881610873\n",
            "Batch #300 Loss: 0.4587983033061028\n",
            "\u001b[92mTrain accuracy: 41694/48000 =  86.86 % ||| loss 0.35878100991249084\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10371/12000 =  86.42 % ||| loss 0.3816896677017212\u001b[0m\n",
            "\u001b[92mTest accuracy: 8585/10000 =  85.85 % ||| loss 0.40532442927360535\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.45275795966386795\n",
            "Batch #200 Loss: 0.4995466899871826\n",
            "Batch #300 Loss: 0.4819935566186905\n",
            "\u001b[92mTrain accuracy: 41755/48000 =  86.99 % ||| loss 0.3474886417388916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10337/12000 =  86.14 % ||| loss 0.38122817873954773\u001b[0m\n",
            "\u001b[92mTest accuracy: 8559/10000 =  85.59 % ||| loss 0.40272057056427\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.44230044558644294\n",
            "Batch #200 Loss: 0.47661807894706726\n",
            "Batch #300 Loss: 0.4763605763018131\n",
            "\u001b[92mTrain accuracy: 41830/48000 =  87.15 % ||| loss 0.33990877866744995\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10400/12000 =  86.67 % ||| loss 0.37344056367874146\u001b[0m\n",
            "\u001b[92mTest accuracy: 8564/10000 =  85.64 % ||| loss 0.40510401129722595\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.43583705961704255\n",
            "Batch #200 Loss: 0.47245222866535186\n",
            "Batch #300 Loss: 0.49658974647521975\n",
            "\u001b[92mTrain accuracy: 42123/48000 =  87.76 % ||| loss 0.3307490944862366\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10383/12000 =  86.52 % ||| loss 0.37749606370925903\u001b[0m\n",
            "\u001b[92mTest accuracy: 8624/10000 =  86.24 % ||| loss 0.39117518067359924\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.45019301056861877\n",
            "Batch #200 Loss: 0.4566549643874168\n",
            "Batch #300 Loss: 0.4562756681442261\n",
            "\u001b[92mTrain accuracy: 42061/48000 =  87.63 % ||| loss 0.33801543712615967\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10384/12000 =  86.53 % ||| loss 0.37794506549835205\u001b[0m\n",
            "\u001b[92mTest accuracy: 8568/10000 =  85.68 % ||| loss 0.40297549962997437\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_5</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_5</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_102310-Lenet5Dropout_1726150070.105228_6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_6' target=\"_blank\">Lenet5Dropout_1726150070.105228_6</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.5610818898677825\n",
            "Batch #200 Loss: 0.7840092271566391\n",
            "Batch #300 Loss: 0.5989536508917809\n",
            "\u001b[92mTrain accuracy: 39865/48000 =  83.05 % ||| loss 0.47874748706817627\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9963/12000 =  83.03 % ||| loss 0.47986599802970886\u001b[0m\n",
            "\u001b[92mTest accuracy: 8223/10000 =  82.23 % ||| loss 0.49890246987342834\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.5023729258775711\n",
            "Batch #200 Loss: 0.4645522171258926\n",
            "Batch #300 Loss: 0.43569749176502226\n",
            "\u001b[92mTrain accuracy: 41281/48000 =  86.0 % ||| loss 0.3790771961212158\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10286/12000 =  85.72 % ||| loss 0.3963050842285156\u001b[0m\n",
            "\u001b[92mTest accuracy: 8543/10000 =  85.43 % ||| loss 0.4095422625541687\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.4120456662774086\n",
            "Batch #200 Loss: 0.390326022207737\n",
            "Batch #300 Loss: 0.37686378359794614\n",
            "\u001b[92mTrain accuracy: 42286/48000 =  88.1 % ||| loss 0.3218477666378021\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10484/12000 =  87.37 % ||| loss 0.3379978537559509\u001b[0m\n",
            "\u001b[92mTest accuracy: 8684/10000 =  86.84 % ||| loss 0.3593723177909851\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.36877249747514723\n",
            "Batch #200 Loss: 0.3636689552664757\n",
            "Batch #300 Loss: 0.34862971782684327\n",
            "\u001b[92mTrain accuracy: 42676/48000 =  88.91 % ||| loss 0.2978096902370453\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10576/12000 =  88.13 % ||| loss 0.32327431440353394\u001b[0m\n",
            "\u001b[92mTest accuracy: 8789/10000 =  87.89 % ||| loss 0.335941880941391\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.32736424311995505\n",
            "Batch #200 Loss: 0.33617748856544494\n",
            "Batch #300 Loss: 0.3344668458402157\n",
            "\u001b[92mTrain accuracy: 42851/48000 =  89.27 % ||| loss 0.2898736596107483\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10587/12000 =  88.22 % ||| loss 0.3228756785392761\u001b[0m\n",
            "\u001b[92mTest accuracy: 8799/10000 =  87.99 % ||| loss 0.3336820602416992\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.3167380757629871\n",
            "Batch #200 Loss: 0.3106853301823139\n",
            "Batch #300 Loss: 0.32830129355192184\n",
            "\u001b[92mTrain accuracy: 43331/48000 =  90.27 % ||| loss 0.26632383465766907\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10709/12000 =  89.24 % ||| loss 0.29734450578689575\u001b[0m\n",
            "\u001b[92mTest accuracy: 8867/10000 =  88.67 % ||| loss 0.3112438917160034\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.30450162395834923\n",
            "Batch #200 Loss: 0.29779618173837663\n",
            "Batch #300 Loss: 0.29361931294202803\n",
            "\u001b[92mTrain accuracy: 43186/48000 =  89.97 % ||| loss 0.26750174164772034\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10660/12000 =  88.83 % ||| loss 0.30379518866539\u001b[0m\n",
            "\u001b[92mTest accuracy: 8792/10000 =  87.92 % ||| loss 0.31754136085510254\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.28227476552128794\n",
            "Batch #200 Loss: 0.2987607944011688\n",
            "Batch #300 Loss: 0.2919876854121685\n",
            "\u001b[92mTrain accuracy: 43787/48000 =  91.22 % ||| loss 0.23504389822483063\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10734/12000 =  89.45 % ||| loss 0.2816269099712372\u001b[0m\n",
            "\u001b[92mTest accuracy: 8942/10000 =  89.42 % ||| loss 0.29554617404937744\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.2775397527217865\n",
            "Batch #200 Loss: 0.2799698163568973\n",
            "Batch #300 Loss: 0.28843295469880104\n",
            "\u001b[92mTrain accuracy: 43746/48000 =  91.14 % ||| loss 0.23809181153774261\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10730/12000 =  89.42 % ||| loss 0.28403744101524353\u001b[0m\n",
            "\u001b[92mTest accuracy: 8881/10000 =  88.81 % ||| loss 0.3009362518787384\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.2651658447086811\n",
            "Batch #200 Loss: 0.27183014154434204\n",
            "Batch #300 Loss: 0.27619227275252345\n",
            "\u001b[92mTrain accuracy: 43787/48000 =  91.22 % ||| loss 0.2321712225675583\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10728/12000 =  89.4 % ||| loss 0.28623130917549133\u001b[0m\n",
            "\u001b[92mTest accuracy: 8880/10000 =  88.8 % ||| loss 0.30228766798973083\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.25942290291190145\n",
            "Batch #200 Loss: 0.2600507007539272\n",
            "Batch #300 Loss: 0.27331659965217114\n",
            "\u001b[92mTrain accuracy: 43894/48000 =  91.45 % ||| loss 0.23289495706558228\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10727/12000 =  89.39 % ||| loss 0.29659661650657654\u001b[0m\n",
            "\u001b[92mTest accuracy: 8911/10000 =  89.11 % ||| loss 0.3071470558643341\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.25220212906599043\n",
            "Batch #200 Loss: 0.24812914729118346\n",
            "Batch #300 Loss: 0.25416535094380377\n",
            "\u001b[92mTrain accuracy: 43921/48000 =  91.5 % ||| loss 0.22238288819789886\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10710/12000 =  89.25 % ||| loss 0.28894516825675964\u001b[0m\n",
            "\u001b[92mTest accuracy: 8878/10000 =  88.78 % ||| loss 0.299981951713562\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.24437399506568908\n",
            "Batch #200 Loss: 0.24057959377765656\n",
            "Batch #300 Loss: 0.2456051804870367\n",
            "\u001b[92mTrain accuracy: 44202/48000 =  92.09 % ||| loss 0.20812246203422546\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10777/12000 =  89.81 % ||| loss 0.2772253155708313\u001b[0m\n",
            "\u001b[92mTest accuracy: 8925/10000 =  89.25 % ||| loss 0.2868449091911316\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.23555632770061494\n",
            "Batch #200 Loss: 0.244580856859684\n",
            "Batch #300 Loss: 0.25058093026280404\n",
            "\u001b[92mTrain accuracy: 44320/48000 =  92.33 % ||| loss 0.2007158398628235\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10783/12000 =  89.86 % ||| loss 0.2757662236690521\u001b[0m\n",
            "\u001b[92mTest accuracy: 8939/10000 =  89.39 % ||| loss 0.2938184142112732\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.23273426957428456\n",
            "Batch #200 Loss: 0.23274992719292642\n",
            "Batch #300 Loss: 0.23965060114860534\n",
            "\u001b[92mTrain accuracy: 44421/48000 =  92.54 % ||| loss 0.19340988993644714\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10778/12000 =  89.82 % ||| loss 0.2752099931240082\u001b[0m\n",
            "\u001b[92mTest accuracy: 8942/10000 =  89.42 % ||| loss 0.28819945454597473\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.22692117162048817\n",
            "Batch #200 Loss: 0.23145146533846855\n",
            "Batch #300 Loss: 0.23798920013010502\n",
            "\u001b[92mTrain accuracy: 44542/48000 =  92.8 % ||| loss 0.19368116557598114\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10794/12000 =  89.95 % ||| loss 0.27635350823402405\u001b[0m\n",
            "\u001b[92mTest accuracy: 8974/10000 =  89.74 % ||| loss 0.29066047072410583\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.22037238843739032\n",
            "Batch #200 Loss: 0.23462362639606\n",
            "Batch #300 Loss: 0.22810580745339393\n",
            "\u001b[92mTrain accuracy: 44668/48000 =  93.06 % ||| loss 0.1833401620388031\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10837/12000 =  90.31 % ||| loss 0.2769802510738373\u001b[0m\n",
            "\u001b[92mTest accuracy: 8953/10000 =  89.53 % ||| loss 0.292033314704895\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.22304407842457294\n",
            "Batch #200 Loss: 0.22551981814205646\n",
            "Batch #300 Loss: 0.2165912339836359\n",
            "\u001b[92mTrain accuracy: 44785/48000 =  93.3 % ||| loss 0.17995668947696686\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10804/12000 =  90.03 % ||| loss 0.2709324061870575\u001b[0m\n",
            "\u001b[92mTest accuracy: 8949/10000 =  89.49 % ||| loss 0.28376495838165283\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.22123695969581603\n",
            "Batch #200 Loss: 0.20648436032235623\n",
            "Batch #300 Loss: 0.2097482931613922\n",
            "\u001b[92mTrain accuracy: 44714/48000 =  93.15 % ||| loss 0.17976105213165283\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10806/12000 =  90.05 % ||| loss 0.2792661786079407\u001b[0m\n",
            "\u001b[92mTest accuracy: 8962/10000 =  89.62 % ||| loss 0.2914009690284729\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.20772993691265584\n",
            "Batch #200 Loss: 0.21374167464673519\n",
            "Batch #300 Loss: 0.20125695027410984\n",
            "\u001b[92mTrain accuracy: 44845/48000 =  93.43 % ||| loss 0.17220385372638702\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10808/12000 =  90.07 % ||| loss 0.28434228897094727\u001b[0m\n",
            "\u001b[92mTest accuracy: 8966/10000 =  89.66 % ||| loss 0.30217650532722473\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.20459501191973686\n",
            "Batch #200 Loss: 0.21105436213314532\n",
            "Batch #300 Loss: 0.19982559829950333\n",
            "\u001b[92mTrain accuracy: 44794/48000 =  93.32 % ||| loss 0.17465493083000183\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10810/12000 =  90.08 % ||| loss 0.2794105112552643\u001b[0m\n",
            "\u001b[92mTest accuracy: 8962/10000 =  89.62 % ||| loss 0.2953934371471405\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.19907282046973707\n",
            "Batch #200 Loss: 0.20824066147208214\n",
            "Batch #300 Loss: 0.20203587837517262\n",
            "\u001b[92mTrain accuracy: 45096/48000 =  93.95 % ||| loss 0.15592440962791443\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10827/12000 =  90.22 % ||| loss 0.2760617733001709\u001b[0m\n",
            "\u001b[92mTest accuracy: 8985/10000 =  89.85 % ||| loss 0.2932371199131012\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.1874849344789982\n",
            "Batch #200 Loss: 0.2055770231038332\n",
            "Batch #300 Loss: 0.20198680117726325\n",
            "\u001b[92mTrain accuracy: 45328/48000 =  94.43 % ||| loss 0.14711813628673553\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10848/12000 =  90.4 % ||| loss 0.2755499482154846\u001b[0m\n",
            "\u001b[92mTest accuracy: 9000/10000 =  90.0 % ||| loss 0.2954900860786438\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.18879750661551953\n",
            "Batch #200 Loss: 0.19596466913819313\n",
            "Batch #300 Loss: 0.19846912108361722\n",
            "\u001b[92mTrain accuracy: 45244/48000 =  94.26 % ||| loss 0.14933475852012634\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10866/12000 =  90.55 % ||| loss 0.2794034779071808\u001b[0m\n",
            "\u001b[92mTest accuracy: 9004/10000 =  90.04 % ||| loss 0.29792580008506775\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.1852247679233551\n",
            "Batch #200 Loss: 0.19853666976094245\n",
            "Batch #300 Loss: 0.1893108492344618\n",
            "\u001b[92mTrain accuracy: 45225/48000 =  94.22 % ||| loss 0.14858408272266388\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10834/12000 =  90.28 % ||| loss 0.2829684019088745\u001b[0m\n",
            "\u001b[92mTest accuracy: 9019/10000 =  90.19 % ||| loss 0.2947283983230591\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_6</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_6</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_102542-Lenet5Dropout_1726150070.105228_7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_7' target=\"_blank\">Lenet5Dropout_1726150070.105228_7</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.645888746380806\n",
            "Batch #200 Loss: 0.8399231839179992\n",
            "Batch #300 Loss: 0.6830816456675529\n",
            "\u001b[92mTrain accuracy: 38755/48000 =  80.74 % ||| loss 0.5139502286911011\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9700/12000 =  80.83 % ||| loss 0.5090766549110413\u001b[0m\n",
            "\u001b[92mTest accuracy: 8010/10000 =  80.1 % ||| loss 0.5296395421028137\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.5631898111104965\n",
            "Batch #200 Loss: 0.537771067917347\n",
            "Batch #300 Loss: 0.5176304730772973\n",
            "\u001b[92mTrain accuracy: 40802/48000 =  85.0 % ||| loss 0.39902210235595703\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10215/12000 =  85.12 % ||| loss 0.39921560883522034\u001b[0m\n",
            "\u001b[92mTest accuracy: 8403/10000 =  84.03 % ||| loss 0.4249015152454376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.442588632106781\n",
            "Batch #200 Loss: 0.4630553779006004\n",
            "Batch #300 Loss: 0.43106226578354834\n",
            "\u001b[92mTrain accuracy: 41568/48000 =  86.6 % ||| loss 0.35865816473960876\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10381/12000 =  86.51 % ||| loss 0.36530202627182007\u001b[0m\n",
            "\u001b[92mTest accuracy: 8521/10000 =  85.21 % ||| loss 0.3888031542301178\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.3958507749438286\n",
            "Batch #200 Loss: 0.4030938175320625\n",
            "Batch #300 Loss: 0.4017297998070717\n",
            "\u001b[92mTrain accuracy: 41579/48000 =  86.62 % ||| loss 0.34944817423820496\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10359/12000 =  86.33 % ||| loss 0.3601229190826416\u001b[0m\n",
            "\u001b[92mTest accuracy: 8541/10000 =  85.41 % ||| loss 0.3895387053489685\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.37560336947441103\n",
            "Batch #200 Loss: 0.36961731299757955\n",
            "Batch #300 Loss: 0.37594166591763495\n",
            "\u001b[92mTrain accuracy: 42489/48000 =  88.52 % ||| loss 0.3046669661998749\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10553/12000 =  87.94 % ||| loss 0.32088714838027954\u001b[0m\n",
            "\u001b[92mTest accuracy: 8721/10000 =  87.21 % ||| loss 0.34282130002975464\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.3493735146522522\n",
            "Batch #200 Loss: 0.35878427907824517\n",
            "Batch #300 Loss: 0.34669412806630134\n",
            "\u001b[92mTrain accuracy: 42708/48000 =  88.98 % ||| loss 0.2918012738227844\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10618/12000 =  88.48 % ||| loss 0.3063657879829407\u001b[0m\n",
            "\u001b[92mTest accuracy: 8747/10000 =  87.47 % ||| loss 0.3353433609008789\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.34344175443053243\n",
            "Batch #200 Loss: 0.33510256826877594\n",
            "Batch #300 Loss: 0.3357254794239998\n",
            "\u001b[92mTrain accuracy: 43294/48000 =  90.2 % ||| loss 0.26534175872802734\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10724/12000 =  89.37 % ||| loss 0.28918319940567017\u001b[0m\n",
            "\u001b[92mTest accuracy: 8860/10000 =  88.6 % ||| loss 0.3174329698085785\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3298607859015465\n",
            "Batch #200 Loss: 0.32483336359262466\n",
            "Batch #300 Loss: 0.3289592608809471\n",
            "\u001b[92mTrain accuracy: 43258/48000 =  90.12 % ||| loss 0.265255331993103\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10721/12000 =  89.34 % ||| loss 0.2925882339477539\u001b[0m\n",
            "\u001b[92mTest accuracy: 8846/10000 =  88.46 % ||| loss 0.3209851384162903\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3237172995507717\n",
            "Batch #200 Loss: 0.3132408219575882\n",
            "Batch #300 Loss: 0.30657013565301894\n",
            "\u001b[92mTrain accuracy: 43324/48000 =  90.26 % ||| loss 0.2613121271133423\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10709/12000 =  89.24 % ||| loss 0.28831636905670166\u001b[0m\n",
            "\u001b[92mTest accuracy: 8834/10000 =  88.34 % ||| loss 0.31613150238990784\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3041326643526554\n",
            "Batch #200 Loss: 0.29871832460165026\n",
            "Batch #300 Loss: 0.31688152104616163\n",
            "\u001b[92mTrain accuracy: 43616/48000 =  90.87 % ||| loss 0.2475677728652954\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10757/12000 =  89.64 % ||| loss 0.28063440322875977\u001b[0m\n",
            "\u001b[92mTest accuracy: 8893/10000 =  88.93 % ||| loss 0.3092060983181\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.30340625301003454\n",
            "Batch #200 Loss: 0.29432047620415686\n",
            "Batch #300 Loss: 0.3032833196222782\n",
            "\u001b[92mTrain accuracy: 43598/48000 =  90.83 % ||| loss 0.24420388042926788\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10758/12000 =  89.65 % ||| loss 0.275398850440979\u001b[0m\n",
            "\u001b[92mTest accuracy: 8888/10000 =  88.88 % ||| loss 0.3014792799949646\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.27813202887773514\n",
            "Batch #200 Loss: 0.2928212907910347\n",
            "Batch #300 Loss: 0.2969161921739578\n",
            "\u001b[92mTrain accuracy: 44018/48000 =  91.7 % ||| loss 0.2235504388809204\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10801/12000 =  90.01 % ||| loss 0.2683468461036682\u001b[0m\n",
            "\u001b[92mTest accuracy: 8949/10000 =  89.49 % ||| loss 0.2913910448551178\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.28870169445872307\n",
            "Batch #200 Loss: 0.2828882221877575\n",
            "Batch #300 Loss: 0.29187756225466727\n",
            "\u001b[92mTrain accuracy: 43975/48000 =  91.61 % ||| loss 0.2233913540840149\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10761/12000 =  89.68 % ||| loss 0.26734811067581177\u001b[0m\n",
            "\u001b[92mTest accuracy: 8935/10000 =  89.35 % ||| loss 0.29843443632125854\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.27748598143458364\n",
            "Batch #200 Loss: 0.2774512395262718\n",
            "Batch #300 Loss: 0.27210708796977995\n",
            "\u001b[92mTrain accuracy: 43779/48000 =  91.21 % ||| loss 0.22842490673065186\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10775/12000 =  89.79 % ||| loss 0.27247750759124756\u001b[0m\n",
            "\u001b[92mTest accuracy: 8901/10000 =  89.01 % ||| loss 0.30077219009399414\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.26570374727249146\n",
            "Batch #200 Loss: 0.2804830427467823\n",
            "Batch #300 Loss: 0.26924175590276717\n",
            "\u001b[92mTrain accuracy: 43708/48000 =  91.06 % ||| loss 0.2311941385269165\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10749/12000 =  89.58 % ||| loss 0.28457626700401306\u001b[0m\n",
            "\u001b[92mTest accuracy: 8880/10000 =  88.8 % ||| loss 0.3218207061290741\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.24869780912995337\n",
            "Batch #200 Loss: 0.2792369721829891\n",
            "Batch #300 Loss: 0.26749688863754273\n",
            "\u001b[92mTrain accuracy: 44279/48000 =  92.25 % ||| loss 0.2038203328847885\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10829/12000 =  90.24 % ||| loss 0.26358047127723694\u001b[0m\n",
            "\u001b[92mTest accuracy: 8953/10000 =  89.53 % ||| loss 0.2927716076374054\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.26282483473420143\n",
            "Batch #200 Loss: 0.2685369815677404\n",
            "Batch #300 Loss: 0.2697034339606762\n",
            "\u001b[92mTrain accuracy: 44216/48000 =  92.12 % ||| loss 0.20534464716911316\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10784/12000 =  89.87 % ||| loss 0.26766639947891235\u001b[0m\n",
            "\u001b[92mTest accuracy: 8939/10000 =  89.39 % ||| loss 0.30433306097984314\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.2616575174033642\n",
            "Batch #200 Loss: 0.2515511105954647\n",
            "Batch #300 Loss: 0.2545843541622162\n",
            "\u001b[92mTrain accuracy: 44487/48000 =  92.68 % ||| loss 0.19559037685394287\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10805/12000 =  90.04 % ||| loss 0.2642683684825897\u001b[0m\n",
            "\u001b[92mTest accuracy: 8976/10000 =  89.76 % ||| loss 0.284919410943985\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.2547039175033569\n",
            "Batch #200 Loss: 0.2592977859824896\n",
            "Batch #300 Loss: 0.26200434401631356\n",
            "\u001b[92mTrain accuracy: 44339/48000 =  92.37 % ||| loss 0.20214375853538513\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10835/12000 =  90.29 % ||| loss 0.26969990134239197\u001b[0m\n",
            "\u001b[92mTest accuracy: 8928/10000 =  89.28 % ||| loss 0.29889827966690063\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.24863395124673843\n",
            "Batch #200 Loss: 0.24464262321591376\n",
            "Batch #300 Loss: 0.252942074239254\n",
            "\u001b[92mTrain accuracy: 44515/48000 =  92.74 % ||| loss 0.19118565320968628\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10856/12000 =  90.47 % ||| loss 0.2605073153972626\u001b[0m\n",
            "\u001b[92mTest accuracy: 8962/10000 =  89.62 % ||| loss 0.29597923159599304\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.23612171590328215\n",
            "Batch #200 Loss: 0.24620101794600488\n",
            "Batch #300 Loss: 0.25933294765651227\n",
            "\u001b[92mTrain accuracy: 44584/48000 =  92.88 % ||| loss 0.1881684511899948\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10826/12000 =  90.22 % ||| loss 0.26634544134140015\u001b[0m\n",
            "\u001b[92mTest accuracy: 8952/10000 =  89.52 % ||| loss 0.2986065149307251\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.23908760637044907\n",
            "Batch #200 Loss: 0.23714628808200358\n",
            "Batch #300 Loss: 0.25241901449859144\n",
            "\u001b[92mTrain accuracy: 44648/48000 =  93.02 % ||| loss 0.1811027079820633\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10839/12000 =  90.33 % ||| loss 0.2603074610233307\u001b[0m\n",
            "\u001b[92mTest accuracy: 8981/10000 =  89.81 % ||| loss 0.2905290126800537\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.21886681877076625\n",
            "Batch #200 Loss: 0.24407849624752997\n",
            "Batch #300 Loss: 0.2567493110895157\n",
            "\u001b[92mTrain accuracy: 44442/48000 =  92.59 % ||| loss 0.19277440011501312\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10801/12000 =  90.01 % ||| loss 0.26809364557266235\u001b[0m\n",
            "\u001b[92mTest accuracy: 8937/10000 =  89.37 % ||| loss 0.30153435468673706\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.23623417474329472\n",
            "Batch #200 Loss: 0.23024736233055593\n",
            "Batch #300 Loss: 0.25173114210367203\n",
            "\u001b[92mTrain accuracy: 44532/48000 =  92.77 % ||| loss 0.1898801028728485\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10807/12000 =  90.06 % ||| loss 0.26409801840782166\u001b[0m\n",
            "\u001b[92mTest accuracy: 8919/10000 =  89.19 % ||| loss 0.30141645669937134\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.2219533294439316\n",
            "Batch #200 Loss: 0.233036488071084\n",
            "Batch #300 Loss: 0.24324489533901214\n",
            "\u001b[92mTrain accuracy: 44742/48000 =  93.21 % ||| loss 0.17762751877307892\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10846/12000 =  90.38 % ||| loss 0.25904473662376404\u001b[0m\n",
            "\u001b[92mTest accuracy: 8963/10000 =  89.63 % ||| loss 0.29476866126060486\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_7</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_7</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_102814-Lenet5Dropout_1726150070.105228_8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_8' target=\"_blank\">Lenet5Dropout_1726150070.105228_8</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.8597227346897125\n",
            "Batch #200 Loss: 1.0219455581903458\n",
            "Batch #300 Loss: 0.8018124186992646\n",
            "\u001b[92mTrain accuracy: 37557/48000 =  78.24 % ||| loss 0.5737919211387634\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9464/12000 =  78.87 % ||| loss 0.566963255405426\u001b[0m\n",
            "\u001b[92mTest accuracy: 7749/10000 =  77.49 % ||| loss 0.5958787798881531\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6717998132109642\n",
            "Batch #200 Loss: 0.6409507375955582\n",
            "Batch #300 Loss: 0.5991936406493187\n",
            "\u001b[92mTrain accuracy: 39898/48000 =  83.12 % ||| loss 0.4660727083683014\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9984/12000 =  83.2 % ||| loss 0.46292105317115784\u001b[0m\n",
            "\u001b[92mTest accuracy: 8197/10000 =  81.97 % ||| loss 0.494060754776001\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5554052790999413\n",
            "Batch #200 Loss: 0.5325806280970573\n",
            "Batch #300 Loss: 0.5217983886599541\n",
            "\u001b[92mTrain accuracy: 41109/48000 =  85.64 % ||| loss 0.3846704959869385\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10272/12000 =  85.6 % ||| loss 0.3875747323036194\u001b[0m\n",
            "\u001b[92mTest accuracy: 8471/10000 =  84.71 % ||| loss 0.411956250667572\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.48747792214155194\n",
            "Batch #200 Loss: 0.464275358915329\n",
            "Batch #300 Loss: 0.4618246978521347\n",
            "\u001b[92mTrain accuracy: 41649/48000 =  86.77 % ||| loss 0.354561984539032\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10376/12000 =  86.47 % ||| loss 0.3672252297401428\u001b[0m\n",
            "\u001b[92mTest accuracy: 8540/10000 =  85.4 % ||| loss 0.38925403356552124\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4356561866402626\n",
            "Batch #200 Loss: 0.44594296932220456\n",
            "Batch #300 Loss: 0.4305574107170105\n",
            "\u001b[92mTrain accuracy: 41966/48000 =  87.43 % ||| loss 0.35246357321739197\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10441/12000 =  87.01 % ||| loss 0.36263129115104675\u001b[0m\n",
            "\u001b[92mTest accuracy: 8612/10000 =  86.12 % ||| loss 0.38934022188186646\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.41468460589647294\n",
            "Batch #200 Loss: 0.41306430667638777\n",
            "Batch #300 Loss: 0.41231444641947745\n",
            "\u001b[92mTrain accuracy: 42259/48000 =  88.04 % ||| loss 0.3298293352127075\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10507/12000 =  87.56 % ||| loss 0.34317877888679504\u001b[0m\n",
            "\u001b[92mTest accuracy: 8671/10000 =  86.71 % ||| loss 0.37226542830467224\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.4226318097114563\n",
            "Batch #200 Loss: 0.4021057069301605\n",
            "Batch #300 Loss: 0.39547701612114905\n",
            "\u001b[92mTrain accuracy: 42676/48000 =  88.91 % ||| loss 0.3051340878009796\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10561/12000 =  88.01 % ||| loss 0.32649514079093933\u001b[0m\n",
            "\u001b[92mTest accuracy: 8716/10000 =  87.16 % ||| loss 0.3507983386516571\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.39271539226174357\n",
            "Batch #200 Loss: 0.3981269110739231\n",
            "Batch #300 Loss: 0.39294735923409463\n",
            "\u001b[92mTrain accuracy: 42660/48000 =  88.88 % ||| loss 0.3021725118160248\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10575/12000 =  88.12 % ||| loss 0.32341256737709045\u001b[0m\n",
            "\u001b[92mTest accuracy: 8700/10000 =  87.0 % ||| loss 0.3508037030696869\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3836506502330303\n",
            "Batch #200 Loss: 0.3856485268473625\n",
            "Batch #300 Loss: 0.3873560681939125\n",
            "\u001b[92mTrain accuracy: 42845/48000 =  89.26 % ||| loss 0.28855404257774353\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10587/12000 =  88.22 % ||| loss 0.31322067975997925\u001b[0m\n",
            "\u001b[92mTest accuracy: 8742/10000 =  87.42 % ||| loss 0.3399335741996765\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.38261616364121437\n",
            "Batch #200 Loss: 0.37864927649497987\n",
            "Batch #300 Loss: 0.37812612280249597\n",
            "\u001b[92mTrain accuracy: 42721/48000 =  89.0 % ||| loss 0.29123032093048096\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10578/12000 =  88.15 % ||| loss 0.3140336275100708\u001b[0m\n",
            "\u001b[92mTest accuracy: 8752/10000 =  87.52 % ||| loss 0.342558890581131\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.3749722334742546\n",
            "Batch #200 Loss: 0.35419987872242925\n",
            "Batch #300 Loss: 0.36515358090400696\n",
            "\u001b[92mTrain accuracy: 43093/48000 =  89.78 % ||| loss 0.28185129165649414\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10629/12000 =  88.58 % ||| loss 0.30831366777420044\u001b[0m\n",
            "\u001b[92mTest accuracy: 8762/10000 =  87.62 % ||| loss 0.33201363682746887\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3559038683772087\n",
            "Batch #200 Loss: 0.3623924069106579\n",
            "Batch #300 Loss: 0.36007911249995234\n",
            "\u001b[92mTrain accuracy: 42793/48000 =  89.15 % ||| loss 0.28240975737571716\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10582/12000 =  88.18 % ||| loss 0.3083239197731018\u001b[0m\n",
            "\u001b[92mTest accuracy: 8726/10000 =  87.26 % ||| loss 0.3414861261844635\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.34881800144910813\n",
            "Batch #200 Loss: 0.3514060194790363\n",
            "Batch #300 Loss: 0.3533050510287285\n",
            "\u001b[92mTrain accuracy: 43117/48000 =  89.83 % ||| loss 0.2725149691104889\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10655/12000 =  88.79 % ||| loss 0.3010832965373993\u001b[0m\n",
            "\u001b[92mTest accuracy: 8798/10000 =  87.98 % ||| loss 0.3289908468723297\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3613443872332573\n",
            "Batch #200 Loss: 0.3434179952740669\n",
            "Batch #300 Loss: 0.3379550565779209\n",
            "\u001b[92mTrain accuracy: 43242/48000 =  90.09 % ||| loss 0.26888442039489746\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10658/12000 =  88.82 % ||| loss 0.30098870396614075\u001b[0m\n",
            "\u001b[92mTest accuracy: 8832/10000 =  88.32 % ||| loss 0.3236696124076843\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.33717231154441835\n",
            "Batch #200 Loss: 0.3335587893426418\n",
            "Batch #300 Loss: 0.3382109209895134\n",
            "\u001b[92mTrain accuracy: 43080/48000 =  89.75 % ||| loss 0.26853346824645996\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10607/12000 =  88.39 % ||| loss 0.30470842123031616\u001b[0m\n",
            "\u001b[92mTest accuracy: 8795/10000 =  87.95 % ||| loss 0.32767200469970703\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3202879196405411\n",
            "Batch #200 Loss: 0.3334891977906227\n",
            "Batch #300 Loss: 0.3317080023884773\n",
            "\u001b[92mTrain accuracy: 43360/48000 =  90.33 % ||| loss 0.2598097026348114\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10666/12000 =  88.88 % ||| loss 0.29270973801612854\u001b[0m\n",
            "\u001b[92mTest accuracy: 8838/10000 =  88.38 % ||| loss 0.3209352493286133\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3176886186003685\n",
            "Batch #200 Loss: 0.3228696331381798\n",
            "Batch #300 Loss: 0.3342741486430168\n",
            "\u001b[92mTrain accuracy: 43378/48000 =  90.37 % ||| loss 0.2512539327144623\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10689/12000 =  89.08 % ||| loss 0.2923850119113922\u001b[0m\n",
            "\u001b[92mTest accuracy: 8867/10000 =  88.67 % ||| loss 0.3260898292064667\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3291663794219494\n",
            "Batch #200 Loss: 0.3252574947476387\n",
            "Batch #300 Loss: 0.3199260947108269\n",
            "\u001b[92mTrain accuracy: 43300/48000 =  90.21 % ||| loss 0.26668909192085266\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10708/12000 =  89.23 % ||| loss 0.3025321662425995\u001b[0m\n",
            "\u001b[92mTest accuracy: 8804/10000 =  88.04 % ||| loss 0.32940348982810974\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.3261721920967102\n",
            "Batch #200 Loss: 0.31749177202582357\n",
            "Batch #300 Loss: 0.3217464929819107\n",
            "\u001b[92mTrain accuracy: 43553/48000 =  90.74 % ||| loss 0.25011423230171204\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10748/12000 =  89.57 % ||| loss 0.28755417466163635\u001b[0m\n",
            "\u001b[92mTest accuracy: 8888/10000 =  88.88 % ||| loss 0.3152715265750885\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.3130983333289623\n",
            "Batch #200 Loss: 0.32982061684131625\n",
            "Batch #300 Loss: 0.3113569663465023\n",
            "\u001b[92mTrain accuracy: 43645/48000 =  90.93 % ||| loss 0.24529707431793213\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10720/12000 =  89.33 % ||| loss 0.2871522009372711\u001b[0m\n",
            "\u001b[92mTest accuracy: 8878/10000 =  88.78 % ||| loss 0.31974345445632935\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.31417386487126353\n",
            "Batch #200 Loss: 0.3231585182249546\n",
            "Batch #300 Loss: 0.30683979257941246\n",
            "\u001b[92mTrain accuracy: 43652/48000 =  90.94 % ||| loss 0.23896977305412292\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10730/12000 =  89.42 % ||| loss 0.2836354970932007\u001b[0m\n",
            "\u001b[92mTest accuracy: 8865/10000 =  88.65 % ||| loss 0.31336429715156555\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.30916038259863854\n",
            "Batch #200 Loss: 0.30989903897047044\n",
            "Batch #300 Loss: 0.30532395735383033\n",
            "\u001b[92mTrain accuracy: 43540/48000 =  90.71 % ||| loss 0.23993656039237976\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10702/12000 =  89.18 % ||| loss 0.28843244910240173\u001b[0m\n",
            "\u001b[92mTest accuracy: 8857/10000 =  88.57 % ||| loss 0.3243224620819092\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.30493113219738005\n",
            "Batch #200 Loss: 0.31160743430256843\n",
            "Batch #300 Loss: 0.3200437793135643\n",
            "\u001b[92mTrain accuracy: 43726/48000 =  91.1 % ||| loss 0.23730063438415527\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10757/12000 =  89.64 % ||| loss 0.28384149074554443\u001b[0m\n",
            "\u001b[92mTest accuracy: 8858/10000 =  88.58 % ||| loss 0.30942070484161377\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.2988463142514229\n",
            "Batch #200 Loss: 0.2993958696722984\n",
            "Batch #300 Loss: 0.3048585493862629\n",
            "\u001b[92mTrain accuracy: 44060/48000 =  91.79 % ||| loss 0.22239357233047485\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10780/12000 =  89.83 % ||| loss 0.2791420817375183\u001b[0m\n",
            "\u001b[92mTest accuracy: 8917/10000 =  89.17 % ||| loss 0.312909871339798\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.302556577026844\n",
            "Batch #200 Loss: 0.30544974938035013\n",
            "Batch #300 Loss: 0.30642504259943965\n",
            "\u001b[92mTrain accuracy: 43405/48000 =  90.43 % ||| loss 0.25661954283714294\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10669/12000 =  88.91 % ||| loss 0.30411434173583984\u001b[0m\n",
            "\u001b[92mTest accuracy: 8796/10000 =  87.96 % ||| loss 0.3363102674484253\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_8</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_8</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_103044-Lenet5Dropout_1726150070.105228_9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_9' target=\"_blank\">Lenet5Dropout_1726150070.105228_9</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_9' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302098753452301\n",
            "Batch #200 Loss: 2.2993353176116944\n",
            "Batch #300 Loss: 2.2943985724449156\n",
            "\u001b[92mTrain accuracy: 7597/48000 =  15.83 % ||| loss 2.2848565578460693\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1922/12000 =  16.02 % ||| loss 2.2848992347717285\u001b[0m\n",
            "\u001b[92mTest accuracy: 1589/10000 =  15.89 % ||| loss 2.2847864627838135\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.279500012397766\n",
            "Batch #200 Loss: 2.2566544890403746\n",
            "Batch #300 Loss: 2.180019202232361\n",
            "\u001b[92mTrain accuracy: 26383/48000 =  54.96 % ||| loss 1.5292656421661377\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6602/12000 =  55.02 % ||| loss 1.526734709739685\u001b[0m\n",
            "\u001b[92mTest accuracy: 5504/10000 =  55.04 % ||| loss 1.5300918817520142\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.3740669250488282\n",
            "Batch #200 Loss: 1.1572610366344451\n",
            "Batch #300 Loss: 1.0417989808321\n",
            "\u001b[92mTrain accuracy: 31959/48000 =  66.58 % ||| loss 0.8998765349388123\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8060/12000 =  67.17 % ||| loss 0.886449933052063\u001b[0m\n",
            "\u001b[92mTest accuracy: 6611/10000 =  66.11 % ||| loss 0.9096981287002563\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.9422947579622268\n",
            "Batch #200 Loss: 0.8985689818859101\n",
            "Batch #300 Loss: 0.8922758740186691\n",
            "\u001b[92mTrain accuracy: 33297/48000 =  69.37 % ||| loss 0.7804366946220398\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8363/12000 =  69.69 % ||| loss 0.7681556940078735\u001b[0m\n",
            "\u001b[92mTest accuracy: 6872/10000 =  68.72 % ||| loss 0.7956076860427856\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.825100502371788\n",
            "Batch #200 Loss: 0.8213262331485748\n",
            "Batch #300 Loss: 0.7906849956512452\n",
            "\u001b[92mTrain accuracy: 34926/48000 =  72.76 % ||| loss 0.713888943195343\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8811/12000 =  73.42 % ||| loss 0.7009194493293762\u001b[0m\n",
            "\u001b[92mTest accuracy: 7235/10000 =  72.35 % ||| loss 0.7326804995536804\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.7582395148277282\n",
            "Batch #200 Loss: 0.746701356768608\n",
            "Batch #300 Loss: 0.7499271762371064\n",
            "\u001b[92mTrain accuracy: 35604/48000 =  74.17 % ||| loss 0.6752009391784668\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8983/12000 =  74.86 % ||| loss 0.663209855556488\u001b[0m\n",
            "\u001b[92mTest accuracy: 7379/10000 =  73.79 % ||| loss 0.691248893737793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.7234606873989106\n",
            "Batch #200 Loss: 0.7150211077928543\n",
            "Batch #300 Loss: 0.7111517488956451\n",
            "\u001b[92mTrain accuracy: 36200/48000 =  75.42 % ||| loss 0.648110568523407\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9100/12000 =  75.83 % ||| loss 0.6369189023971558\u001b[0m\n",
            "\u001b[92mTest accuracy: 7505/10000 =  75.05 % ||| loss 0.6698110699653625\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.6982696497440338\n",
            "Batch #200 Loss: 0.680171931385994\n",
            "Batch #300 Loss: 0.6872042316198349\n",
            "\u001b[92mTrain accuracy: 35931/48000 =  74.86 % ||| loss 0.6318974494934082\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9029/12000 =  75.24 % ||| loss 0.6239432692527771\u001b[0m\n",
            "\u001b[92mTest accuracy: 7411/10000 =  74.11 % ||| loss 0.6522723436355591\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.6718851801753044\n",
            "Batch #200 Loss: 0.6503952187299729\n",
            "Batch #300 Loss: 0.662864972949028\n",
            "\u001b[92mTrain accuracy: 37131/48000 =  77.36 % ||| loss 0.5847346782684326\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9315/12000 =  77.62 % ||| loss 0.5777941346168518\u001b[0m\n",
            "\u001b[92mTest accuracy: 7703/10000 =  77.03 % ||| loss 0.6056845188140869\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.6438865509629249\n",
            "Batch #200 Loss: 0.6201118350028991\n",
            "Batch #300 Loss: 0.638551903963089\n",
            "\u001b[92mTrain accuracy: 37354/48000 =  77.82 % ||| loss 0.5678237080574036\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9389/12000 =  78.24 % ||| loss 0.5598915815353394\u001b[0m\n",
            "\u001b[92mTest accuracy: 7720/10000 =  77.2 % ||| loss 0.5877143740653992\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6314730140566825\n",
            "Batch #200 Loss: 0.6124752825498581\n",
            "Batch #300 Loss: 0.5985130828619003\n",
            "\u001b[92mTrain accuracy: 38009/48000 =  79.19 % ||| loss 0.5484413504600525\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9516/12000 =  79.3 % ||| loss 0.5442025661468506\u001b[0m\n",
            "\u001b[92mTest accuracy: 7843/10000 =  78.43 % ||| loss 0.572961151599884\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.6106785047054291\n",
            "Batch #200 Loss: 0.5963240569829941\n",
            "Batch #300 Loss: 0.5790951880812645\n",
            "\u001b[92mTrain accuracy: 38071/48000 =  79.31 % ||| loss 0.5318906903266907\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9557/12000 =  79.64 % ||| loss 0.5254296064376831\u001b[0m\n",
            "\u001b[92mTest accuracy: 7856/10000 =  78.56 % ||| loss 0.5530806183815002\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5835483804345131\n",
            "Batch #200 Loss: 0.5802659144997597\n",
            "Batch #300 Loss: 0.5874414205551147\n",
            "\u001b[92mTrain accuracy: 38299/48000 =  79.79 % ||| loss 0.5180155634880066\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9587/12000 =  79.89 % ||| loss 0.5148584246635437\u001b[0m\n",
            "\u001b[92mTest accuracy: 7896/10000 =  78.96 % ||| loss 0.5435335636138916\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.5571182417869568\n",
            "Batch #200 Loss: 0.5752936908602715\n",
            "Batch #300 Loss: 0.5642710644006729\n",
            "\u001b[92mTrain accuracy: 38332/48000 =  79.86 % ||| loss 0.5158600211143494\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9613/12000 =  80.11 % ||| loss 0.513304591178894\u001b[0m\n",
            "\u001b[92mTest accuracy: 7890/10000 =  78.9 % ||| loss 0.5406583547592163\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5554467004537582\n",
            "Batch #200 Loss: 0.5668734282255172\n",
            "Batch #300 Loss: 0.5518503868579865\n",
            "\u001b[92mTrain accuracy: 39097/48000 =  81.45 % ||| loss 0.496543824672699\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9786/12000 =  81.55 % ||| loss 0.49389827251434326\u001b[0m\n",
            "\u001b[92mTest accuracy: 8043/10000 =  80.43 % ||| loss 0.5191422700881958\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.542803253531456\n",
            "Batch #200 Loss: 0.5381466799974441\n",
            "Batch #300 Loss: 0.5385295268893242\n",
            "\u001b[92mTrain accuracy: 39468/48000 =  82.23 % ||| loss 0.47857117652893066\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9911/12000 =  82.59 % ||| loss 0.4787648022174835\u001b[0m\n",
            "\u001b[92mTest accuracy: 8149/10000 =  81.49 % ||| loss 0.5050463080406189\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5399063405394554\n",
            "Batch #200 Loss: 0.5162715792655945\n",
            "Batch #300 Loss: 0.5298475679755211\n",
            "\u001b[92mTrain accuracy: 39512/48000 =  82.32 % ||| loss 0.4729866683483124\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9899/12000 =  82.49 % ||| loss 0.4730972647666931\u001b[0m\n",
            "\u001b[92mTest accuracy: 8130/10000 =  81.3 % ||| loss 0.4953944981098175\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5200106930732727\n",
            "Batch #200 Loss: 0.521305935382843\n",
            "Batch #300 Loss: 0.5136706680059433\n",
            "\u001b[92mTrain accuracy: 39608/48000 =  82.52 % ||| loss 0.4662219285964966\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9931/12000 =  82.76 % ||| loss 0.4671984016895294\u001b[0m\n",
            "\u001b[92mTest accuracy: 8167/10000 =  81.67 % ||| loss 0.49373573064804077\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5156259563565254\n",
            "Batch #200 Loss: 0.49992635786533357\n",
            "Batch #300 Loss: 0.5117684462666512\n",
            "\u001b[92mTrain accuracy: 39886/48000 =  83.1 % ||| loss 0.4519904851913452\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10012/12000 =  83.43 % ||| loss 0.4527615010738373\u001b[0m\n",
            "\u001b[92mTest accuracy: 8216/10000 =  82.16 % ||| loss 0.4826057553291321\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.5011325803399086\n",
            "Batch #200 Loss: 0.5027295696735382\n",
            "Batch #300 Loss: 0.5096368202567101\n",
            "\u001b[92mTrain accuracy: 40107/48000 =  83.56 % ||| loss 0.44644689559936523\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10072/12000 =  83.93 % ||| loss 0.44684725999832153\u001b[0m\n",
            "\u001b[92mTest accuracy: 8265/10000 =  82.65 % ||| loss 0.4770093560218811\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.48472828716039656\n",
            "Batch #200 Loss: 0.500048255622387\n",
            "Batch #300 Loss: 0.48484416127204893\n",
            "\u001b[92mTrain accuracy: 40206/48000 =  83.76 % ||| loss 0.43889153003692627\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10064/12000 =  83.87 % ||| loss 0.4435814917087555\u001b[0m\n",
            "\u001b[92mTest accuracy: 8274/10000 =  82.74 % ||| loss 0.46817639470100403\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.47756152659654616\n",
            "Batch #200 Loss: 0.4797678142786026\n",
            "Batch #300 Loss: 0.48215047746896744\n",
            "\u001b[92mTrain accuracy: 40410/48000 =  84.19 % ||| loss 0.4283944070339203\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10116/12000 =  84.3 % ||| loss 0.432679682970047\u001b[0m\n",
            "\u001b[92mTest accuracy: 8339/10000 =  83.39 % ||| loss 0.4561796188354492\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.48291219532489776\n",
            "Batch #200 Loss: 0.4721627944707871\n",
            "Batch #300 Loss: 0.4740520411729813\n",
            "\u001b[92mTrain accuracy: 39749/48000 =  82.81 % ||| loss 0.45598554611206055\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9962/12000 =  83.02 % ||| loss 0.4602271020412445\u001b[0m\n",
            "\u001b[92mTest accuracy: 8203/10000 =  82.03 % ||| loss 0.48378342390060425\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.4633171156048775\n",
            "Batch #200 Loss: 0.4683904418349266\n",
            "Batch #300 Loss: 0.47175578743219376\n",
            "\u001b[92mTrain accuracy: 40305/48000 =  83.97 % ||| loss 0.4267815053462982\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10097/12000 =  84.14 % ||| loss 0.4367629587650299\u001b[0m\n",
            "\u001b[92mTest accuracy: 8306/10000 =  83.06 % ||| loss 0.4549556076526642\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.4739342668652535\n",
            "Batch #200 Loss: 0.4591010373830795\n",
            "Batch #300 Loss: 0.46498198688030246\n",
            "\u001b[92mTrain accuracy: 40770/48000 =  84.94 % ||| loss 0.41117414832115173\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10182/12000 =  84.85 % ||| loss 0.41789862513542175\u001b[0m\n",
            "\u001b[92mTest accuracy: 8417/10000 =  84.17 % ||| loss 0.43897953629493713\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_9</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_9' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_9</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_103317-Lenet5Dropout_1726150070.105228_10</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_10' target=\"_blank\">Lenet5Dropout_1726150070.105228_10</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_10' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_10</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.303086953163147\n",
            "Batch #200 Loss: 2.300490770339966\n",
            "Batch #300 Loss: 2.296530284881592\n",
            "\u001b[92mTrain accuracy: 5607/48000 =  11.68 % ||| loss 2.2888920307159424\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1419/12000 =  11.82 % ||| loss 2.288456678390503\u001b[0m\n",
            "\u001b[92mTest accuracy: 1134/10000 =  11.34 % ||| loss 2.2889955043792725\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.286197052001953\n",
            "Batch #200 Loss: 2.2719551587104796\n",
            "Batch #300 Loss: 2.241679837703705\n",
            "\u001b[92mTrain accuracy: 16085/48000 =  33.51 % ||| loss 2.037224054336548\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4089/12000 =  34.08 % ||| loss 2.0349879264831543\u001b[0m\n",
            "\u001b[92mTest accuracy: 3341/10000 =  33.41 % ||| loss 2.036313772201538\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.8477481603622437\n",
            "Batch #200 Loss: 1.4348972868919372\n",
            "Batch #300 Loss: 1.263448921442032\n",
            "\u001b[92mTrain accuracy: 28189/48000 =  58.73 % ||| loss 1.0150200128555298\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7027/12000 =  58.56 % ||| loss 1.0089746713638306\u001b[0m\n",
            "\u001b[92mTest accuracy: 5828/10000 =  58.28 % ||| loss 1.0225731134414673\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.1470225220918655\n",
            "Batch #200 Loss: 1.0717751425504685\n",
            "Batch #300 Loss: 1.0511256259679795\n",
            "\u001b[92mTrain accuracy: 32243/48000 =  67.17 % ||| loss 0.8550944924354553\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8112/12000 =  67.6 % ||| loss 0.8433520793914795\u001b[0m\n",
            "\u001b[92mTest accuracy: 6649/10000 =  66.49 % ||| loss 0.8635138273239136\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.971655016541481\n",
            "Batch #200 Loss: 0.9346226614713669\n",
            "Batch #300 Loss: 0.9061373674869537\n",
            "\u001b[92mTrain accuracy: 33715/48000 =  70.24 % ||| loss 0.7673987746238708\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8499/12000 =  70.83 % ||| loss 0.7545861601829529\u001b[0m\n",
            "\u001b[92mTest accuracy: 7035/10000 =  70.35 % ||| loss 0.7764632105827332\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.869351487159729\n",
            "Batch #200 Loss: 0.8408619558811188\n",
            "Batch #300 Loss: 0.838093244433403\n",
            "\u001b[92mTrain accuracy: 34977/48000 =  72.87 % ||| loss 0.705713152885437\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8793/12000 =  73.28 % ||| loss 0.6932027339935303\u001b[0m\n",
            "\u001b[92mTest accuracy: 7243/10000 =  72.43 % ||| loss 0.719581127166748\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.8142994308471679\n",
            "Batch #200 Loss: 0.8002532446384429\n",
            "Batch #300 Loss: 0.8004484272003174\n",
            "\u001b[92mTrain accuracy: 35498/48000 =  73.95 % ||| loss 0.6803452968597412\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8887/12000 =  74.06 % ||| loss 0.6688898205757141\u001b[0m\n",
            "\u001b[92mTest accuracy: 7349/10000 =  73.49 % ||| loss 0.6948933601379395\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.7753187710046768\n",
            "Batch #200 Loss: 0.7669560384750366\n",
            "Batch #300 Loss: 0.7566355109214783\n",
            "\u001b[92mTrain accuracy: 35523/48000 =  74.01 % ||| loss 0.6583479046821594\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8895/12000 =  74.12 % ||| loss 0.6504008173942566\u001b[0m\n",
            "\u001b[92mTest accuracy: 7302/10000 =  73.02 % ||| loss 0.6801162362098694\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.7397504073381423\n",
            "Batch #200 Loss: 0.7367215865850448\n",
            "Batch #300 Loss: 0.7106162697076798\n",
            "\u001b[92mTrain accuracy: 36266/48000 =  75.55 % ||| loss 0.6292495727539062\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9064/12000 =  75.53 % ||| loss 0.6185427308082581\u001b[0m\n",
            "\u001b[92mTest accuracy: 7489/10000 =  74.89 % ||| loss 0.6498582363128662\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.7134900206327438\n",
            "Batch #200 Loss: 0.7156987535953522\n",
            "Batch #300 Loss: 0.6973000133037567\n",
            "\u001b[92mTrain accuracy: 36318/48000 =  75.66 % ||| loss 0.6136777400970459\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9102/12000 =  75.85 % ||| loss 0.6022037863731384\u001b[0m\n",
            "\u001b[92mTest accuracy: 7489/10000 =  74.89 % ||| loss 0.6294813752174377\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6890169167518616\n",
            "Batch #200 Loss: 0.7004156994819641\n",
            "Batch #300 Loss: 0.6672337287664414\n",
            "\u001b[92mTrain accuracy: 36579/48000 =  76.21 % ||| loss 0.5998448729515076\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9196/12000 =  76.63 % ||| loss 0.5872065424919128\u001b[0m\n",
            "\u001b[92mTest accuracy: 7533/10000 =  75.33 % ||| loss 0.6165605187416077\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.6641119319200516\n",
            "Batch #200 Loss: 0.6695692232251167\n",
            "Batch #300 Loss: 0.6601150006055831\n",
            "\u001b[92mTrain accuracy: 36918/48000 =  76.91 % ||| loss 0.5771858096122742\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9260/12000 =  77.17 % ||| loss 0.5685640573501587\u001b[0m\n",
            "\u001b[92mTest accuracy: 7588/10000 =  75.88 % ||| loss 0.5973287224769592\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.6442875194549561\n",
            "Batch #200 Loss: 0.6443994921445847\n",
            "Batch #300 Loss: 0.6475853970646859\n",
            "\u001b[92mTrain accuracy: 37505/48000 =  78.14 % ||| loss 0.5497875809669495\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9398/12000 =  78.32 % ||| loss 0.5404748916625977\u001b[0m\n",
            "\u001b[92mTest accuracy: 7741/10000 =  77.41 % ||| loss 0.5658047199249268\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.6303785544633865\n",
            "Batch #200 Loss: 0.6312765410542488\n",
            "Batch #300 Loss: 0.621568218767643\n",
            "\u001b[92mTrain accuracy: 37603/48000 =  78.34 % ||| loss 0.5475701093673706\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9392/12000 =  78.27 % ||| loss 0.5401196479797363\u001b[0m\n",
            "\u001b[92mTest accuracy: 7750/10000 =  77.5 % ||| loss 0.5671423673629761\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.6114303934574127\n",
            "Batch #200 Loss: 0.621858240365982\n",
            "Batch #300 Loss: 0.6053214198350907\n",
            "\u001b[92mTrain accuracy: 37981/48000 =  79.13 % ||| loss 0.5279631614685059\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9515/12000 =  79.29 % ||| loss 0.5224275588989258\u001b[0m\n",
            "\u001b[92mTest accuracy: 7818/10000 =  78.18 % ||| loss 0.544941246509552\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.6036598011851311\n",
            "Batch #200 Loss: 0.6023569449782371\n",
            "Batch #300 Loss: 0.5999117255210876\n",
            "\u001b[92mTrain accuracy: 38539/48000 =  80.29 % ||| loss 0.5109134316444397\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9651/12000 =  80.42 % ||| loss 0.5056252479553223\u001b[0m\n",
            "\u001b[92mTest accuracy: 7929/10000 =  79.29 % ||| loss 0.5308599472045898\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5764984011650085\n",
            "Batch #200 Loss: 0.5919330096244813\n",
            "Batch #300 Loss: 0.59599406093359\n",
            "\u001b[92mTrain accuracy: 38885/48000 =  81.01 % ||| loss 0.504092812538147\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9723/12000 =  81.03 % ||| loss 0.49723830819129944\u001b[0m\n",
            "\u001b[92mTest accuracy: 7984/10000 =  79.84 % ||| loss 0.5256518125534058\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5822894370555878\n",
            "Batch #200 Loss: 0.5731561571359635\n",
            "Batch #300 Loss: 0.5611976963281632\n",
            "\u001b[92mTrain accuracy: 38963/48000 =  81.17 % ||| loss 0.49406901001930237\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9758/12000 =  81.32 % ||| loss 0.48870038986206055\u001b[0m\n",
            "\u001b[92mTest accuracy: 7998/10000 =  79.98 % ||| loss 0.5190227627754211\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5666010585427285\n",
            "Batch #200 Loss: 0.5580865752696991\n",
            "Batch #300 Loss: 0.5698474624752998\n",
            "\u001b[92mTrain accuracy: 39484/48000 =  82.26 % ||| loss 0.4811009168624878\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9940/12000 =  82.83 % ||| loss 0.47710666060447693\u001b[0m\n",
            "\u001b[92mTest accuracy: 8129/10000 =  81.29 % ||| loss 0.5013261437416077\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.5584445023536682\n",
            "Batch #200 Loss: 0.5418105238676071\n",
            "Batch #300 Loss: 0.5636102083325386\n",
            "\u001b[92mTrain accuracy: 39573/48000 =  82.44 % ||| loss 0.47478193044662476\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9932/12000 =  82.77 % ||| loss 0.4736294746398926\u001b[0m\n",
            "\u001b[92mTest accuracy: 8126/10000 =  81.26 % ||| loss 0.5007731318473816\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.557925443649292\n",
            "Batch #200 Loss: 0.5375288307666779\n",
            "Batch #300 Loss: 0.5340831223130226\n",
            "\u001b[92mTrain accuracy: 39724/48000 =  82.76 % ||| loss 0.46388623118400574\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9970/12000 =  83.08 % ||| loss 0.46027645468711853\u001b[0m\n",
            "\u001b[92mTest accuracy: 8187/10000 =  81.87 % ||| loss 0.49057623744010925\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.543198409974575\n",
            "Batch #200 Loss: 0.5311703664064408\n",
            "Batch #300 Loss: 0.5470708486437798\n",
            "\u001b[92mTrain accuracy: 39861/48000 =  83.04 % ||| loss 0.4622029662132263\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9990/12000 =  83.25 % ||| loss 0.4588531255722046\u001b[0m\n",
            "\u001b[92mTest accuracy: 8205/10000 =  82.05 % ||| loss 0.4858803451061249\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5352986180782318\n",
            "Batch #200 Loss: 0.5318346068263053\n",
            "Batch #300 Loss: 0.5394536611437798\n",
            "\u001b[92mTrain accuracy: 40039/48000 =  83.41 % ||| loss 0.4520762860774994\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10067/12000 =  83.89 % ||| loss 0.45148926973342896\u001b[0m\n",
            "\u001b[92mTest accuracy: 8217/10000 =  82.17 % ||| loss 0.4749240279197693\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5219899103045463\n",
            "Batch #200 Loss: 0.5214108857512474\n",
            "Batch #300 Loss: 0.5317338335514069\n",
            "\u001b[92mTrain accuracy: 39978/48000 =  83.29 % ||| loss 0.4519471824169159\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10037/12000 =  83.64 % ||| loss 0.4534982740879059\u001b[0m\n",
            "\u001b[92mTest accuracy: 8215/10000 =  82.15 % ||| loss 0.47646090388298035\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5038331082463264\n",
            "Batch #200 Loss: 0.5189031633734703\n",
            "Batch #300 Loss: 0.5242385813593864\n",
            "\u001b[92mTrain accuracy: 40254/48000 =  83.86 % ||| loss 0.4411904811859131\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10088/12000 =  84.07 % ||| loss 0.4430927634239197\u001b[0m\n",
            "\u001b[92mTest accuracy: 8287/10000 =  82.87 % ||| loss 0.46575260162353516\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_10</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_10' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_10</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_103549-Lenet5Dropout_1726150070.105228_11</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_11' target=\"_blank\">Lenet5Dropout_1726150070.105228_11</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_11' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3017009973526\n",
            "Batch #200 Loss: 2.2920155715942383\n",
            "Batch #300 Loss: 2.274660391807556\n",
            "\u001b[92mTrain accuracy: 16931/48000 =  35.27 % ||| loss 2.1934380531311035\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4298/12000 =  35.82 % ||| loss 2.1921098232269287\u001b[0m\n",
            "\u001b[92mTest accuracy: 3480/10000 =  34.8 % ||| loss 2.1939964294433594\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.139511206150055\n",
            "Batch #200 Loss: 1.827471648454666\n",
            "Batch #300 Loss: 1.5143725550174714\n",
            "\u001b[92mTrain accuracy: 29589/48000 =  61.64 % ||| loss 1.1053675413131714\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7440/12000 =  62.0 % ||| loss 1.0989711284637451\u001b[0m\n",
            "\u001b[92mTest accuracy: 6117/10000 =  61.17 % ||| loss 1.114680290222168\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.2835008323192596\n",
            "Batch #200 Loss: 1.2137700641155242\n",
            "Batch #300 Loss: 1.1771391123533248\n",
            "\u001b[92mTrain accuracy: 30507/48000 =  63.56 % ||| loss 0.9392378926277161\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7665/12000 =  63.88 % ||| loss 0.929391086101532\u001b[0m\n",
            "\u001b[92mTest accuracy: 6278/10000 =  62.78 % ||| loss 0.950880229473114\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.1070387667417527\n",
            "Batch #200 Loss: 1.07249489068985\n",
            "Batch #300 Loss: 1.0413473963737487\n",
            "\u001b[92mTrain accuracy: 32065/48000 =  66.8 % ||| loss 0.8465083241462708\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8066/12000 =  67.22 % ||| loss 0.8343570828437805\u001b[0m\n",
            "\u001b[92mTest accuracy: 6671/10000 =  66.71 % ||| loss 0.8636465072631836\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 1.01427497446537\n",
            "Batch #200 Loss: 0.9850587475299836\n",
            "Batch #300 Loss: 0.9702023380994796\n",
            "\u001b[92mTrain accuracy: 34082/48000 =  71.0 % ||| loss 0.7800565361976624\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8611/12000 =  71.76 % ||| loss 0.7650574445724487\u001b[0m\n",
            "\u001b[92mTest accuracy: 7026/10000 =  70.26 % ||| loss 0.7935329079627991\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.9498479282855987\n",
            "Batch #200 Loss: 0.9225752604007721\n",
            "Batch #300 Loss: 0.9306077462434769\n",
            "\u001b[92mTrain accuracy: 33912/48000 =  70.65 % ||| loss 0.7676426768302917\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8512/12000 =  70.93 % ||| loss 0.7524616718292236\u001b[0m\n",
            "\u001b[92mTest accuracy: 7008/10000 =  70.08 % ||| loss 0.7827098965644836\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.9110123884677886\n",
            "Batch #200 Loss: 0.8906507325172425\n",
            "Batch #300 Loss: 0.8831406211853028\n",
            "\u001b[92mTrain accuracy: 34675/48000 =  72.24 % ||| loss 0.7184242606163025\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8743/12000 =  72.86 % ||| loss 0.6991083025932312\u001b[0m\n",
            "\u001b[92mTest accuracy: 7188/10000 =  71.88 % ||| loss 0.732387900352478\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.8498020362854004\n",
            "Batch #200 Loss: 0.8578873455524445\n",
            "Batch #300 Loss: 0.8607304149866104\n",
            "\u001b[92mTrain accuracy: 35057/48000 =  73.04 % ||| loss 0.6915062665939331\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8869/12000 =  73.91 % ||| loss 0.6760377287864685\u001b[0m\n",
            "\u001b[92mTest accuracy: 7252/10000 =  72.52 % ||| loss 0.7092702984809875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.8287113571166992\n",
            "Batch #200 Loss: 0.8186097621917725\n",
            "Batch #300 Loss: 0.8285613840818405\n",
            "\u001b[92mTrain accuracy: 35658/48000 =  74.29 % ||| loss 0.6705738306045532\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9004/12000 =  75.03 % ||| loss 0.6553753614425659\u001b[0m\n",
            "\u001b[92mTest accuracy: 7357/10000 =  73.57 % ||| loss 0.683796763420105\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.8173355841636658\n",
            "Batch #200 Loss: 0.8016407638788223\n",
            "Batch #300 Loss: 0.7943067574501037\n",
            "\u001b[92mTrain accuracy: 36070/48000 =  75.15 % ||| loss 0.644633948802948\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9076/12000 =  75.63 % ||| loss 0.6272262930870056\u001b[0m\n",
            "\u001b[92mTest accuracy: 7474/10000 =  74.74 % ||| loss 0.6692762970924377\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.7870771098136902\n",
            "Batch #200 Loss: 0.7740542620420456\n",
            "Batch #300 Loss: 0.7667456889152526\n",
            "\u001b[92mTrain accuracy: 36168/48000 =  75.35 % ||| loss 0.6383457779884338\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9052/12000 =  75.43 % ||| loss 0.62337726354599\u001b[0m\n",
            "\u001b[92mTest accuracy: 7477/10000 =  74.77 % ||| loss 0.657120943069458\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.7647390043735505\n",
            "Batch #200 Loss: 0.7486213541030884\n",
            "Batch #300 Loss: 0.7514046716690064\n",
            "\u001b[92mTrain accuracy: 36825/48000 =  76.72 % ||| loss 0.6038180589675903\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9249/12000 =  77.08 % ||| loss 0.5906023979187012\u001b[0m\n",
            "\u001b[92mTest accuracy: 7583/10000 =  75.83 % ||| loss 0.6241939663887024\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.7436738085746765\n",
            "Batch #200 Loss: 0.7336564099788666\n",
            "Batch #300 Loss: 0.7273434501886368\n",
            "\u001b[92mTrain accuracy: 37294/48000 =  77.7 % ||| loss 0.5888922810554504\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9359/12000 =  77.99 % ||| loss 0.5773205161094666\u001b[0m\n",
            "\u001b[92mTest accuracy: 7697/10000 =  76.97 % ||| loss 0.6122683882713318\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.7125012594461441\n",
            "Batch #200 Loss: 0.7328356450796127\n",
            "Batch #300 Loss: 0.7023960393667221\n",
            "\u001b[92mTrain accuracy: 37138/48000 =  77.37 % ||| loss 0.5731903314590454\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9278/12000 =  77.32 % ||| loss 0.5608620047569275\u001b[0m\n",
            "\u001b[92mTest accuracy: 7648/10000 =  76.48 % ||| loss 0.5952975749969482\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.687585089802742\n",
            "Batch #200 Loss: 0.7033907389640808\n",
            "Batch #300 Loss: 0.6859250038862228\n",
            "\u001b[92mTrain accuracy: 37385/48000 =  77.89 % ||| loss 0.5655179619789124\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9360/12000 =  78.0 % ||| loss 0.553438127040863\u001b[0m\n",
            "\u001b[92mTest accuracy: 7708/10000 =  77.08 % ||| loss 0.5849290490150452\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.6985235130786895\n",
            "Batch #200 Loss: 0.6676317793130875\n",
            "Batch #300 Loss: 0.6752985325455666\n",
            "\u001b[92mTrain accuracy: 37375/48000 =  77.86 % ||| loss 0.5541762113571167\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9357/12000 =  77.98 % ||| loss 0.5447225570678711\u001b[0m\n",
            "\u001b[92mTest accuracy: 7680/10000 =  76.8 % ||| loss 0.582523763179779\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.6846432629227638\n",
            "Batch #200 Loss: 0.6662243735790253\n",
            "Batch #300 Loss: 0.6626989871263504\n",
            "\u001b[92mTrain accuracy: 37666/48000 =  78.47 % ||| loss 0.5530866384506226\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9439/12000 =  78.66 % ||| loss 0.5445240139961243\u001b[0m\n",
            "\u001b[92mTest accuracy: 7771/10000 =  77.71 % ||| loss 0.5729150176048279\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.6516866120696068\n",
            "Batch #200 Loss: 0.641611744761467\n",
            "Batch #300 Loss: 0.6605929392576217\n",
            "\u001b[92mTrain accuracy: 37910/48000 =  78.98 % ||| loss 0.5309658646583557\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9497/12000 =  79.14 % ||| loss 0.5221640467643738\u001b[0m\n",
            "\u001b[92mTest accuracy: 7817/10000 =  78.17 % ||| loss 0.5503426790237427\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.648999882042408\n",
            "Batch #200 Loss: 0.6356601068377494\n",
            "Batch #300 Loss: 0.6344136002659798\n",
            "\u001b[92mTrain accuracy: 37970/48000 =  79.1 % ||| loss 0.5256167054176331\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9497/12000 =  79.14 % ||| loss 0.5190632343292236\u001b[0m\n",
            "\u001b[92mTest accuracy: 7833/10000 =  78.33 % ||| loss 0.5487738251686096\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.6355689138174057\n",
            "Batch #200 Loss: 0.6200968560576439\n",
            "Batch #300 Loss: 0.6257407113909721\n",
            "\u001b[92mTrain accuracy: 38131/48000 =  79.44 % ||| loss 0.5160273313522339\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9527/12000 =  79.39 % ||| loss 0.5096004009246826\u001b[0m\n",
            "\u001b[92mTest accuracy: 7837/10000 =  78.37 % ||| loss 0.5387473702430725\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.6243008267879486\n",
            "Batch #200 Loss: 0.6295772224664689\n",
            "Batch #300 Loss: 0.6247023296356201\n",
            "\u001b[92mTrain accuracy: 38787/48000 =  80.81 % ||| loss 0.5052627325057983\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9727/12000 =  81.06 % ||| loss 0.4999745488166809\u001b[0m\n",
            "\u001b[92mTest accuracy: 8009/10000 =  80.09 % ||| loss 0.5281350016593933\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.6130559143424034\n",
            "Batch #200 Loss: 0.6182871773838997\n",
            "Batch #300 Loss: 0.6179892709851265\n",
            "\u001b[92mTrain accuracy: 38370/48000 =  79.94 % ||| loss 0.5012589693069458\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9612/12000 =  80.1 % ||| loss 0.4957173466682434\u001b[0m\n",
            "\u001b[92mTest accuracy: 7915/10000 =  79.15 % ||| loss 0.5316290855407715\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.6113393789529801\n",
            "Batch #200 Loss: 0.5946622863411903\n",
            "Batch #300 Loss: 0.6044152602553368\n",
            "\u001b[92mTrain accuracy: 39070/48000 =  81.4 % ||| loss 0.49170035123825073\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9775/12000 =  81.46 % ||| loss 0.4905015528202057\u001b[0m\n",
            "\u001b[92mTest accuracy: 8069/10000 =  80.69 % ||| loss 0.5192642211914062\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.6033002001047134\n",
            "Batch #200 Loss: 0.6009680587053299\n",
            "Batch #300 Loss: 0.595360703766346\n",
            "\u001b[92mTrain accuracy: 38904/48000 =  81.05 % ||| loss 0.48622363805770874\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9729/12000 =  81.08 % ||| loss 0.48478689789772034\u001b[0m\n",
            "\u001b[92mTest accuracy: 7995/10000 =  79.95 % ||| loss 0.5166147351264954\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5938089776039124\n",
            "Batch #200 Loss: 0.5930814865231514\n",
            "Batch #300 Loss: 0.5881936365365982\n",
            "\u001b[92mTrain accuracy: 39327/48000 =  81.93 % ||| loss 0.4793056547641754\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9861/12000 =  82.17 % ||| loss 0.47820228338241577\u001b[0m\n",
            "\u001b[92mTest accuracy: 8116/10000 =  81.16 % ||| loss 0.5106833577156067\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_11</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_11' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_11</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_103821-Lenet5Dropout_1726150070.105228_12</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_12' target=\"_blank\">Lenet5Dropout_1726150070.105228_12</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_12' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_12</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.1411598932743074\n",
            "Batch #200 Loss: 1.0890086108446122\n",
            "Batch #300 Loss: 0.8594185072183609\n",
            "\u001b[92mTrain accuracy: 35789/48000 =  74.56 % ||| loss 0.6887146830558777\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8908/12000 =  74.23 % ||| loss 0.6799325346946716\u001b[0m\n",
            "\u001b[92mTest accuracy: 7376/10000 =  73.76 % ||| loss 0.7095046043395996\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.7256701242923737\n",
            "Batch #200 Loss: 0.6587885817885399\n",
            "Batch #300 Loss: 0.609614118039608\n",
            "\u001b[92mTrain accuracy: 39188/48000 =  81.64 % ||| loss 0.502434253692627\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9804/12000 =  81.7 % ||| loss 0.49834832549095154\u001b[0m\n",
            "\u001b[92mTest accuracy: 8074/10000 =  80.74 % ||| loss 0.527953565120697\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5629346492886543\n",
            "Batch #200 Loss: 0.5444649487733841\n",
            "Batch #300 Loss: 0.5350810369849205\n",
            "\u001b[92mTrain accuracy: 40119/48000 =  83.58 % ||| loss 0.4624076187610626\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10085/12000 =  84.04 % ||| loss 0.4626582860946655\u001b[0m\n",
            "\u001b[92mTest accuracy: 8259/10000 =  82.59 % ||| loss 0.485804945230484\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5053029704093933\n",
            "Batch #200 Loss: 0.47710899770259857\n",
            "Batch #300 Loss: 0.4618551376461983\n",
            "\u001b[92mTrain accuracy: 40625/48000 =  84.64 % ||| loss 0.4152161180973053\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10114/12000 =  84.28 % ||| loss 0.4234236180782318\u001b[0m\n",
            "\u001b[92mTest accuracy: 8361/10000 =  83.61 % ||| loss 0.44650620222091675\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4478477454185486\n",
            "Batch #200 Loss: 0.44489569902420045\n",
            "Batch #300 Loss: 0.4369894900918007\n",
            "\u001b[92mTrain accuracy: 41179/48000 =  85.79 % ||| loss 0.3807947039604187\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10295/12000 =  85.79 % ||| loss 0.3926047384738922\u001b[0m\n",
            "\u001b[92mTest accuracy: 8511/10000 =  85.11 % ||| loss 0.41238030791282654\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.42455788284540175\n",
            "Batch #200 Loss: 0.4111879554390907\n",
            "Batch #300 Loss: 0.40983199223876\n",
            "\u001b[92mTrain accuracy: 41726/48000 =  86.93 % ||| loss 0.35667985677719116\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10347/12000 =  86.22 % ||| loss 0.3694905638694763\u001b[0m\n",
            "\u001b[92mTest accuracy: 8576/10000 =  85.76 % ||| loss 0.39640337228775024\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.4060238856077194\n",
            "Batch #200 Loss: 0.3859623783826828\n",
            "Batch #300 Loss: 0.38467989072203634\n",
            "\u001b[92mTrain accuracy: 41839/48000 =  87.16 % ||| loss 0.34789714217185974\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10329/12000 =  86.08 % ||| loss 0.3670092523097992\u001b[0m\n",
            "\u001b[92mTest accuracy: 8580/10000 =  85.8 % ||| loss 0.3932960033416748\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3701900626718998\n",
            "Batch #200 Loss: 0.3705830919742584\n",
            "Batch #300 Loss: 0.3756876407563686\n",
            "\u001b[92mTrain accuracy: 41887/48000 =  87.26 % ||| loss 0.34103089570999146\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10391/12000 =  86.59 % ||| loss 0.36061233282089233\u001b[0m\n",
            "\u001b[92mTest accuracy: 8622/10000 =  86.22 % ||| loss 0.3893938660621643\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.36877315312623976\n",
            "Batch #200 Loss: 0.36084652572870257\n",
            "Batch #300 Loss: 0.3650672382116318\n",
            "\u001b[92mTrain accuracy: 42248/48000 =  88.02 % ||| loss 0.3228011131286621\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10443/12000 =  87.02 % ||| loss 0.3439558446407318\u001b[0m\n",
            "\u001b[92mTest accuracy: 8647/10000 =  86.47 % ||| loss 0.36992108821868896\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3507585841417313\n",
            "Batch #200 Loss: 0.3513133196532726\n",
            "Batch #300 Loss: 0.3474554443359375\n",
            "\u001b[92mTrain accuracy: 42711/48000 =  88.98 % ||| loss 0.2990487217903137\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10588/12000 =  88.23 % ||| loss 0.3229064345359802\u001b[0m\n",
            "\u001b[92mTest accuracy: 8736/10000 =  87.36 % ||| loss 0.34535402059555054\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.3324363876879215\n",
            "Batch #200 Loss: 0.33951183542609215\n",
            "Batch #300 Loss: 0.33839687541127206\n",
            "\u001b[92mTrain accuracy: 42810/48000 =  89.19 % ||| loss 0.2931917607784271\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10535/12000 =  87.79 % ||| loss 0.32192784547805786\u001b[0m\n",
            "\u001b[92mTest accuracy: 8744/10000 =  87.44 % ||| loss 0.3407924473285675\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.33485372364521027\n",
            "Batch #200 Loss: 0.3240532141923904\n",
            "Batch #300 Loss: 0.32694959953427316\n",
            "\u001b[92mTrain accuracy: 42897/48000 =  89.37 % ||| loss 0.2837623357772827\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10604/12000 =  88.37 % ||| loss 0.31185299158096313\u001b[0m\n",
            "\u001b[92mTest accuracy: 8777/10000 =  87.77 % ||| loss 0.3432852327823639\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.32429432615637777\n",
            "Batch #200 Loss: 0.32245156362652777\n",
            "Batch #300 Loss: 0.3259713141620159\n",
            "\u001b[92mTrain accuracy: 43065/48000 =  89.72 % ||| loss 0.27743804454803467\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10597/12000 =  88.31 % ||| loss 0.3157973289489746\u001b[0m\n",
            "\u001b[92mTest accuracy: 8797/10000 =  87.97 % ||| loss 0.3452049493789673\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.30205235674977304\n",
            "Batch #200 Loss: 0.32350499883294104\n",
            "Batch #300 Loss: 0.3133176402747631\n",
            "\u001b[92mTrain accuracy: 43137/48000 =  89.87 % ||| loss 0.2737607955932617\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10602/12000 =  88.35 % ||| loss 0.30845287442207336\u001b[0m\n",
            "\u001b[92mTest accuracy: 8759/10000 =  87.59 % ||| loss 0.3344862163066864\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3037692278623581\n",
            "Batch #200 Loss: 0.3097766064107418\n",
            "Batch #300 Loss: 0.29553346082568166\n",
            "\u001b[92mTrain accuracy: 43492/48000 =  90.61 % ||| loss 0.25482669472694397\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10700/12000 =  89.17 % ||| loss 0.289845734834671\u001b[0m\n",
            "\u001b[92mTest accuracy: 8848/10000 =  88.48 % ||| loss 0.31337466835975647\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.2935883831977844\n",
            "Batch #200 Loss: 0.30647684589028357\n",
            "Batch #300 Loss: 0.2923262557387352\n",
            "\u001b[92mTrain accuracy: 43077/48000 =  89.74 % ||| loss 0.2723369002342224\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10659/12000 =  88.83 % ||| loss 0.3049560785293579\u001b[0m\n",
            "\u001b[92mTest accuracy: 8783/10000 =  87.83 % ||| loss 0.33861103653907776\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.2773963814973831\n",
            "Batch #200 Loss: 0.29869697585701943\n",
            "Batch #300 Loss: 0.30067572221159933\n",
            "\u001b[92mTrain accuracy: 43486/48000 =  90.6 % ||| loss 0.25048479437828064\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10662/12000 =  88.85 % ||| loss 0.2937069833278656\u001b[0m\n",
            "\u001b[92mTest accuracy: 8817/10000 =  88.17 % ||| loss 0.32017990946769714\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.2805150307714939\n",
            "Batch #200 Loss: 0.27917585879564283\n",
            "Batch #300 Loss: 0.2980794429779053\n",
            "\u001b[92mTrain accuracy: 43694/48000 =  91.03 % ||| loss 0.23922555148601532\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10749/12000 =  89.58 % ||| loss 0.2811526954174042\u001b[0m\n",
            "\u001b[92mTest accuracy: 8899/10000 =  88.99 % ||| loss 0.3069073557853699\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.2764997544884682\n",
            "Batch #200 Loss: 0.27456970244646073\n",
            "Batch #300 Loss: 0.28433073550462723\n",
            "\u001b[92mTrain accuracy: 43752/48000 =  91.15 % ||| loss 0.23614603281021118\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10756/12000 =  89.63 % ||| loss 0.28055474162101746\u001b[0m\n",
            "\u001b[92mTest accuracy: 8873/10000 =  88.73 % ||| loss 0.307163268327713\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.2712156046926975\n",
            "Batch #200 Loss: 0.27124576590955257\n",
            "Batch #300 Loss: 0.2737241491675377\n",
            "\u001b[92mTrain accuracy: 43665/48000 =  90.97 % ||| loss 0.2373361885547638\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10726/12000 =  89.38 % ||| loss 0.28501400351524353\u001b[0m\n",
            "\u001b[92mTest accuracy: 8861/10000 =  88.61 % ||| loss 0.30869463086128235\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.2678436768054962\n",
            "Batch #200 Loss: 0.2637086036801338\n",
            "Batch #300 Loss: 0.2692385175824165\n",
            "\u001b[92mTrain accuracy: 43908/48000 =  91.47 % ||| loss 0.22665250301361084\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10770/12000 =  89.75 % ||| loss 0.2755056619644165\u001b[0m\n",
            "\u001b[92mTest accuracy: 8877/10000 =  88.77 % ||| loss 0.30343443155288696\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.2723610173165798\n",
            "Batch #200 Loss: 0.2578883954882622\n",
            "Batch #300 Loss: 0.26224189981818197\n",
            "\u001b[92mTrain accuracy: 44077/48000 =  91.83 % ||| loss 0.21905763447284698\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10804/12000 =  90.03 % ||| loss 0.2725909352302551\u001b[0m\n",
            "\u001b[92mTest accuracy: 8922/10000 =  89.22 % ||| loss 0.29665443301200867\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.2525814625620842\n",
            "Batch #200 Loss: 0.2632700665295124\n",
            "Batch #300 Loss: 0.2692797338962555\n",
            "\u001b[92mTrain accuracy: 43896/48000 =  91.45 % ||| loss 0.22427873313426971\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10745/12000 =  89.54 % ||| loss 0.2769228219985962\u001b[0m\n",
            "\u001b[92mTest accuracy: 8893/10000 =  88.93 % ||| loss 0.30431637167930603\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.2456136716157198\n",
            "Batch #200 Loss: 0.2563522684574127\n",
            "Batch #300 Loss: 0.25860069021582605\n",
            "\u001b[92mTrain accuracy: 44182/48000 =  92.05 % ||| loss 0.21301324665546417\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10792/12000 =  89.93 % ||| loss 0.2693868577480316\u001b[0m\n",
            "\u001b[92mTest accuracy: 8925/10000 =  89.25 % ||| loss 0.2940879166126251\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.24146112591028213\n",
            "Batch #200 Loss: 0.250345119535923\n",
            "Batch #300 Loss: 0.2529354843497276\n",
            "\u001b[92mTrain accuracy: 44345/48000 =  92.39 % ||| loss 0.20963461697101593\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10797/12000 =  89.98 % ||| loss 0.2659412920475006\u001b[0m\n",
            "\u001b[92mTest accuracy: 8918/10000 =  89.18 % ||| loss 0.2919853925704956\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_12</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_12' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_12</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_104055-Lenet5Dropout_1726150070.105228_13</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_13' target=\"_blank\">Lenet5Dropout_1726150070.105228_13</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_13' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_13</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.188194535970688\n",
            "Batch #200 Loss: 1.1891801643371582\n",
            "Batch #300 Loss: 0.9898767817020416\n",
            "\u001b[92mTrain accuracy: 34525/48000 =  71.93 % ||| loss 0.7560358047485352\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8699/12000 =  72.49 % ||| loss 0.7440714836120605\u001b[0m\n",
            "\u001b[92mTest accuracy: 7163/10000 =  71.63 % ||| loss 0.7706251740455627\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.8191384083032608\n",
            "Batch #200 Loss: 0.7461777681112289\n",
            "Batch #300 Loss: 0.7062287840247155\n",
            "\u001b[92mTrain accuracy: 36892/48000 =  76.86 % ||| loss 0.5911802649497986\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9224/12000 =  76.87 % ||| loss 0.5876333117485046\u001b[0m\n",
            "\u001b[92mTest accuracy: 7639/10000 =  76.39 % ||| loss 0.6079347133636475\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6511291950941086\n",
            "Batch #200 Loss: 0.6220064482092857\n",
            "Batch #300 Loss: 0.5901070389151574\n",
            "\u001b[92mTrain accuracy: 38519/48000 =  80.25 % ||| loss 0.5002022385597229\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9655/12000 =  80.46 % ||| loss 0.49575909972190857\u001b[0m\n",
            "\u001b[92mTest accuracy: 7923/10000 =  79.23 % ||| loss 0.5211802124977112\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5654862895607948\n",
            "Batch #200 Loss: 0.5391962283849716\n",
            "Batch #300 Loss: 0.5326611176133156\n",
            "\u001b[92mTrain accuracy: 40168/48000 =  83.68 % ||| loss 0.4465073347091675\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10088/12000 =  84.07 % ||| loss 0.4486669898033142\u001b[0m\n",
            "\u001b[92mTest accuracy: 8299/10000 =  82.99 % ||| loss 0.4733785390853882\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5146653887629509\n",
            "Batch #200 Loss: 0.509023298919201\n",
            "Batch #300 Loss: 0.48548886060714724\n",
            "\u001b[92mTrain accuracy: 40766/48000 =  84.93 % ||| loss 0.40569132566452026\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10133/12000 =  84.44 % ||| loss 0.4126879572868347\u001b[0m\n",
            "\u001b[92mTest accuracy: 8403/10000 =  84.03 % ||| loss 0.430244505405426\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.4804783546924591\n",
            "Batch #200 Loss: 0.47072780311107637\n",
            "Batch #300 Loss: 0.4618274024128914\n",
            "\u001b[92mTrain accuracy: 41362/48000 =  86.17 % ||| loss 0.3804267942905426\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10310/12000 =  85.92 % ||| loss 0.3863600492477417\u001b[0m\n",
            "\u001b[92mTest accuracy: 8535/10000 =  85.35 % ||| loss 0.4086134731769562\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.4476551952958107\n",
            "Batch #200 Loss: 0.44683317810297013\n",
            "Batch #300 Loss: 0.43151736676692964\n",
            "\u001b[92mTrain accuracy: 41715/48000 =  86.91 % ||| loss 0.352771520614624\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10391/12000 =  86.59 % ||| loss 0.3663656711578369\u001b[0m\n",
            "\u001b[92mTest accuracy: 8583/10000 =  85.83 % ||| loss 0.388156920671463\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.42843083828687667\n",
            "Batch #200 Loss: 0.4234794908761978\n",
            "Batch #300 Loss: 0.4073366792500019\n",
            "\u001b[92mTrain accuracy: 42108/48000 =  87.72 % ||| loss 0.33265867829322815\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10458/12000 =  87.15 % ||| loss 0.3498644232749939\u001b[0m\n",
            "\u001b[92mTest accuracy: 8683/10000 =  86.83 % ||| loss 0.37343910336494446\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.41285064101219177\n",
            "Batch #200 Loss: 0.40354442194104195\n",
            "Batch #300 Loss: 0.4027924017608166\n",
            "\u001b[92mTrain accuracy: 42328/48000 =  88.18 % ||| loss 0.32123807072639465\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10509/12000 =  87.58 % ||| loss 0.33830392360687256\u001b[0m\n",
            "\u001b[92mTest accuracy: 8688/10000 =  86.88 % ||| loss 0.365623414516449\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.4005686117708683\n",
            "Batch #200 Loss: 0.37577288284897803\n",
            "Batch #300 Loss: 0.38966679960489276\n",
            "\u001b[92mTrain accuracy: 42619/48000 =  88.79 % ||| loss 0.3076696991920471\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10563/12000 =  88.02 % ||| loss 0.3304857015609741\u001b[0m\n",
            "\u001b[92mTest accuracy: 8767/10000 =  87.67 % ||| loss 0.35061126947402954\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.3801960317790508\n",
            "Batch #200 Loss: 0.3692787159979343\n",
            "Batch #300 Loss: 0.3796288569271564\n",
            "\u001b[92mTrain accuracy: 42586/48000 =  88.72 % ||| loss 0.30404603481292725\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10573/12000 =  88.11 % ||| loss 0.3265705406665802\u001b[0m\n",
            "\u001b[92mTest accuracy: 8761/10000 =  87.61 % ||| loss 0.34730181097984314\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.3636588762700558\n",
            "Batch #200 Loss: 0.360175654143095\n",
            "Batch #300 Loss: 0.3630228056013584\n",
            "\u001b[92mTrain accuracy: 42911/48000 =  89.4 % ||| loss 0.28578537702560425\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10589/12000 =  88.24 % ||| loss 0.30980101227760315\u001b[0m\n",
            "\u001b[92mTest accuracy: 8818/10000 =  88.18 % ||| loss 0.32824385166168213\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.3559456305205822\n",
            "Batch #200 Loss: 0.34218878343701364\n",
            "Batch #300 Loss: 0.3536878180503845\n",
            "\u001b[92mTrain accuracy: 42781/48000 =  89.13 % ||| loss 0.28916940093040466\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10568/12000 =  88.07 % ||| loss 0.3140282928943634\u001b[0m\n",
            "\u001b[92mTest accuracy: 8768/10000 =  87.68 % ||| loss 0.33100515604019165\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3455688087642193\n",
            "Batch #200 Loss: 0.3446220476925373\n",
            "Batch #300 Loss: 0.3412864406406879\n",
            "\u001b[92mTrain accuracy: 42988/48000 =  89.56 % ||| loss 0.2843123972415924\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10595/12000 =  88.29 % ||| loss 0.3133877217769623\u001b[0m\n",
            "\u001b[92mTest accuracy: 8807/10000 =  88.07 % ||| loss 0.3334611654281616\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3289564672112465\n",
            "Batch #200 Loss: 0.3272338169813156\n",
            "Batch #300 Loss: 0.34078523620963097\n",
            "\u001b[92mTrain accuracy: 43227/48000 =  90.06 % ||| loss 0.2663944959640503\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10680/12000 =  89.0 % ||| loss 0.29382574558258057\u001b[0m\n",
            "\u001b[92mTest accuracy: 8868/10000 =  88.68 % ||| loss 0.31399497389793396\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.32132021978497505\n",
            "Batch #200 Loss: 0.3285989670455456\n",
            "Batch #300 Loss: 0.3343315055966377\n",
            "\u001b[92mTrain accuracy: 43069/48000 =  89.73 % ||| loss 0.27510344982147217\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10638/12000 =  88.65 % ||| loss 0.30870866775512695\u001b[0m\n",
            "\u001b[92mTest accuracy: 8849/10000 =  88.49 % ||| loss 0.3320131301879883\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.31979847356677055\n",
            "Batch #200 Loss: 0.33495371356606485\n",
            "Batch #300 Loss: 0.3199076786637306\n",
            "\u001b[92mTrain accuracy: 43185/48000 =  89.97 % ||| loss 0.26709067821502686\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10696/12000 =  89.13 % ||| loss 0.29824522137641907\u001b[0m\n",
            "\u001b[92mTest accuracy: 8843/10000 =  88.43 % ||| loss 0.31979453563690186\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.32059662982821463\n",
            "Batch #200 Loss: 0.30810054779052737\n",
            "Batch #300 Loss: 0.32140738859772683\n",
            "\u001b[92mTrain accuracy: 43092/48000 =  89.78 % ||| loss 0.2695159316062927\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10612/12000 =  88.43 % ||| loss 0.30139005184173584\u001b[0m\n",
            "\u001b[92mTest accuracy: 8815/10000 =  88.15 % ||| loss 0.3183465600013733\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.30414765268564226\n",
            "Batch #200 Loss: 0.31164953917264937\n",
            "Batch #300 Loss: 0.2967483814060688\n",
            "\u001b[92mTrain accuracy: 43527/48000 =  90.68 % ||| loss 0.2518201768398285\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10732/12000 =  89.43 % ||| loss 0.2899724841117859\u001b[0m\n",
            "\u001b[92mTest accuracy: 8910/10000 =  89.1 % ||| loss 0.3112080991268158\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.30712774753570554\n",
            "Batch #200 Loss: 0.30247032046318056\n",
            "Batch #300 Loss: 0.30697611182928086\n",
            "\u001b[92mTrain accuracy: 43718/48000 =  91.08 % ||| loss 0.2428591102361679\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10748/12000 =  89.57 % ||| loss 0.28329896926879883\u001b[0m\n",
            "\u001b[92mTest accuracy: 8945/10000 =  89.45 % ||| loss 0.3028661906719208\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.29698238268494603\n",
            "Batch #200 Loss: 0.2978546266257763\n",
            "Batch #300 Loss: 0.2891163270175457\n",
            "\u001b[92mTrain accuracy: 43691/48000 =  91.02 % ||| loss 0.23912909626960754\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10732/12000 =  89.43 % ||| loss 0.2808167338371277\u001b[0m\n",
            "\u001b[92mTest accuracy: 8948/10000 =  89.48 % ||| loss 0.2941991686820984\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.29336063787341116\n",
            "Batch #200 Loss: 0.2904904143512249\n",
            "Batch #300 Loss: 0.28109087541699407\n",
            "\u001b[92mTrain accuracy: 43763/48000 =  91.17 % ||| loss 0.23512490093708038\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10772/12000 =  89.77 % ||| loss 0.2818755805492401\u001b[0m\n",
            "\u001b[92mTest accuracy: 8962/10000 =  89.62 % ||| loss 0.29337939620018005\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.29129784777760503\n",
            "Batch #200 Loss: 0.28594562888145447\n",
            "Batch #300 Loss: 0.28404712185263636\n",
            "\u001b[92mTrain accuracy: 43857/48000 =  91.37 % ||| loss 0.22828523814678192\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10727/12000 =  89.39 % ||| loss 0.27929607033729553\u001b[0m\n",
            "\u001b[92mTest accuracy: 8968/10000 =  89.68 % ||| loss 0.2971154749393463\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.2798499934375286\n",
            "Batch #200 Loss: 0.286433003693819\n",
            "Batch #300 Loss: 0.2842777010798454\n",
            "\u001b[92mTrain accuracy: 43934/48000 =  91.53 % ||| loss 0.226845383644104\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10740/12000 =  89.5 % ||| loss 0.27428165078163147\u001b[0m\n",
            "\u001b[92mTest accuracy: 8973/10000 =  89.73 % ||| loss 0.28686273097991943\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.27353469118475915\n",
            "Batch #200 Loss: 0.2810544565320015\n",
            "Batch #300 Loss: 0.27650711476802825\n",
            "\u001b[92mTrain accuracy: 44023/48000 =  91.71 % ||| loss 0.22484220564365387\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10776/12000 =  89.8 % ||| loss 0.27528300881385803\u001b[0m\n",
            "\u001b[92mTest accuracy: 8973/10000 =  89.73 % ||| loss 0.2893821597099304\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_13</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_13' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_13</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_104334-Lenet5Dropout_1726150070.105228_14</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_14' target=\"_blank\">Lenet5Dropout_1726150070.105228_14</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_14' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_14</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2580567002296448\n",
            "Batch #200 Loss: 1.407309672832489\n",
            "Batch #300 Loss: 1.0929121041297913\n",
            "\u001b[92mTrain accuracy: 34379/48000 =  71.62 % ||| loss 0.7476775050163269\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8687/12000 =  72.39 % ||| loss 0.7327271103858948\u001b[0m\n",
            "\u001b[92mTest accuracy: 7170/10000 =  71.7 % ||| loss 0.7620355486869812\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.8601019382476807\n",
            "Batch #200 Loss: 0.789571437239647\n",
            "Batch #300 Loss: 0.7531198507547379\n",
            "\u001b[92mTrain accuracy: 37214/48000 =  77.53 % ||| loss 0.5642780065536499\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9379/12000 =  78.16 % ||| loss 0.5520737171173096\u001b[0m\n",
            "\u001b[92mTest accuracy: 7692/10000 =  76.92 % ||| loss 0.5843858122825623\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.6901409077644348\n",
            "Batch #200 Loss: 0.6463116469979286\n",
            "Batch #300 Loss: 0.6427973780035973\n",
            "\u001b[92mTrain accuracy: 38178/48000 =  79.54 % ||| loss 0.5268488526344299\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9583/12000 =  79.86 % ||| loss 0.5234676599502563\u001b[0m\n",
            "\u001b[92mTest accuracy: 7898/10000 =  78.98 % ||| loss 0.547182559967041\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.6217720338702202\n",
            "Batch #200 Loss: 0.6042741948366165\n",
            "Batch #300 Loss: 0.5772494435310364\n",
            "\u001b[92mTrain accuracy: 38591/48000 =  80.4 % ||| loss 0.47800204157829285\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9598/12000 =  79.98 % ||| loss 0.4853142201900482\u001b[0m\n",
            "\u001b[92mTest accuracy: 7972/10000 =  79.72 % ||| loss 0.5042250752449036\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5804814371466637\n",
            "Batch #200 Loss: 0.5461314818263054\n",
            "Batch #300 Loss: 0.5611198210716247\n",
            "\u001b[92mTrain accuracy: 39667/48000 =  82.64 % ||| loss 0.4468664824962616\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9913/12000 =  82.61 % ||| loss 0.4503817558288574\u001b[0m\n",
            "\u001b[92mTest accuracy: 8119/10000 =  81.19 % ||| loss 0.47371819615364075\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5246507868170738\n",
            "Batch #200 Loss: 0.5246732980012894\n",
            "Batch #300 Loss: 0.5277686131000519\n",
            "\u001b[92mTrain accuracy: 40550/48000 =  84.48 % ||| loss 0.40819689631462097\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10160/12000 =  84.67 % ||| loss 0.4134402573108673\u001b[0m\n",
            "\u001b[92mTest accuracy: 8342/10000 =  83.42 % ||| loss 0.43699684739112854\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.49962725073099135\n",
            "Batch #200 Loss: 0.5058646634221077\n",
            "Batch #300 Loss: 0.496235608458519\n",
            "\u001b[92mTrain accuracy: 41039/48000 =  85.5 % ||| loss 0.3873923420906067\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10233/12000 =  85.28 % ||| loss 0.39624154567718506\u001b[0m\n",
            "\u001b[92mTest accuracy: 8445/10000 =  84.45 % ||| loss 0.4213182330131531\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.4816819533705711\n",
            "Batch #200 Loss: 0.4722199761867523\n",
            "Batch #300 Loss: 0.48074268490076066\n",
            "\u001b[92mTrain accuracy: 41073/48000 =  85.57 % ||| loss 0.38531774282455444\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10228/12000 =  85.23 % ||| loss 0.3928041160106659\u001b[0m\n",
            "\u001b[92mTest accuracy: 8446/10000 =  84.46 % ||| loss 0.4224281311035156\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.454697350859642\n",
            "Batch #200 Loss: 0.4667796120047569\n",
            "Batch #300 Loss: 0.45801705062389375\n",
            "\u001b[92mTrain accuracy: 41811/48000 =  87.11 % ||| loss 0.3497615158557892\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10442/12000 =  87.02 % ||| loss 0.36020010709762573\u001b[0m\n",
            "\u001b[92mTest accuracy: 8612/10000 =  86.12 % ||| loss 0.38636380434036255\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.4359573346376419\n",
            "Batch #200 Loss: 0.443907730281353\n",
            "Batch #300 Loss: 0.4552502590417862\n",
            "\u001b[92mTrain accuracy: 41966/48000 =  87.43 % ||| loss 0.33932000398635864\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10441/12000 =  87.01 % ||| loss 0.35112592577934265\u001b[0m\n",
            "\u001b[92mTest accuracy: 8597/10000 =  85.97 % ||| loss 0.3817611038684845\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.4329783067107201\n",
            "Batch #200 Loss: 0.41827845722436907\n",
            "Batch #300 Loss: 0.43526641637086866\n",
            "\u001b[92mTrain accuracy: 42107/48000 =  87.72 % ||| loss 0.32718539237976074\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10450/12000 =  87.08 % ||| loss 0.34301233291625977\u001b[0m\n",
            "\u001b[92mTest accuracy: 8652/10000 =  86.52 % ||| loss 0.36824148893356323\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.4093921120464802\n",
            "Batch #200 Loss: 0.40086490124464036\n",
            "Batch #300 Loss: 0.4272615987062454\n",
            "\u001b[92mTrain accuracy: 42146/48000 =  87.8 % ||| loss 0.32727381587028503\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10488/12000 =  87.4 % ||| loss 0.3432433307170868\u001b[0m\n",
            "\u001b[92mTest accuracy: 8652/10000 =  86.52 % ||| loss 0.36344024538993835\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.40399043083190916\n",
            "Batch #200 Loss: 0.40666108697652814\n",
            "Batch #300 Loss: 0.40583431601524356\n",
            "\u001b[92mTrain accuracy: 42352/48000 =  88.23 % ||| loss 0.3211740255355835\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10534/12000 =  87.78 % ||| loss 0.3387894034385681\u001b[0m\n",
            "\u001b[92mTest accuracy: 8696/10000 =  86.96 % ||| loss 0.36579540371894836\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.38834241032600403\n",
            "Batch #200 Loss: 0.3964930075407028\n",
            "Batch #300 Loss: 0.4078256732225418\n",
            "\u001b[92mTrain accuracy: 42382/48000 =  88.3 % ||| loss 0.30915147066116333\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10518/12000 =  87.65 % ||| loss 0.33041155338287354\u001b[0m\n",
            "\u001b[92mTest accuracy: 8663/10000 =  86.63 % ||| loss 0.355696439743042\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.38697059467434886\n",
            "Batch #200 Loss: 0.37228316351771357\n",
            "Batch #300 Loss: 0.39287261337041857\n",
            "\u001b[92mTrain accuracy: 42607/48000 =  88.76 % ||| loss 0.30367642641067505\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10576/12000 =  88.13 % ||| loss 0.32457399368286133\u001b[0m\n",
            "\u001b[92mTest accuracy: 8736/10000 =  87.36 % ||| loss 0.3460005521774292\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.38317866027355196\n",
            "Batch #200 Loss: 0.3850945697724819\n",
            "Batch #300 Loss: 0.3753976994752884\n",
            "\u001b[92mTrain accuracy: 42635/48000 =  88.82 % ||| loss 0.301235169172287\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10553/12000 =  87.94 % ||| loss 0.32514768838882446\u001b[0m\n",
            "\u001b[92mTest accuracy: 8718/10000 =  87.18 % ||| loss 0.3485229015350342\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.37075338512659073\n",
            "Batch #200 Loss: 0.3703575935959816\n",
            "Batch #300 Loss: 0.37685469269752503\n",
            "\u001b[92mTrain accuracy: 42834/48000 =  89.24 % ||| loss 0.2870045006275177\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10625/12000 =  88.54 % ||| loss 0.3110373616218567\u001b[0m\n",
            "\u001b[92mTest accuracy: 8765/10000 =  87.65 % ||| loss 0.34668877720832825\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.37549247026443483\n",
            "Batch #200 Loss: 0.3624282793700695\n",
            "Batch #300 Loss: 0.35510715052485464\n",
            "\u001b[92mTrain accuracy: 42893/48000 =  89.36 % ||| loss 0.28794801235198975\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10616/12000 =  88.47 % ||| loss 0.3102407157421112\u001b[0m\n",
            "\u001b[92mTest accuracy: 8780/10000 =  87.8 % ||| loss 0.3415585160255432\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.35981875598430635\n",
            "Batch #200 Loss: 0.36923537388443944\n",
            "Batch #300 Loss: 0.36297467067837713\n",
            "\u001b[92mTrain accuracy: 42875/48000 =  89.32 % ||| loss 0.2801469564437866\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10613/12000 =  88.44 % ||| loss 0.3118227422237396\u001b[0m\n",
            "\u001b[92mTest accuracy: 8744/10000 =  87.44 % ||| loss 0.33801621198654175\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.33744326651096346\n",
            "Batch #200 Loss: 0.35509994626045227\n",
            "Batch #300 Loss: 0.3566053578257561\n",
            "\u001b[92mTrain accuracy: 43048/48000 =  89.68 % ||| loss 0.2768440246582031\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10678/12000 =  88.98 % ||| loss 0.30500808358192444\u001b[0m\n",
            "\u001b[92mTest accuracy: 8815/10000 =  88.15 % ||| loss 0.3362296223640442\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.35784418135881424\n",
            "Batch #200 Loss: 0.3514516094326973\n",
            "Batch #300 Loss: 0.3425886745750904\n",
            "\u001b[92mTrain accuracy: 43294/48000 =  90.2 % ||| loss 0.26773735880851746\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10660/12000 =  88.83 % ||| loss 0.2979431450366974\u001b[0m\n",
            "\u001b[92mTest accuracy: 8847/10000 =  88.47 % ||| loss 0.3243471086025238\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.33594448536634447\n",
            "Batch #200 Loss: 0.35048735931515695\n",
            "Batch #300 Loss: 0.35202216893434524\n",
            "\u001b[92mTrain accuracy: 43332/48000 =  90.28 % ||| loss 0.2616869807243347\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10706/12000 =  89.22 % ||| loss 0.2948988974094391\u001b[0m\n",
            "\u001b[92mTest accuracy: 8860/10000 =  88.6 % ||| loss 0.31971606612205505\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.34300673082470895\n",
            "Batch #200 Loss: 0.3438850677013397\n",
            "Batch #300 Loss: 0.33644754633307455\n",
            "\u001b[92mTrain accuracy: 43469/48000 =  90.56 % ||| loss 0.261304646730423\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10712/12000 =  89.27 % ||| loss 0.29920196533203125\u001b[0m\n",
            "\u001b[92mTest accuracy: 8871/10000 =  88.71 % ||| loss 0.32534554600715637\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.33411090686917305\n",
            "Batch #200 Loss: 0.3311097924411297\n",
            "Batch #300 Loss: 0.3390392652153969\n",
            "\u001b[92mTrain accuracy: 43453/48000 =  90.53 % ||| loss 0.2564049959182739\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10736/12000 =  89.47 % ||| loss 0.2924231290817261\u001b[0m\n",
            "\u001b[92mTest accuracy: 8886/10000 =  88.86 % ||| loss 0.32899436354637146\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3312814350426197\n",
            "Batch #200 Loss: 0.3369801165163517\n",
            "Batch #300 Loss: 0.3268413975834846\n",
            "\u001b[92mTrain accuracy: 43593/48000 =  90.82 % ||| loss 0.25019654631614685\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10734/12000 =  89.45 % ||| loss 0.2913351058959961\u001b[0m\n",
            "\u001b[92mTest accuracy: 8887/10000 =  88.87 % ||| loss 0.3141544759273529\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_14</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_14' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_14</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_104606-Lenet5Dropout_1726150070.105228_15</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_15' target=\"_blank\">Lenet5Dropout_1726150070.105228_15</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_15' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_15</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.293517487049103\n",
            "Batch #200 Loss: 1.9318579483032225\n",
            "Batch #300 Loss: 1.1460096168518066\n",
            "\u001b[92mTrain accuracy: 31631/48000 =  65.9 % ||| loss 0.8932272791862488\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7926/12000 =  66.05 % ||| loss 0.8810895681381226\u001b[0m\n",
            "\u001b[92mTest accuracy: 6559/10000 =  65.59 % ||| loss 0.9049525260925293\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.9118527811765671\n",
            "Batch #200 Loss: 0.8793692922592163\n",
            "Batch #300 Loss: 0.8529411691427231\n",
            "\u001b[92mTrain accuracy: 34716/48000 =  72.32 % ||| loss 0.7297939658164978\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8712/12000 =  72.6 % ||| loss 0.7191603779792786\u001b[0m\n",
            "\u001b[92mTest accuracy: 7152/10000 =  71.52 % ||| loss 0.7528018355369568\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.7890446001291275\n",
            "Batch #200 Loss: 0.7634806489944458\n",
            "Batch #300 Loss: 0.7345670515298843\n",
            "\u001b[92mTrain accuracy: 35745/48000 =  74.47 % ||| loss 0.6530313491821289\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8965/12000 =  74.71 % ||| loss 0.6400824189186096\u001b[0m\n",
            "\u001b[92mTest accuracy: 7375/10000 =  73.75 % ||| loss 0.6805956959724426\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.6924914443492889\n",
            "Batch #200 Loss: 0.6916957470774651\n",
            "Batch #300 Loss: 0.6655624312162399\n",
            "\u001b[92mTrain accuracy: 37409/48000 =  77.94 % ||| loss 0.5834046602249146\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9363/12000 =  78.03 % ||| loss 0.5740633606910706\u001b[0m\n",
            "\u001b[92mTest accuracy: 7694/10000 =  76.94 % ||| loss 0.6066981554031372\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.6375322231650352\n",
            "Batch #200 Loss: 0.6193864184617996\n",
            "Batch #300 Loss: 0.6305570849776267\n",
            "\u001b[92mTrain accuracy: 37920/48000 =  79.0 % ||| loss 0.5399527549743652\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9478/12000 =  78.98 % ||| loss 0.537346601486206\u001b[0m\n",
            "\u001b[92mTest accuracy: 7806/10000 =  78.06 % ||| loss 0.5643563866615295\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5980344769358635\n",
            "Batch #200 Loss: 0.5750821930170059\n",
            "Batch #300 Loss: 0.5771397012472153\n",
            "\u001b[92mTrain accuracy: 38890/48000 =  81.02 % ||| loss 0.5006890296936035\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9755/12000 =  81.29 % ||| loss 0.4962780177593231\u001b[0m\n",
            "\u001b[92mTest accuracy: 8024/10000 =  80.24 % ||| loss 0.5286353230476379\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5520359158515931\n",
            "Batch #200 Loss: 0.5487916651368141\n",
            "Batch #300 Loss: 0.5367721822857857\n",
            "\u001b[92mTrain accuracy: 39553/48000 =  82.4 % ||| loss 0.47161829471588135\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9909/12000 =  82.58 % ||| loss 0.4691849946975708\u001b[0m\n",
            "\u001b[92mTest accuracy: 8146/10000 =  81.46 % ||| loss 0.5044460892677307\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.5204353570938111\n",
            "Batch #200 Loss: 0.5218837672472\n",
            "Batch #300 Loss: 0.5125437796115875\n",
            "\u001b[92mTrain accuracy: 39658/48000 =  82.62 % ||| loss 0.4641437530517578\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9887/12000 =  82.39 % ||| loss 0.4661415219306946\u001b[0m\n",
            "\u001b[92mTest accuracy: 8189/10000 =  81.89 % ||| loss 0.4916304647922516\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.4987296035885811\n",
            "Batch #200 Loss: 0.5077851176261902\n",
            "Batch #300 Loss: 0.4890021738409996\n",
            "\u001b[92mTrain accuracy: 40318/48000 =  84.0 % ||| loss 0.4316146671772003\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10111/12000 =  84.26 % ||| loss 0.43269357085227966\u001b[0m\n",
            "\u001b[92mTest accuracy: 8290/10000 =  82.9 % ||| loss 0.4623483717441559\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.488798485994339\n",
            "Batch #200 Loss: 0.47625385612249377\n",
            "Batch #300 Loss: 0.47049802154302595\n",
            "\u001b[92mTrain accuracy: 40584/48000 =  84.55 % ||| loss 0.4169350266456604\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10141/12000 =  84.51 % ||| loss 0.42019227147102356\u001b[0m\n",
            "\u001b[92mTest accuracy: 8359/10000 =  83.59 % ||| loss 0.4463779628276825\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.4635391691327095\n",
            "Batch #200 Loss: 0.4565913027524948\n",
            "Batch #300 Loss: 0.45327556878328323\n",
            "\u001b[92mTrain accuracy: 40538/48000 =  84.45 % ||| loss 0.4197198748588562\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10140/12000 =  84.5 % ||| loss 0.4228735864162445\u001b[0m\n",
            "\u001b[92mTest accuracy: 8330/10000 =  83.3 % ||| loss 0.4553389847278595\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.4654946407675743\n",
            "Batch #200 Loss: 0.4510262253880501\n",
            "Batch #300 Loss: 0.4357536619901657\n",
            "\u001b[92mTrain accuracy: 41156/48000 =  85.74 % ||| loss 0.38565322756767273\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10260/12000 =  85.5 % ||| loss 0.39527028799057007\u001b[0m\n",
            "\u001b[92mTest accuracy: 8509/10000 =  85.09 % ||| loss 0.41354697942733765\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.4350747361779213\n",
            "Batch #200 Loss: 0.4347405228018761\n",
            "Batch #300 Loss: 0.4332710194587708\n",
            "\u001b[92mTrain accuracy: 41355/48000 =  86.16 % ||| loss 0.37906017899513245\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10291/12000 =  85.76 % ||| loss 0.39124295115470886\u001b[0m\n",
            "\u001b[92mTest accuracy: 8517/10000 =  85.17 % ||| loss 0.41718730330467224\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.41787458688020707\n",
            "Batch #200 Loss: 0.4222107508778572\n",
            "Batch #300 Loss: 0.42081554770469665\n",
            "\u001b[92mTrain accuracy: 41464/48000 =  86.38 % ||| loss 0.3701820969581604\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10338/12000 =  86.15 % ||| loss 0.38009074330329895\u001b[0m\n",
            "\u001b[92mTest accuracy: 8539/10000 =  85.39 % ||| loss 0.4010087549686432\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.42108131483197214\n",
            "Batch #200 Loss: 0.3981625573337078\n",
            "Batch #300 Loss: 0.42447917550802233\n",
            "\u001b[92mTrain accuracy: 41610/48000 =  86.69 % ||| loss 0.36207836866378784\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10321/12000 =  86.01 % ||| loss 0.3747937083244324\u001b[0m\n",
            "\u001b[92mTest accuracy: 8579/10000 =  85.79 % ||| loss 0.39300569891929626\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3959687700867653\n",
            "Batch #200 Loss: 0.40086388528347017\n",
            "Batch #300 Loss: 0.4130269254744053\n",
            "\u001b[92mTrain accuracy: 41629/48000 =  86.73 % ||| loss 0.3535016179084778\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10385/12000 =  86.54 % ||| loss 0.36699825525283813\u001b[0m\n",
            "\u001b[92mTest accuracy: 8602/10000 =  86.02 % ||| loss 0.3911193609237671\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3942820882797241\n",
            "Batch #200 Loss: 0.39883639618754385\n",
            "Batch #300 Loss: 0.40837200224399567\n",
            "\u001b[92mTrain accuracy: 41823/48000 =  87.13 % ||| loss 0.346117228269577\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10446/12000 =  87.05 % ||| loss 0.35926511883735657\u001b[0m\n",
            "\u001b[92mTest accuracy: 8599/10000 =  85.99 % ||| loss 0.38087591528892517\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3870282258093357\n",
            "Batch #200 Loss: 0.3956040653586388\n",
            "Batch #300 Loss: 0.3881583897769451\n",
            "\u001b[92mTrain accuracy: 41988/48000 =  87.48 % ||| loss 0.3391116261482239\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10442/12000 =  87.02 % ||| loss 0.3524874448776245\u001b[0m\n",
            "\u001b[92mTest accuracy: 8635/10000 =  86.35 % ||| loss 0.3734952509403229\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.38699928104877473\n",
            "Batch #200 Loss: 0.3868860211968422\n",
            "Batch #300 Loss: 0.3717093820869923\n",
            "\u001b[92mTrain accuracy: 42163/48000 =  87.84 % ||| loss 0.32742756605148315\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10480/12000 =  87.33 % ||| loss 0.3476911187171936\u001b[0m\n",
            "\u001b[92mTest accuracy: 8665/10000 =  86.65 % ||| loss 0.36352813243865967\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.3771718482673168\n",
            "Batch #200 Loss: 0.376931736767292\n",
            "Batch #300 Loss: 0.37955826580524443\n",
            "\u001b[92mTrain accuracy: 42054/48000 =  87.61 % ||| loss 0.33138924837112427\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10471/12000 =  87.26 % ||| loss 0.34904852509498596\u001b[0m\n",
            "\u001b[92mTest accuracy: 8629/10000 =  86.29 % ||| loss 0.3687800168991089\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.3795010356605053\n",
            "Batch #200 Loss: 0.3632981212437153\n",
            "Batch #300 Loss: 0.3636234514415264\n",
            "\u001b[92mTrain accuracy: 42407/48000 =  88.35 % ||| loss 0.31567302346229553\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10524/12000 =  87.7 % ||| loss 0.336825966835022\u001b[0m\n",
            "\u001b[92mTest accuracy: 8691/10000 =  86.91 % ||| loss 0.35617542266845703\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.362628664970398\n",
            "Batch #200 Loss: 0.35750447228550913\n",
            "Batch #300 Loss: 0.36577062636613844\n",
            "\u001b[92mTrain accuracy: 42354/48000 =  88.24 % ||| loss 0.3174114227294922\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10561/12000 =  88.01 % ||| loss 0.33947476744651794\u001b[0m\n",
            "\u001b[92mTest accuracy: 8685/10000 =  86.85 % ||| loss 0.3621516525745392\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.3535151968896389\n",
            "Batch #200 Loss: 0.36302301451563834\n",
            "Batch #300 Loss: 0.36554130762815473\n",
            "\u001b[92mTrain accuracy: 42633/48000 =  88.82 % ||| loss 0.3069339990615845\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10578/12000 =  88.15 % ||| loss 0.3282979726791382\u001b[0m\n",
            "\u001b[92mTest accuracy: 8715/10000 =  87.15 % ||| loss 0.3530675172805786\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.35128364607691764\n",
            "Batch #200 Loss: 0.34748363435268403\n",
            "Batch #300 Loss: 0.3501780463755131\n",
            "\u001b[92mTrain accuracy: 42678/48000 =  88.91 % ||| loss 0.30678778886795044\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10569/12000 =  88.08 % ||| loss 0.32943153381347656\u001b[0m\n",
            "\u001b[92mTest accuracy: 8753/10000 =  87.53 % ||| loss 0.3445990979671478\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3464048819243908\n",
            "Batch #200 Loss: 0.3506665104627609\n",
            "Batch #300 Loss: 0.35053749427199365\n",
            "\u001b[92mTrain accuracy: 42481/48000 =  88.5 % ||| loss 0.3117387294769287\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10548/12000 =  87.9 % ||| loss 0.3348012864589691\u001b[0m\n",
            "\u001b[92mTest accuracy: 8679/10000 =  86.79 % ||| loss 0.35546794533729553\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_15</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_15' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_15</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_104839-Lenet5Dropout_1726150070.105228_16</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_16' target=\"_blank\">Lenet5Dropout_1726150070.105228_16</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_16' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_16</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3020011353492738\n",
            "Batch #200 Loss: 2.290576915740967\n",
            "Batch #300 Loss: 2.1691760873794554\n",
            "\u001b[92mTrain accuracy: 27482/48000 =  57.25 % ||| loss 1.0578373670578003\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6875/12000 =  57.29 % ||| loss 1.052351713180542\u001b[0m\n",
            "\u001b[92mTest accuracy: 5695/10000 =  56.95 % ||| loss 1.0661407709121704\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 1.1077301728725433\n",
            "Batch #200 Loss: 1.007960010766983\n",
            "Batch #300 Loss: 0.9222389531135559\n",
            "\u001b[92mTrain accuracy: 34511/48000 =  71.9 % ||| loss 0.7527535557746887\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8660/12000 =  72.17 % ||| loss 0.7443299889564514\u001b[0m\n",
            "\u001b[92mTest accuracy: 7116/10000 =  71.16 % ||| loss 0.7626650333404541\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.8285734736919403\n",
            "Batch #200 Loss: 0.7805064529180527\n",
            "Batch #300 Loss: 0.7637519890069961\n",
            "\u001b[92mTrain accuracy: 36094/48000 =  75.2 % ||| loss 0.6420518159866333\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8987/12000 =  74.89 % ||| loss 0.6316531300544739\u001b[0m\n",
            "\u001b[92mTest accuracy: 7428/10000 =  74.28 % ||| loss 0.662501871585846\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.7043241304159165\n",
            "Batch #200 Loss: 0.6822258245944977\n",
            "Batch #300 Loss: 0.6660000723600388\n",
            "\u001b[92mTrain accuracy: 36882/48000 =  76.84 % ||| loss 0.5860773324966431\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9255/12000 =  77.12 % ||| loss 0.576272189617157\u001b[0m\n",
            "\u001b[92mTest accuracy: 7585/10000 =  75.85 % ||| loss 0.6083455085754395\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.6558359816670418\n",
            "Batch #200 Loss: 0.6305235788226128\n",
            "Batch #300 Loss: 0.6191220924258232\n",
            "\u001b[92mTrain accuracy: 37880/48000 =  78.92 % ||| loss 0.5243958234786987\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9502/12000 =  79.18 % ||| loss 0.520868718624115\u001b[0m\n",
            "\u001b[92mTest accuracy: 7809/10000 =  78.09 % ||| loss 0.5442913174629211\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.6150532752275467\n",
            "Batch #200 Loss: 0.5998385199904441\n",
            "Batch #300 Loss: 0.5788052687048912\n",
            "\u001b[92mTrain accuracy: 38548/48000 =  80.31 % ||| loss 0.5002085566520691\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9640/12000 =  80.33 % ||| loss 0.49806514382362366\u001b[0m\n",
            "\u001b[92mTest accuracy: 7950/10000 =  79.5 % ||| loss 0.5231021642684937\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5712313377857208\n",
            "Batch #200 Loss: 0.5689006280899048\n",
            "Batch #300 Loss: 0.5362857976555824\n",
            "\u001b[92mTrain accuracy: 38756/48000 =  80.74 % ||| loss 0.4886121451854706\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9685/12000 =  80.71 % ||| loss 0.4854901134967804\u001b[0m\n",
            "\u001b[92mTest accuracy: 7981/10000 =  79.81 % ||| loss 0.5087677836418152\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.5428619369864464\n",
            "Batch #200 Loss: 0.5310143339633941\n",
            "Batch #300 Loss: 0.535299876332283\n",
            "\u001b[92mTrain accuracy: 39983/48000 =  83.3 % ||| loss 0.4496332108974457\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10021/12000 =  83.51 % ||| loss 0.45001351833343506\u001b[0m\n",
            "\u001b[92mTest accuracy: 8262/10000 =  82.62 % ||| loss 0.4760866165161133\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.5282283160090446\n",
            "Batch #200 Loss: 0.5276164057850837\n",
            "Batch #300 Loss: 0.5117272779345512\n",
            "\u001b[92mTrain accuracy: 40326/48000 =  84.01 % ||| loss 0.4355136752128601\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10096/12000 =  84.13 % ||| loss 0.4375198781490326\u001b[0m\n",
            "\u001b[92mTest accuracy: 8304/10000 =  83.04 % ||| loss 0.4604380130767822\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.5050548732280731\n",
            "Batch #200 Loss: 0.5032431107759475\n",
            "Batch #300 Loss: 0.498186768591404\n",
            "\u001b[92mTrain accuracy: 40532/48000 =  84.44 % ||| loss 0.4215047061443329\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10137/12000 =  84.47 % ||| loss 0.42551589012145996\u001b[0m\n",
            "\u001b[92mTest accuracy: 8353/10000 =  83.53 % ||| loss 0.44934722781181335\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.4739228951931\n",
            "Batch #200 Loss: 0.49701688677072525\n",
            "Batch #300 Loss: 0.49265537425875666\n",
            "\u001b[92mTrain accuracy: 40650/48000 =  84.69 % ||| loss 0.41344600915908813\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10116/12000 =  84.3 % ||| loss 0.42020323872566223\u001b[0m\n",
            "\u001b[92mTest accuracy: 8401/10000 =  84.01 % ||| loss 0.4398295283317566\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.4661160713434219\n",
            "Batch #200 Loss: 0.4659159418940544\n",
            "Batch #300 Loss: 0.4712513828277588\n",
            "\u001b[92mTrain accuracy: 41173/48000 =  85.78 % ||| loss 0.38968905806541443\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10275/12000 =  85.62 % ||| loss 0.39766618609428406\u001b[0m\n",
            "\u001b[92mTest accuracy: 8485/10000 =  84.85 % ||| loss 0.41818130016326904\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.47205381155014037\n",
            "Batch #200 Loss: 0.46467020362615585\n",
            "Batch #300 Loss: 0.44756056636571884\n",
            "\u001b[92mTrain accuracy: 41396/48000 =  86.24 % ||| loss 0.3770073354244232\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10308/12000 =  85.9 % ||| loss 0.38424018025398254\u001b[0m\n",
            "\u001b[92mTest accuracy: 8534/10000 =  85.34 % ||| loss 0.41108259558677673\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.45406775146722794\n",
            "Batch #200 Loss: 0.44930750131607056\n",
            "Batch #300 Loss: 0.44126820474863054\n",
            "\u001b[92mTrain accuracy: 41495/48000 =  86.45 % ||| loss 0.368889719247818\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10350/12000 =  86.25 % ||| loss 0.3763737380504608\u001b[0m\n",
            "\u001b[92mTest accuracy: 8563/10000 =  85.63 % ||| loss 0.402508944272995\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.4335689082741737\n",
            "Batch #200 Loss: 0.43528244078159334\n",
            "Batch #300 Loss: 0.4355745854973793\n",
            "\u001b[92mTrain accuracy: 41509/48000 =  86.48 % ||| loss 0.3644999563694\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10346/12000 =  86.22 % ||| loss 0.3724539279937744\u001b[0m\n",
            "\u001b[92mTest accuracy: 8530/10000 =  85.3 % ||| loss 0.3992120623588562\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.41086417958140375\n",
            "Batch #200 Loss: 0.42232743978500364\n",
            "Batch #300 Loss: 0.4313347619771957\n",
            "\u001b[92mTrain accuracy: 41725/48000 =  86.93 % ||| loss 0.3534301221370697\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10379/12000 =  86.49 % ||| loss 0.3657688498497009\u001b[0m\n",
            "\u001b[92mTest accuracy: 8598/10000 =  85.98 % ||| loss 0.3841642141342163\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.412913366407156\n",
            "Batch #200 Loss: 0.4225423315167427\n",
            "Batch #300 Loss: 0.41851294979453085\n",
            "\u001b[92mTrain accuracy: 41865/48000 =  87.22 % ||| loss 0.3403796851634979\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10431/12000 =  86.92 % ||| loss 0.35212942957878113\u001b[0m\n",
            "\u001b[92mTest accuracy: 8634/10000 =  86.34 % ||| loss 0.37639614939689636\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.4215054020285606\n",
            "Batch #200 Loss: 0.3908242151141167\n",
            "Batch #300 Loss: 0.40248069882392884\n",
            "\u001b[92mTrain accuracy: 41997/48000 =  87.49 % ||| loss 0.33960574865341187\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10458/12000 =  87.15 % ||| loss 0.35295814275741577\u001b[0m\n",
            "\u001b[92mTest accuracy: 8662/10000 =  86.62 % ||| loss 0.3754275441169739\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.40059910625219347\n",
            "Batch #200 Loss: 0.4044328659772873\n",
            "Batch #300 Loss: 0.3968327213823795\n",
            "\u001b[92mTrain accuracy: 42090/48000 =  87.69 % ||| loss 0.3332526385784149\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10459/12000 =  87.16 % ||| loss 0.34913188219070435\u001b[0m\n",
            "\u001b[92mTest accuracy: 8688/10000 =  86.88 % ||| loss 0.36870595812797546\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.38165930584073066\n",
            "Batch #200 Loss: 0.3985732758045197\n",
            "Batch #300 Loss: 0.39670306742191314\n",
            "\u001b[92mTrain accuracy: 42325/48000 =  88.18 % ||| loss 0.3207511305809021\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10545/12000 =  87.88 % ||| loss 0.3332405686378479\u001b[0m\n",
            "\u001b[92mTest accuracy: 8720/10000 =  87.2 % ||| loss 0.36061662435531616\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.38858783796429636\n",
            "Batch #200 Loss: 0.3783905862271786\n",
            "Batch #300 Loss: 0.392680662125349\n",
            "\u001b[92mTrain accuracy: 42239/48000 =  88.0 % ||| loss 0.321327269077301\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10530/12000 =  87.75 % ||| loss 0.33703672885894775\u001b[0m\n",
            "\u001b[92mTest accuracy: 8689/10000 =  86.89 % ||| loss 0.36121681332588196\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.389746183604002\n",
            "Batch #200 Loss: 0.3693027536571026\n",
            "Batch #300 Loss: 0.38613908872008323\n",
            "\u001b[92mTrain accuracy: 42403/48000 =  88.34 % ||| loss 0.3135678768157959\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10553/12000 =  87.94 % ||| loss 0.33057475090026855\u001b[0m\n",
            "\u001b[92mTest accuracy: 8723/10000 =  87.23 % ||| loss 0.3540474474430084\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.37147207245230673\n",
            "Batch #200 Loss: 0.37714872375130654\n",
            "Batch #300 Loss: 0.3685251522064209\n",
            "\u001b[92mTrain accuracy: 42271/48000 =  88.06 % ||| loss 0.32049405574798584\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10535/12000 =  87.79 % ||| loss 0.33471497893333435\u001b[0m\n",
            "\u001b[92mTest accuracy: 8664/10000 =  86.64 % ||| loss 0.3637801706790924\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.3683637221157551\n",
            "Batch #200 Loss: 0.379096322208643\n",
            "Batch #300 Loss: 0.36607425212860106\n",
            "\u001b[92mTrain accuracy: 42506/48000 =  88.55 % ||| loss 0.30647146701812744\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10571/12000 =  88.09 % ||| loss 0.3263954520225525\u001b[0m\n",
            "\u001b[92mTest accuracy: 8753/10000 =  87.53 % ||| loss 0.34777987003326416\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.36091105207800867\n",
            "Batch #200 Loss: 0.3683892804384232\n",
            "Batch #300 Loss: 0.3606813636422157\n",
            "\u001b[92mTrain accuracy: 42706/48000 =  88.97 % ||| loss 0.29778313636779785\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10596/12000 =  88.3 % ||| loss 0.31783565878868103\u001b[0m\n",
            "\u001b[92mTest accuracy: 8762/10000 =  87.62 % ||| loss 0.34268423914909363\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_16</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_16' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_16</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_105115-Lenet5Dropout_1726150070.105228_17</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_17' target=\"_blank\">Lenet5Dropout_1726150070.105228_17</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_17' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_17</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3022067499160768\n",
            "Batch #200 Loss: 2.279677739143372\n",
            "Batch #300 Loss: 1.9147353267669678\n",
            "\u001b[92mTrain accuracy: 28310/48000 =  58.98 % ||| loss 1.0592073202133179\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7055/12000 =  58.79 % ||| loss 1.053767442703247\u001b[0m\n",
            "\u001b[92mTest accuracy: 5841/10000 =  58.41 % ||| loss 1.0679233074188232\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 1.2040006697177887\n",
            "Batch #200 Loss: 1.0927796483039856\n",
            "Batch #300 Loss: 1.0236218267679214\n",
            "\u001b[92mTrain accuracy: 32792/48000 =  68.32 % ||| loss 0.7887073755264282\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8224/12000 =  68.53 % ||| loss 0.7772501707077026\u001b[0m\n",
            "\u001b[92mTest accuracy: 6794/10000 =  67.94 % ||| loss 0.7981659770011902\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.9287192177772522\n",
            "Batch #200 Loss: 0.8827586227655411\n",
            "Batch #300 Loss: 0.8632055628299713\n",
            "\u001b[92mTrain accuracy: 35205/48000 =  73.34 % ||| loss 0.6748508214950562\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8853/12000 =  73.78 % ||| loss 0.6646682024002075\u001b[0m\n",
            "\u001b[92mTest accuracy: 7274/10000 =  72.74 % ||| loss 0.6907204985618591\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.7974044632911682\n",
            "Batch #200 Loss: 0.7897284942865371\n",
            "Batch #300 Loss: 0.7579816085100174\n",
            "\u001b[92mTrain accuracy: 36567/48000 =  76.18 % ||| loss 0.6196286678314209\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9127/12000 =  76.06 % ||| loss 0.6114590167999268\u001b[0m\n",
            "\u001b[92mTest accuracy: 7542/10000 =  75.42 % ||| loss 0.646345317363739\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.7463305109739303\n",
            "Batch #200 Loss: 0.7166388690471649\n",
            "Batch #300 Loss: 0.7131809228658677\n",
            "\u001b[92mTrain accuracy: 37008/48000 =  77.1 % ||| loss 0.5828417539596558\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9269/12000 =  77.24 % ||| loss 0.576636552810669\u001b[0m\n",
            "\u001b[92mTest accuracy: 7610/10000 =  76.1 % ||| loss 0.598718523979187\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.679129291176796\n",
            "Batch #200 Loss: 0.6821134647727013\n",
            "Batch #300 Loss: 0.679269432425499\n",
            "\u001b[92mTrain accuracy: 37525/48000 =  78.18 % ||| loss 0.5442311763763428\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9398/12000 =  78.32 % ||| loss 0.5394870042800903\u001b[0m\n",
            "\u001b[92mTest accuracy: 7715/10000 =  77.15 % ||| loss 0.566081702709198\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.651897314786911\n",
            "Batch #200 Loss: 0.6412159770727157\n",
            "Batch #300 Loss: 0.6208062160015106\n",
            "\u001b[92mTrain accuracy: 37849/48000 =  78.85 % ||| loss 0.5208070278167725\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9488/12000 =  79.07 % ||| loss 0.519609808921814\u001b[0m\n",
            "\u001b[92mTest accuracy: 7828/10000 =  78.28 % ||| loss 0.5404254794120789\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.6143368139863015\n",
            "Batch #200 Loss: 0.6093920874595642\n",
            "Batch #300 Loss: 0.6047363075613975\n",
            "\u001b[92mTrain accuracy: 37817/48000 =  78.79 % ||| loss 0.5152122378349304\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9444/12000 =  78.7 % ||| loss 0.5174000859260559\u001b[0m\n",
            "\u001b[92mTest accuracy: 7757/10000 =  77.57 % ||| loss 0.5500717163085938\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.6077580159902572\n",
            "Batch #200 Loss: 0.5710017818212509\n",
            "Batch #300 Loss: 0.5789325174689293\n",
            "\u001b[92mTrain accuracy: 38717/48000 =  80.66 % ||| loss 0.48020222783088684\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9686/12000 =  80.72 % ||| loss 0.4785815179347992\u001b[0m\n",
            "\u001b[92mTest accuracy: 7997/10000 =  79.97 % ||| loss 0.5069304704666138\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.5722873100638389\n",
            "Batch #200 Loss: 0.5692712458968162\n",
            "Batch #300 Loss: 0.5873920106887818\n",
            "\u001b[92mTrain accuracy: 39628/48000 =  82.56 % ||| loss 0.4622359573841095\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9914/12000 =  82.62 % ||| loss 0.4636838436126709\u001b[0m\n",
            "\u001b[92mTest accuracy: 8169/10000 =  81.69 % ||| loss 0.4876648485660553\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.5602410987019539\n",
            "Batch #200 Loss: 0.5552871766686439\n",
            "Batch #300 Loss: 0.5450142300128937\n",
            "\u001b[92mTrain accuracy: 39658/48000 =  82.62 % ||| loss 0.45261311531066895\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9952/12000 =  82.93 % ||| loss 0.4581427574157715\u001b[0m\n",
            "\u001b[92mTest accuracy: 8192/10000 =  81.92 % ||| loss 0.4773789942264557\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5557476398348808\n",
            "Batch #200 Loss: 0.545198038816452\n",
            "Batch #300 Loss: 0.5480852329730987\n",
            "\u001b[92mTrain accuracy: 39133/48000 =  81.53 % ||| loss 0.46130892634391785\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9774/12000 =  81.45 % ||| loss 0.46442410349845886\u001b[0m\n",
            "\u001b[92mTest accuracy: 8050/10000 =  80.5 % ||| loss 0.48998531699180603\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5309377771615982\n",
            "Batch #200 Loss: 0.5412501713633537\n",
            "Batch #300 Loss: 0.5264602744579315\n",
            "\u001b[92mTrain accuracy: 40288/48000 =  83.93 % ||| loss 0.4325167238712311\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10081/12000 =  84.01 % ||| loss 0.4369419515132904\u001b[0m\n",
            "\u001b[92mTest accuracy: 8273/10000 =  82.73 % ||| loss 0.4597441852092743\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.5189834234118461\n",
            "Batch #200 Loss: 0.5254595845937728\n",
            "Batch #300 Loss: 0.5102881360054016\n",
            "\u001b[92mTrain accuracy: 40280/48000 =  83.92 % ||| loss 0.4274253845214844\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10053/12000 =  83.78 % ||| loss 0.43369269371032715\u001b[0m\n",
            "\u001b[92mTest accuracy: 8275/10000 =  82.75 % ||| loss 0.46099185943603516\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5203350374102592\n",
            "Batch #200 Loss: 0.5173299396038056\n",
            "Batch #300 Loss: 0.5188175028562546\n",
            "\u001b[92mTrain accuracy: 39925/48000 =  83.18 % ||| loss 0.42693212628364563\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9916/12000 =  82.63 % ||| loss 0.4338279962539673\u001b[0m\n",
            "\u001b[92mTest accuracy: 8226/10000 =  82.26 % ||| loss 0.4555513262748718\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.4914768362045288\n",
            "Batch #200 Loss: 0.4964421653747559\n",
            "Batch #300 Loss: 0.5196745035052299\n",
            "\u001b[92mTrain accuracy: 40639/48000 =  84.66 % ||| loss 0.40998876094818115\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10131/12000 =  84.42 % ||| loss 0.4153200387954712\u001b[0m\n",
            "\u001b[92mTest accuracy: 8335/10000 =  83.35 % ||| loss 0.43958237767219543\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.49273885756731034\n",
            "Batch #200 Loss: 0.501718507707119\n",
            "Batch #300 Loss: 0.48262462735176087\n",
            "\u001b[92mTrain accuracy: 40946/48000 =  85.3 % ||| loss 0.4079830050468445\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10233/12000 =  85.28 % ||| loss 0.41627010703086853\u001b[0m\n",
            "\u001b[92mTest accuracy: 8423/10000 =  84.23 % ||| loss 0.43229714035987854\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.4932138335704803\n",
            "Batch #200 Loss: 0.47522193640470506\n",
            "Batch #300 Loss: 0.479303865134716\n",
            "\u001b[92mTrain accuracy: 41330/48000 =  86.1 % ||| loss 0.3889230489730835\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10298/12000 =  85.82 % ||| loss 0.3996426463127136\u001b[0m\n",
            "\u001b[92mTest accuracy: 8473/10000 =  84.73 % ||| loss 0.4205034375190735\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.48103142708539964\n",
            "Batch #200 Loss: 0.481230431497097\n",
            "Batch #300 Loss: 0.4807338297367096\n",
            "\u001b[92mTrain accuracy: 41273/48000 =  85.99 % ||| loss 0.37994128465652466\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10302/12000 =  85.85 % ||| loss 0.38673022389411926\u001b[0m\n",
            "\u001b[92mTest accuracy: 8467/10000 =  84.67 % ||| loss 0.4098239839076996\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.4703897887468338\n",
            "Batch #200 Loss: 0.4754677951335907\n",
            "Batch #300 Loss: 0.46262900322675704\n",
            "\u001b[92mTrain accuracy: 41505/48000 =  86.47 % ||| loss 0.3735467195510864\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10327/12000 =  86.06 % ||| loss 0.3842206597328186\u001b[0m\n",
            "\u001b[92mTest accuracy: 8517/10000 =  85.17 % ||| loss 0.4049733579158783\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.46047368735074995\n",
            "Batch #200 Loss: 0.45453759133815763\n",
            "Batch #300 Loss: 0.45794760942459106\n",
            "\u001b[92mTrain accuracy: 41284/48000 =  86.01 % ||| loss 0.3711926341056824\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10296/12000 =  85.8 % ||| loss 0.3801325559616089\u001b[0m\n",
            "\u001b[92mTest accuracy: 8456/10000 =  84.56 % ||| loss 0.4038352966308594\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.4453090026974678\n",
            "Batch #200 Loss: 0.4565475386381149\n",
            "Batch #300 Loss: 0.4538093444705009\n",
            "\u001b[92mTrain accuracy: 41804/48000 =  87.09 % ||| loss 0.3566896617412567\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10376/12000 =  86.47 % ||| loss 0.3687146008014679\u001b[0m\n",
            "\u001b[92mTest accuracy: 8594/10000 =  85.94 % ||| loss 0.39102640748023987\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.44161642909049986\n",
            "Batch #200 Loss: 0.43706709653139114\n",
            "Batch #300 Loss: 0.4471940115094185\n",
            "\u001b[92mTrain accuracy: 41930/48000 =  87.35 % ||| loss 0.34360745549201965\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10422/12000 =  86.85 % ||| loss 0.35743555426597595\u001b[0m\n",
            "\u001b[92mTest accuracy: 8579/10000 =  85.79 % ||| loss 0.37934625148773193\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.44647157430648804\n",
            "Batch #200 Loss: 0.42941228330135345\n",
            "Batch #300 Loss: 0.44104053735733034\n",
            "\u001b[92mTrain accuracy: 41985/48000 =  87.47 % ||| loss 0.3370966911315918\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10436/12000 =  86.97 % ||| loss 0.350852370262146\u001b[0m\n",
            "\u001b[92mTest accuracy: 8603/10000 =  86.03 % ||| loss 0.37271279096603394\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.42885974049568176\n",
            "Batch #200 Loss: 0.428064204454422\n",
            "Batch #300 Loss: 0.4304750409722328\n",
            "\u001b[92mTrain accuracy: 41726/48000 =  86.93 % ||| loss 0.35470038652420044\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10376/12000 =  86.47 % ||| loss 0.3698435425758362\u001b[0m\n",
            "\u001b[92mTest accuracy: 8547/10000 =  85.47 % ||| loss 0.39568081498146057\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_17</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_17' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_17</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_105348-Lenet5Dropout_1726150070.105228_18</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_18' target=\"_blank\">Lenet5Dropout_1726150070.105228_18</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_18' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_18</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3031236481666566\n",
            "Batch #200 Loss: 2.3046129631996153\n",
            "Batch #300 Loss: 2.304068639278412\n",
            "\u001b[92mTrain accuracy: 4268/48000 =  8.892 % ||| loss 2.3024966716766357\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1101/12000 =  9.175 % ||| loss 2.302544116973877\u001b[0m\n",
            "\u001b[92mTest accuracy: 895/10000 =  8.95 % ||| loss 2.302701711654663\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3027929067611694\n",
            "Batch #200 Loss: 2.302429475784302\n",
            "Batch #300 Loss: 2.3015481305122374\n",
            "\u001b[92mTrain accuracy: 4551/48000 =  9.481 % ||| loss 2.3008577823638916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1158/12000 =  9.65 % ||| loss 2.3008668422698975\u001b[0m\n",
            "\u001b[92mTest accuracy: 950/10000 =  9.5 % ||| loss 2.301159143447876\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.300244855880737\n",
            "Batch #200 Loss: 2.3012944936752318\n",
            "Batch #300 Loss: 2.3002477383613584\n",
            "\u001b[92mTrain accuracy: 5339/48000 =  11.12 % ||| loss 2.299036979675293\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1375/12000 =  11.46 % ||| loss 2.2990469932556152\u001b[0m\n",
            "\u001b[92mTest accuracy: 1103/10000 =  11.03 % ||| loss 2.2991273403167725\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.299059069156647\n",
            "Batch #200 Loss: 2.2990506052970887\n",
            "Batch #300 Loss: 2.298617675304413\n",
            "\u001b[92mTrain accuracy: 6767/48000 =  14.1 % ||| loss 2.2970495223999023\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1744/12000 =  14.53 % ||| loss 2.2970030307769775\u001b[0m\n",
            "\u001b[92mTest accuracy: 1406/10000 =  14.06 % ||| loss 2.297353744506836\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.297442288398743\n",
            "Batch #200 Loss: 2.2973238039016723\n",
            "Batch #300 Loss: 2.295904190540314\n",
            "\u001b[92mTrain accuracy: 7566/48000 =  15.76 % ||| loss 2.2948102951049805\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1924/12000 =  16.03 % ||| loss 2.294761896133423\u001b[0m\n",
            "\u001b[92mTest accuracy: 1569/10000 =  15.69 % ||| loss 2.2949624061584473\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.295046350955963\n",
            "Batch #200 Loss: 2.2946013045310973\n",
            "Batch #300 Loss: 2.294131817817688\n",
            "\u001b[92mTrain accuracy: 7847/48000 =  16.35 % ||| loss 2.2921688556671143\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1982/12000 =  16.52 % ||| loss 2.2920544147491455\u001b[0m\n",
            "\u001b[92mTest accuracy: 1632/10000 =  16.32 % ||| loss 2.292405366897583\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.293150429725647\n",
            "Batch #200 Loss: 2.291589891910553\n",
            "Batch #300 Loss: 2.290333273410797\n",
            "\u001b[92mTrain accuracy: 8054/48000 =  16.78 % ||| loss 2.2888855934143066\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2041/12000 =  17.01 % ||| loss 2.288726568222046\u001b[0m\n",
            "\u001b[92mTest accuracy: 1659/10000 =  16.59 % ||| loss 2.2887277603149414\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.289697434902191\n",
            "Batch #200 Loss: 2.2883390092849734\n",
            "Batch #300 Loss: 2.287520263195038\n",
            "\u001b[92mTrain accuracy: 8196/48000 =  17.08 % ||| loss 2.2846438884735107\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2088/12000 =  17.4 % ||| loss 2.2844200134277344\u001b[0m\n",
            "\u001b[92mTest accuracy: 1686/10000 =  16.86 % ||| loss 2.2846434116363525\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.285246057510376\n",
            "Batch #200 Loss: 2.284055745601654\n",
            "Batch #300 Loss: 2.2822508716583254\n",
            "\u001b[92mTrain accuracy: 8323/48000 =  17.34 % ||| loss 2.2787396907806396\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2144/12000 =  17.87 % ||| loss 2.2784249782562256\u001b[0m\n",
            "\u001b[92mTest accuracy: 1716/10000 =  17.16 % ||| loss 2.2787368297576904\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.279250769615173\n",
            "Batch #200 Loss: 2.277520761489868\n",
            "Batch #300 Loss: 2.2751530766487122\n",
            "\u001b[92mTrain accuracy: 8420/48000 =  17.54 % ||| loss 2.2704646587371826\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2165/12000 =  18.04 % ||| loss 2.269996404647827\u001b[0m\n",
            "\u001b[92mTest accuracy: 1751/10000 =  17.51 % ||| loss 2.2705371379852295\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.2703826093673705\n",
            "Batch #200 Loss: 2.2679573345184325\n",
            "Batch #300 Loss: 2.2656479382514956\n",
            "\u001b[92mTrain accuracy: 8462/48000 =  17.63 % ||| loss 2.2578375339508057\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2173/12000 =  18.11 % ||| loss 2.2571263313293457\u001b[0m\n",
            "\u001b[92mTest accuracy: 1765/10000 =  17.65 % ||| loss 2.2579264640808105\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.2595778036117555\n",
            "Batch #200 Loss: 2.2533149194717406\n",
            "Batch #300 Loss: 2.247168800830841\n",
            "\u001b[92mTrain accuracy: 8474/48000 =  17.65 % ||| loss 2.2369234561920166\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2185/12000 =  18.21 % ||| loss 2.235848903656006\u001b[0m\n",
            "\u001b[92mTest accuracy: 1780/10000 =  17.8 % ||| loss 2.2373244762420654\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.2363970851898194\n",
            "Batch #200 Loss: 2.230608735084534\n",
            "Batch #300 Loss: 2.218581938743591\n",
            "\u001b[92mTrain accuracy: 10262/48000 =  21.38 % ||| loss 2.1991467475891113\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2659/12000 =  22.16 % ||| loss 2.1974620819091797\u001b[0m\n",
            "\u001b[92mTest accuracy: 2162/10000 =  21.62 % ||| loss 2.1992154121398926\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.1988020181655883\n",
            "Batch #200 Loss: 2.1799525451660156\n",
            "Batch #300 Loss: 2.1642422246932984\n",
            "\u001b[92mTrain accuracy: 12865/48000 =  26.8 % ||| loss 2.1228973865509033\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3290/12000 =  27.42 % ||| loss 2.1201980113983154\u001b[0m\n",
            "\u001b[92mTest accuracy: 2712/10000 =  27.12 % ||| loss 2.1230106353759766\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.1194866824150087\n",
            "Batch #200 Loss: 2.0859693670272828\n",
            "Batch #300 Loss: 2.045636441707611\n",
            "\u001b[92mTrain accuracy: 18059/48000 =  37.62 % ||| loss 1.9575231075286865\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4585/12000 =  38.21 % ||| loss 1.9531490802764893\u001b[0m\n",
            "\u001b[92mTest accuracy: 3792/10000 =  37.92 % ||| loss 1.9571006298065186\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 1.9492632174491882\n",
            "Batch #200 Loss: 1.8957795584201813\n",
            "Batch #300 Loss: 1.8249156868457794\n",
            "\u001b[92mTrain accuracy: 19930/48000 =  41.52 % ||| loss 1.6775157451629639\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5018/12000 =  41.82 % ||| loss 1.6724426746368408\u001b[0m\n",
            "\u001b[92mTest accuracy: 4141/10000 =  41.41 % ||| loss 1.6804150342941284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 1.7094875407218932\n",
            "Batch #200 Loss: 1.6469395911693574\n",
            "Batch #300 Loss: 1.584955359697342\n",
            "\u001b[92mTrain accuracy: 24926/48000 =  51.93 % ||| loss 1.4237207174301147\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6245/12000 =  52.04 % ||| loss 1.4188857078552246\u001b[0m\n",
            "\u001b[92mTest accuracy: 5212/10000 =  52.12 % ||| loss 1.4280366897583008\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 1.5037062931060792\n",
            "Batch #200 Loss: 1.4582313215732574\n",
            "Batch #300 Loss: 1.4236561810970307\n",
            "\u001b[92mTrain accuracy: 26274/48000 =  54.74 % ||| loss 1.2644412517547607\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6579/12000 =  54.83 % ||| loss 1.2596869468688965\u001b[0m\n",
            "\u001b[92mTest accuracy: 5461/10000 =  54.61 % ||| loss 1.2689646482467651\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 1.3697624552249907\n",
            "Batch #200 Loss: 1.3427824640274049\n",
            "Batch #300 Loss: 1.3195609056949615\n",
            "\u001b[92mTrain accuracy: 27253/48000 =  56.78 % ||| loss 1.1651405096054077\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6781/12000 =  56.51 % ||| loss 1.160530686378479\u001b[0m\n",
            "\u001b[92mTest accuracy: 5682/10000 =  56.82 % ||| loss 1.1754480600357056\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 1.277470828294754\n",
            "Batch #200 Loss: 1.2554083251953125\n",
            "Batch #300 Loss: 1.235857127904892\n",
            "\u001b[92mTrain accuracy: 28448/48000 =  59.27 % ||| loss 1.0972306728363037\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7085/12000 =  59.04 % ||| loss 1.0923247337341309\u001b[0m\n",
            "\u001b[92mTest accuracy: 5960/10000 =  59.6 % ||| loss 1.1070021390914917\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 1.2043048655986786\n",
            "Batch #200 Loss: 1.2048592352867127\n",
            "Batch #300 Loss: 1.1709450483322144\n",
            "\u001b[92mTrain accuracy: 28877/48000 =  60.16 % ||| loss 1.0498743057250977\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7196/12000 =  59.97 % ||| loss 1.0449564456939697\u001b[0m\n",
            "\u001b[92mTest accuracy: 5999/10000 =  59.99 % ||| loss 1.0579570531845093\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 1.1594431626796722\n",
            "Batch #200 Loss: 1.1375273269414903\n",
            "Batch #300 Loss: 1.1392262107133866\n",
            "\u001b[92mTrain accuracy: 29470/48000 =  61.4 % ||| loss 1.0103892087936401\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7324/12000 =  61.03 % ||| loss 1.004716396331787\u001b[0m\n",
            "\u001b[92mTest accuracy: 6102/10000 =  61.02 % ||| loss 1.019261360168457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 1.1122580742836\n",
            "Batch #200 Loss: 1.0946425169706344\n",
            "Batch #300 Loss: 1.0998862564563752\n",
            "\u001b[92mTrain accuracy: 30385/48000 =  63.3 % ||| loss 0.9787564873695374\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7603/12000 =  63.36 % ||| loss 0.9728092551231384\u001b[0m\n",
            "\u001b[92mTest accuracy: 6245/10000 =  62.45 % ||| loss 0.9914935827255249\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 1.083260977268219\n",
            "Batch #200 Loss: 1.0672799569368363\n",
            "Batch #300 Loss: 1.0613046580553054\n",
            "\u001b[92mTrain accuracy: 31066/48000 =  64.72 % ||| loss 0.9524723291397095\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7744/12000 =  64.53 % ||| loss 0.9465594291687012\u001b[0m\n",
            "\u001b[92mTest accuracy: 6386/10000 =  63.86 % ||| loss 0.9672837257385254\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 1.0475762385129928\n",
            "Batch #200 Loss: 1.0501918137073516\n",
            "Batch #300 Loss: 1.0368120348453522\n",
            "\u001b[92mTrain accuracy: 31605/48000 =  65.84 % ||| loss 0.9274938702583313\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7913/12000 =  65.94 % ||| loss 0.9196045994758606\u001b[0m\n",
            "\u001b[92mTest accuracy: 6512/10000 =  65.12 % ||| loss 0.9388564825057983\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_18</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_18' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_18</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_105619-Lenet5Dropout_1726150070.105228_19</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_19' target=\"_blank\">Lenet5Dropout_1726150070.105228_19</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_19' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_19</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302646791934967\n",
            "Batch #200 Loss: 2.3021067070960997\n",
            "Batch #300 Loss: 2.302387368679047\n",
            "\u001b[92mTrain accuracy: 4670/48000 =  9.729 % ||| loss 2.300774335861206\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1179/12000 =  9.825 % ||| loss 2.3005387783050537\u001b[0m\n",
            "\u001b[92mTest accuracy: 977/10000 =  9.77 % ||| loss 2.3007845878601074\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.30093478679657\n",
            "Batch #200 Loss: 2.3007828330993654\n",
            "Batch #300 Loss: 2.3004507923126223\n",
            "\u001b[92mTrain accuracy: 4771/48000 =  9.94 % ||| loss 2.2987148761749268\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1209/12000 =  10.08 % ||| loss 2.298483371734619\u001b[0m\n",
            "\u001b[92mTest accuracy: 1003/10000 =  10.03 % ||| loss 2.2987449169158936\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.2988392019271853\n",
            "Batch #200 Loss: 2.298688242435455\n",
            "Batch #300 Loss: 2.2983074975013733\n",
            "\u001b[92mTrain accuracy: 4760/48000 =  9.917 % ||| loss 2.2965357303619385\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1202/12000 =  10.02 % ||| loss 2.29630970954895\u001b[0m\n",
            "\u001b[92mTest accuracy: 998/10000 =  9.98 % ||| loss 2.2965381145477295\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.297166769504547\n",
            "Batch #200 Loss: 2.296140367984772\n",
            "Batch #300 Loss: 2.2960682559013366\n",
            "\u001b[92mTrain accuracy: 4624/48000 =  9.633 % ||| loss 2.293886661529541\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1190/12000 =  9.917 % ||| loss 2.293652296066284\u001b[0m\n",
            "\u001b[92mTest accuracy: 975/10000 =  9.75 % ||| loss 2.2938406467437744\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.2947130489349363\n",
            "Batch #200 Loss: 2.293665452003479\n",
            "Batch #300 Loss: 2.293257415294647\n",
            "\u001b[92mTrain accuracy: 6147/48000 =  12.81 % ||| loss 2.2904152870178223\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1564/12000 =  13.03 % ||| loss 2.290199041366577\u001b[0m\n",
            "\u001b[92mTest accuracy: 1283/10000 =  12.83 % ||| loss 2.290309190750122\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.291200382709503\n",
            "Batch #200 Loss: 2.2911198687553407\n",
            "Batch #300 Loss: 2.2886002016067506\n",
            "\u001b[92mTrain accuracy: 8249/48000 =  17.19 % ||| loss 2.2857675552368164\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2072/12000 =  17.27 % ||| loss 2.2855722904205322\u001b[0m\n",
            "\u001b[92mTest accuracy: 1710/10000 =  17.1 % ||| loss 2.2859156131744385\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.2870630192756654\n",
            "Batch #200 Loss: 2.2858591866493225\n",
            "Batch #300 Loss: 2.2850813245773316\n",
            "\u001b[92mTrain accuracy: 8876/48000 =  18.49 % ||| loss 2.279508113861084\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2219/12000 =  18.49 % ||| loss 2.2793126106262207\u001b[0m\n",
            "\u001b[92mTest accuracy: 1824/10000 =  18.24 % ||| loss 2.279797077178955\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.2811125135421753\n",
            "Batch #200 Loss: 2.278975133895874\n",
            "Batch #300 Loss: 2.2778082489967346\n",
            "\u001b[92mTrain accuracy: 9435/48000 =  19.66 % ||| loss 2.2705435752868652\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2378/12000 =  19.82 % ||| loss 2.270376443862915\u001b[0m\n",
            "\u001b[92mTest accuracy: 1923/10000 =  19.23 % ||| loss 2.270503282546997\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.2736064076423643\n",
            "Batch #200 Loss: 2.2710687041282656\n",
            "Batch #300 Loss: 2.267289674282074\n",
            "\u001b[92mTrain accuracy: 10493/48000 =  21.86 % ||| loss 2.256826400756836\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2625/12000 =  21.88 % ||| loss 2.2567036151885986\u001b[0m\n",
            "\u001b[92mTest accuracy: 2134/10000 =  21.34 % ||| loss 2.2569990158081055\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.2591242480278013\n",
            "Batch #200 Loss: 2.2567338275909425\n",
            "Batch #300 Loss: 2.2515772461891173\n",
            "\u001b[92mTrain accuracy: 11860/48000 =  24.71 % ||| loss 2.2345235347747803\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2993/12000 =  24.94 % ||| loss 2.2344090938568115\u001b[0m\n",
            "\u001b[92mTest accuracy: 2477/10000 =  24.77 % ||| loss 2.2350964546203613\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.239833824634552\n",
            "Batch #200 Loss: 2.2335129880905153\n",
            "Batch #300 Loss: 2.2214764189720153\n",
            "\u001b[92mTrain accuracy: 13229/48000 =  27.56 % ||| loss 2.195436477661133\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3311/12000 =  27.59 % ||| loss 2.1954398155212402\u001b[0m\n",
            "\u001b[92mTest accuracy: 2744/10000 =  27.44 % ||| loss 2.1962833404541016\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.2030290937423707\n",
            "Batch #200 Loss: 2.1887929129600523\n",
            "Batch #300 Loss: 2.1715453100204467\n",
            "\u001b[92mTrain accuracy: 16347/48000 =  34.06 % ||| loss 2.119906187057495\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4105/12000 =  34.21 % ||| loss 2.1199328899383545\u001b[0m\n",
            "\u001b[92mTest accuracy: 3390/10000 =  33.9 % ||| loss 2.1199522018432617\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.130326292514801\n",
            "Batch #200 Loss: 2.100204520225525\n",
            "Batch #300 Loss: 2.0699959588050842\n",
            "\u001b[92mTrain accuracy: 19915/48000 =  41.49 % ||| loss 1.9671928882598877\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4958/12000 =  41.32 % ||| loss 1.9673314094543457\u001b[0m\n",
            "\u001b[92mTest accuracy: 4158/10000 =  41.58 % ||| loss 1.9682161808013916\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 1.992713282108307\n",
            "Batch #200 Loss: 1.9369771730899812\n",
            "Batch #300 Loss: 1.8862560343742372\n",
            "\u001b[92mTrain accuracy: 22201/48000 =  46.25 % ||| loss 1.7271391153335571\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5558/12000 =  46.32 % ||| loss 1.7267675399780273\u001b[0m\n",
            "\u001b[92mTest accuracy: 4637/10000 =  46.37 % ||| loss 1.7297297716140747\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 1.7908929455280305\n",
            "Batch #200 Loss: 1.730612680912018\n",
            "Batch #300 Loss: 1.686306574344635\n",
            "\u001b[92mTrain accuracy: 25030/48000 =  52.15 % ||| loss 1.5033907890319824\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6223/12000 =  51.86 % ||| loss 1.5021896362304688\u001b[0m\n",
            "\u001b[92mTest accuracy: 5214/10000 =  52.14 % ||| loss 1.5081870555877686\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 1.6093256437778474\n",
            "Batch #200 Loss: 1.5666799008846284\n",
            "Batch #300 Loss: 1.5370390164852141\n",
            "\u001b[92mTrain accuracy: 26162/48000 =  54.5 % ||| loss 1.340839147567749\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6514/12000 =  54.28 % ||| loss 1.3389391899108887\u001b[0m\n",
            "\u001b[92mTest accuracy: 5463/10000 =  54.63 % ||| loss 1.3457908630371094\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 1.4877187740802764\n",
            "Batch #200 Loss: 1.4507659566402435\n",
            "Batch #300 Loss: 1.4144357740879059\n",
            "\u001b[92mTrain accuracy: 26734/48000 =  55.7 % ||| loss 1.2276784181594849\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6662/12000 =  55.52 % ||| loss 1.2250628471374512\u001b[0m\n",
            "\u001b[92mTest accuracy: 5580/10000 =  55.8 % ||| loss 1.2334139347076416\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 1.3771974289417266\n",
            "Batch #200 Loss: 1.3589991092681886\n",
            "Batch #300 Loss: 1.3293307685852052\n",
            "\u001b[92mTrain accuracy: 27431/48000 =  57.15 % ||| loss 1.1506969928741455\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6831/12000 =  56.93 % ||| loss 1.1480045318603516\u001b[0m\n",
            "\u001b[92mTest accuracy: 5702/10000 =  57.02 % ||| loss 1.1585502624511719\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 1.3081809198856353\n",
            "Batch #200 Loss: 1.2895491707324982\n",
            "Batch #300 Loss: 1.269780865907669\n",
            "\u001b[92mTrain accuracy: 28393/48000 =  59.15 % ||| loss 1.095998764038086\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7081/12000 =  59.01 % ||| loss 1.0921348333358765\u001b[0m\n",
            "\u001b[92mTest accuracy: 5888/10000 =  58.88 % ||| loss 1.1040194034576416\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 1.242359187602997\n",
            "Batch #200 Loss: 1.2284964215755463\n",
            "Batch #300 Loss: 1.2108840620517731\n",
            "\u001b[92mTrain accuracy: 28925/48000 =  60.26 % ||| loss 1.0565431118011475\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7210/12000 =  60.08 % ||| loss 1.0522435903549194\u001b[0m\n",
            "\u001b[92mTest accuracy: 5994/10000 =  59.94 % ||| loss 1.0649442672729492\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 1.193321280479431\n",
            "Batch #200 Loss: 1.1990272176265717\n",
            "Batch #300 Loss: 1.1788174533843994\n",
            "\u001b[92mTrain accuracy: 29509/48000 =  61.48 % ||| loss 1.0257004499435425\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7376/12000 =  61.47 % ||| loss 1.0205450057983398\u001b[0m\n",
            "\u001b[92mTest accuracy: 6116/10000 =  61.16 % ||| loss 1.0362975597381592\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 1.1682333129644393\n",
            "Batch #200 Loss: 1.1718469226360322\n",
            "Batch #300 Loss: 1.1473453259468078\n",
            "\u001b[92mTrain accuracy: 29906/48000 =  62.3 % ||| loss 1.0001124143600464\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7462/12000 =  62.18 % ||| loss 0.9949495792388916\u001b[0m\n",
            "\u001b[92mTest accuracy: 6169/10000 =  61.69 % ||| loss 1.0107306241989136\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 1.1403258752822876\n",
            "Batch #200 Loss: 1.132102215886116\n",
            "Batch #300 Loss: 1.1242600184679032\n",
            "\u001b[92mTrain accuracy: 29950/48000 =  62.4 % ||| loss 0.9797694683074951\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7493/12000 =  62.44 % ||| loss 0.97374427318573\u001b[0m\n",
            "\u001b[92mTest accuracy: 6181/10000 =  61.81 % ||| loss 0.9908943772315979\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 1.1151934790611266\n",
            "Batch #200 Loss: 1.0953406596183777\n",
            "Batch #300 Loss: 1.0958616119623183\n",
            "\u001b[92mTrain accuracy: 30528/48000 =  63.6 % ||| loss 0.9567746520042419\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7620/12000 =  63.5 % ||| loss 0.9498980045318604\u001b[0m\n",
            "\u001b[92mTest accuracy: 6304/10000 =  63.04 % ||| loss 0.9738931655883789\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 1.0886697566509247\n",
            "Batch #200 Loss: 1.0816607081890106\n",
            "Batch #300 Loss: 1.0776441377401351\n",
            "\u001b[92mTrain accuracy: 30691/48000 =  63.94 % ||| loss 0.9382872581481934\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7675/12000 =  63.96 % ||| loss 0.9311732649803162\u001b[0m\n",
            "\u001b[92mTest accuracy: 6343/10000 =  63.43 % ||| loss 0.9539900422096252\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_19</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_19' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_19</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_105849-Lenet5Dropout_1726150070.105228_20</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_20' target=\"_blank\">Lenet5Dropout_1726150070.105228_20</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_20' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_20</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.305018491744995\n",
            "Batch #200 Loss: 2.3065198707580565\n",
            "Batch #300 Loss: 2.3051770520210266\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.304286241531372\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3043699264526367\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.304431676864624\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.305083236694336\n",
            "Batch #200 Loss: 2.304404218196869\n",
            "Batch #300 Loss: 2.303911347389221\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3034567832946777\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3035645484924316\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303480863571167\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3040161156654357\n",
            "Batch #200 Loss: 2.303719527721405\n",
            "Batch #300 Loss: 2.302600462436676\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302675724029541\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3027751445770264\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025059700012207\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3019549465179443\n",
            "Batch #200 Loss: 2.3032949209213256\n",
            "Batch #300 Loss: 2.303252007961273\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.301896810531616\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.301981210708618\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3017797470092773\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3020356726646423\n",
            "Batch #200 Loss: 2.3024563550949098\n",
            "Batch #300 Loss: 2.3017741942405703\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3010752201080322\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3011634349823\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3007287979125977\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3009236884117126\n",
            "Batch #200 Loss: 2.3021231508255005\n",
            "Batch #300 Loss: 2.3008689308166503\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3001999855041504\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3002734184265137\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3003697395324707\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.3005360412597655\n",
            "Batch #200 Loss: 2.299955701828003\n",
            "Batch #300 Loss: 2.3003030800819397\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.299262762069702\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.2993438243865967\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.299285888671875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.3000023436546324\n",
            "Batch #200 Loss: 2.300061776638031\n",
            "Batch #300 Loss: 2.29856255531311\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.298245429992676\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.2983312606811523\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2982711791992188\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.2983577179908754\n",
            "Batch #200 Loss: 2.2981327986717224\n",
            "Batch #300 Loss: 2.2982342720031737\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.2970800399780273\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.297152042388916\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2972612380981445\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.2979125356674195\n",
            "Batch #200 Loss: 2.296984543800354\n",
            "Batch #300 Loss: 2.296879518032074\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.2957746982574463\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.295825481414795\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.295457363128662\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.295572180747986\n",
            "Batch #200 Loss: 2.296318066120148\n",
            "Batch #300 Loss: 2.2955619263648988\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.294217824935913\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.294252395629883\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2941792011260986\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.2952781987190245\n",
            "Batch #200 Loss: 2.2938737058639527\n",
            "Batch #300 Loss: 2.294076874256134\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.2923243045806885\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.2923495769500732\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.292400598526001\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.2934881949424746\n",
            "Batch #200 Loss: 2.2921336817741396\n",
            "Batch #300 Loss: 2.2919632172584534\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.2899911403656006\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.2899904251098633\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2900545597076416\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.2901867175102235\n",
            "Batch #200 Loss: 2.290374507904053\n",
            "Batch #300 Loss: 2.2886822628974914\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.2869791984558105\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.2869443893432617\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2871270179748535\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.2887764835357665\n",
            "Batch #200 Loss: 2.2873737454414367\n",
            "Batch #300 Loss: 2.286008698940277\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.2831454277038574\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.283071756362915\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2833755016326904\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.284778380393982\n",
            "Batch #200 Loss: 2.282989270687103\n",
            "Batch #300 Loss: 2.2813074827194213\n",
            "\u001b[92mTrain accuracy: 4849/48000 =  10.1 % ||| loss 2.2779457569122314\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1201/12000 =  10.01 % ||| loss 2.2778260707855225\u001b[0m\n",
            "\u001b[92mTest accuracy: 1009/10000 =  10.09 % ||| loss 2.2780282497406006\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.279160099029541\n",
            "Batch #200 Loss: 2.2783021783828734\n",
            "Batch #300 Loss: 2.2766796731948853\n",
            "\u001b[92mTrain accuracy: 7472/48000 =  15.57 % ||| loss 2.270613670349121\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1880/12000 =  15.67 % ||| loss 2.2704532146453857\u001b[0m\n",
            "\u001b[92mTest accuracy: 1565/10000 =  15.65 % ||| loss 2.2708775997161865\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.274378290176392\n",
            "Batch #200 Loss: 2.271290240287781\n",
            "Batch #300 Loss: 2.268298909664154\n",
            "\u001b[92mTrain accuracy: 10347/48000 =  21.56 % ||| loss 2.26005220413208\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2601/12000 =  21.68 % ||| loss 2.2598249912261963\u001b[0m\n",
            "\u001b[92mTest accuracy: 2176/10000 =  21.76 % ||| loss 2.260272741317749\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.2634843230247497\n",
            "Batch #200 Loss: 2.261411883831024\n",
            "Batch #300 Loss: 2.2566702938079835\n",
            "\u001b[92mTrain accuracy: 11762/48000 =  24.5 % ||| loss 2.2442972660064697\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2971/12000 =  24.76 % ||| loss 2.243964672088623\u001b[0m\n",
            "\u001b[92mTest accuracy: 2463/10000 =  24.63 % ||| loss 2.2441482543945312\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.2515823912620543\n",
            "Batch #200 Loss: 2.24564715385437\n",
            "Batch #300 Loss: 2.2385235357284547\n",
            "\u001b[92mTrain accuracy: 12509/48000 =  26.06 % ||| loss 2.219611406326294\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3127/12000 =  26.06 % ||| loss 2.2191314697265625\u001b[0m\n",
            "\u001b[92mTest accuracy: 2612/10000 =  26.12 % ||| loss 2.2198779582977295\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.230267779827118\n",
            "Batch #200 Loss: 2.217307231426239\n",
            "Batch #300 Loss: 2.211867940425873\n",
            "\u001b[92mTrain accuracy: 12414/48000 =  25.86 % ||| loss 2.1774837970733643\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3112/12000 =  25.93 % ||| loss 2.1767046451568604\u001b[0m\n",
            "\u001b[92mTest accuracy: 2588/10000 =  25.88 % ||| loss 2.177870273590088\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.1898225212097167\n",
            "Batch #200 Loss: 2.172306706905365\n",
            "Batch #300 Loss: 2.158936154842377\n",
            "\u001b[92mTrain accuracy: 13213/48000 =  27.53 % ||| loss 2.098418951034546\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3320/12000 =  27.67 % ||| loss 2.0972163677215576\u001b[0m\n",
            "\u001b[92mTest accuracy: 2740/10000 =  27.4 % ||| loss 2.0991451740264893\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.1199153566360476\n",
            "Batch #200 Loss: 2.0913800621032714\n",
            "Batch #300 Loss: 2.058252030611038\n",
            "\u001b[92mTrain accuracy: 13847/48000 =  28.85 % ||| loss 1.958140254020691\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3476/12000 =  28.97 % ||| loss 1.9559167623519897\u001b[0m\n",
            "\u001b[92mTest accuracy: 2882/10000 =  28.82 % ||| loss 1.9599781036376953\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.009085999727249\n",
            "Batch #200 Loss: 1.9641397070884705\n",
            "Batch #300 Loss: 1.926435910463333\n",
            "\u001b[92mTrain accuracy: 15763/48000 =  32.84 % ||| loss 1.7819257974624634\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3983/12000 =  33.19 % ||| loss 1.7786734104156494\u001b[0m\n",
            "\u001b[92mTest accuracy: 3273/10000 =  32.73 % ||| loss 1.7848153114318848\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 1.860751746892929\n",
            "Batch #200 Loss: 1.8297794699668883\n",
            "Batch #300 Loss: 1.7950078284740447\n",
            "\u001b[92mTrain accuracy: 17878/48000 =  37.25 % ||| loss 1.6221303939819336\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4544/12000 =  37.87 % ||| loss 1.6186281442642212\u001b[0m\n",
            "\u001b[92mTest accuracy: 3735/10000 =  37.35 % ||| loss 1.6232481002807617\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_20</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_20' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_20</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_110122-Lenet5Dropout_1726150070.105228_21</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_21' target=\"_blank\">Lenet5Dropout_1726150070.105228_21</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_21' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_21</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3043922781944275\n",
            "Batch #200 Loss: 2.300562553405762\n",
            "Batch #300 Loss: 2.295116183757782\n",
            "\u001b[92mTrain accuracy: 8631/48000 =  17.98 % ||| loss 2.2861557006835938\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2099/12000 =  17.49 % ||| loss 2.2861387729644775\u001b[0m\n",
            "\u001b[92mTest accuracy: 1785/10000 =  17.85 % ||| loss 2.2859342098236084\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.281436975002289\n",
            "Batch #200 Loss: 2.2571098279953\n",
            "Batch #300 Loss: 2.1772727179527283\n",
            "\u001b[92mTrain accuracy: 24016/48000 =  50.03 % ||| loss 1.6069906949996948\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5988/12000 =  49.9 % ||| loss 1.6079474687576294\u001b[0m\n",
            "\u001b[92mTest accuracy: 5012/10000 =  50.12 % ||| loss 1.607662558555603\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.4430294942855835\n",
            "Batch #200 Loss: 1.179949425458908\n",
            "Batch #300 Loss: 1.0863630598783494\n",
            "\u001b[92mTrain accuracy: 31180/48000 =  64.96 % ||| loss 0.9138348698616028\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7813/12000 =  65.11 % ||| loss 0.9058757424354553\u001b[0m\n",
            "\u001b[92mTest accuracy: 6415/10000 =  64.15 % ||| loss 0.9250815510749817\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.9794737952947616\n",
            "Batch #200 Loss: 0.9494232732057571\n",
            "Batch #300 Loss: 0.8971730327606201\n",
            "\u001b[92mTrain accuracy: 33367/48000 =  69.51 % ||| loss 0.783743679523468\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8419/12000 =  70.16 % ||| loss 0.7711211442947388\u001b[0m\n",
            "\u001b[92mTest accuracy: 6893/10000 =  68.93 % ||| loss 0.8024041652679443\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.8595836848020554\n",
            "Batch #200 Loss: 0.8265554583072663\n",
            "Batch #300 Loss: 0.8208336335420608\n",
            "\u001b[92mTrain accuracy: 34622/48000 =  72.13 % ||| loss 0.7318958044052124\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8726/12000 =  72.72 % ||| loss 0.7190103530883789\u001b[0m\n",
            "\u001b[92mTest accuracy: 7169/10000 =  71.69 % ||| loss 0.7430405020713806\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.7809365427494049\n",
            "Batch #200 Loss: 0.7682436537742615\n",
            "Batch #300 Loss: 0.7681093043088913\n",
            "\u001b[92mTrain accuracy: 35285/48000 =  73.51 % ||| loss 0.6864202618598938\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8857/12000 =  73.81 % ||| loss 0.6776922941207886\u001b[0m\n",
            "\u001b[92mTest accuracy: 7259/10000 =  72.59 % ||| loss 0.7047776579856873\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.7292332953214645\n",
            "Batch #200 Loss: 0.7344536471366883\n",
            "Batch #300 Loss: 0.7207233327627182\n",
            "\u001b[92mTrain accuracy: 36103/48000 =  75.21 % ||| loss 0.6493238210678101\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9113/12000 =  75.94 % ||| loss 0.6369325518608093\u001b[0m\n",
            "\u001b[92mTest accuracy: 7492/10000 =  74.92 % ||| loss 0.6739271283149719\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.6948984342813492\n",
            "Batch #200 Loss: 0.7057522308826446\n",
            "Batch #300 Loss: 0.6992653077840805\n",
            "\u001b[92mTrain accuracy: 36303/48000 =  75.63 % ||| loss 0.6328004002571106\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9137/12000 =  76.14 % ||| loss 0.6229571104049683\u001b[0m\n",
            "\u001b[92mTest accuracy: 7511/10000 =  75.11 % ||| loss 0.653099775314331\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.6744222867488862\n",
            "Batch #200 Loss: 0.6735780245065689\n",
            "Batch #300 Loss: 0.6693026471138\n",
            "\u001b[92mTrain accuracy: 36745/48000 =  76.55 % ||| loss 0.6071109771728516\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9198/12000 =  76.65 % ||| loss 0.5984892845153809\u001b[0m\n",
            "\u001b[92mTest accuracy: 7568/10000 =  75.68 % ||| loss 0.6261636018753052\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.6479476910829544\n",
            "Batch #200 Loss: 0.6456319722533226\n",
            "Batch #300 Loss: 0.6645045107603074\n",
            "\u001b[92mTrain accuracy: 37366/48000 =  77.85 % ||| loss 0.5810494422912598\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9364/12000 =  78.03 % ||| loss 0.5707301497459412\u001b[0m\n",
            "\u001b[92mTest accuracy: 7728/10000 =  77.28 % ||| loss 0.6029486656188965\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6380498585104942\n",
            "Batch #200 Loss: 0.6335157132148743\n",
            "Batch #300 Loss: 0.6247791299223899\n",
            "\u001b[92mTrain accuracy: 37422/48000 =  77.96 % ||| loss 0.5622798800468445\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9382/12000 =  78.18 % ||| loss 0.5549405217170715\u001b[0m\n",
            "\u001b[92mTest accuracy: 7717/10000 =  77.17 % ||| loss 0.5860093235969543\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.6224960857629775\n",
            "Batch #200 Loss: 0.6092903625965118\n",
            "Batch #300 Loss: 0.6057905039191246\n",
            "\u001b[92mTrain accuracy: 37708/48000 =  78.56 % ||| loss 0.5470240712165833\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9444/12000 =  78.7 % ||| loss 0.538807213306427\u001b[0m\n",
            "\u001b[92mTest accuracy: 7783/10000 =  77.83 % ||| loss 0.568557858467102\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5917950814962387\n",
            "Batch #200 Loss: 0.5988731896877288\n",
            "Batch #300 Loss: 0.5911895790696144\n",
            "\u001b[92mTrain accuracy: 37830/48000 =  78.81 % ||| loss 0.5397307276725769\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9481/12000 =  79.01 % ||| loss 0.5335291624069214\u001b[0m\n",
            "\u001b[92mTest accuracy: 7779/10000 =  77.79 % ||| loss 0.5690581798553467\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.5818100732564926\n",
            "Batch #200 Loss: 0.5857473969459533\n",
            "Batch #300 Loss: 0.5794822981953621\n",
            "\u001b[92mTrain accuracy: 38184/48000 =  79.55 % ||| loss 0.530251681804657\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9586/12000 =  79.88 % ||| loss 0.5231372117996216\u001b[0m\n",
            "\u001b[92mTest accuracy: 7864/10000 =  78.64 % ||| loss 0.5523847937583923\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5604034143686295\n",
            "Batch #200 Loss: 0.574016483426094\n",
            "Batch #300 Loss: 0.5748383870720863\n",
            "\u001b[92mTrain accuracy: 38502/48000 =  80.21 % ||| loss 0.5076190233230591\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9662/12000 =  80.52 % ||| loss 0.5035595297813416\u001b[0m\n",
            "\u001b[92mTest accuracy: 7927/10000 =  79.27 % ||| loss 0.5337910652160645\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.5624917528033256\n",
            "Batch #200 Loss: 0.5597887739539147\n",
            "Batch #300 Loss: 0.5474204367399216\n",
            "\u001b[92mTrain accuracy: 38858/48000 =  80.95 % ||| loss 0.49693563580513\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9751/12000 =  81.26 % ||| loss 0.49369901418685913\u001b[0m\n",
            "\u001b[92mTest accuracy: 8046/10000 =  80.46 % ||| loss 0.5265159010887146\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5563653814792633\n",
            "Batch #200 Loss: 0.5416514667868614\n",
            "Batch #300 Loss: 0.5370571750402451\n",
            "\u001b[92mTrain accuracy: 39224/48000 =  81.72 % ||| loss 0.48080456256866455\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9840/12000 =  82.0 % ||| loss 0.47762760519981384\u001b[0m\n",
            "\u001b[92mTest accuracy: 8075/10000 =  80.75 % ||| loss 0.50857013463974\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5334077242016793\n",
            "Batch #200 Loss: 0.5373913249373437\n",
            "Batch #300 Loss: 0.5375885033607483\n",
            "\u001b[92mTrain accuracy: 39127/48000 =  81.51 % ||| loss 0.48144426941871643\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9822/12000 =  81.85 % ||| loss 0.47974181175231934\u001b[0m\n",
            "\u001b[92mTest accuracy: 8045/10000 =  80.45 % ||| loss 0.5127764940261841\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5311321264505386\n",
            "Batch #200 Loss: 0.5247496324777603\n",
            "Batch #300 Loss: 0.5148171439766884\n",
            "\u001b[92mTrain accuracy: 39625/48000 =  82.55 % ||| loss 0.4678114056587219\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9977/12000 =  83.14 % ||| loss 0.46594148874282837\u001b[0m\n",
            "\u001b[92mTest accuracy: 8134/10000 =  81.34 % ||| loss 0.4944474995136261\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.515466159582138\n",
            "Batch #200 Loss: 0.5244768106937409\n",
            "Batch #300 Loss: 0.5106381362676621\n",
            "\u001b[92mTrain accuracy: 39550/48000 =  82.4 % ||| loss 0.46441686153411865\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9938/12000 =  82.82 % ||| loss 0.46421554684638977\u001b[0m\n",
            "\u001b[92mTest accuracy: 8104/10000 =  81.04 % ||| loss 0.49195384979248047\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.5021906894445419\n",
            "Batch #200 Loss: 0.4998471614718437\n",
            "Batch #300 Loss: 0.5058275955915451\n",
            "\u001b[92mTrain accuracy: 40019/48000 =  83.37 % ||| loss 0.451090931892395\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10037/12000 =  83.64 % ||| loss 0.45092979073524475\u001b[0m\n",
            "\u001b[92mTest accuracy: 8233/10000 =  82.33 % ||| loss 0.48277413845062256\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.49538383215665815\n",
            "Batch #200 Loss: 0.4968138766288757\n",
            "Batch #300 Loss: 0.4963579472899437\n",
            "\u001b[92mTrain accuracy: 40016/48000 =  83.37 % ||| loss 0.44548848271369934\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10032/12000 =  83.6 % ||| loss 0.4470302164554596\u001b[0m\n",
            "\u001b[92mTest accuracy: 8246/10000 =  82.46 % ||| loss 0.4827772080898285\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5008963063359261\n",
            "Batch #200 Loss: 0.4848938709497452\n",
            "Batch #300 Loss: 0.48636759638786314\n",
            "\u001b[92mTrain accuracy: 40461/48000 =  84.29 % ||| loss 0.43254515528678894\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10113/12000 =  84.28 % ||| loss 0.43521440029144287\u001b[0m\n",
            "\u001b[92mTest accuracy: 8320/10000 =  83.2 % ||| loss 0.46243444085121155\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.49122214615345\n",
            "Batch #200 Loss: 0.4822435677051544\n",
            "Batch #300 Loss: 0.4884887048602104\n",
            "\u001b[92mTrain accuracy: 40184/48000 =  83.72 % ||| loss 0.43343114852905273\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10073/12000 =  83.94 % ||| loss 0.4332989752292633\u001b[0m\n",
            "\u001b[92mTest accuracy: 8282/10000 =  82.82 % ||| loss 0.46647268533706665\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.47561264902353284\n",
            "Batch #200 Loss: 0.4680211043357849\n",
            "Batch #300 Loss: 0.47461326092481615\n",
            "\u001b[92mTrain accuracy: 40652/48000 =  84.69 % ||| loss 0.42109057307243347\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10185/12000 =  84.88 % ||| loss 0.4239140748977661\u001b[0m\n",
            "\u001b[92mTest accuracy: 8327/10000 =  83.27 % ||| loss 0.44981640577316284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_21</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_21' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_21</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_110357-Lenet5Dropout_1726150070.105228_22</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_22' target=\"_blank\">Lenet5Dropout_1726150070.105228_22</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_22' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_22</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3004505181312562\n",
            "Batch #200 Loss: 2.295376741886139\n",
            "Batch #300 Loss: 2.2854590821266174\n",
            "\u001b[92mTrain accuracy: 21850/48000 =  45.52 % ||| loss 2.252627372741699\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5459/12000 =  45.49 % ||| loss 2.252657175064087\u001b[0m\n",
            "\u001b[92mTest accuracy: 4533/10000 =  45.33 % ||| loss 2.2525784969329834\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.224454417228699\n",
            "Batch #200 Loss: 1.989561220407486\n",
            "Batch #300 Loss: 1.5408701312541961\n",
            "\u001b[92mTrain accuracy: 27704/48000 =  57.72 % ||| loss 1.10164213180542\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6943/12000 =  57.86 % ||| loss 1.0967167615890503\u001b[0m\n",
            "\u001b[92mTest accuracy: 5768/10000 =  57.68 % ||| loss 1.114051103591919\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.2089000892639161\n",
            "Batch #200 Loss: 1.1299997854232788\n",
            "Batch #300 Loss: 1.0929117983579635\n",
            "\u001b[92mTrain accuracy: 31935/48000 =  66.53 % ||| loss 0.8976168632507324\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7986/12000 =  66.55 % ||| loss 0.8872408866882324\u001b[0m\n",
            "\u001b[92mTest accuracy: 6565/10000 =  65.65 % ||| loss 0.919320285320282\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.007323145866394\n",
            "Batch #200 Loss: 0.9772236424684525\n",
            "Batch #300 Loss: 0.9467528921365738\n",
            "\u001b[92mTrain accuracy: 33401/48000 =  69.59 % ||| loss 0.7896721959114075\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8412/12000 =  70.1 % ||| loss 0.7754524946212769\u001b[0m\n",
            "\u001b[92mTest accuracy: 6897/10000 =  68.97 % ||| loss 0.8039713501930237\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.9036498063802719\n",
            "Batch #200 Loss: 0.8893569397926331\n",
            "Batch #300 Loss: 0.8724742984771728\n",
            "\u001b[92mTrain accuracy: 33409/48000 =  69.6 % ||| loss 0.7588922381401062\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8427/12000 =  70.23 % ||| loss 0.7423639297485352\u001b[0m\n",
            "\u001b[92mTest accuracy: 6967/10000 =  69.67 % ||| loss 0.7698951363563538\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.8533655691146851\n",
            "Batch #200 Loss: 0.8451034504175187\n",
            "Batch #300 Loss: 0.8174260973930358\n",
            "\u001b[92mTrain accuracy: 35076/48000 =  73.08 % ||| loss 0.7087729573249817\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8765/12000 =  73.04 % ||| loss 0.6965228319168091\u001b[0m\n",
            "\u001b[92mTest accuracy: 7214/10000 =  72.14 % ||| loss 0.7263770699501038\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.8169086235761642\n",
            "Batch #200 Loss: 0.8093831884860992\n",
            "Batch #300 Loss: 0.7862306123971939\n",
            "\u001b[92mTrain accuracy: 35270/48000 =  73.48 % ||| loss 0.6874057054519653\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8832/12000 =  73.6 % ||| loss 0.6729978322982788\u001b[0m\n",
            "\u001b[92mTest accuracy: 7317/10000 =  73.17 % ||| loss 0.7103989720344543\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.7949802780151367\n",
            "Batch #200 Loss: 0.774887643456459\n",
            "Batch #300 Loss: 0.7514404463768005\n",
            "\u001b[92mTrain accuracy: 35503/48000 =  73.96 % ||| loss 0.6662683486938477\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8904/12000 =  74.2 % ||| loss 0.6493879556655884\u001b[0m\n",
            "\u001b[92mTest accuracy: 7354/10000 =  73.54 % ||| loss 0.6865963935852051\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.7602214729785919\n",
            "Batch #200 Loss: 0.7460089087486267\n",
            "Batch #300 Loss: 0.7308953815698623\n",
            "\u001b[92mTrain accuracy: 36282/48000 =  75.59 % ||| loss 0.6378055214881897\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9086/12000 =  75.72 % ||| loss 0.6225658059120178\u001b[0m\n",
            "\u001b[92mTest accuracy: 7511/10000 =  75.11 % ||| loss 0.6589784026145935\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.7317525130510331\n",
            "Batch #200 Loss: 0.7135796850919723\n",
            "Batch #300 Loss: 0.7140040320158004\n",
            "\u001b[92mTrain accuracy: 36759/48000 =  76.58 % ||| loss 0.6144079566001892\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9168/12000 =  76.4 % ||| loss 0.6028212904930115\u001b[0m\n",
            "\u001b[92mTest accuracy: 7553/10000 =  75.53 % ||| loss 0.6432870030403137\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6987191212177276\n",
            "Batch #200 Loss: 0.6938982611894607\n",
            "Batch #300 Loss: 0.7012045323848725\n",
            "\u001b[92mTrain accuracy: 37027/48000 =  77.14 % ||| loss 0.5984660983085632\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9240/12000 =  77.0 % ||| loss 0.5834353566169739\u001b[0m\n",
            "\u001b[92mTest accuracy: 7640/10000 =  76.4 % ||| loss 0.6184356212615967\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.6850748401880264\n",
            "Batch #200 Loss: 0.6587706685066224\n",
            "Batch #300 Loss: 0.6689243394136429\n",
            "\u001b[92mTrain accuracy: 37443/48000 =  78.01 % ||| loss 0.5726808309555054\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9367/12000 =  78.06 % ||| loss 0.5599825382232666\u001b[0m\n",
            "\u001b[92mTest accuracy: 7737/10000 =  77.37 % ||| loss 0.5939837098121643\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.6537973192334176\n",
            "Batch #200 Loss: 0.6566492199897767\n",
            "Batch #300 Loss: 0.6571874228119851\n",
            "\u001b[92mTrain accuracy: 37325/48000 =  77.76 % ||| loss 0.5717966556549072\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9372/12000 =  78.1 % ||| loss 0.56106036901474\u001b[0m\n",
            "\u001b[92mTest accuracy: 7694/10000 =  76.94 % ||| loss 0.5918760895729065\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.6408474633097648\n",
            "Batch #200 Loss: 0.6451375994086266\n",
            "Batch #300 Loss: 0.6328136143088341\n",
            "\u001b[92mTrain accuracy: 37667/48000 =  78.47 % ||| loss 0.5501996278762817\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9407/12000 =  78.39 % ||| loss 0.5383303761482239\u001b[0m\n",
            "\u001b[92mTest accuracy: 7780/10000 =  77.8 % ||| loss 0.575393795967102\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.6197095829248428\n",
            "Batch #200 Loss: 0.6106610184907914\n",
            "Batch #300 Loss: 0.6207795459032058\n",
            "\u001b[92mTrain accuracy: 37982/48000 =  79.13 % ||| loss 0.5349099636077881\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9491/12000 =  79.09 % ||| loss 0.5269045829772949\u001b[0m\n",
            "\u001b[92mTest accuracy: 7833/10000 =  78.33 % ||| loss 0.5606391429901123\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.5982821109890938\n",
            "Batch #200 Loss: 0.6126126968860626\n",
            "Batch #300 Loss: 0.614407200217247\n",
            "\u001b[92mTrain accuracy: 38242/48000 =  79.67 % ||| loss 0.5202494263648987\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9578/12000 =  79.82 % ||| loss 0.5159156322479248\u001b[0m\n",
            "\u001b[92mTest accuracy: 7890/10000 =  78.9 % ||| loss 0.5474134087562561\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5936023962497711\n",
            "Batch #200 Loss: 0.5988627362251282\n",
            "Batch #300 Loss: 0.5939296925067902\n",
            "\u001b[92mTrain accuracy: 38396/48000 =  79.99 % ||| loss 0.5246351361274719\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9579/12000 =  79.83 % ||| loss 0.5209032893180847\u001b[0m\n",
            "\u001b[92mTest accuracy: 7909/10000 =  79.09 % ||| loss 0.5475409030914307\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5796136084198952\n",
            "Batch #200 Loss: 0.5988620427250863\n",
            "Batch #300 Loss: 0.5821370476484299\n",
            "\u001b[92mTrain accuracy: 38183/48000 =  79.55 % ||| loss 0.5120118856430054\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9518/12000 =  79.32 % ||| loss 0.5085042715072632\u001b[0m\n",
            "\u001b[92mTest accuracy: 7907/10000 =  79.07 % ||| loss 0.5362386107444763\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5755685997009278\n",
            "Batch #200 Loss: 0.5866856664419174\n",
            "Batch #300 Loss: 0.5658919697999955\n",
            "\u001b[92mTrain accuracy: 38662/48000 =  80.55 % ||| loss 0.49209073185920715\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9666/12000 =  80.55 % ||| loss 0.4858163893222809\u001b[0m\n",
            "\u001b[92mTest accuracy: 7957/10000 =  79.57 % ||| loss 0.5210463404655457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.5697496184706687\n",
            "Batch #200 Loss: 0.5584922417998314\n",
            "Batch #300 Loss: 0.5663247296214103\n",
            "\u001b[92mTrain accuracy: 39190/48000 =  81.65 % ||| loss 0.48259204626083374\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9809/12000 =  81.74 % ||| loss 0.4833013117313385\u001b[0m\n",
            "\u001b[92mTest accuracy: 8083/10000 =  80.83 % ||| loss 0.5162453055381775\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.5558392742276191\n",
            "Batch #200 Loss: 0.5661132121086121\n",
            "Batch #300 Loss: 0.5579618906974793\n",
            "\u001b[92mTrain accuracy: 38907/48000 =  81.06 % ||| loss 0.4851139187812805\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9686/12000 =  80.72 % ||| loss 0.48718568682670593\u001b[0m\n",
            "\u001b[92mTest accuracy: 8018/10000 =  80.18 % ||| loss 0.5118467807769775\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.5478041568398475\n",
            "Batch #200 Loss: 0.5366378039121628\n",
            "Batch #300 Loss: 0.562155424952507\n",
            "\u001b[92mTrain accuracy: 39491/48000 =  82.27 % ||| loss 0.47246992588043213\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9862/12000 =  82.18 % ||| loss 0.473078191280365\u001b[0m\n",
            "\u001b[92mTest accuracy: 8150/10000 =  81.5 % ||| loss 0.5000473260879517\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5634774157404899\n",
            "Batch #200 Loss: 0.555257927775383\n",
            "Batch #300 Loss: 0.5314886638522148\n",
            "\u001b[92mTrain accuracy: 39632/48000 =  82.57 % ||| loss 0.46211543679237366\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9920/12000 =  82.67 % ||| loss 0.4644850194454193\u001b[0m\n",
            "\u001b[92mTest accuracy: 8176/10000 =  81.76 % ||| loss 0.4926820993423462\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5304163938760758\n",
            "Batch #200 Loss: 0.529590731561184\n",
            "Batch #300 Loss: 0.5394987305998802\n",
            "\u001b[92mTrain accuracy: 39514/48000 =  82.32 % ||| loss 0.45806294679641724\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9881/12000 =  82.34 % ||| loss 0.46006760001182556\u001b[0m\n",
            "\u001b[92mTest accuracy: 8164/10000 =  81.64 % ||| loss 0.4847581088542938\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5306041657924652\n",
            "Batch #200 Loss: 0.5225704598426819\n",
            "Batch #300 Loss: 0.52614082634449\n",
            "\u001b[92mTrain accuracy: 40198/48000 =  83.75 % ||| loss 0.4549618363380432\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10025/12000 =  83.54 % ||| loss 0.4591512084007263\u001b[0m\n",
            "\u001b[92mTest accuracy: 8254/10000 =  82.54 % ||| loss 0.4824846088886261\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_22</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_22' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_22</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_110636-Lenet5Dropout_1726150070.105228_23</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_23' target=\"_blank\">Lenet5Dropout_1726150070.105228_23</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_23' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_23</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.304215502738953\n",
            "Batch #200 Loss: 2.302432436943054\n",
            "Batch #300 Loss: 2.298287422657013\n",
            "\u001b[92mTrain accuracy: 11241/48000 =  23.42 % ||| loss 2.293630599975586\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2788/12000 =  23.23 % ||| loss 2.2937815189361572\u001b[0m\n",
            "\u001b[92mTest accuracy: 2338/10000 =  23.38 % ||| loss 2.2937474250793457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2925801849365235\n",
            "Batch #200 Loss: 2.285189027786255\n",
            "Batch #300 Loss: 2.2723970103263853\n",
            "\u001b[92mTrain accuracy: 13834/48000 =  28.82 % ||| loss 2.2139816284179688\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3437/12000 =  28.64 % ||| loss 2.2138991355895996\u001b[0m\n",
            "\u001b[92mTest accuracy: 2873/10000 =  28.73 % ||| loss 2.214221477508545\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.178763782978058\n",
            "Batch #200 Loss: 1.969856481552124\n",
            "Batch #300 Loss: 1.6667991149425507\n",
            "\u001b[92mTrain accuracy: 26628/48000 =  55.47 % ||| loss 1.1918941736221313\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6685/12000 =  55.71 % ||| loss 1.1888067722320557\u001b[0m\n",
            "\u001b[92mTest accuracy: 5547/10000 =  55.47 % ||| loss 1.1968228816986084\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 1.3320500123500825\n",
            "Batch #200 Loss: 1.242277556657791\n",
            "Batch #300 Loss: 1.185230745077133\n",
            "\u001b[92mTrain accuracy: 30865/48000 =  64.3 % ||| loss 0.9280012249946594\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7690/12000 =  64.08 % ||| loss 0.9193029403686523\u001b[0m\n",
            "\u001b[92mTest accuracy: 6353/10000 =  63.53 % ||| loss 0.9358801245689392\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 1.0977819466590881\n",
            "Batch #200 Loss: 1.064279221892357\n",
            "Batch #300 Loss: 1.0203300768136978\n",
            "\u001b[92mTrain accuracy: 32425/48000 =  67.55 % ||| loss 0.8475936651229858\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8136/12000 =  67.8 % ||| loss 0.8373448848724365\u001b[0m\n",
            "\u001b[92mTest accuracy: 6680/10000 =  66.8 % ||| loss 0.8552421927452087\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.9827508354187011\n",
            "Batch #200 Loss: 0.9782652658224106\n",
            "Batch #300 Loss: 0.9558420777320862\n",
            "\u001b[92mTrain accuracy: 33968/48000 =  70.77 % ||| loss 0.7806559801101685\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8597/12000 =  71.64 % ||| loss 0.7656764388084412\u001b[0m\n",
            "\u001b[92mTest accuracy: 7018/10000 =  70.18 % ||| loss 0.7950562834739685\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.9263425129652023\n",
            "Batch #200 Loss: 0.9131488925218583\n",
            "Batch #300 Loss: 0.892107127904892\n",
            "\u001b[92mTrain accuracy: 34607/48000 =  72.1 % ||| loss 0.7375054359436035\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8739/12000 =  72.82 % ||| loss 0.7211660742759705\u001b[0m\n",
            "\u001b[92mTest accuracy: 7165/10000 =  71.65 % ||| loss 0.748114824295044\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.892444389462471\n",
            "Batch #200 Loss: 0.8653716158866882\n",
            "Batch #300 Loss: 0.8422266906499862\n",
            "\u001b[92mTrain accuracy: 35245/48000 =  73.43 % ||| loss 0.7029918432235718\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8855/12000 =  73.79 % ||| loss 0.6866692900657654\u001b[0m\n",
            "\u001b[92mTest accuracy: 7293/10000 =  72.93 % ||| loss 0.7158210873603821\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.8485101401805878\n",
            "Batch #200 Loss: 0.831468203663826\n",
            "Batch #300 Loss: 0.8136174726486206\n",
            "\u001b[92mTrain accuracy: 35511/48000 =  73.98 % ||| loss 0.6769625544548035\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8928/12000 =  74.4 % ||| loss 0.657983660697937\u001b[0m\n",
            "\u001b[92mTest accuracy: 7359/10000 =  73.59 % ||| loss 0.6925700902938843\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.8095808172225952\n",
            "Batch #200 Loss: 0.7970572566986084\n",
            "Batch #300 Loss: 0.7986363846063614\n",
            "\u001b[92mTrain accuracy: 35452/48000 =  73.86 % ||| loss 0.6679670810699463\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8910/12000 =  74.25 % ||| loss 0.6551992893218994\u001b[0m\n",
            "\u001b[92mTest accuracy: 7316/10000 =  73.16 % ||| loss 0.6824442744255066\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.7880683964490891\n",
            "Batch #200 Loss: 0.7742815655469895\n",
            "Batch #300 Loss: 0.7581103789806366\n",
            "\u001b[92mTrain accuracy: 36471/48000 =  75.98 % ||| loss 0.6368691325187683\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9169/12000 =  76.41 % ||| loss 0.6240702271461487\u001b[0m\n",
            "\u001b[92mTest accuracy: 7549/10000 =  75.49 % ||| loss 0.6497484445571899\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.7543120670318604\n",
            "Batch #200 Loss: 0.7639924728870392\n",
            "Batch #300 Loss: 0.745442407131195\n",
            "\u001b[92mTrain accuracy: 36658/48000 =  76.37 % ||| loss 0.6161558628082275\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9238/12000 =  76.98 % ||| loss 0.6007514595985413\u001b[0m\n",
            "\u001b[92mTest accuracy: 7593/10000 =  75.93 % ||| loss 0.6343863010406494\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.7584044349193573\n",
            "Batch #200 Loss: 0.7383376610279083\n",
            "Batch #300 Loss: 0.7383888751268387\n",
            "\u001b[92mTrain accuracy: 36923/48000 =  76.92 % ||| loss 0.6000638604164124\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9282/12000 =  77.35 % ||| loss 0.5858032703399658\u001b[0m\n",
            "\u001b[92mTest accuracy: 7614/10000 =  76.14 % ||| loss 0.618032693862915\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.7209733664989472\n",
            "Batch #200 Loss: 0.7233604347705841\n",
            "Batch #300 Loss: 0.7124340504407882\n",
            "\u001b[92mTrain accuracy: 37080/48000 =  77.25 % ||| loss 0.588474452495575\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9298/12000 =  77.48 % ||| loss 0.5762302279472351\u001b[0m\n",
            "\u001b[92mTest accuracy: 7648/10000 =  76.48 % ||| loss 0.6075882911682129\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.6998023891448975\n",
            "Batch #200 Loss: 0.7012684983015061\n",
            "Batch #300 Loss: 0.6985941433906555\n",
            "\u001b[92mTrain accuracy: 37239/48000 =  77.58 % ||| loss 0.5732517838478088\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9344/12000 =  77.87 % ||| loss 0.5613781213760376\u001b[0m\n",
            "\u001b[92mTest accuracy: 7686/10000 =  76.86 % ||| loss 0.5969875454902649\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.6919141620397568\n",
            "Batch #200 Loss: 0.6827276074886321\n",
            "Batch #300 Loss: 0.6788874790072441\n",
            "\u001b[92mTrain accuracy: 37540/48000 =  78.21 % ||| loss 0.5621219277381897\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9410/12000 =  78.42 % ||| loss 0.5513572096824646\u001b[0m\n",
            "\u001b[92mTest accuracy: 7713/10000 =  77.13 % ||| loss 0.582705557346344\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.6862929508090019\n",
            "Batch #200 Loss: 0.6640067410469055\n",
            "Batch #300 Loss: 0.6698057851195336\n",
            "\u001b[92mTrain accuracy: 37591/48000 =  78.31 % ||| loss 0.5559388995170593\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9447/12000 =  78.72 % ||| loss 0.543454647064209\u001b[0m\n",
            "\u001b[92mTest accuracy: 7754/10000 =  77.54 % ||| loss 0.5775017738342285\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.6429586124420166\n",
            "Batch #200 Loss: 0.6918674328923226\n",
            "Batch #300 Loss: 0.6402744060754776\n",
            "\u001b[92mTrain accuracy: 37665/48000 =  78.47 % ||| loss 0.5423084497451782\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9462/12000 =  78.85 % ||| loss 0.5340996384620667\u001b[0m\n",
            "\u001b[92mTest accuracy: 7764/10000 =  77.64 % ||| loss 0.5620855689048767\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.6473956900835037\n",
            "Batch #200 Loss: 0.6446349239349365\n",
            "Batch #300 Loss: 0.6482382434606552\n",
            "\u001b[92mTrain accuracy: 38029/48000 =  79.23 % ||| loss 0.5317862033843994\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9582/12000 =  79.85 % ||| loss 0.522061288356781\u001b[0m\n",
            "\u001b[92mTest accuracy: 7833/10000 =  78.33 % ||| loss 0.551796019077301\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.6325177544355393\n",
            "Batch #200 Loss: 0.6453600022196769\n",
            "Batch #300 Loss: 0.6279675897955894\n",
            "\u001b[92mTrain accuracy: 38121/48000 =  79.42 % ||| loss 0.5160912275314331\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9553/12000 =  79.61 % ||| loss 0.5085586905479431\u001b[0m\n",
            "\u001b[92mTest accuracy: 7858/10000 =  78.58 % ||| loss 0.5380786657333374\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.6322132885456085\n",
            "Batch #200 Loss: 0.605893904864788\n",
            "Batch #300 Loss: 0.6447069850564003\n",
            "\u001b[92mTrain accuracy: 38237/48000 =  79.66 % ||| loss 0.5089570879936218\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9602/12000 =  80.02 % ||| loss 0.5017880201339722\u001b[0m\n",
            "\u001b[92mTest accuracy: 7888/10000 =  78.88 % ||| loss 0.5340108275413513\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.6092368850111961\n",
            "Batch #200 Loss: 0.6113545599579812\n",
            "Batch #300 Loss: 0.624257670044899\n",
            "\u001b[92mTrain accuracy: 38932/48000 =  81.11 % ||| loss 0.5031843185424805\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9779/12000 =  81.49 % ||| loss 0.4973551034927368\u001b[0m\n",
            "\u001b[92mTest accuracy: 8015/10000 =  80.15 % ||| loss 0.5254273414611816\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.6025470095872879\n",
            "Batch #200 Loss: 0.6176287576556205\n",
            "Batch #300 Loss: 0.6099486926198006\n",
            "\u001b[92mTrain accuracy: 38670/48000 =  80.56 % ||| loss 0.5027181506156921\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9679/12000 =  80.66 % ||| loss 0.49878230690956116\u001b[0m\n",
            "\u001b[92mTest accuracy: 7945/10000 =  79.45 % ||| loss 0.5269025564193726\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.6041516155004502\n",
            "Batch #200 Loss: 0.5958118432760239\n",
            "Batch #300 Loss: 0.5938892048597336\n",
            "\u001b[92mTrain accuracy: 39112/48000 =  81.48 % ||| loss 0.48193907737731934\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9805/12000 =  81.71 % ||| loss 0.47807300090789795\u001b[0m\n",
            "\u001b[92mTest accuracy: 8059/10000 =  80.59 % ||| loss 0.5036352872848511\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5932771810889244\n",
            "Batch #200 Loss: 0.5868651181459427\n",
            "Batch #300 Loss: 0.5840641269087792\n",
            "\u001b[92mTrain accuracy: 39162/48000 =  81.59 % ||| loss 0.47887304425239563\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9817/12000 =  81.81 % ||| loss 0.4764120578765869\u001b[0m\n",
            "\u001b[92mTest accuracy: 8041/10000 =  80.41 % ||| loss 0.5029711127281189\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_23</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_23' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_23</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_110909-Lenet5Dropout_1726150070.105228_24</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_24' target=\"_blank\">Lenet5Dropout_1726150070.105228_24</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_24' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_24</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7, 'dropout': 0.2}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.303526463508606\n",
            "Batch #200 Loss: 2.302427933216095\n",
            "Batch #300 Loss: 2.3007312750816347\n",
            "\u001b[92mTrain accuracy: 5051/48000 =  10.52 % ||| loss 2.2976512908935547\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1356/12000 =  11.3 % ||| loss 2.2972991466522217\u001b[0m\n",
            "\u001b[92mTest accuracy: 1073/10000 =  10.73 % ||| loss 2.297583818435669\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2968850898742676\n",
            "Batch #200 Loss: 2.294793884754181\n",
            "Batch #300 Loss: 2.2923253989219665\n",
            "\u001b[92mTrain accuracy: 6629/48000 =  13.81 % ||| loss 2.286254644393921\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1735/12000 =  14.46 % ||| loss 2.285844564437866\u001b[0m\n",
            "\u001b[92mTest accuracy: 1395/10000 =  13.95 % ||| loss 2.286229372024536\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.2853662729263307\n",
            "Batch #200 Loss: 2.2790872263908386\n",
            "Batch #300 Loss: 2.27057537317276\n",
            "\u001b[92mTrain accuracy: 10839/48000 =  22.58 % ||| loss 2.2493934631347656\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2748/12000 =  22.9 % ||| loss 2.248756170272827\u001b[0m\n",
            "\u001b[92mTest accuracy: 2265/10000 =  22.65 % ||| loss 2.249157190322876\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.245441918373108\n",
            "Batch #200 Loss: 2.216442232131958\n",
            "Batch #300 Loss: 2.1668482041358947\n",
            "\u001b[92mTrain accuracy: 14658/48000 =  30.54 % ||| loss 2.0088093280792236\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3714/12000 =  30.95 % ||| loss 2.0077481269836426\u001b[0m\n",
            "\u001b[92mTest accuracy: 3055/10000 =  30.55 % ||| loss 2.008913278579712\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 1.9439148473739625\n",
            "Batch #200 Loss: 1.7245964848995208\n",
            "Batch #300 Loss: 1.5224075996875763\n",
            "\u001b[92mTrain accuracy: 28794/48000 =  59.99 % ||| loss 1.2310712337493896\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7195/12000 =  59.96 % ||| loss 1.22810697555542\u001b[0m\n",
            "\u001b[92mTest accuracy: 5990/10000 =  59.9 % ||| loss 1.2345889806747437\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 1.2953411543369293\n",
            "Batch #200 Loss: 1.2136435103416443\n",
            "Batch #300 Loss: 1.1701738715171814\n",
            "\u001b[92mTrain accuracy: 30129/48000 =  62.77 % ||| loss 0.9968363642692566\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7554/12000 =  62.95 % ||| loss 0.9914149045944214\u001b[0m\n",
            "\u001b[92mTest accuracy: 6251/10000 =  62.51 % ||| loss 1.0058917999267578\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 1.1043850934505464\n",
            "Batch #200 Loss: 1.0543021512031556\n",
            "Batch #300 Loss: 1.0343043744564056\n",
            "\u001b[92mTrain accuracy: 30952/48000 =  64.48 % ||| loss 0.9186723828315735\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7726/12000 =  64.38 % ||| loss 0.9117830991744995\u001b[0m\n",
            "\u001b[92mTest accuracy: 6355/10000 =  63.55 % ||| loss 0.9312373995780945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 1.0082870733737945\n",
            "Batch #200 Loss: 0.9955717074871063\n",
            "Batch #300 Loss: 0.9808253765106201\n",
            "\u001b[92mTrain accuracy: 31792/48000 =  66.23 % ||| loss 0.8745812177658081\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7963/12000 =  66.36 % ||| loss 0.8670011758804321\u001b[0m\n",
            "\u001b[92mTest accuracy: 6556/10000 =  65.56 % ||| loss 0.8857317566871643\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.9613689321279526\n",
            "Batch #200 Loss: 0.9369872504472733\n",
            "Batch #300 Loss: 0.9460544496774673\n",
            "\u001b[92mTrain accuracy: 32517/48000 =  67.74 % ||| loss 0.8393171429634094\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8154/12000 =  67.95 % ||| loss 0.8307231068611145\u001b[0m\n",
            "\u001b[92mTest accuracy: 6712/10000 =  67.12 % ||| loss 0.8611942529678345\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.9157618063688279\n",
            "Batch #200 Loss: 0.9112118124961853\n",
            "Batch #300 Loss: 0.9056164091825485\n",
            "\u001b[92mTrain accuracy: 33042/48000 =  68.84 % ||| loss 0.811496913433075\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8354/12000 =  69.62 % ||| loss 0.8009910583496094\u001b[0m\n",
            "\u001b[92mTest accuracy: 6814/10000 =  68.14 % ||| loss 0.828095555305481\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.8854873168468476\n",
            "Batch #200 Loss: 0.8833068001270294\n",
            "Batch #300 Loss: 0.8629226005077362\n",
            "\u001b[92mTrain accuracy: 33637/48000 =  70.08 % ||| loss 0.7882080674171448\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8489/12000 =  70.74 % ||| loss 0.7769274711608887\u001b[0m\n",
            "\u001b[92mTest accuracy: 6948/10000 =  69.48 % ||| loss 0.809032678604126\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.8577831214666367\n",
            "Batch #200 Loss: 0.8698250353336334\n",
            "Batch #300 Loss: 0.8392671298980713\n",
            "\u001b[92mTrain accuracy: 33898/48000 =  70.62 % ||| loss 0.7698983550071716\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8568/12000 =  71.4 % ||| loss 0.7588223218917847\u001b[0m\n",
            "\u001b[92mTest accuracy: 7014/10000 =  70.14 % ||| loss 0.7851929068565369\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.8296871757507325\n",
            "Batch #200 Loss: 0.8356698310375213\n",
            "Batch #300 Loss: 0.8279413449764251\n",
            "\u001b[92mTrain accuracy: 34148/48000 =  71.14 % ||| loss 0.7524198293685913\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8623/12000 =  71.86 % ||| loss 0.7396177053451538\u001b[0m\n",
            "\u001b[92mTest accuracy: 7066/10000 =  70.66 % ||| loss 0.7723516821861267\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.8283871471881866\n",
            "Batch #200 Loss: 0.8241513437032699\n",
            "Batch #300 Loss: 0.8127577352523804\n",
            "\u001b[92mTrain accuracy: 34392/48000 =  71.65 % ||| loss 0.7408854365348816\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8678/12000 =  72.32 % ||| loss 0.7274842858314514\u001b[0m\n",
            "\u001b[92mTest accuracy: 7134/10000 =  71.34 % ||| loss 0.7580164670944214\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.8150021660327912\n",
            "Batch #200 Loss: 0.8056774443387985\n",
            "Batch #300 Loss: 0.7880602169036866\n",
            "\u001b[92mTrain accuracy: 34611/48000 =  72.11 % ||| loss 0.7301670908927917\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8722/12000 =  72.68 % ||| loss 0.7181374430656433\u001b[0m\n",
            "\u001b[92mTest accuracy: 7157/10000 =  71.57 % ||| loss 0.7537180781364441\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.7978142195940018\n",
            "Batch #200 Loss: 0.7775925666093826\n",
            "Batch #300 Loss: 0.79638492166996\n",
            "\u001b[92mTrain accuracy: 34831/48000 =  72.56 % ||| loss 0.7213078737258911\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8748/12000 =  72.9 % ||| loss 0.7081515789031982\u001b[0m\n",
            "\u001b[92mTest accuracy: 7206/10000 =  72.06 % ||| loss 0.7401300072669983\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.7868620502948761\n",
            "Batch #200 Loss: 0.7876584309339524\n",
            "Batch #300 Loss: 0.7791787713766098\n",
            "\u001b[92mTrain accuracy: 34902/48000 =  72.71 % ||| loss 0.7095399498939514\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8776/12000 =  73.13 % ||| loss 0.6973755359649658\u001b[0m\n",
            "\u001b[92mTest accuracy: 7214/10000 =  72.14 % ||| loss 0.7298932075500488\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.7647284913063049\n",
            "Batch #200 Loss: 0.7840626281499863\n",
            "Batch #300 Loss: 0.7607761114835739\n",
            "\u001b[92mTrain accuracy: 35084/48000 =  73.09 % ||| loss 0.7012438178062439\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8797/12000 =  73.31 % ||| loss 0.6883852481842041\u001b[0m\n",
            "\u001b[92mTest accuracy: 7267/10000 =  72.67 % ||| loss 0.7192639708518982\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.7569409227371215\n",
            "Batch #200 Loss: 0.7775354743003845\n",
            "Batch #300 Loss: 0.7647042179107666\n",
            "\u001b[92mTrain accuracy: 35178/48000 =  73.29 % ||| loss 0.6910662651062012\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8834/12000 =  73.62 % ||| loss 0.6784026026725769\u001b[0m\n",
            "\u001b[92mTest accuracy: 7262/10000 =  72.62 % ||| loss 0.7151533961296082\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.73917799949646\n",
            "Batch #200 Loss: 0.7614576715230942\n",
            "Batch #300 Loss: 0.765195289850235\n",
            "\u001b[92mTrain accuracy: 35311/48000 =  73.56 % ||| loss 0.689158022403717\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8866/12000 =  73.88 % ||| loss 0.6750192046165466\u001b[0m\n",
            "\u001b[92mTest accuracy: 7312/10000 =  73.12 % ||| loss 0.7075210213661194\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.7344744300842285\n",
            "Batch #200 Loss: 0.7395328652858734\n",
            "Batch #300 Loss: 0.7502000069618225\n",
            "\u001b[92mTrain accuracy: 35353/48000 =  73.65 % ||| loss 0.6838400959968567\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8895/12000 =  74.12 % ||| loss 0.670505166053772\u001b[0m\n",
            "\u001b[92mTest accuracy: 7334/10000 =  73.34 % ||| loss 0.7028681635856628\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.7479911911487579\n",
            "Batch #200 Loss: 0.7239011245965957\n",
            "Batch #300 Loss: 0.7420431303977967\n",
            "\u001b[92mTrain accuracy: 35568/48000 =  74.1 % ||| loss 0.6711738705635071\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8915/12000 =  74.29 % ||| loss 0.6591877341270447\u001b[0m\n",
            "\u001b[92mTest accuracy: 7342/10000 =  73.42 % ||| loss 0.6927869915962219\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.7255015838146209\n",
            "Batch #200 Loss: 0.7285668957233429\n",
            "Batch #300 Loss: 0.7309377855062484\n",
            "\u001b[92mTrain accuracy: 35671/48000 =  74.31 % ||| loss 0.6649007797241211\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8952/12000 =  74.6 % ||| loss 0.6509135365486145\u001b[0m\n",
            "\u001b[92mTest accuracy: 7377/10000 =  73.77 % ||| loss 0.6893817782402039\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.725760225057602\n",
            "Batch #200 Loss: 0.7227572131156922\n",
            "Batch #300 Loss: 0.7186283093690872\n",
            "\u001b[92mTrain accuracy: 35879/48000 =  74.75 % ||| loss 0.65521240234375\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8987/12000 =  74.89 % ||| loss 0.6428070664405823\u001b[0m\n",
            "\u001b[92mTest accuracy: 7432/10000 =  74.32 % ||| loss 0.6858952641487122\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.7213701993227005\n",
            "Batch #200 Loss: 0.7209285688400269\n",
            "Batch #300 Loss: 0.7023979061841965\n",
            "\u001b[92mTrain accuracy: 36030/48000 =  75.06 % ||| loss 0.6488426327705383\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9054/12000 =  75.45 % ||| loss 0.6358947157859802\u001b[0m\n",
            "\u001b[92mTest accuracy: 7436/10000 =  74.36 % ||| loss 0.6727247834205627\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_24</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_24' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_24</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_111141-Lenet5Dropout_1726150070.105228_25</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_25' target=\"_blank\">Lenet5Dropout_1726150070.105228_25</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_25' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_25</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7, 'dropout': 0.35}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3033937335014345\n",
            "Batch #200 Loss: 2.3021716618537904\n",
            "Batch #300 Loss: 2.3011268377304077\n",
            "\u001b[92mTrain accuracy: 6527/48000 =  13.6 % ||| loss 2.2983202934265137\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1586/12000 =  13.22 % ||| loss 2.2986419200897217\u001b[0m\n",
            "\u001b[92mTest accuracy: 1375/10000 =  13.75 % ||| loss 2.29848575592041\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.298263931274414\n",
            "Batch #200 Loss: 2.295878756046295\n",
            "Batch #300 Loss: 2.293563702106476\n",
            "\u001b[92mTrain accuracy: 7204/48000 =  15.01 % ||| loss 2.2892391681671143\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1766/12000 =  14.72 % ||| loss 2.289527654647827\u001b[0m\n",
            "\u001b[92mTest accuracy: 1516/10000 =  15.16 % ||| loss 2.289341688156128\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.2883187079429628\n",
            "Batch #200 Loss: 2.2852773427963258\n",
            "Batch #300 Loss: 2.27938809633255\n",
            "\u001b[92mTrain accuracy: 12042/48000 =  25.09 % ||| loss 2.2655885219573975\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2998/12000 =  24.98 % ||| loss 2.2657320499420166\u001b[0m\n",
            "\u001b[92mTest accuracy: 2498/10000 =  24.98 % ||| loss 2.265726327896118\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.2634042072296143\n",
            "Batch #200 Loss: 2.249988956451416\n",
            "Batch #300 Loss: 2.2261212635040284\n",
            "\u001b[92mTrain accuracy: 12097/48000 =  25.2 % ||| loss 2.156693458557129\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3040/12000 =  25.33 % ||| loss 2.15643572807312\u001b[0m\n",
            "\u001b[92mTest accuracy: 2488/10000 =  24.88 % ||| loss 2.157522439956665\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.1427512407302856\n",
            "Batch #200 Loss: 2.031741840839386\n",
            "Batch #300 Loss: 1.8629216349124909\n",
            "\u001b[92mTrain accuracy: 23755/48000 =  49.49 % ||| loss 1.5092394351959229\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6033/12000 =  50.28 % ||| loss 1.506485104560852\u001b[0m\n",
            "\u001b[92mTest accuracy: 4947/10000 =  49.47 % ||| loss 1.5109150409698486\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 1.5671029472351075\n",
            "Batch #200 Loss: 1.4507931351661683\n",
            "Batch #300 Loss: 1.3773511946201324\n",
            "\u001b[92mTrain accuracy: 28335/48000 =  59.03 % ||| loss 1.1500378847122192\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7084/12000 =  59.03 % ||| loss 1.1462041139602661\u001b[0m\n",
            "\u001b[92mTest accuracy: 5862/10000 =  58.62 % ||| loss 1.158475637435913\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 1.2741817438602447\n",
            "Batch #200 Loss: 1.2360379493236542\n",
            "Batch #300 Loss: 1.2162917512655258\n",
            "\u001b[92mTrain accuracy: 29773/48000 =  62.03 % ||| loss 1.0222618579864502\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7481/12000 =  62.34 % ||| loss 1.016983985900879\u001b[0m\n",
            "\u001b[92mTest accuracy: 6156/10000 =  61.56 % ||| loss 1.0319823026657104\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 1.1530560976266861\n",
            "Batch #200 Loss: 1.1284567373991012\n",
            "Batch #300 Loss: 1.1105452758073806\n",
            "\u001b[92mTrain accuracy: 30859/48000 =  64.29 % ||| loss 0.9423836469650269\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7715/12000 =  64.29 % ||| loss 0.9363930225372314\u001b[0m\n",
            "\u001b[92mTest accuracy: 6364/10000 =  63.64 % ||| loss 0.9565613269805908\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 1.0611586302518845\n",
            "Batch #200 Loss: 1.0421529769897462\n",
            "Batch #300 Loss: 1.0271087932586669\n",
            "\u001b[92mTrain accuracy: 32053/48000 =  66.78 % ||| loss 0.8834530711174011\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8032/12000 =  66.93 % ||| loss 0.8765568137168884\u001b[0m\n",
            "\u001b[92mTest accuracy: 6604/10000 =  66.04 % ||| loss 0.8983932733535767\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 1.0051353830099106\n",
            "Batch #200 Loss: 0.9907497179508209\n",
            "Batch #300 Loss: 0.9731993544101715\n",
            "\u001b[92mTrain accuracy: 32523/48000 =  67.76 % ||| loss 0.8381063342094421\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8177/12000 =  68.14 % ||| loss 0.8301130533218384\u001b[0m\n",
            "\u001b[92mTest accuracy: 6709/10000 =  67.09 % ||| loss 0.8540767431259155\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.9620247477293015\n",
            "Batch #200 Loss: 0.9401451861858368\n",
            "Batch #300 Loss: 0.9242667132616043\n",
            "\u001b[92mTrain accuracy: 32964/48000 =  68.67 % ||| loss 0.8101847171783447\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8304/12000 =  69.2 % ||| loss 0.8022236227989197\u001b[0m\n",
            "\u001b[92mTest accuracy: 6784/10000 =  67.84 % ||| loss 0.8298211693763733\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.9075056260824204\n",
            "Batch #200 Loss: 0.9157053685188293\n",
            "Batch #300 Loss: 0.8902954226732254\n",
            "\u001b[92mTrain accuracy: 33692/48000 =  70.19 % ||| loss 0.7800009846687317\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8469/12000 =  70.58 % ||| loss 0.771889328956604\u001b[0m\n",
            "\u001b[92mTest accuracy: 6938/10000 =  69.38 % ||| loss 0.7964583039283752\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.878175613284111\n",
            "Batch #200 Loss: 0.8841805243492127\n",
            "Batch #300 Loss: 0.8861353814601898\n",
            "\u001b[92mTrain accuracy: 34103/48000 =  71.05 % ||| loss 0.7625885605812073\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8562/12000 =  71.35 % ||| loss 0.7531890869140625\u001b[0m\n",
            "\u001b[92mTest accuracy: 7035/10000 =  70.35 % ||| loss 0.7815581560134888\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.8595626145601273\n",
            "Batch #200 Loss: 0.8582368505001068\n",
            "Batch #300 Loss: 0.8540736323595047\n",
            "\u001b[92mTrain accuracy: 34265/48000 =  71.39 % ||| loss 0.7494696378707886\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8587/12000 =  71.56 % ||| loss 0.7393057942390442\u001b[0m\n",
            "\u001b[92mTest accuracy: 7094/10000 =  70.94 % ||| loss 0.7679588794708252\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.8344360566139222\n",
            "Batch #200 Loss: 0.8555918896198272\n",
            "Batch #300 Loss: 0.848146955370903\n",
            "\u001b[92mTrain accuracy: 34588/48000 =  72.06 % ||| loss 0.7325544357299805\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8679/12000 =  72.32 % ||| loss 0.7221816182136536\u001b[0m\n",
            "\u001b[92mTest accuracy: 7165/10000 =  71.65 % ||| loss 0.7511254549026489\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.8325163757801056\n",
            "Batch #200 Loss: 0.822014080286026\n",
            "Batch #300 Loss: 0.827473583817482\n",
            "\u001b[92mTrain accuracy: 34761/48000 =  72.42 % ||| loss 0.7205484509468079\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8684/12000 =  72.37 % ||| loss 0.7103182077407837\u001b[0m\n",
            "\u001b[92mTest accuracy: 7185/10000 =  71.85 % ||| loss 0.739372730255127\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.8230883461236954\n",
            "Batch #200 Loss: 0.8126479429006577\n",
            "Batch #300 Loss: 0.8165544670820236\n",
            "\u001b[92mTrain accuracy: 34650/48000 =  72.19 % ||| loss 0.7178026437759399\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8679/12000 =  72.32 % ||| loss 0.7065351605415344\u001b[0m\n",
            "\u001b[92mTest accuracy: 7173/10000 =  71.73 % ||| loss 0.7346205115318298\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.806199135184288\n",
            "Batch #200 Loss: 0.8054942816495896\n",
            "Batch #300 Loss: 0.7954360538721085\n",
            "\u001b[92mTrain accuracy: 35053/48000 =  73.03 % ||| loss 0.7019743919372559\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8785/12000 =  73.21 % ||| loss 0.6922110915184021\u001b[0m\n",
            "\u001b[92mTest accuracy: 7246/10000 =  72.46 % ||| loss 0.7265059351921082\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.7986079001426697\n",
            "Batch #200 Loss: 0.7933285784721374\n",
            "Batch #300 Loss: 0.7882348209619522\n",
            "\u001b[92mTrain accuracy: 35091/48000 =  73.11 % ||| loss 0.6958880424499512\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8767/12000 =  73.06 % ||| loss 0.6846223473548889\u001b[0m\n",
            "\u001b[92mTest accuracy: 7249/10000 =  72.49 % ||| loss 0.7186816930770874\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.7870126634836196\n",
            "Batch #200 Loss: 0.7855666315555573\n",
            "Batch #300 Loss: 0.7761124813556671\n",
            "\u001b[92mTrain accuracy: 35365/48000 =  73.68 % ||| loss 0.6837911605834961\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8844/12000 =  73.7 % ||| loss 0.6725611090660095\u001b[0m\n",
            "\u001b[92mTest accuracy: 7302/10000 =  73.02 % ||| loss 0.7026419639587402\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.7676437562704086\n",
            "Batch #200 Loss: 0.7600348734855652\n",
            "Batch #300 Loss: 0.7698635321855545\n",
            "\u001b[92mTrain accuracy: 35629/48000 =  74.23 % ||| loss 0.6749753355979919\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8925/12000 =  74.38 % ||| loss 0.6650546789169312\u001b[0m\n",
            "\u001b[92mTest accuracy: 7361/10000 =  73.61 % ||| loss 0.6984038352966309\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.7614550602436065\n",
            "Batch #200 Loss: 0.7588714683055877\n",
            "Batch #300 Loss: 0.7514921915531159\n",
            "\u001b[92mTrain accuracy: 35741/48000 =  74.46 % ||| loss 0.6668077111244202\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8920/12000 =  74.33 % ||| loss 0.6568385362625122\u001b[0m\n",
            "\u001b[92mTest accuracy: 7392/10000 =  73.92 % ||| loss 0.6846731901168823\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.7633886843919754\n",
            "Batch #200 Loss: 0.7520020645856857\n",
            "Batch #300 Loss: 0.7456971621513366\n",
            "\u001b[92mTrain accuracy: 35858/48000 =  74.7 % ||| loss 0.660641610622406\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8960/12000 =  74.67 % ||| loss 0.6472115516662598\u001b[0m\n",
            "\u001b[92mTest accuracy: 7404/10000 =  74.04 % ||| loss 0.6837031841278076\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.7317167717218399\n",
            "Batch #200 Loss: 0.752547157406807\n",
            "Batch #300 Loss: 0.7454647219181061\n",
            "\u001b[92mTrain accuracy: 35867/48000 =  74.72 % ||| loss 0.6566070318222046\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8974/12000 =  74.78 % ||| loss 0.6438871026039124\u001b[0m\n",
            "\u001b[92mTest accuracy: 7407/10000 =  74.07 % ||| loss 0.678683340549469\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.7342966413497924\n",
            "Batch #200 Loss: 0.7364158087968826\n",
            "Batch #300 Loss: 0.7358369797468185\n",
            "\u001b[92mTrain accuracy: 36253/48000 =  75.53 % ||| loss 0.6421851515769958\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9059/12000 =  75.49 % ||| loss 0.6323614716529846\u001b[0m\n",
            "\u001b[92mTest accuracy: 7474/10000 =  74.74 % ||| loss 0.6618617177009583\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_25</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_25' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_25</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_111412-Lenet5Dropout_1726150070.105228_26</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_26' target=\"_blank\">Lenet5Dropout_1726150070.105228_26</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_26' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_26</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Dropout XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7, 'dropout': 0.5}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3041425490379335\n",
            "Batch #200 Loss: 2.304863700866699\n",
            "Batch #300 Loss: 2.3036313486099242\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.302396059036255\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.30202054977417\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302281141281128\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.302511532306671\n",
            "Batch #200 Loss: 2.302117955684662\n",
            "Batch #300 Loss: 2.3014575719833372\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.3000619411468506\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.29976487159729\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2999026775360107\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3006633043289186\n",
            "Batch #200 Loss: 2.299994120597839\n",
            "Batch #300 Loss: 2.298824849128723\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.2968623638153076\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.2966513633728027\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2968273162841797\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.2972800946235656\n",
            "Batch #200 Loss: 2.296611454486847\n",
            "Batch #300 Loss: 2.29504456281662\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.2909772396087646\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.2908010482788086\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2908401489257812\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.2910460090637206\n",
            "Batch #200 Loss: 2.2887480998039247\n",
            "Batch #300 Loss: 2.2856633067131042\n",
            "\u001b[92mTrain accuracy: 9629/48000 =  20.06 % ||| loss 2.2762675285339355\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2406/12000 =  20.05 % ||| loss 2.276106595993042\u001b[0m\n",
            "\u001b[92mTest accuracy: 2066/10000 =  20.66 % ||| loss 2.2762718200683594\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.276280221939087\n",
            "Batch #200 Loss: 2.2688707876205445\n",
            "Batch #300 Loss: 2.2589867091178895\n",
            "\u001b[92mTrain accuracy: 14096/48000 =  29.37 % ||| loss 2.226531505584717\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3550/12000 =  29.58 % ||| loss 2.2263283729553223\u001b[0m\n",
            "\u001b[92mTest accuracy: 2968/10000 =  29.68 % ||| loss 2.2263693809509277\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.2257526445388796\n",
            "Batch #200 Loss: 2.190047142505646\n",
            "Batch #300 Loss: 2.1433802127838133\n",
            "\u001b[92mTrain accuracy: 22214/48000 =  46.28 % ||| loss 1.957434892654419\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5575/12000 =  46.46 % ||| loss 1.9566981792449951\u001b[0m\n",
            "\u001b[92mTest accuracy: 4530/10000 =  45.3 % ||| loss 1.9571484327316284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 1.978423264026642\n",
            "Batch #200 Loss: 1.844252840280533\n",
            "Batch #300 Loss: 1.7067877125740052\n",
            "\u001b[92mTrain accuracy: 26921/48000 =  56.09 % ||| loss 1.3736871480941772\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6760/12000 =  56.33 % ||| loss 1.3702569007873535\u001b[0m\n",
            "\u001b[92mTest accuracy: 5607/10000 =  56.07 % ||| loss 1.3777992725372314\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 1.5169322407245636\n",
            "Batch #200 Loss: 1.4528176367282868\n",
            "Batch #300 Loss: 1.392780739068985\n",
            "\u001b[92mTrain accuracy: 28272/48000 =  58.9 % ||| loss 1.14280366897583\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7103/12000 =  59.19 % ||| loss 1.1375051736831665\u001b[0m\n",
            "\u001b[92mTest accuracy: 5886/10000 =  58.86 % ||| loss 1.1515698432922363\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 1.3321768987178801\n",
            "Batch #200 Loss: 1.302784583568573\n",
            "Batch #300 Loss: 1.273570350408554\n",
            "\u001b[92mTrain accuracy: 29750/48000 =  61.98 % ||| loss 1.0446561574935913\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7455/12000 =  62.12 % ||| loss 1.0382492542266846\u001b[0m\n",
            "\u001b[92mTest accuracy: 6151/10000 =  61.51 % ||| loss 1.0542752742767334\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 1.2157766342163085\n",
            "Batch #200 Loss: 1.2091775274276733\n",
            "Batch #300 Loss: 1.2002341103553773\n",
            "\u001b[92mTrain accuracy: 30446/48000 =  63.43 % ||| loss 0.9855148196220398\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7635/12000 =  63.62 % ||| loss 0.9785739183425903\u001b[0m\n",
            "\u001b[92mTest accuracy: 6259/10000 =  62.59 % ||| loss 0.9983457922935486\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 1.170736334323883\n",
            "Batch #200 Loss: 1.1409012591838836\n",
            "Batch #300 Loss: 1.140383432507515\n",
            "\u001b[92mTrain accuracy: 31052/48000 =  64.69 % ||| loss 0.9414920210838318\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7801/12000 =  65.01 % ||| loss 0.9335351586341858\u001b[0m\n",
            "\u001b[92mTest accuracy: 6399/10000 =  63.99 % ||| loss 0.9527604579925537\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 1.1230916899442673\n",
            "Batch #200 Loss: 1.0984853100776673\n",
            "Batch #300 Loss: 1.086889221072197\n",
            "\u001b[92mTrain accuracy: 31638/48000 =  65.91 % ||| loss 0.8988115191459656\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7924/12000 =  66.03 % ||| loss 0.8901232481002808\u001b[0m\n",
            "\u001b[92mTest accuracy: 6510/10000 =  65.1 % ||| loss 0.9091096520423889\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 1.066242863535881\n",
            "Batch #200 Loss: 1.0450508415699005\n",
            "Batch #300 Loss: 1.0534071826934814\n",
            "\u001b[92mTrain accuracy: 32335/48000 =  67.36 % ||| loss 0.8713192343711853\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8118/12000 =  67.65 % ||| loss 0.8617315292358398\u001b[0m\n",
            "\u001b[92mTest accuracy: 6645/10000 =  66.45 % ||| loss 0.8841476440429688\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 1.0377861720323562\n",
            "Batch #200 Loss: 0.99751609146595\n",
            "Batch #300 Loss: 1.008509984612465\n",
            "\u001b[92mTrain accuracy: 33349/48000 =  69.48 % ||| loss 0.8375694155693054\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8388/12000 =  69.9 % ||| loss 0.8282510638237\u001b[0m\n",
            "\u001b[92mTest accuracy: 6868/10000 =  68.68 % ||| loss 0.8520851731300354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.9910530692338944\n",
            "Batch #200 Loss: 0.9830931067466736\n",
            "Batch #300 Loss: 0.9792141819000244\n",
            "\u001b[92mTrain accuracy: 33581/48000 =  69.96 % ||| loss 0.8146500587463379\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8467/12000 =  70.56 % ||| loss 0.8033784627914429\u001b[0m\n",
            "\u001b[92mTest accuracy: 6909/10000 =  69.09 % ||| loss 0.8308977484703064\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.9700916284322738\n",
            "Batch #200 Loss: 0.9634450626373291\n",
            "Batch #300 Loss: 0.958270725607872\n",
            "\u001b[92mTrain accuracy: 33860/48000 =  70.54 % ||| loss 0.7947474122047424\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8517/12000 =  70.97 % ||| loss 0.7846837043762207\u001b[0m\n",
            "\u001b[92mTest accuracy: 6988/10000 =  69.88 % ||| loss 0.8120973110198975\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.9669234848022461\n",
            "Batch #200 Loss: 0.9386023515462876\n",
            "Batch #300 Loss: 0.9318385344743728\n",
            "\u001b[92mTrain accuracy: 34179/48000 =  71.21 % ||| loss 0.7758049368858337\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8583/12000 =  71.53 % ||| loss 0.7649920582771301\u001b[0m\n",
            "\u001b[92mTest accuracy: 7070/10000 =  70.7 % ||| loss 0.7911540865898132\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.9230194783210754\n",
            "Batch #200 Loss: 0.9380334913730621\n",
            "Batch #300 Loss: 0.908972271680832\n",
            "\u001b[92mTrain accuracy: 34169/48000 =  71.19 % ||| loss 0.7646521925926208\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8596/12000 =  71.63 % ||| loss 0.7515023350715637\u001b[0m\n",
            "\u001b[92mTest accuracy: 7090/10000 =  70.9 % ||| loss 0.7762206792831421\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.9192132651805878\n",
            "Batch #200 Loss: 0.9074332416057587\n",
            "Batch #300 Loss: 0.8988120019435882\n",
            "\u001b[92mTrain accuracy: 34491/48000 =  71.86 % ||| loss 0.7472932934761047\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8668/12000 =  72.23 % ||| loss 0.7355378866195679\u001b[0m\n",
            "\u001b[92mTest accuracy: 7130/10000 =  71.3 % ||| loss 0.7628032565116882\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.8984603637456894\n",
            "Batch #200 Loss: 0.8811514014005661\n",
            "Batch #300 Loss: 0.8956616550683976\n",
            "\u001b[92mTrain accuracy: 34708/48000 =  72.31 % ||| loss 0.7268930673599243\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8717/12000 =  72.64 % ||| loss 0.7142667770385742\u001b[0m\n",
            "\u001b[92mTest accuracy: 7193/10000 =  71.93 % ||| loss 0.7426508665084839\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.8753010028600693\n",
            "Batch #200 Loss: 0.8776867407560348\n",
            "Batch #300 Loss: 0.8761534970998764\n",
            "\u001b[92mTrain accuracy: 34796/48000 =  72.49 % ||| loss 0.7251025438308716\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8744/12000 =  72.87 % ||| loss 0.7114672660827637\u001b[0m\n",
            "\u001b[92mTest accuracy: 7213/10000 =  72.13 % ||| loss 0.7442556619644165\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.8591567760705948\n",
            "Batch #200 Loss: 0.8631522208452225\n",
            "Batch #300 Loss: 0.8732188457250595\n",
            "\u001b[92mTrain accuracy: 34883/48000 =  72.67 % ||| loss 0.7146902084350586\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8710/12000 =  72.58 % ||| loss 0.7024380564689636\u001b[0m\n",
            "\u001b[92mTest accuracy: 7204/10000 =  72.04 % ||| loss 0.7324978113174438\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.8531534135341644\n",
            "Batch #200 Loss: 0.8636122643947601\n",
            "Batch #300 Loss: 0.8400415521860123\n",
            "\u001b[92mTrain accuracy: 34976/48000 =  72.87 % ||| loss 0.7050582766532898\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8754/12000 =  72.95 % ||| loss 0.6902602910995483\u001b[0m\n",
            "\u001b[92mTest accuracy: 7238/10000 =  72.38 % ||| loss 0.7226035594940186\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.8402900785207749\n",
            "Batch #200 Loss: 0.8305597925186157\n",
            "Batch #300 Loss: 0.8403235512971878\n",
            "\u001b[92mTrain accuracy: 35272/48000 =  73.48 % ||| loss 0.6988115906715393\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8838/12000 =  73.65 % ||| loss 0.6865620613098145\u001b[0m\n",
            "\u001b[92mTest accuracy: 7288/10000 =  72.88 % ||| loss 0.7165620923042297\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Dropout_1726150070.105228_26</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_26' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Dropout_1726150070.105228_26</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[93m!!!!!!! Hyper Param Tuning Finished!!!!!!!!!!!\u001b[0m\n",
            "Best Model: Lenet5Dropout(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "wandb name: Lenet5Dropout_1726150070.105228_0\n",
            "\n",
            "HyperParams: {'learning_rate': 0.001, 'momentum': 0.7, 'dropout': 0.5}\n",
            "\n",
            "Accuracies: {'train': 0.9314166666666667, 'val': 0.90425, 'test': 0.8996}\n"
          ]
        }
      ],
      "source": [
        "class Lenet5Dropout(Lenet5):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(kwargs['dropout'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.max_pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.max_pool1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    \n",
        "param_grid = {\n",
        "  'learning_rate':[0.1, 0.01,0.001],\n",
        "  'momentum':[0, 0.9, 0.7],\n",
        "  'dropout':[0.2, 0.35, 0.5]\n",
        "}\n",
        "\n",
        "best_dropout = hyperparameter_tuning(Lenet5Dropout, dataloaders, device, 25, **param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiiV2yXT4CFj"
      },
      "source": [
        "#### Using Weight Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHirXkQmNu2V",
        "outputId": "cd74126e-da53-4703-9710-93a3b01165ba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_113046-Lenet5Decay_1726155046.322212_0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_0' target=\"_blank\">Lenet5Decay_1726155046.322212_0</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3026756548881533\n",
            "Batch #200 Loss: 2.3027213835716247\n",
            "Batch #300 Loss: 2.3027830123901367\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3026111125946045\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302717924118042\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026750087738037\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.302762153148651\n",
            "Batch #200 Loss: 2.3027604484558104\n",
            "Batch #300 Loss: 2.302781307697296\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.3026351928710938\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.3026492595672607\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026533126831055\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3026110672950746\n",
            "Batch #200 Loss: 2.3027409410476682\n",
            "Batch #300 Loss: 2.302689712047577\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3027563095092773\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3028316497802734\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302744150161743\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3027427697181704\n",
            "Batch #200 Loss: 2.302687964439392\n",
            "Batch #300 Loss: 2.302718288898468\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3026859760284424\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3027260303497314\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027091026306152\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3025993585586546\n",
            "Batch #200 Loss: 2.3027941608428955\n",
            "Batch #300 Loss: 2.302648379802704\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3026528358459473\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.302827835083008\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302690029144287\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.30281055688858\n",
            "Batch #200 Loss: 2.3027020645141603\n",
            "Batch #300 Loss: 2.302829864025116\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3026435375213623\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302549362182617\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302604913711548\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.3027503657341004\n",
            "Batch #200 Loss: 2.3027115416526795\n",
            "Batch #300 Loss: 2.3027045154571533\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3026082515716553\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302682638168335\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302609920501709\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.3026898288726807\n",
            "Batch #200 Loss: 2.3027888226509092\n",
            "Batch #300 Loss: 2.302627568244934\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.302607536315918\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.3027071952819824\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302609443664551\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.302757341861725\n",
            "Batch #200 Loss: 2.3027452850341796\n",
            "Batch #300 Loss: 2.30253381729126\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.3027150630950928\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.302828311920166\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027286529541016\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.3027417254447937\n",
            "Batch #200 Loss: 2.3027157616615295\n",
            "Batch #300 Loss: 2.3027333784103394\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.3026065826416016\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.30269193649292\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026187419891357\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.3025529646873473\n",
            "Batch #200 Loss: 2.302724940776825\n",
            "Batch #300 Loss: 2.3028324913978575\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.302607536315918\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.3026857376098633\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302665948867798\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.3026911306381224\n",
            "Batch #200 Loss: 2.302653954029083\n",
            "Batch #300 Loss: 2.302753007411957\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3026278018951416\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302726984024048\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026318550109863\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.302788083553314\n",
            "Batch #200 Loss: 2.302761378288269\n",
            "Batch #300 Loss: 2.302619915008545\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3026413917541504\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.3027091026306152\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026912212371826\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.3027116417884828\n",
            "Batch #200 Loss: 2.3027974438667296\n",
            "Batch #300 Loss: 2.3027863073349\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302617073059082\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3028311729431152\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302659034729004\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.3027053904533386\n",
            "Batch #200 Loss: 2.3027264404296877\n",
            "Batch #300 Loss: 2.302843081951141\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302664279937744\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3025999069213867\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026010990142822\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.302690668106079\n",
            "Batch #200 Loss: 2.30268816947937\n",
            "Batch #300 Loss: 2.302752914428711\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3026299476623535\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.302598237991333\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026273250579834\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.3027938413619995\n",
            "Batch #200 Loss: 2.302714331150055\n",
            "Batch #300 Loss: 2.3026952052116396\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3025827407836914\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3027007579803467\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026187419891357\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.302722647190094\n",
            "Batch #200 Loss: 2.30270085811615\n",
            "Batch #300 Loss: 2.3027686309814452\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3026514053344727\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.3026676177978516\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302617311477661\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.302683346271515\n",
            "Batch #200 Loss: 2.3026856637001036\n",
            "Batch #300 Loss: 2.302659282684326\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3026318550109863\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3028526306152344\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302690029144287\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.3027498149871826\n",
            "Batch #200 Loss: 2.3027620053291322\n",
            "Batch #300 Loss: 2.302766377925873\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302640676498413\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302870512008667\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027005195617676\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.3027280664443968\n",
            "Batch #200 Loss: 2.3027316689491273\n",
            "Batch #300 Loss: 2.3028364896774294\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3026158809661865\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3027873039245605\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026280403137207\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.302662706375122\n",
            "Batch #200 Loss: 2.302668797969818\n",
            "Batch #300 Loss: 2.302751636505127\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3026204109191895\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302629232406616\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302638530731201\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.3027145648002625\n",
            "Batch #200 Loss: 2.302703456878662\n",
            "Batch #300 Loss: 2.302762899398804\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302638530731201\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3027074337005615\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302664279937744\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.302657732963562\n",
            "Batch #200 Loss: 2.302740409374237\n",
            "Batch #300 Loss: 2.302541596889496\n",
            "\u001b[92mTrain accuracy: 4739/48000 =  9.873 % ||| loss 2.3026421070098877\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1261/12000 =  10.51 % ||| loss 2.3025319576263428\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302605628967285\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3027187037467955\n",
            "Batch #200 Loss: 2.302791004180908\n",
            "Batch #300 Loss: 2.3027127981185913\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3026766777038574\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3029227256774902\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30269718170166\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_0</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_0' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_0</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_113322-Lenet5Decay_1726155046.322212_1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_1' target=\"_blank\">Lenet5Decay_1726155046.322212_1</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.0947487485408782\n",
            "Batch #200 Loss: 1.0852064913511277\n",
            "Batch #300 Loss: 0.8171360576152802\n",
            "\u001b[92mTrain accuracy: 34054/48000 =  70.95 % ||| loss 0.7452675104141235\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8577/12000 =  71.47 % ||| loss 0.7276946306228638\u001b[0m\n",
            "\u001b[92mTest accuracy: 7063/10000 =  70.63 % ||| loss 0.7577359080314636\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6781147867441177\n",
            "Batch #200 Loss: 0.6417628601193428\n",
            "Batch #300 Loss: 0.5929045671224594\n",
            "\u001b[92mTrain accuracy: 38141/48000 =  79.46 % ||| loss 0.5445351600646973\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9616/12000 =  80.13 % ||| loss 0.5392951369285583\u001b[0m\n",
            "\u001b[92mTest accuracy: 7872/10000 =  78.72 % ||| loss 0.567488968372345\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5753523609042168\n",
            "Batch #200 Loss: 0.5444029885530471\n",
            "Batch #300 Loss: 0.5304844823479652\n",
            "\u001b[92mTrain accuracy: 38878/48000 =  81.0 % ||| loss 0.526229977607727\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9721/12000 =  81.01 % ||| loss 0.5248546004295349\u001b[0m\n",
            "\u001b[92mTest accuracy: 7982/10000 =  79.82 % ||| loss 0.5473369359970093\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.511691238284111\n",
            "Batch #200 Loss: 0.4895991709828377\n",
            "Batch #300 Loss: 0.5005043590068817\n",
            "\u001b[92mTrain accuracy: 39951/48000 =  83.23 % ||| loss 0.45217618346214294\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9978/12000 =  83.15 % ||| loss 0.45604828000068665\u001b[0m\n",
            "\u001b[92mTest accuracy: 8232/10000 =  82.32 % ||| loss 0.4766543507575989\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4707052400708199\n",
            "Batch #200 Loss: 0.48738951325416563\n",
            "Batch #300 Loss: 0.46673675447702406\n",
            "\u001b[92mTrain accuracy: 39738/48000 =  82.79 % ||| loss 0.47001683712005615\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9938/12000 =  82.82 % ||| loss 0.4732895791530609\u001b[0m\n",
            "\u001b[92mTest accuracy: 8191/10000 =  81.91 % ||| loss 0.49670323729515076\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.47173763245344164\n",
            "Batch #200 Loss: 0.4499310231208801\n",
            "Batch #300 Loss: 0.45284785717725756\n",
            "\u001b[92mTrain accuracy: 40246/48000 =  83.85 % ||| loss 0.43572014570236206\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10071/12000 =  83.93 % ||| loss 0.43806129693984985\u001b[0m\n",
            "\u001b[92mTest accuracy: 8294/10000 =  82.94 % ||| loss 0.4593344032764435\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.4467557588219643\n",
            "Batch #200 Loss: 0.4451691988110542\n",
            "Batch #300 Loss: 0.45004780024290086\n",
            "\u001b[92mTrain accuracy: 40229/48000 =  83.81 % ||| loss 0.4446839690208435\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10021/12000 =  83.51 % ||| loss 0.44940030574798584\u001b[0m\n",
            "\u001b[92mTest accuracy: 8294/10000 =  82.94 % ||| loss 0.47398027777671814\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.4387331482768059\n",
            "Batch #200 Loss: 0.42349466770887373\n",
            "Batch #300 Loss: 0.4360368195176125\n",
            "\u001b[92mTrain accuracy: 40823/48000 =  85.05 % ||| loss 0.41274675726890564\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10195/12000 =  84.96 % ||| loss 0.4145043194293976\u001b[0m\n",
            "\u001b[92mTest accuracy: 8425/10000 =  84.25 % ||| loss 0.43594035506248474\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.42147658452391623\n",
            "Batch #200 Loss: 0.42841544359922407\n",
            "Batch #300 Loss: 0.4256527808308601\n",
            "\u001b[92mTrain accuracy: 40445/48000 =  84.26 % ||| loss 0.4217747747898102\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10069/12000 =  83.91 % ||| loss 0.42872247099876404\u001b[0m\n",
            "\u001b[92mTest accuracy: 8311/10000 =  83.11 % ||| loss 0.44348227977752686\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.41825583294034\n",
            "Batch #200 Loss: 0.4188552838563919\n",
            "Batch #300 Loss: 0.4195872063934803\n",
            "\u001b[92mTrain accuracy: 41053/48000 =  85.53 % ||| loss 0.4018308222293854\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10221/12000 =  85.17 % ||| loss 0.4097987711429596\u001b[0m\n",
            "\u001b[92mTest accuracy: 8480/10000 =  84.8 % ||| loss 0.42163288593292236\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.4159161481261253\n",
            "Batch #200 Loss: 0.4315575134754181\n",
            "Batch #300 Loss: 0.406179760992527\n",
            "\u001b[92mTrain accuracy: 40665/48000 =  84.72 % ||| loss 0.4162329435348511\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10132/12000 =  84.43 % ||| loss 0.4235524833202362\u001b[0m\n",
            "\u001b[92mTest accuracy: 8378/10000 =  83.78 % ||| loss 0.4376954734325409\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.42175926923751833\n",
            "Batch #200 Loss: 0.4093378746509552\n",
            "Batch #300 Loss: 0.4225264129042625\n",
            "\u001b[92mTrain accuracy: 41192/48000 =  85.82 % ||| loss 0.39524486660957336\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10257/12000 =  85.47 % ||| loss 0.4031640589237213\u001b[0m\n",
            "\u001b[92mTest accuracy: 8466/10000 =  84.66 % ||| loss 0.422953724861145\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.41268700391054153\n",
            "Batch #200 Loss: 0.40397791504859926\n",
            "Batch #300 Loss: 0.415858911126852\n",
            "\u001b[92mTrain accuracy: 40433/48000 =  84.24 % ||| loss 0.43898311257362366\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10111/12000 =  84.26 % ||| loss 0.4396439790725708\u001b[0m\n",
            "\u001b[92mTest accuracy: 8324/10000 =  83.24 % ||| loss 0.4623579680919647\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3938099879026413\n",
            "Batch #200 Loss: 0.4253808709979057\n",
            "Batch #300 Loss: 0.3988765922188759\n",
            "\u001b[92mTrain accuracy: 40853/48000 =  85.11 % ||| loss 0.4035584330558777\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10219/12000 =  85.16 % ||| loss 0.40941840410232544\u001b[0m\n",
            "\u001b[92mTest accuracy: 8432/10000 =  84.32 % ||| loss 0.42999136447906494\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.40642244815826417\n",
            "Batch #200 Loss: 0.4136061976850033\n",
            "Batch #300 Loss: 0.39514189437031744\n",
            "\u001b[92mTrain accuracy: 41576/48000 =  86.62 % ||| loss 0.3881559669971466\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10363/12000 =  86.36 % ||| loss 0.3971509337425232\u001b[0m\n",
            "\u001b[92mTest accuracy: 8567/10000 =  85.67 % ||| loss 0.41292262077331543\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.41018064379692076\n",
            "Batch #200 Loss: 0.40487521678209304\n",
            "Batch #300 Loss: 0.40138547524809837\n",
            "\u001b[92mTrain accuracy: 40863/48000 =  85.13 % ||| loss 0.40286821126937866\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10224/12000 =  85.2 % ||| loss 0.40724292397499084\u001b[0m\n",
            "\u001b[92mTest accuracy: 8414/10000 =  84.14 % ||| loss 0.4280736446380615\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.4002902176976204\n",
            "Batch #200 Loss: 0.4057340097427368\n",
            "Batch #300 Loss: 0.40890860199928286\n",
            "\u001b[92mTrain accuracy: 41036/48000 =  85.49 % ||| loss 0.3966295123100281\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10296/12000 =  85.8 % ||| loss 0.4049004018306732\u001b[0m\n",
            "\u001b[92mTest accuracy: 8459/10000 =  84.59 % ||| loss 0.43163105845451355\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.4169508355855942\n",
            "Batch #200 Loss: 0.39503618463873863\n",
            "Batch #300 Loss: 0.40329704865813254\n",
            "\u001b[92mTrain accuracy: 41366/48000 =  86.18 % ||| loss 0.3849095106124878\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10275/12000 =  85.62 % ||| loss 0.39474913477897644\u001b[0m\n",
            "\u001b[92mTest accuracy: 8521/10000 =  85.21 % ||| loss 0.4164981245994568\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.4136278024315834\n",
            "Batch #200 Loss: 0.3982901833951473\n",
            "Batch #300 Loss: 0.3987509709596634\n",
            "\u001b[92mTrain accuracy: 41377/48000 =  86.2 % ||| loss 0.3820381164550781\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10304/12000 =  85.87 % ||| loss 0.39058640599250793\u001b[0m\n",
            "\u001b[92mTest accuracy: 8525/10000 =  85.25 % ||| loss 0.4095664620399475\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.3979622822999954\n",
            "Batch #200 Loss: 0.39457751899957655\n",
            "Batch #300 Loss: 0.3985976266860962\n",
            "\u001b[92mTrain accuracy: 41781/48000 =  87.04 % ||| loss 0.37333232164382935\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10380/12000 =  86.5 % ||| loss 0.3851771652698517\u001b[0m\n",
            "\u001b[92mTest accuracy: 8587/10000 =  85.87 % ||| loss 0.40339162945747375\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.3894123025238514\n",
            "Batch #200 Loss: 0.4028938725590706\n",
            "Batch #300 Loss: 0.3948918642103672\n",
            "\u001b[92mTrain accuracy: 41236/48000 =  85.91 % ||| loss 0.40313977003097534\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10285/12000 =  85.71 % ||| loss 0.40886297821998596\u001b[0m\n",
            "\u001b[92mTest accuracy: 8507/10000 =  85.07 % ||| loss 0.42562058568000793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.39492890775203704\n",
            "Batch #200 Loss: 0.3764783227443695\n",
            "Batch #300 Loss: 0.41936218455433844\n",
            "\u001b[92mTrain accuracy: 40784/48000 =  84.97 % ||| loss 0.40129756927490234\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10164/12000 =  84.7 % ||| loss 0.40522825717926025\u001b[0m\n",
            "\u001b[92mTest accuracy: 8375/10000 =  83.75 % ||| loss 0.4270128309726715\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.40043467298150065\n",
            "Batch #200 Loss: 0.3952659797668457\n",
            "Batch #300 Loss: 0.3845334467291832\n",
            "\u001b[92mTrain accuracy: 41767/48000 =  87.01 % ||| loss 0.3664199113845825\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10379/12000 =  86.49 % ||| loss 0.3775249123573303\u001b[0m\n",
            "\u001b[92mTest accuracy: 8580/10000 =  85.8 % ||| loss 0.39478325843811035\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.38480156496167184\n",
            "Batch #200 Loss: 0.393511995524168\n",
            "Batch #300 Loss: 0.3901749849319458\n",
            "\u001b[92mTrain accuracy: 41831/48000 =  87.15 % ||| loss 0.3575418293476105\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10432/12000 =  86.93 % ||| loss 0.36494043469429016\u001b[0m\n",
            "\u001b[92mTest accuracy: 8636/10000 =  86.36 % ||| loss 0.3851041793823242\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.39437075093388557\n",
            "Batch #200 Loss: 0.3877772691845894\n",
            "Batch #300 Loss: 0.39379251018166544\n",
            "\u001b[92mTrain accuracy: 39968/48000 =  83.27 % ||| loss 0.4403684139251709\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9984/12000 =  83.2 % ||| loss 0.45123979449272156\u001b[0m\n",
            "\u001b[92mTest accuracy: 8178/10000 =  81.78 % ||| loss 0.47845950722694397\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_1</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_1</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_113553-Lenet5Decay_1726155046.322212_2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_2' target=\"_blank\">Lenet5Decay_1726155046.322212_2</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.126390256881714\n",
            "Batch #200 Loss: 1.0910818678140641\n",
            "Batch #300 Loss: 0.7965350931882859\n",
            "\u001b[92mTrain accuracy: 36008/48000 =  75.02 % ||| loss 0.6485482454299927\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9058/12000 =  75.48 % ||| loss 0.6396463513374329\u001b[0m\n",
            "\u001b[92mTest accuracy: 7374/10000 =  73.74 % ||| loss 0.6670076251029968\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6681223648786545\n",
            "Batch #200 Loss: 0.601432491838932\n",
            "Batch #300 Loss: 0.562272193133831\n",
            "\u001b[92mTrain accuracy: 38495/48000 =  80.2 % ||| loss 0.5232740044593811\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9657/12000 =  80.47 % ||| loss 0.5166505575180054\u001b[0m\n",
            "\u001b[92mTest accuracy: 7924/10000 =  79.24 % ||| loss 0.5533588528633118\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5089040794968605\n",
            "Batch #200 Loss: 0.4795686122775078\n",
            "Batch #300 Loss: 0.47344632029533384\n",
            "\u001b[92mTrain accuracy: 40370/48000 =  84.1 % ||| loss 0.43263086676597595\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10074/12000 =  83.95 % ||| loss 0.4358028769493103\u001b[0m\n",
            "\u001b[92mTest accuracy: 8301/10000 =  83.01 % ||| loss 0.4593014717102051\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.43717020124197004\n",
            "Batch #200 Loss: 0.438266262114048\n",
            "Batch #300 Loss: 0.42633337825536727\n",
            "\u001b[92mTrain accuracy: 40945/48000 =  85.3 % ||| loss 0.3922670781612396\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10219/12000 =  85.16 % ||| loss 0.39928895235061646\u001b[0m\n",
            "\u001b[92mTest accuracy: 8448/10000 =  84.48 % ||| loss 0.4244159162044525\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.40165887013077733\n",
            "Batch #200 Loss: 0.38523060753941535\n",
            "Batch #300 Loss: 0.3917604431509972\n",
            "\u001b[92mTrain accuracy: 41517/48000 =  86.49 % ||| loss 0.3633668124675751\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10317/12000 =  85.97 % ||| loss 0.37900999188423157\u001b[0m\n",
            "\u001b[92mTest accuracy: 8545/10000 =  85.45 % ||| loss 0.3959357738494873\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.380273819565773\n",
            "Batch #200 Loss: 0.3587840886414051\n",
            "Batch #300 Loss: 0.3585544587671757\n",
            "\u001b[92mTrain accuracy: 41444/48000 =  86.34 % ||| loss 0.3692622780799866\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10262/12000 =  85.52 % ||| loss 0.39212095737457275\u001b[0m\n",
            "\u001b[92mTest accuracy: 8532/10000 =  85.32 % ||| loss 0.39964407682418823\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.35765880018472673\n",
            "Batch #200 Loss: 0.3453275540471077\n",
            "Batch #300 Loss: 0.34885593995451925\n",
            "\u001b[92mTrain accuracy: 42290/48000 =  88.1 % ||| loss 0.32741579413414\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10474/12000 =  87.28 % ||| loss 0.35153526067733765\u001b[0m\n",
            "\u001b[92mTest accuracy: 8673/10000 =  86.73 % ||| loss 0.366707444190979\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.34291187331080436\n",
            "Batch #200 Loss: 0.3245482507348061\n",
            "Batch #300 Loss: 0.3314349235594273\n",
            "\u001b[92mTrain accuracy: 42194/48000 =  87.9 % ||| loss 0.32493484020233154\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10481/12000 =  87.34 % ||| loss 0.35198354721069336\u001b[0m\n",
            "\u001b[92mTest accuracy: 8627/10000 =  86.27 % ||| loss 0.3703515827655792\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.31724516332149505\n",
            "Batch #200 Loss: 0.3129622521996498\n",
            "Batch #300 Loss: 0.32504290476441383\n",
            "\u001b[92mTrain accuracy: 42792/48000 =  89.15 % ||| loss 0.29550454020500183\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10617/12000 =  88.48 % ||| loss 0.3209313750267029\u001b[0m\n",
            "\u001b[92mTest accuracy: 8758/10000 =  87.58 % ||| loss 0.3395676910877228\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.30005222737789156\n",
            "Batch #200 Loss: 0.30800728395581245\n",
            "Batch #300 Loss: 0.3115078341960907\n",
            "\u001b[92mTrain accuracy: 42707/48000 =  88.97 % ||| loss 0.2975841164588928\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10637/12000 =  88.64 % ||| loss 0.32088157534599304\u001b[0m\n",
            "\u001b[92mTest accuracy: 8760/10000 =  87.6 % ||| loss 0.3518555760383606\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.29474280431866645\n",
            "Batch #200 Loss: 0.30347401335835456\n",
            "Batch #300 Loss: 0.29990381449460984\n",
            "\u001b[92mTrain accuracy: 42374/48000 =  88.28 % ||| loss 0.3138611614704132\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10486/12000 =  87.38 % ||| loss 0.34970104694366455\u001b[0m\n",
            "\u001b[92mTest accuracy: 8694/10000 =  86.94 % ||| loss 0.36298513412475586\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.29156030803918837\n",
            "Batch #200 Loss: 0.2890345618128777\n",
            "Batch #300 Loss: 0.2966328364610672\n",
            "\u001b[92mTrain accuracy: 43024/48000 =  89.63 % ||| loss 0.28360921144485474\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10601/12000 =  88.34 % ||| loss 0.3123627305030823\u001b[0m\n",
            "\u001b[92mTest accuracy: 8773/10000 =  87.73 % ||| loss 0.3285522162914276\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.27705894023180005\n",
            "Batch #200 Loss: 0.3000648362934589\n",
            "Batch #300 Loss: 0.27624016150832176\n",
            "\u001b[92mTrain accuracy: 43245/48000 =  90.09 % ||| loss 0.27429914474487305\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10674/12000 =  88.95 % ||| loss 0.30681276321411133\u001b[0m\n",
            "\u001b[92mTest accuracy: 8834/10000 =  88.34 % ||| loss 0.32468003034591675\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.28008690908551215\n",
            "Batch #200 Loss: 0.27988194808363914\n",
            "Batch #300 Loss: 0.27426092863082885\n",
            "\u001b[92mTrain accuracy: 43394/48000 =  90.4 % ||| loss 0.2610865831375122\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10689/12000 =  89.08 % ||| loss 0.30168041586875916\u001b[0m\n",
            "\u001b[92mTest accuracy: 8874/10000 =  88.74 % ||| loss 0.31942984461784363\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.2810893280804157\n",
            "Batch #200 Loss: 0.26943215042352675\n",
            "Batch #300 Loss: 0.27385683730244637\n",
            "\u001b[92mTrain accuracy: 43539/48000 =  90.71 % ||| loss 0.2544251084327698\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10741/12000 =  89.51 % ||| loss 0.2953389286994934\u001b[0m\n",
            "\u001b[92mTest accuracy: 8880/10000 =  88.8 % ||| loss 0.3155971169471741\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.2605741754174232\n",
            "Batch #200 Loss: 0.28029267862439156\n",
            "Batch #300 Loss: 0.26794742569327357\n",
            "\u001b[92mTrain accuracy: 43178/48000 =  89.95 % ||| loss 0.26645520329475403\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10591/12000 =  88.26 % ||| loss 0.3097638785839081\u001b[0m\n",
            "\u001b[92mTest accuracy: 8784/10000 =  87.84 % ||| loss 0.33326777815818787\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.2617048518359661\n",
            "Batch #200 Loss: 0.25738973274827004\n",
            "Batch #300 Loss: 0.2745291140675545\n",
            "\u001b[92mTrain accuracy: 42989/48000 =  89.56 % ||| loss 0.27571722865104675\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10583/12000 =  88.19 % ||| loss 0.3151535987854004\u001b[0m\n",
            "\u001b[92mTest accuracy: 8755/10000 =  87.55 % ||| loss 0.339851051568985\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.26557023733854296\n",
            "Batch #200 Loss: 0.2506057343631983\n",
            "Batch #300 Loss: 0.2538127090036869\n",
            "\u001b[92mTrain accuracy: 43800/48000 =  91.25 % ||| loss 0.23629699647426605\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10744/12000 =  89.53 % ||| loss 0.2833386957645416\u001b[0m\n",
            "\u001b[92mTest accuracy: 8940/10000 =  89.4 % ||| loss 0.3034915626049042\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.24909490965306758\n",
            "Batch #200 Loss: 0.26143774144351484\n",
            "Batch #300 Loss: 0.25720173701643945\n",
            "\u001b[92mTrain accuracy: 40540/48000 =  84.46 % ||| loss 0.3949323296546936\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9992/12000 =  83.27 % ||| loss 0.43629318475723267\u001b[0m\n",
            "\u001b[92mTest accuracy: 8228/10000 =  82.28 % ||| loss 0.45322364568710327\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.2464493941515684\n",
            "Batch #200 Loss: 0.24501966267824174\n",
            "Batch #300 Loss: 0.26150074556469916\n",
            "\u001b[92mTrain accuracy: 44059/48000 =  91.79 % ||| loss 0.22533422708511353\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10750/12000 =  89.58 % ||| loss 0.27752891182899475\u001b[0m\n",
            "\u001b[92mTest accuracy: 8932/10000 =  89.32 % ||| loss 0.2997053563594818\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.24195681661367416\n",
            "Batch #200 Loss: 0.2453474621474743\n",
            "Batch #300 Loss: 0.2467542351782322\n",
            "\u001b[92mTrain accuracy: 43660/48000 =  90.96 % ||| loss 0.2401210516691208\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10717/12000 =  89.31 % ||| loss 0.29477307200431824\u001b[0m\n",
            "\u001b[92mTest accuracy: 8856/10000 =  88.56 % ||| loss 0.31743890047073364\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.24173336431384088\n",
            "Batch #200 Loss: 0.23716474868357182\n",
            "Batch #300 Loss: 0.24299525409936906\n",
            "\u001b[92mTrain accuracy: 43957/48000 =  91.58 % ||| loss 0.22997000813484192\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10732/12000 =  89.43 % ||| loss 0.287397176027298\u001b[0m\n",
            "\u001b[92mTest accuracy: 8929/10000 =  89.29 % ||| loss 0.30844247341156006\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.23759178817272186\n",
            "Batch #200 Loss: 0.23844274252653122\n",
            "Batch #300 Loss: 0.240673236399889\n",
            "\u001b[92mTrain accuracy: 44195/48000 =  92.07 % ||| loss 0.21404677629470825\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10806/12000 =  90.05 % ||| loss 0.2744511663913727\u001b[0m\n",
            "\u001b[92mTest accuracy: 8960/10000 =  89.6 % ||| loss 0.2908181846141815\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.2261452452093363\n",
            "Batch #200 Loss: 0.2379232170432806\n",
            "Batch #300 Loss: 0.23434185303747654\n",
            "\u001b[92mTrain accuracy: 44087/48000 =  91.85 % ||| loss 0.218641459941864\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10793/12000 =  89.94 % ||| loss 0.2824355661869049\u001b[0m\n",
            "\u001b[92mTest accuracy: 8939/10000 =  89.39 % ||| loss 0.3031587600708008\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.222305456250906\n",
            "Batch #200 Loss: 0.23862529039382935\n",
            "Batch #300 Loss: 0.23136781066656112\n",
            "\u001b[92mTrain accuracy: 44047/48000 =  91.76 % ||| loss 0.2271929383277893\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10726/12000 =  89.38 % ||| loss 0.2885904610157013\u001b[0m\n",
            "\u001b[92mTest accuracy: 8929/10000 =  89.29 % ||| loss 0.3046254813671112\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_2</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_2</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_113821-Lenet5Decay_1726155046.322212_3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_3' target=\"_blank\">Lenet5Decay_1726155046.322212_3</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302839913368225\n",
            "Batch #200 Loss: 2.303093957901001\n",
            "Batch #300 Loss: 2.303577604293823\n",
            "\u001b[92mTrain accuracy: 4739/48000 =  9.873 % ||| loss 2.3034589290618896\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1261/12000 =  10.51 % ||| loss 2.3031668663024902\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3034403324127197\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.303311779499054\n",
            "Batch #200 Loss: 2.303526885509491\n",
            "Batch #300 Loss: 2.303621349334717\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.303335666656494\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3034276962280273\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3034162521362305\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3038113784790037\n",
            "Batch #200 Loss: 2.3039122080802916\n",
            "Batch #300 Loss: 2.3030872535705567\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.3034727573394775\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.303452491760254\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303287982940674\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3034898328781126\n",
            "Batch #200 Loss: 2.303543713092804\n",
            "Batch #300 Loss: 2.3037024116516114\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.303980588912964\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.304685592651367\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3040428161621094\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3029792737960815\n",
            "Batch #200 Loss: 2.3033873414993287\n",
            "Batch #300 Loss: 2.303550806045532\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.303013324737549\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3029251098632812\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028857707977295\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3037246298789977\n",
            "Batch #200 Loss: 2.303902361392975\n",
            "Batch #300 Loss: 2.3029851841926576\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3031668663024902\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.303483486175537\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303241491317749\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.303447918891907\n",
            "Batch #200 Loss: 2.303602080345154\n",
            "Batch #300 Loss: 2.3037243247032166\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.303870439529419\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3041129112243652\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30409836769104\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.3035350823402405\n",
            "Batch #200 Loss: 2.3037567019462584\n",
            "Batch #300 Loss: 2.303642017841339\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3032093048095703\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.303093671798706\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3031396865844727\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.303108744621277\n",
            "Batch #200 Loss: 2.3039586997032164\n",
            "Batch #300 Loss: 2.303401412963867\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.303778886795044\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3040578365325928\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303919792175293\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.3035714745521547\n",
            "Batch #200 Loss: 2.303845398426056\n",
            "Batch #300 Loss: 2.303574950695038\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.303302049636841\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.3033812046051025\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303288221359253\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.302604522705078\n",
            "Batch #200 Loss: 2.3034809684753417\n",
            "Batch #300 Loss: 2.3033197331428528\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3034067153930664\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3033673763275146\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3033905029296875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.303546254634857\n",
            "Batch #200 Loss: 2.3030187630653383\n",
            "Batch #300 Loss: 2.303271088600159\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.30293869972229\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302976131439209\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3029963970184326\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.3033507537841795\n",
            "Batch #200 Loss: 2.303061857223511\n",
            "Batch #300 Loss: 2.3033655881881714\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3033668994903564\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3036625385284424\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3033764362335205\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.3041070389747618\n",
            "Batch #200 Loss: 2.3037762308120726\n",
            "Batch #300 Loss: 2.303592264652252\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.3034422397613525\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.303316354751587\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303379774093628\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.303151910305023\n",
            "Batch #200 Loss: 2.3030746817588805\n",
            "Batch #300 Loss: 2.303307664394379\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.304077386856079\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.3044447898864746\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.304103136062622\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.303594138622284\n",
            "Batch #200 Loss: 2.3036647820472718\n",
            "Batch #300 Loss: 2.3035738849639893\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.303046226501465\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.30329966545105\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303037643432617\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.303452563285828\n",
            "Batch #200 Loss: 2.303642747402191\n",
            "Batch #300 Loss: 2.303575325012207\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302765369415283\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302887201309204\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302849292755127\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.303218672275543\n",
            "Batch #200 Loss: 2.3033035039901733\n",
            "Batch #300 Loss: 2.3039113593101503\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3029658794403076\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.303060531616211\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3030803203582764\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.303783881664276\n",
            "Batch #200 Loss: 2.3033415055274964\n",
            "Batch #300 Loss: 2.3033638286590574\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.304201602935791\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.304434061050415\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3043880462646484\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.3033367133140565\n",
            "Batch #200 Loss: 2.303813586235046\n",
            "Batch #300 Loss: 2.303751142024994\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3029067516326904\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3030343055725098\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30291485786438\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.303659250736237\n",
            "Batch #200 Loss: 2.30321457862854\n",
            "Batch #300 Loss: 2.3035931158065797\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3034310340881348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.3030920028686523\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3033106327056885\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.30403395652771\n",
            "Batch #200 Loss: 2.3036769938468935\n",
            "Batch #300 Loss: 2.303755087852478\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.303438425064087\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.3033435344696045\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303436756134033\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.303687942028046\n",
            "Batch #200 Loss: 2.3031638646125794\n",
            "Batch #300 Loss: 2.303256723880768\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.30311918258667\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3031411170959473\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3032047748565674\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.303892958164215\n",
            "Batch #200 Loss: 2.30382586479187\n",
            "Batch #300 Loss: 2.303911712169647\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3041656017303467\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3044192790985107\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3043415546417236\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3041058778762817\n",
            "Batch #200 Loss: 2.303990080356598\n",
            "Batch #300 Loss: 2.303578267097473\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.303424835205078\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3040900230407715\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3037829399108887\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_3</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_3' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_3</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_114056-Lenet5Decay_1726155046.322212_4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_4' target=\"_blank\">Lenet5Decay_1726155046.322212_4</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.053508312702179\n",
            "Batch #200 Loss: 0.8479281806945801\n",
            "Batch #300 Loss: 0.6694279837608338\n",
            "\u001b[92mTrain accuracy: 34081/48000 =  71.0 % ||| loss 0.784342348575592\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8493/12000 =  70.78 % ||| loss 0.7830308079719543\u001b[0m\n",
            "\u001b[92mTest accuracy: 7062/10000 =  70.62 % ||| loss 0.7982873320579529\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6246040958166122\n",
            "Batch #200 Loss: 0.5981770044565201\n",
            "Batch #300 Loss: 0.5721274343132973\n",
            "\u001b[92mTrain accuracy: 38472/48000 =  80.15 % ||| loss 0.5783445239067078\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9573/12000 =  79.77 % ||| loss 0.5783422589302063\u001b[0m\n",
            "\u001b[92mTest accuracy: 7952/10000 =  79.52 % ||| loss 0.5912169814109802\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.566903832256794\n",
            "Batch #200 Loss: 0.5582227486371995\n",
            "Batch #300 Loss: 0.5505937466025352\n",
            "\u001b[92mTrain accuracy: 39411/48000 =  82.11 % ||| loss 0.5178987979888916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9892/12000 =  82.43 % ||| loss 0.5140958428382874\u001b[0m\n",
            "\u001b[92mTest accuracy: 8123/10000 =  81.23 % ||| loss 0.5402282476425171\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5597087574005127\n",
            "Batch #200 Loss: 0.5724404725432396\n",
            "Batch #300 Loss: 0.5631949141621589\n",
            "\u001b[92mTrain accuracy: 37871/48000 =  78.9 % ||| loss 0.6006131768226624\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9415/12000 =  78.46 % ||| loss 0.6005110740661621\u001b[0m\n",
            "\u001b[92mTest accuracy: 7791/10000 =  77.91 % ||| loss 0.6216924786567688\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5512061470746994\n",
            "Batch #200 Loss: 0.5544230040907859\n",
            "Batch #300 Loss: 0.5438577619194984\n",
            "\u001b[92mTrain accuracy: 38893/48000 =  81.03 % ||| loss 0.5246127843856812\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9690/12000 =  80.75 % ||| loss 0.5221534371376038\u001b[0m\n",
            "\u001b[92mTest accuracy: 8047/10000 =  80.47 % ||| loss 0.5409239530563354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5566689389944076\n",
            "Batch #200 Loss: 0.5418885749578476\n",
            "Batch #300 Loss: 0.5492196083068848\n",
            "\u001b[92mTrain accuracy: 38173/48000 =  79.53 % ||| loss 0.5498244166374207\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9514/12000 =  79.28 % ||| loss 0.5487098693847656\u001b[0m\n",
            "\u001b[92mTest accuracy: 7861/10000 =  78.61 % ||| loss 0.5647132992744446\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5427900713682174\n",
            "Batch #200 Loss: 0.5389609569311142\n",
            "Batch #300 Loss: 0.5224104714393616\n",
            "\u001b[92mTrain accuracy: 39167/48000 =  81.6 % ||| loss 0.5464725494384766\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9818/12000 =  81.82 % ||| loss 0.5444284677505493\u001b[0m\n",
            "\u001b[92mTest accuracy: 8130/10000 =  81.3 % ||| loss 0.5540409088134766\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.532030316889286\n",
            "Batch #200 Loss: 0.5331596046686172\n",
            "Batch #300 Loss: 0.5340285819768905\n",
            "\u001b[92mTrain accuracy: 37071/48000 =  77.23 % ||| loss 0.5870245695114136\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9235/12000 =  76.96 % ||| loss 0.5867784023284912\u001b[0m\n",
            "\u001b[92mTest accuracy: 7651/10000 =  76.51 % ||| loss 0.610177218914032\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.5356313827633857\n",
            "Batch #200 Loss: 0.5394527572393417\n",
            "Batch #300 Loss: 0.529321790933609\n",
            "\u001b[92mTrain accuracy: 39514/48000 =  82.32 % ||| loss 0.5247305035591125\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9850/12000 =  82.08 % ||| loss 0.5237628817558289\u001b[0m\n",
            "\u001b[92mTest accuracy: 8179/10000 =  81.79 % ||| loss 0.5409310460090637\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.5499896195530891\n",
            "Batch #200 Loss: 0.5366880291700363\n",
            "Batch #300 Loss: 0.5425862312316895\n",
            "\u001b[92mTrain accuracy: 39174/48000 =  81.61 % ||| loss 0.5136240124702454\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9750/12000 =  81.25 % ||| loss 0.5142917037010193\u001b[0m\n",
            "\u001b[92mTest accuracy: 8078/10000 =  80.78 % ||| loss 0.5326995849609375\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.5256396514177323\n",
            "Batch #200 Loss: 0.5555880010128021\n",
            "Batch #300 Loss: 0.553270226418972\n",
            "\u001b[92mTrain accuracy: 38579/48000 =  80.37 % ||| loss 0.5458556413650513\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9608/12000 =  80.07 % ||| loss 0.5466111898422241\u001b[0m\n",
            "\u001b[92mTest accuracy: 7952/10000 =  79.52 % ||| loss 0.559788703918457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5460338446497918\n",
            "Batch #200 Loss: 0.5222093310952186\n",
            "Batch #300 Loss: 0.5172612223029137\n",
            "\u001b[92mTrain accuracy: 39481/48000 =  82.25 % ||| loss 0.4771384596824646\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9870/12000 =  82.25 % ||| loss 0.4747828245162964\u001b[0m\n",
            "\u001b[92mTest accuracy: 8136/10000 =  81.36 % ||| loss 0.48834291100502014\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5290940850973129\n",
            "Batch #200 Loss: 0.5361946830153466\n",
            "Batch #300 Loss: 0.5462138134241105\n",
            "\u001b[92mTrain accuracy: 39541/48000 =  82.38 % ||| loss 0.5273774266242981\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9929/12000 =  82.74 % ||| loss 0.5233564376831055\u001b[0m\n",
            "\u001b[92mTest accuracy: 8117/10000 =  81.17 % ||| loss 0.5499312281608582\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.5411091715097427\n",
            "Batch #200 Loss: 0.5519008114933968\n",
            "Batch #300 Loss: 0.5393139204382896\n",
            "\u001b[92mTrain accuracy: 39104/48000 =  81.47 % ||| loss 0.5152357816696167\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9772/12000 =  81.43 % ||| loss 0.5091556906700134\u001b[0m\n",
            "\u001b[92mTest accuracy: 8072/10000 =  80.72 % ||| loss 0.5350165963172913\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5548151567578316\n",
            "Batch #200 Loss: 0.5339497238397598\n",
            "Batch #300 Loss: 0.5434302508831024\n",
            "\u001b[92mTrain accuracy: 39543/48000 =  82.38 % ||| loss 0.5047406554222107\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9908/12000 =  82.57 % ||| loss 0.4974740147590637\u001b[0m\n",
            "\u001b[92mTest accuracy: 8235/10000 =  82.35 % ||| loss 0.5164490938186646\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.5395499882102013\n",
            "Batch #200 Loss: 0.5386393690109252\n",
            "Batch #300 Loss: 0.5531578519940377\n",
            "\u001b[92mTrain accuracy: 39202/48000 =  81.67 % ||| loss 0.5484442710876465\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9791/12000 =  81.59 % ||| loss 0.5474467277526855\u001b[0m\n",
            "\u001b[92mTest accuracy: 8107/10000 =  81.07 % ||| loss 0.5653549432754517\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5397448071837425\n",
            "Batch #200 Loss: 0.5506452706456184\n",
            "Batch #300 Loss: 0.5423720026016235\n",
            "\u001b[92mTrain accuracy: 39558/48000 =  82.41 % ||| loss 0.4789196252822876\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9895/12000 =  82.46 % ||| loss 0.47391170263290405\u001b[0m\n",
            "\u001b[92mTest accuracy: 8162/10000 =  81.62 % ||| loss 0.49512559175491333\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5372556674480439\n",
            "Batch #200 Loss: 0.5391606739163399\n",
            "Batch #300 Loss: 0.5349825796484947\n",
            "\u001b[92mTrain accuracy: 37927/48000 =  79.01 % ||| loss 0.615234911441803\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9492/12000 =  79.1 % ||| loss 0.6118177175521851\u001b[0m\n",
            "\u001b[92mTest accuracy: 7844/10000 =  78.44 % ||| loss 0.6440960168838501\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5339585819840431\n",
            "Batch #200 Loss: 0.5468052181601525\n",
            "Batch #300 Loss: 0.5195575109124184\n",
            "\u001b[92mTrain accuracy: 39235/48000 =  81.74 % ||| loss 0.5148569941520691\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9755/12000 =  81.29 % ||| loss 0.5198121070861816\u001b[0m\n",
            "\u001b[92mTest accuracy: 8061/10000 =  80.61 % ||| loss 0.5398819446563721\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.544565811753273\n",
            "Batch #200 Loss: 0.5294884461164474\n",
            "Batch #300 Loss: 0.5492774313688278\n",
            "\u001b[92mTrain accuracy: 39329/48000 =  81.94 % ||| loss 0.5109412670135498\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9879/12000 =  82.33 % ||| loss 0.5034927725791931\u001b[0m\n",
            "\u001b[92mTest accuracy: 8097/10000 =  80.97 % ||| loss 0.5241096615791321\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.5375263807177544\n",
            "Batch #200 Loss: 0.5249411183595657\n",
            "Batch #300 Loss: 0.537522883117199\n",
            "\u001b[92mTrain accuracy: 38638/48000 =  80.5 % ||| loss 0.5474929809570312\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9673/12000 =  80.61 % ||| loss 0.5457869172096252\u001b[0m\n",
            "\u001b[92mTest accuracy: 7985/10000 =  79.85 % ||| loss 0.5662609338760376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.5331840831041336\n",
            "Batch #200 Loss: 0.5344680771231651\n",
            "Batch #300 Loss: 0.5399904584884644\n",
            "\u001b[92mTrain accuracy: 39118/48000 =  81.5 % ||| loss 0.5286437273025513\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9802/12000 =  81.68 % ||| loss 0.5260128378868103\u001b[0m\n",
            "\u001b[92mTest accuracy: 8047/10000 =  80.47 % ||| loss 0.5510828495025635\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5252951109409332\n",
            "Batch #200 Loss: 0.5402307996153831\n",
            "Batch #300 Loss: 0.5326442784070968\n",
            "\u001b[92mTrain accuracy: 39654/48000 =  82.61 % ||| loss 0.4848875403404236\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9935/12000 =  82.79 % ||| loss 0.4772375524044037\u001b[0m\n",
            "\u001b[92mTest accuracy: 8205/10000 =  82.05 % ||| loss 0.5032241344451904\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5348650440573692\n",
            "Batch #200 Loss: 0.5253199958801269\n",
            "Batch #300 Loss: 0.5195152053236961\n",
            "\u001b[92mTrain accuracy: 39444/48000 =  82.17 % ||| loss 0.5032383799552917\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9867/12000 =  82.23 % ||| loss 0.5058412551879883\u001b[0m\n",
            "\u001b[92mTest accuracy: 8122/10000 =  81.22 % ||| loss 0.523727536201477\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5196147355437278\n",
            "Batch #200 Loss: 0.5559539312124252\n",
            "Batch #300 Loss: 0.5294422641396522\n",
            "\u001b[92mTrain accuracy: 38928/48000 =  81.1 % ||| loss 0.5454802513122559\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9677/12000 =  80.64 % ||| loss 0.5490866899490356\u001b[0m\n",
            "\u001b[92mTest accuracy: 8021/10000 =  80.21 % ||| loss 0.5599114298820496\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_4</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_4</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_114329-Lenet5Decay_1726155046.322212_5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_5' target=\"_blank\">Lenet5Decay_1726155046.322212_5</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.9, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 1.761655519604683\n",
            "Batch #200 Loss: 0.756212274134159\n",
            "Batch #300 Loss: 0.6211975505948066\n",
            "\u001b[92mTrain accuracy: 38826/48000 =  80.89 % ||| loss 0.5221537947654724\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9700/12000 =  80.83 % ||| loss 0.521485447883606\u001b[0m\n",
            "\u001b[92mTest accuracy: 8010/10000 =  80.1 % ||| loss 0.5444061756134033\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.5084913682937622\n",
            "Batch #200 Loss: 0.4809216833114624\n",
            "Batch #300 Loss: 0.4693568259477615\n",
            "\u001b[92mTrain accuracy: 40311/48000 =  83.98 % ||| loss 0.4336735010147095\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10068/12000 =  83.9 % ||| loss 0.44081366062164307\u001b[0m\n",
            "\u001b[92mTest accuracy: 8330/10000 =  83.3 % ||| loss 0.46262988448143005\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.43583082646131516\n",
            "Batch #200 Loss: 0.4617518442869186\n",
            "Batch #300 Loss: 0.4475122570991516\n",
            "\u001b[92mTrain accuracy: 40894/48000 =  85.2 % ||| loss 0.403260201215744\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10161/12000 =  84.67 % ||| loss 0.4212515950202942\u001b[0m\n",
            "\u001b[92mTest accuracy: 8387/10000 =  83.87 % ||| loss 0.4465172290802002\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.41840509593486785\n",
            "Batch #200 Loss: 0.43464999765157697\n",
            "Batch #300 Loss: 0.4203266626596451\n",
            "\u001b[92mTrain accuracy: 40785/48000 =  84.97 % ||| loss 0.40637364983558655\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10102/12000 =  84.18 % ||| loss 0.42718732357025146\u001b[0m\n",
            "\u001b[92mTest accuracy: 8395/10000 =  83.95 % ||| loss 0.4471840560436249\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.40881179183721544\n",
            "Batch #200 Loss: 0.4198606136441231\n",
            "Batch #300 Loss: 0.4128061231970787\n",
            "\u001b[92mTrain accuracy: 40802/48000 =  85.0 % ||| loss 0.3998248279094696\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10136/12000 =  84.47 % ||| loss 0.4165129065513611\u001b[0m\n",
            "\u001b[92mTest accuracy: 8375/10000 =  83.75 % ||| loss 0.4351136386394501\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.4024149975180626\n",
            "Batch #200 Loss: 0.39822837501764297\n",
            "Batch #300 Loss: 0.41477523058652876\n",
            "\u001b[92mTrain accuracy: 41510/48000 =  86.48 % ||| loss 0.36446261405944824\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10310/12000 =  85.92 % ||| loss 0.3838423490524292\u001b[0m\n",
            "\u001b[92mTest accuracy: 8503/10000 =  85.03 % ||| loss 0.401668518781662\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3955558842420578\n",
            "Batch #200 Loss: 0.4071482437849045\n",
            "Batch #300 Loss: 0.37952463001012804\n",
            "\u001b[92mTrain accuracy: 41486/48000 =  86.43 % ||| loss 0.3736491799354553\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10272/12000 =  85.6 % ||| loss 0.39122018218040466\u001b[0m\n",
            "\u001b[92mTest accuracy: 8499/10000 =  84.99 % ||| loss 0.41066306829452515\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3777246716618538\n",
            "Batch #200 Loss: 0.40213985443115235\n",
            "Batch #300 Loss: 0.388297119140625\n",
            "\u001b[92mTrain accuracy: 41738/48000 =  86.95 % ||| loss 0.357959508895874\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10326/12000 =  86.05 % ||| loss 0.3839828372001648\u001b[0m\n",
            "\u001b[92mTest accuracy: 8580/10000 =  85.8 % ||| loss 0.411866158246994\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3869816449284553\n",
            "Batch #200 Loss: 0.393571098446846\n",
            "Batch #300 Loss: 0.39859504014253616\n",
            "\u001b[92mTrain accuracy: 41759/48000 =  87.0 % ||| loss 0.3534389138221741\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10319/12000 =  85.99 % ||| loss 0.3828808665275574\u001b[0m\n",
            "\u001b[92mTest accuracy: 8538/10000 =  85.38 % ||| loss 0.40696465969085693\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3922905384004116\n",
            "Batch #200 Loss: 0.4025737138092518\n",
            "Batch #300 Loss: 0.38749586284160614\n",
            "\u001b[92mTrain accuracy: 41232/48000 =  85.9 % ||| loss 0.37871798872947693\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10243/12000 =  85.36 % ||| loss 0.40179693698883057\u001b[0m\n",
            "\u001b[92mTest accuracy: 8467/10000 =  84.67 % ||| loss 0.4178902804851532\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.3713936670124531\n",
            "Batch #200 Loss: 0.3817401270568371\n",
            "Batch #300 Loss: 0.3777821870148182\n",
            "\u001b[92mTrain accuracy: 41419/48000 =  86.29 % ||| loss 0.3643215298652649\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10232/12000 =  85.27 % ||| loss 0.3987175524234772\u001b[0m\n",
            "\u001b[92mTest accuracy: 8433/10000 =  84.33 % ||| loss 0.4217839241027832\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.37462689861655235\n",
            "Batch #200 Loss: 0.3753432221710682\n",
            "Batch #300 Loss: 0.38636840403079986\n",
            "\u001b[92mTrain accuracy: 40978/48000 =  85.37 % ||| loss 0.3972141444683075\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10092/12000 =  84.1 % ||| loss 0.4264349341392517\u001b[0m\n",
            "\u001b[92mTest accuracy: 8348/10000 =  83.48 % ||| loss 0.45023468136787415\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.3870095032453537\n",
            "Batch #200 Loss: 0.378332422375679\n",
            "Batch #300 Loss: 0.37831479355692865\n",
            "\u001b[92mTrain accuracy: 41714/48000 =  86.9 % ||| loss 0.3574574291706085\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10286/12000 =  85.72 % ||| loss 0.3789035677909851\u001b[0m\n",
            "\u001b[92mTest accuracy: 8486/10000 =  84.86 % ||| loss 0.4060434401035309\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3616116572916508\n",
            "Batch #200 Loss: 0.3687539176642895\n",
            "Batch #300 Loss: 0.36733760327100756\n",
            "\u001b[92mTrain accuracy: 41987/48000 =  87.47 % ||| loss 0.3357312083244324\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10396/12000 =  86.63 % ||| loss 0.3616301417350769\u001b[0m\n",
            "\u001b[92mTest accuracy: 8540/10000 =  85.4 % ||| loss 0.3816198408603668\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.3493299466371536\n",
            "Batch #200 Loss: 0.3699601548910141\n",
            "Batch #300 Loss: 0.3693062788248062\n",
            "\u001b[92mTrain accuracy: 41733/48000 =  86.94 % ||| loss 0.360615611076355\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10359/12000 =  86.33 % ||| loss 0.3790642023086548\u001b[0m\n",
            "\u001b[92mTest accuracy: 8555/10000 =  85.55 % ||| loss 0.4012213945388794\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3485057045519352\n",
            "Batch #200 Loss: 0.353835658878088\n",
            "Batch #300 Loss: 0.3642573456466198\n",
            "\u001b[92mTrain accuracy: 41759/48000 =  87.0 % ||| loss 0.3595269024372101\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10379/12000 =  86.49 % ||| loss 0.38181066513061523\u001b[0m\n",
            "\u001b[92mTest accuracy: 8576/10000 =  85.76 % ||| loss 0.41685813665390015\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3429599466919899\n",
            "Batch #200 Loss: 0.3428379340469837\n",
            "Batch #300 Loss: 0.3436642874777317\n",
            "\u001b[92mTrain accuracy: 42198/48000 =  87.91 % ||| loss 0.3190707862377167\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10448/12000 =  87.07 % ||| loss 0.34516873955726624\u001b[0m\n",
            "\u001b[92mTest accuracy: 8574/10000 =  85.74 % ||| loss 0.37511420249938965\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.3381297606229782\n",
            "Batch #200 Loss: 0.3320857451856136\n",
            "Batch #300 Loss: 0.35672004014253617\n",
            "\u001b[92mTrain accuracy: 42351/48000 =  88.23 % ||| loss 0.32111743092536926\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10475/12000 =  87.29 % ||| loss 0.3503425717353821\u001b[0m\n",
            "\u001b[92mTest accuracy: 8635/10000 =  86.35 % ||| loss 0.37418654561042786\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.33168996185064314\n",
            "Batch #200 Loss: 0.335798536837101\n",
            "Batch #300 Loss: 0.33491350457072255\n",
            "\u001b[92mTrain accuracy: 42342/48000 =  88.21 % ||| loss 0.31719470024108887\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10486/12000 =  87.38 % ||| loss 0.3376741111278534\u001b[0m\n",
            "\u001b[92mTest accuracy: 8647/10000 =  86.47 % ||| loss 0.3666311502456665\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.33643167838454247\n",
            "Batch #200 Loss: 0.3287126550078392\n",
            "Batch #300 Loss: 0.34982536137104037\n",
            "\u001b[92mTrain accuracy: 42522/48000 =  88.59 % ||| loss 0.3160971403121948\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10497/12000 =  87.48 % ||| loss 0.3485962152481079\u001b[0m\n",
            "\u001b[92mTest accuracy: 8691/10000 =  86.91 % ||| loss 0.36365336179733276\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.32351734027266504\n",
            "Batch #200 Loss: 0.32931107982993124\n",
            "Batch #300 Loss: 0.34402206480503084\n",
            "\u001b[92mTrain accuracy: 41904/48000 =  87.3 % ||| loss 0.3442592918872833\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10355/12000 =  86.29 % ||| loss 0.3726687431335449\u001b[0m\n",
            "\u001b[92mTest accuracy: 8578/10000 =  85.78 % ||| loss 0.39550742506980896\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.32553932279348374\n",
            "Batch #200 Loss: 0.3379290436208248\n",
            "Batch #300 Loss: 0.33347478568553923\n",
            "\u001b[92mTrain accuracy: 42502/48000 =  88.55 % ||| loss 0.3136701285839081\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10484/12000 =  87.37 % ||| loss 0.34387508034706116\u001b[0m\n",
            "\u001b[92mTest accuracy: 8675/10000 =  86.75 % ||| loss 0.366193950176239\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.3240837711840868\n",
            "Batch #200 Loss: 0.3408715255558491\n",
            "Batch #300 Loss: 0.3378875993192196\n",
            "\u001b[92mTrain accuracy: 42694/48000 =  88.95 % ||| loss 0.3093334138393402\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10527/12000 =  87.72 % ||| loss 0.34002941846847534\u001b[0m\n",
            "\u001b[92mTest accuracy: 8733/10000 =  87.33 % ||| loss 0.35657835006713867\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.3121862931549549\n",
            "Batch #200 Loss: 0.33344949677586555\n",
            "Batch #300 Loss: 0.3510740797221661\n",
            "\u001b[92mTrain accuracy: 42457/48000 =  88.45 % ||| loss 0.30981528759002686\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10497/12000 =  87.48 % ||| loss 0.3429931402206421\u001b[0m\n",
            "\u001b[92mTest accuracy: 8705/10000 =  87.05 % ||| loss 0.3600594997406006\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3179194115102291\n",
            "Batch #200 Loss: 0.34020081236958505\n",
            "Batch #300 Loss: 0.3308780691027641\n",
            "\u001b[92mTrain accuracy: 42596/48000 =  88.74 % ||| loss 0.3054058253765106\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10536/12000 =  87.8 % ||| loss 0.33444884419441223\u001b[0m\n",
            "\u001b[92mTest accuracy: 8712/10000 =  87.12 % ||| loss 0.354445219039917\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_5</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_5' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_5</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_114604-Lenet5Decay_1726155046.322212_6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_6' target=\"_blank\">Lenet5Decay_1726155046.322212_6</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302575480937958\n",
            "Batch #200 Loss: 2.3029043173789976\n",
            "Batch #300 Loss: 2.3030543661117555\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3031420707702637\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.303023099899292\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302964687347412\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.302912995815277\n",
            "Batch #200 Loss: 2.303015577793121\n",
            "Batch #300 Loss: 2.3029992270469664\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302863597869873\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302961587905884\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027942180633545\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.302957992553711\n",
            "Batch #200 Loss: 2.3027507734298704\n",
            "Batch #300 Loss: 2.3028581070899965\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3027262687683105\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3030037879943848\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028321266174316\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3029674291610718\n",
            "Batch #200 Loss: 2.303205268383026\n",
            "Batch #300 Loss: 2.3030125427246095\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3029465675354004\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.302567720413208\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028786182403564\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.303140866756439\n",
            "Batch #200 Loss: 2.303091206550598\n",
            "Batch #300 Loss: 2.302734739780426\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.302900552749634\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.3026957511901855\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028292655944824\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.302828993797302\n",
            "Batch #200 Loss: 2.30289231300354\n",
            "Batch #300 Loss: 2.302943482398987\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3029603958129883\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.303302764892578\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3031301498413086\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.3030561709403994\n",
            "Batch #200 Loss: 2.3029400873184205\n",
            "Batch #300 Loss: 2.303081042766571\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302896738052368\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.303269147872925\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3029160499572754\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.3028688168525697\n",
            "Batch #200 Loss: 2.3032350516319275\n",
            "Batch #300 Loss: 2.302822995185852\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3028531074523926\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302828311920166\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028385639190674\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.303058829307556\n",
            "Batch #200 Loss: 2.3029340958595275\n",
            "Batch #300 Loss: 2.3030444836616515\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3031070232391357\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3029706478118896\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3031299114227295\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.3029373574256895\n",
            "Batch #200 Loss: 2.302743811607361\n",
            "Batch #300 Loss: 2.302917742729187\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302630662918091\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3027384281158447\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026204109191895\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.3029998183250426\n",
            "Batch #200 Loss: 2.303052191734314\n",
            "Batch #300 Loss: 2.302759895324707\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.3027915954589844\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.3026750087738037\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027536869049072\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.30287043094635\n",
            "Batch #200 Loss: 2.3027575755119325\n",
            "Batch #300 Loss: 2.3029383277893065\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.3028721809387207\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.302921772003174\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302844524383545\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.3030530405044556\n",
            "Batch #200 Loss: 2.3030175399780273\n",
            "Batch #300 Loss: 2.3030733370780947\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.303250551223755\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.303356647491455\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30336594581604\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.3029978275299072\n",
            "Batch #200 Loss: 2.3029793429374696\n",
            "Batch #300 Loss: 2.302866654396057\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3030197620391846\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.303083896636963\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3031437397003174\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.303102796077728\n",
            "Batch #200 Loss: 2.3028034782409668\n",
            "Batch #300 Loss: 2.3029865837097168\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.3028430938720703\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3030107021331787\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303048610687256\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.303062198162079\n",
            "Batch #200 Loss: 2.30323712348938\n",
            "Batch #300 Loss: 2.30286899805069\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302809715270996\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3031036853790283\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3028223514556885\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.303002598285675\n",
            "Batch #200 Loss: 2.3028286337852477\n",
            "Batch #300 Loss: 2.302855031490326\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.303342819213867\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3034141063690186\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3031234741210938\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.3028160524368286\n",
            "Batch #200 Loss: 2.303144054412842\n",
            "Batch #300 Loss: 2.3030351567268372\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.303358554840088\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.3028600215911865\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3032522201538086\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.3029409527778624\n",
            "Batch #200 Loss: 2.303154020309448\n",
            "Batch #300 Loss: 2.3029595971107484\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302915573120117\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.303347110748291\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3030290603637695\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.3027560186386107\n",
            "Batch #200 Loss: 2.3031794738769533\n",
            "Batch #300 Loss: 2.303039288520813\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302717685699463\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3027162551879883\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302698850631714\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.302716779708862\n",
            "Batch #200 Loss: 2.3029198598861695\n",
            "Batch #300 Loss: 2.303161287307739\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302988052368164\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3030598163604736\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3030173778533936\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.302816617488861\n",
            "Batch #200 Loss: 2.3032030773162844\n",
            "Batch #300 Loss: 2.302821652889252\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.3029603958129883\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.303144693374634\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.303018093109131\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.3026324915885925\n",
            "Batch #200 Loss: 2.303009216785431\n",
            "Batch #300 Loss: 2.303034508228302\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3027172088623047\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3029744625091553\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027546405792236\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.302969927787781\n",
            "Batch #200 Loss: 2.3030691266059877\n",
            "Batch #300 Loss: 2.302984790802002\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3028347492218018\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3027544021606445\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302805185317993\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3029563784599305\n",
            "Batch #200 Loss: 2.3030364656448366\n",
            "Batch #300 Loss: 2.3027075695991517\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302891492843628\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3031809329986572\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302997350692749\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_6</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_6</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_114834-Lenet5Decay_1726155046.322212_7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_7' target=\"_blank\">Lenet5Decay_1726155046.322212_7</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2105327892303466\n",
            "Batch #200 Loss: 1.1856917583942412\n",
            "Batch #300 Loss: 0.7763883155584336\n",
            "\u001b[92mTrain accuracy: 36295/48000 =  75.61 % ||| loss 0.6519725918769836\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9064/12000 =  75.53 % ||| loss 0.651132345199585\u001b[0m\n",
            "\u001b[92mTest accuracy: 7474/10000 =  74.74 % ||| loss 0.674680769443512\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6148496690392494\n",
            "Batch #200 Loss: 0.5856157782673835\n",
            "Batch #300 Loss: 0.5504357275366784\n",
            "\u001b[92mTrain accuracy: 39620/48000 =  82.54 % ||| loss 0.48557713627815247\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9884/12000 =  82.37 % ||| loss 0.486971914768219\u001b[0m\n",
            "\u001b[92mTest accuracy: 8178/10000 =  81.78 % ||| loss 0.4994029700756073\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5107596436142922\n",
            "Batch #200 Loss: 0.5082593926787377\n",
            "Batch #300 Loss: 0.5043293124437332\n",
            "\u001b[92mTrain accuracy: 39486/48000 =  82.26 % ||| loss 0.4903707802295685\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9797/12000 =  81.64 % ||| loss 0.5008711218833923\u001b[0m\n",
            "\u001b[92mTest accuracy: 8144/10000 =  81.44 % ||| loss 0.5127894878387451\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.48284746795892713\n",
            "Batch #200 Loss: 0.47581558018922804\n",
            "Batch #300 Loss: 0.4780874526500702\n",
            "\u001b[92mTrain accuracy: 39418/48000 =  82.12 % ||| loss 0.5029926896095276\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9846/12000 =  82.05 % ||| loss 0.5056772232055664\u001b[0m\n",
            "\u001b[92mTest accuracy: 8121/10000 =  81.21 % ||| loss 0.521902859210968\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4641276985406876\n",
            "Batch #200 Loss: 0.4814376500248909\n",
            "Batch #300 Loss: 0.4664414837956429\n",
            "\u001b[92mTrain accuracy: 40697/48000 =  84.79 % ||| loss 0.4197036921977997\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10213/12000 =  85.11 % ||| loss 0.42242735624313354\u001b[0m\n",
            "\u001b[92mTest accuracy: 8415/10000 =  84.15 % ||| loss 0.43931087851524353\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.45775471270084384\n",
            "Batch #200 Loss: 0.47118026435375215\n",
            "Batch #300 Loss: 0.4538279005885124\n",
            "\u001b[92mTrain accuracy: 40678/48000 =  84.75 % ||| loss 0.4237944483757019\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10156/12000 =  84.63 % ||| loss 0.4279484748840332\u001b[0m\n",
            "\u001b[92mTest accuracy: 8401/10000 =  84.01 % ||| loss 0.44685131311416626\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.4484750133752823\n",
            "Batch #200 Loss: 0.45689266800880435\n",
            "Batch #300 Loss: 0.4586030927300453\n",
            "\u001b[92mTrain accuracy: 40522/48000 =  84.42 % ||| loss 0.43057766556739807\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10191/12000 =  84.92 % ||| loss 0.4303341805934906\u001b[0m\n",
            "\u001b[92mTest accuracy: 8360/10000 =  83.6 % ||| loss 0.4546372592449188\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.46373035907745364\n",
            "Batch #200 Loss: 0.4507723766565323\n",
            "Batch #300 Loss: 0.44739651679992676\n",
            "\u001b[92mTrain accuracy: 38617/48000 =  80.45 % ||| loss 0.51608806848526\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9617/12000 =  80.14 % ||| loss 0.5196248292922974\u001b[0m\n",
            "\u001b[92mTest accuracy: 8017/10000 =  80.17 % ||| loss 0.5348004102706909\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.4468370997905731\n",
            "Batch #200 Loss: 0.4428840786218643\n",
            "Batch #300 Loss: 0.4671029683947563\n",
            "\u001b[92mTrain accuracy: 39665/48000 =  82.64 % ||| loss 0.46271389722824097\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9866/12000 =  82.22 % ||| loss 0.4668038487434387\u001b[0m\n",
            "\u001b[92mTest accuracy: 8152/10000 =  81.52 % ||| loss 0.48616883158683777\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.4385280394554138\n",
            "Batch #200 Loss: 0.4553442773222923\n",
            "Batch #300 Loss: 0.45402113139629363\n",
            "\u001b[92mTrain accuracy: 40213/48000 =  83.78 % ||| loss 0.4494466185569763\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10076/12000 =  83.97 % ||| loss 0.45322707295417786\u001b[0m\n",
            "\u001b[92mTest accuracy: 8332/10000 =  83.32 % ||| loss 0.46622776985168457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.44822135984897615\n",
            "Batch #200 Loss: 0.4466192027926445\n",
            "Batch #300 Loss: 0.43724255576729776\n",
            "\u001b[92mTrain accuracy: 40650/48000 =  84.69 % ||| loss 0.42087438702583313\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10222/12000 =  85.18 % ||| loss 0.4207887351512909\u001b[0m\n",
            "\u001b[92mTest accuracy: 8408/10000 =  84.08 % ||| loss 0.4376986622810364\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.4499490490555763\n",
            "Batch #200 Loss: 0.4377357751131058\n",
            "Batch #300 Loss: 0.44709893614053725\n",
            "\u001b[92mTrain accuracy: 38773/48000 =  80.78 % ||| loss 0.5357642769813538\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9717/12000 =  80.97 % ||| loss 0.5422398447990417\u001b[0m\n",
            "\u001b[92mTest accuracy: 7996/10000 =  79.96 % ||| loss 0.5581382513046265\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.4450607779622078\n",
            "Batch #200 Loss: 0.4315720272064209\n",
            "Batch #300 Loss: 0.4413526329398155\n",
            "\u001b[92mTrain accuracy: 40449/48000 =  84.27 % ||| loss 0.43963485956192017\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10131/12000 =  84.42 % ||| loss 0.4428059160709381\u001b[0m\n",
            "\u001b[92mTest accuracy: 8359/10000 =  83.59 % ||| loss 0.4608350694179535\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.4401256912946701\n",
            "Batch #200 Loss: 0.44372919738292693\n",
            "Batch #300 Loss: 0.45015484720468524\n",
            "\u001b[92mTrain accuracy: 40985/48000 =  85.39 % ||| loss 0.40500766038894653\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10294/12000 =  85.78 % ||| loss 0.40712589025497437\u001b[0m\n",
            "\u001b[92mTest accuracy: 8454/10000 =  84.54 % ||| loss 0.42810705304145813\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.44445316910743715\n",
            "Batch #200 Loss: 0.43890872806310655\n",
            "Batch #300 Loss: 0.4396583679318428\n",
            "\u001b[92mTrain accuracy: 40995/48000 =  85.41 % ||| loss 0.4094271957874298\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10238/12000 =  85.32 % ||| loss 0.4134000837802887\u001b[0m\n",
            "\u001b[92mTest accuracy: 8484/10000 =  84.84 % ||| loss 0.4276200234889984\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.43902852445840834\n",
            "Batch #200 Loss: 0.42638314187526705\n",
            "Batch #300 Loss: 0.4245430353283882\n",
            "\u001b[92mTrain accuracy: 41061/48000 =  85.54 % ||| loss 0.40349963307380676\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10269/12000 =  85.58 % ||| loss 0.40760114789009094\u001b[0m\n",
            "\u001b[92mTest accuracy: 8470/10000 =  84.7 % ||| loss 0.42496761679649353\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.4511476156115532\n",
            "Batch #200 Loss: 0.42709635734558105\n",
            "Batch #300 Loss: 0.43413057893514634\n",
            "\u001b[92mTrain accuracy: 40020/48000 =  83.38 % ||| loss 0.44769129157066345\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10003/12000 =  83.36 % ||| loss 0.4535115957260132\u001b[0m\n",
            "\u001b[92mTest accuracy: 8304/10000 =  83.04 % ||| loss 0.4644932448863983\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.435209384560585\n",
            "Batch #200 Loss: 0.44141095370054245\n",
            "Batch #300 Loss: 0.4222116994857788\n",
            "\u001b[92mTrain accuracy: 40536/48000 =  84.45 % ||| loss 0.4401841163635254\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10108/12000 =  84.23 % ||| loss 0.44191470742225647\u001b[0m\n",
            "\u001b[92mTest accuracy: 8356/10000 =  83.56 % ||| loss 0.46529334783554077\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.44193999350070956\n",
            "Batch #200 Loss: 0.4311961001157761\n",
            "Batch #300 Loss: 0.428210569024086\n",
            "\u001b[92mTrain accuracy: 40637/48000 =  84.66 % ||| loss 0.422891229391098\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10201/12000 =  85.01 % ||| loss 0.4212521016597748\u001b[0m\n",
            "\u001b[92mTest accuracy: 8406/10000 =  84.06 % ||| loss 0.4419969916343689\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.42323474526405336\n",
            "Batch #200 Loss: 0.4362431591749191\n",
            "Batch #300 Loss: 0.43745881706476214\n",
            "\u001b[92mTrain accuracy: 40527/48000 =  84.43 % ||| loss 0.42433422803878784\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10123/12000 =  84.36 % ||| loss 0.42567211389541626\u001b[0m\n",
            "\u001b[92mTest accuracy: 8346/10000 =  83.46 % ||| loss 0.4515596330165863\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.42222837775945665\n",
            "Batch #200 Loss: 0.42739283591508864\n",
            "Batch #300 Loss: 0.43340119153261186\n",
            "\u001b[92mTrain accuracy: 40332/48000 =  84.03 % ||| loss 0.4372790455818176\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10118/12000 =  84.32 % ||| loss 0.44189777970314026\u001b[0m\n",
            "\u001b[92mTest accuracy: 8314/10000 =  83.14 % ||| loss 0.46439462900161743\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.43380476951599123\n",
            "Batch #200 Loss: 0.4286141459643841\n",
            "Batch #300 Loss: 0.4332192659378052\n",
            "\u001b[92mTrain accuracy: 40299/48000 =  83.96 % ||| loss 0.44359859824180603\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10091/12000 =  84.09 % ||| loss 0.44556525349617004\u001b[0m\n",
            "\u001b[92mTest accuracy: 8323/10000 =  83.23 % ||| loss 0.4639160633087158\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.42312831938266754\n",
            "Batch #200 Loss: 0.42156691730022433\n",
            "Batch #300 Loss: 0.4487509861588478\n",
            "\u001b[92mTrain accuracy: 40700/48000 =  84.79 % ||| loss 0.41507527232170105\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10194/12000 =  84.95 % ||| loss 0.41436663269996643\u001b[0m\n",
            "\u001b[92mTest accuracy: 8420/10000 =  84.2 % ||| loss 0.43501022458076477\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.42640478134155274\n",
            "Batch #200 Loss: 0.42812481969594957\n",
            "Batch #300 Loss: 0.44380713880062106\n",
            "\u001b[92mTrain accuracy: 41075/48000 =  85.57 % ||| loss 0.39909031987190247\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10263/12000 =  85.52 % ||| loss 0.4031608998775482\u001b[0m\n",
            "\u001b[92mTest accuracy: 8504/10000 =  85.04 % ||| loss 0.4170328974723816\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.4144947965443134\n",
            "Batch #200 Loss: 0.4361746203899384\n",
            "Batch #300 Loss: 0.4332770448923111\n",
            "\u001b[92mTrain accuracy: 40114/48000 =  83.57 % ||| loss 0.4498951733112335\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10045/12000 =  83.71 % ||| loss 0.4515587091445923\u001b[0m\n",
            "\u001b[92mTest accuracy: 8330/10000 =  83.3 % ||| loss 0.4662238359451294\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_7</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_7' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_7</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_115104-Lenet5Decay_1726155046.322212_8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_8' target=\"_blank\">Lenet5Decay_1726155046.322212_8</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.1, 'momentum': 0.7, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.1369668292999267\n",
            "Batch #200 Loss: 0.818001464009285\n",
            "Batch #300 Loss: 0.6159687069058418\n",
            "\u001b[92mTrain accuracy: 39233/48000 =  81.74 % ||| loss 0.5000623464584351\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9787/12000 =  81.56 % ||| loss 0.4996545612812042\u001b[0m\n",
            "\u001b[92mTest accuracy: 8092/10000 =  80.92 % ||| loss 0.5292457342147827\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.5008178836107254\n",
            "Batch #200 Loss: 0.4791209182143211\n",
            "Batch #300 Loss: 0.43796371400356293\n",
            "\u001b[92mTrain accuracy: 41227/48000 =  85.89 % ||| loss 0.39180245995521545\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10302/12000 =  85.85 % ||| loss 0.3969862461090088\u001b[0m\n",
            "\u001b[92mTest accuracy: 8511/10000 =  85.11 % ||| loss 0.4262877404689789\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.39964251160621644\n",
            "Batch #200 Loss: 0.3941985151171684\n",
            "Batch #300 Loss: 0.37479057669639587\n",
            "\u001b[92mTrain accuracy: 41555/48000 =  86.57 % ||| loss 0.36231374740600586\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10319/12000 =  85.99 % ||| loss 0.3737841248512268\u001b[0m\n",
            "\u001b[92mTest accuracy: 8518/10000 =  85.18 % ||| loss 0.3981928527355194\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.3586741955578327\n",
            "Batch #200 Loss: 0.34822831600904464\n",
            "Batch #300 Loss: 0.3522798678278923\n",
            "\u001b[92mTrain accuracy: 42236/48000 =  87.99 % ||| loss 0.3219987154006958\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10499/12000 =  87.49 % ||| loss 0.3373298943042755\u001b[0m\n",
            "\u001b[92mTest accuracy: 8652/10000 =  86.52 % ||| loss 0.3608512878417969\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.32679921835660936\n",
            "Batch #200 Loss: 0.33259492307901384\n",
            "Batch #300 Loss: 0.3447303709387779\n",
            "\u001b[92mTrain accuracy: 42427/48000 =  88.39 % ||| loss 0.3112657070159912\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10489/12000 =  87.41 % ||| loss 0.3360145092010498\u001b[0m\n",
            "\u001b[92mTest accuracy: 8701/10000 =  87.01 % ||| loss 0.35480964183807373\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.32025874584913255\n",
            "Batch #200 Loss: 0.3233149217069149\n",
            "Batch #300 Loss: 0.31576848790049555\n",
            "\u001b[92mTrain accuracy: 42396/48000 =  88.33 % ||| loss 0.31235501170158386\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10483/12000 =  87.36 % ||| loss 0.34059590101242065\u001b[0m\n",
            "\u001b[92mTest accuracy: 8700/10000 =  87.0 % ||| loss 0.3603879511356354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.30196350246667863\n",
            "Batch #200 Loss: 0.30188075989484786\n",
            "Batch #300 Loss: 0.3237477312982082\n",
            "\u001b[92mTrain accuracy: 42579/48000 =  88.71 % ||| loss 0.2979232966899872\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10532/12000 =  87.77 % ||| loss 0.324870228767395\u001b[0m\n",
            "\u001b[92mTest accuracy: 8717/10000 =  87.17 % ||| loss 0.3524492681026459\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.29264107063412664\n",
            "Batch #200 Loss: 0.3056969690322876\n",
            "Batch #300 Loss: 0.302917248159647\n",
            "\u001b[92mTrain accuracy: 42932/48000 =  89.44 % ||| loss 0.28243178129196167\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10580/12000 =  88.17 % ||| loss 0.31356146931648254\u001b[0m\n",
            "\u001b[92mTest accuracy: 8761/10000 =  87.61 % ||| loss 0.33283284306526184\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.28892706379294397\n",
            "Batch #200 Loss: 0.2913927574455738\n",
            "Batch #300 Loss: 0.29316563412547114\n",
            "\u001b[92mTrain accuracy: 42814/48000 =  89.2 % ||| loss 0.2857212424278259\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10578/12000 =  88.15 % ||| loss 0.3211575746536255\u001b[0m\n",
            "\u001b[92mTest accuracy: 8732/10000 =  87.32 % ||| loss 0.3410297632217407\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.2766335244476795\n",
            "Batch #200 Loss: 0.2844124050438404\n",
            "Batch #300 Loss: 0.2852481831610203\n",
            "\u001b[92mTrain accuracy: 43124/48000 =  89.84 % ||| loss 0.2728089690208435\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10622/12000 =  88.52 % ||| loss 0.3037062883377075\u001b[0m\n",
            "\u001b[92mTest accuracy: 8791/10000 =  87.91 % ||| loss 0.3226381540298462\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.27304249376058576\n",
            "Batch #200 Loss: 0.28313228830695153\n",
            "Batch #300 Loss: 0.27958480194211005\n",
            "\u001b[92mTrain accuracy: 43356/48000 =  90.33 % ||| loss 0.2530243396759033\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10668/12000 =  88.9 % ||| loss 0.2905140817165375\u001b[0m\n",
            "\u001b[92mTest accuracy: 8808/10000 =  88.08 % ||| loss 0.32124650478363037\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.2635093033313751\n",
            "Batch #200 Loss: 0.27115694239735605\n",
            "Batch #300 Loss: 0.2741017255187035\n",
            "\u001b[92mTrain accuracy: 43470/48000 =  90.56 % ||| loss 0.2561452090740204\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10678/12000 =  88.98 % ||| loss 0.29546481370925903\u001b[0m\n",
            "\u001b[92mTest accuracy: 8847/10000 =  88.47 % ||| loss 0.31309962272644043\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.2528423695266247\n",
            "Batch #200 Loss: 0.2551255026459694\n",
            "Batch #300 Loss: 0.27980893656611444\n",
            "\u001b[92mTrain accuracy: 43663/48000 =  90.96 % ||| loss 0.24587687849998474\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10711/12000 =  89.26 % ||| loss 0.2899567782878876\u001b[0m\n",
            "\u001b[92mTest accuracy: 8893/10000 =  88.93 % ||| loss 0.3116680085659027\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.2688710202276707\n",
            "Batch #200 Loss: 0.2582039976119995\n",
            "Batch #300 Loss: 0.2603064504265785\n",
            "\u001b[92mTrain accuracy: 43515/48000 =  90.66 % ||| loss 0.2518983483314514\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10690/12000 =  89.08 % ||| loss 0.2920916676521301\u001b[0m\n",
            "\u001b[92mTest accuracy: 8849/10000 =  88.49 % ||| loss 0.31772345304489136\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.2581928136944771\n",
            "Batch #200 Loss: 0.25327142581343653\n",
            "Batch #300 Loss: 0.253846927434206\n",
            "\u001b[92mTrain accuracy: 43457/48000 =  90.54 % ||| loss 0.2519443929195404\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10642/12000 =  88.68 % ||| loss 0.2977108955383301\u001b[0m\n",
            "\u001b[92mTest accuracy: 8849/10000 =  88.49 % ||| loss 0.31750303506851196\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.249088666588068\n",
            "Batch #200 Loss: 0.2599842518568039\n",
            "Batch #300 Loss: 0.25951786786317826\n",
            "\u001b[92mTrain accuracy: 43881/48000 =  91.42 % ||| loss 0.23833845555782318\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10717/12000 =  89.31 % ||| loss 0.2876189649105072\u001b[0m\n",
            "\u001b[92mTest accuracy: 8888/10000 =  88.88 % ||| loss 0.3092631697654724\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.24671574860811232\n",
            "Batch #200 Loss: 0.2476473069190979\n",
            "Batch #300 Loss: 0.24680110976099967\n",
            "\u001b[92mTrain accuracy: 43689/48000 =  91.02 % ||| loss 0.2430437207221985\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10691/12000 =  89.09 % ||| loss 0.2918436825275421\u001b[0m\n",
            "\u001b[92mTest accuracy: 8857/10000 =  88.57 % ||| loss 0.31277307868003845\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.24137253671884537\n",
            "Batch #200 Loss: 0.25368991151452064\n",
            "Batch #300 Loss: 0.23966634184122085\n",
            "\u001b[92mTrain accuracy: 44104/48000 =  91.88 % ||| loss 0.22651347517967224\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10745/12000 =  89.54 % ||| loss 0.27548378705978394\u001b[0m\n",
            "\u001b[92mTest accuracy: 8930/10000 =  89.3 % ||| loss 0.2917194068431854\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.23248319134116172\n",
            "Batch #200 Loss: 0.247990210801363\n",
            "Batch #300 Loss: 0.2504977846890688\n",
            "\u001b[92mTrain accuracy: 43717/48000 =  91.08 % ||| loss 0.238244891166687\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10727/12000 =  89.39 % ||| loss 0.28745153546333313\u001b[0m\n",
            "\u001b[92mTest accuracy: 8863/10000 =  88.63 % ||| loss 0.3160686492919922\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.23171892553567885\n",
            "Batch #200 Loss: 0.23635667324066162\n",
            "Batch #300 Loss: 0.241365235298872\n",
            "\u001b[92mTrain accuracy: 43819/48000 =  91.29 % ||| loss 0.23577044904232025\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10710/12000 =  89.25 % ||| loss 0.29193180799484253\u001b[0m\n",
            "\u001b[92mTest accuracy: 8878/10000 =  88.78 % ||| loss 0.3110159933567047\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.22822139479219913\n",
            "Batch #200 Loss: 0.2416100873053074\n",
            "Batch #300 Loss: 0.23512175902724267\n",
            "\u001b[92mTrain accuracy: 43843/48000 =  91.34 % ||| loss 0.23384127020835876\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10719/12000 =  89.33 % ||| loss 0.2920161187648773\u001b[0m\n",
            "\u001b[92mTest accuracy: 8906/10000 =  89.06 % ||| loss 0.3084571063518524\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.22672255359590054\n",
            "Batch #200 Loss: 0.23537022575736047\n",
            "Batch #300 Loss: 0.23950115874409675\n",
            "\u001b[92mTrain accuracy: 44106/48000 =  91.89 % ||| loss 0.22691625356674194\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10714/12000 =  89.28 % ||| loss 0.29039520025253296\u001b[0m\n",
            "\u001b[92mTest accuracy: 8899/10000 =  88.99 % ||| loss 0.3121265769004822\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.23407957419753075\n",
            "Batch #200 Loss: 0.22246076799929143\n",
            "Batch #300 Loss: 0.24254011437296868\n",
            "\u001b[92mTrain accuracy: 44221/48000 =  92.13 % ||| loss 0.21649017930030823\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10742/12000 =  89.52 % ||| loss 0.2820475101470947\u001b[0m\n",
            "\u001b[92mTest accuracy: 8927/10000 =  89.27 % ||| loss 0.29878026247024536\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.22044130116701127\n",
            "Batch #200 Loss: 0.22820126630365847\n",
            "Batch #300 Loss: 0.23957885459065437\n",
            "\u001b[92mTrain accuracy: 44475/48000 =  92.66 % ||| loss 0.2026212215423584\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10795/12000 =  89.96 % ||| loss 0.2695966958999634\u001b[0m\n",
            "\u001b[92mTest accuracy: 8975/10000 =  89.75 % ||| loss 0.28449204564094543\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.22051189050078393\n",
            "Batch #200 Loss: 0.2344923270493746\n",
            "Batch #300 Loss: 0.23057831019163133\n",
            "\u001b[92mTrain accuracy: 44458/48000 =  92.62 % ||| loss 0.20292040705680847\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10785/12000 =  89.88 % ||| loss 0.27211812138557434\u001b[0m\n",
            "\u001b[92mTest accuracy: 9005/10000 =  90.05 % ||| loss 0.28890570998191833\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_8</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_8</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_115334-Lenet5Decay_1726155046.322212_9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_9' target=\"_blank\">Lenet5Decay_1726155046.322212_9</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_9' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302008354663849\n",
            "Batch #200 Loss: 2.2999162006378175\n",
            "Batch #300 Loss: 2.2996698093414305\n",
            "\u001b[92mTrain accuracy: 5568/48000 =  11.6 % ||| loss 2.300185441970825\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1346/12000 =  11.22 % ||| loss 2.30025315284729\u001b[0m\n",
            "\u001b[92mTest accuracy: 1172/10000 =  11.72 % ||| loss 2.300137996673584\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.300343859195709\n",
            "Batch #200 Loss: 2.3008951473236086\n",
            "Batch #300 Loss: 2.301470754146576\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3018128871917725\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3018953800201416\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.301848888397217\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3019537281990052\n",
            "Batch #200 Loss: 2.3022219705581666\n",
            "Batch #300 Loss: 2.302279727458954\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302419662475586\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302502155303955\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3024442195892334\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3024439334869387\n",
            "Batch #200 Loss: 2.3025567507743836\n",
            "Batch #300 Loss: 2.302552456855774\n",
            "\u001b[92mTrain accuracy: 9067/48000 =  18.89 % ||| loss 2.3025505542755127\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2218/12000 =  18.48 % ||| loss 2.3026297092437744\u001b[0m\n",
            "\u001b[92mTest accuracy: 1885/10000 =  18.85 % ||| loss 2.302579879760742\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3026115608215334\n",
            "Batch #200 Loss: 2.3025825548172\n",
            "Batch #300 Loss: 2.3025856733322145\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302569627761841\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026511669158936\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025941848754883\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.302574632167816\n",
            "Batch #200 Loss: 2.3025947284698485\n",
            "Batch #300 Loss: 2.3026600956916807\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025734424591064\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026511669158936\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30259108543396\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.302566957473755\n",
            "Batch #200 Loss: 2.302633273601532\n",
            "Batch #300 Loss: 2.3025892639160155\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302644729614258\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025853633880615\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.302588155269623\n",
            "Batch #200 Loss: 2.302575178146362\n",
            "Batch #300 Loss: 2.3026205945014953\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026416301727295\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025925159454346\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.302565326690674\n",
            "Batch #200 Loss: 2.3025991225242617\n",
            "Batch #300 Loss: 2.302612204551697\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025741577148438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302638292312622\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025877475738525\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.302580575942993\n",
            "Batch #200 Loss: 2.302604103088379\n",
            "Batch #300 Loss: 2.3026254844665526\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025758266448975\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026390075683594\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302586317062378\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.3025857639312743\n",
            "Batch #200 Loss: 2.3025674390792847\n",
            "Batch #300 Loss: 2.3026473569869994\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025760650634766\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026368618011475\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302582263946533\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.302582845687866\n",
            "Batch #200 Loss: 2.3026390886306762\n",
            "Batch #300 Loss: 2.3026177382469175\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302645683288574\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302579164505005\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.302614724636078\n",
            "Batch #200 Loss: 2.302609622478485\n",
            "Batch #300 Loss: 2.302592089176178\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3025753498077393\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3026492595672607\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30259370803833\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.302604100704193\n",
            "Batch #200 Loss: 2.3025365400314333\n",
            "Batch #300 Loss: 2.3026333045959473\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3025765419006348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302639961242676\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302582025527954\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.3025817704200744\n",
            "Batch #200 Loss: 2.302557785511017\n",
            "Batch #300 Loss: 2.3026524996757507\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302576780319214\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302633762359619\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302595615386963\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.302585232257843\n",
            "Batch #200 Loss: 2.30263258934021\n",
            "Batch #300 Loss: 2.302610101699829\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025753498077393\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302649736404419\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025882244110107\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.302539792060852\n",
            "Batch #200 Loss: 2.302613637447357\n",
            "Batch #300 Loss: 2.3026362538337706\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025758266448975\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302638530731201\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302584648132324\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.302612006664276\n",
            "Batch #200 Loss: 2.302600028514862\n",
            "Batch #300 Loss: 2.3025947642326354\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302574872970581\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302644729614258\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302600145339966\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.302578628063202\n",
            "Batch #200 Loss: 2.3026085257530213\n",
            "Batch #300 Loss: 2.3026370930671693\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025732040405273\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302650213241577\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302581310272217\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.302603666782379\n",
            "Batch #200 Loss: 2.302606999874115\n",
            "Batch #300 Loss: 2.3026151728630064\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025741577148438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026533126831055\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025779724121094\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.302487416267395\n",
            "Batch #200 Loss: 2.3026745891571045\n",
            "Batch #300 Loss: 2.3026149129867552\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025755882263184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026347160339355\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025996685028076\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.3026118969917295\n",
            "Batch #200 Loss: 2.30258898973465\n",
            "Batch #300 Loss: 2.302626111507416\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025753498077393\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026416301727295\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025896549224854\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.30256929397583\n",
            "Batch #200 Loss: 2.3026160764694215\n",
            "Batch #300 Loss: 2.3025989365577697\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302637815475464\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302588939666748\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.3025960302352906\n",
            "Batch #200 Loss: 2.30261691570282\n",
            "Batch #300 Loss: 2.3026241207122804\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025729656219482\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026516437530518\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025949001312256\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.302603189945221\n",
            "Batch #200 Loss: 2.3026145482063294\n",
            "Batch #300 Loss: 2.3025972628593445\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025741577148438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302652597427368\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302595615386963\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_9</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_9' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_9</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_115605-Lenet5Decay_1726155046.322212_10</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_10' target=\"_blank\">Lenet5Decay_1726155046.322212_10</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_10' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_10</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3029189467430116\n",
            "Batch #200 Loss: 2.2984030771255495\n",
            "Batch #300 Loss: 2.2954772758483886\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.2879867553710938\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.287625551223755\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.2879228591918945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.282655429840088\n",
            "Batch #200 Loss: 2.262349865436554\n",
            "Batch #300 Loss: 2.1830800938606263\n",
            "\u001b[92mTrain accuracy: 23915/48000 =  49.82 % ||| loss 1.560214638710022\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5989/12000 =  49.91 % ||| loss 1.5574084520339966\u001b[0m\n",
            "\u001b[92mTest accuracy: 4982/10000 =  49.82 % ||| loss 1.5636119842529297\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 1.254001897573471\n",
            "Batch #200 Loss: 1.0190924030542374\n",
            "Batch #300 Loss: 0.9587710160017013\n",
            "\u001b[92mTrain accuracy: 31901/48000 =  66.46 % ||| loss 0.8803964257240295\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7983/12000 =  66.53 % ||| loss 0.8695483803749084\u001b[0m\n",
            "\u001b[92mTest accuracy: 6615/10000 =  66.15 % ||| loss 0.8978682160377502\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.8900978869199753\n",
            "Batch #200 Loss: 0.8503910356760025\n",
            "Batch #300 Loss: 0.8605176669359207\n",
            "\u001b[92mTrain accuracy: 34245/48000 =  71.34 % ||| loss 0.782139003276825\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8624/12000 =  71.87 % ||| loss 0.7716649770736694\u001b[0m\n",
            "\u001b[92mTest accuracy: 7067/10000 =  70.67 % ||| loss 0.7960110306739807\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.8021821975708008\n",
            "Batch #200 Loss: 0.8087547099590302\n",
            "Batch #300 Loss: 0.7609050267934799\n",
            "\u001b[92mTrain accuracy: 34052/48000 =  70.94 % ||| loss 0.7655483484268188\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8556/12000 =  71.3 % ||| loss 0.7580404877662659\u001b[0m\n",
            "\u001b[92mTest accuracy: 7014/10000 =  70.14 % ||| loss 0.7829311490058899\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.765383038520813\n",
            "Batch #200 Loss: 0.7476079374551773\n",
            "Batch #300 Loss: 0.7387216109037399\n",
            "\u001b[92mTrain accuracy: 33943/48000 =  70.71 % ||| loss 0.7645992040634155\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8496/12000 =  70.8 % ||| loss 0.754982590675354\u001b[0m\n",
            "\u001b[92mTest accuracy: 7029/10000 =  70.29 % ||| loss 0.7906594276428223\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.7214275962114334\n",
            "Batch #200 Loss: 0.7169811582565307\n",
            "Batch #300 Loss: 0.7118681374192238\n",
            "\u001b[92mTrain accuracy: 35783/48000 =  74.55 % ||| loss 0.6865237355232239\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8924/12000 =  74.37 % ||| loss 0.6804859042167664\u001b[0m\n",
            "\u001b[92mTest accuracy: 7357/10000 =  73.57 % ||| loss 0.70661860704422\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.6920583164691925\n",
            "Batch #200 Loss: 0.6796374905109406\n",
            "Batch #300 Loss: 0.6831585049629212\n",
            "\u001b[92mTrain accuracy: 36392/48000 =  75.82 % ||| loss 0.6478120684623718\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9121/12000 =  76.01 % ||| loss 0.6403205394744873\u001b[0m\n",
            "\u001b[92mTest accuracy: 7492/10000 =  74.92 % ||| loss 0.6665106415748596\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.6668391731381417\n",
            "Batch #200 Loss: 0.6805211082100868\n",
            "Batch #300 Loss: 0.6612728887796402\n",
            "\u001b[92mTrain accuracy: 36162/48000 =  75.34 % ||| loss 0.6462356448173523\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9102/12000 =  75.85 % ||| loss 0.6364666223526001\u001b[0m\n",
            "\u001b[92mTest accuracy: 7417/10000 =  74.17 % ||| loss 0.6686640381813049\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.6416594809293747\n",
            "Batch #200 Loss: 0.6462474471330643\n",
            "Batch #300 Loss: 0.6374460026621819\n",
            "\u001b[92mTrain accuracy: 36677/48000 =  76.41 % ||| loss 0.6239708065986633\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9168/12000 =  76.4 % ||| loss 0.6174586415290833\u001b[0m\n",
            "\u001b[92mTest accuracy: 7567/10000 =  75.67 % ||| loss 0.6479963660240173\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6229904970526695\n",
            "Batch #200 Loss: 0.6272740897536278\n",
            "Batch #300 Loss: 0.6100765216350555\n",
            "\u001b[92mTrain accuracy: 36966/48000 =  77.01 % ||| loss 0.6137488484382629\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9247/12000 =  77.06 % ||| loss 0.6112590432167053\u001b[0m\n",
            "\u001b[92mTest accuracy: 7634/10000 =  76.34 % ||| loss 0.6336115002632141\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.612766125202179\n",
            "Batch #200 Loss: 0.5845304787158966\n",
            "Batch #300 Loss: 0.6076497095823288\n",
            "\u001b[92mTrain accuracy: 37548/48000 =  78.22 % ||| loss 0.5779064893722534\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9404/12000 =  78.37 % ||| loss 0.5724149942398071\u001b[0m\n",
            "\u001b[92mTest accuracy: 7767/10000 =  77.67 % ||| loss 0.5999720096588135\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5783044895529748\n",
            "Batch #200 Loss: 0.5906432282924652\n",
            "Batch #300 Loss: 0.59932832300663\n",
            "\u001b[92mTrain accuracy: 37824/48000 =  78.8 % ||| loss 0.5817170739173889\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9425/12000 =  78.54 % ||| loss 0.5783597230911255\u001b[0m\n",
            "\u001b[92mTest accuracy: 7745/10000 =  77.45 % ||| loss 0.6093407869338989\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.5853207609057427\n",
            "Batch #200 Loss: 0.5841843289136887\n",
            "Batch #300 Loss: 0.5730607816576958\n",
            "\u001b[92mTrain accuracy: 37362/48000 =  77.84 % ||| loss 0.5812428593635559\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9347/12000 =  77.89 % ||| loss 0.576576292514801\u001b[0m\n",
            "\u001b[92mTest accuracy: 7723/10000 =  77.23 % ||| loss 0.6082174777984619\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5778714394569398\n",
            "Batch #200 Loss: 0.5665695717930794\n",
            "Batch #300 Loss: 0.555920989215374\n",
            "\u001b[92mTrain accuracy: 38401/48000 =  80.0 % ||| loss 0.5453004837036133\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9684/12000 =  80.7 % ||| loss 0.5379548072814941\u001b[0m\n",
            "\u001b[92mTest accuracy: 7907/10000 =  79.07 % ||| loss 0.5673173069953918\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.552519844174385\n",
            "Batch #200 Loss: 0.5651497969031334\n",
            "Batch #300 Loss: 0.555558224618435\n",
            "\u001b[92mTrain accuracy: 37226/48000 =  77.55 % ||| loss 0.5856506824493408\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9323/12000 =  77.69 % ||| loss 0.5804482102394104\u001b[0m\n",
            "\u001b[92mTest accuracy: 7663/10000 =  76.63 % ||| loss 0.6123477816581726\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5503713303804397\n",
            "Batch #200 Loss: 0.5590507626533509\n",
            "Batch #300 Loss: 0.5322984573245049\n",
            "\u001b[92mTrain accuracy: 38349/48000 =  79.89 % ||| loss 0.5467279553413391\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9566/12000 =  79.72 % ||| loss 0.5459540486335754\u001b[0m\n",
            "\u001b[92mTest accuracy: 7882/10000 =  78.82 % ||| loss 0.5756556987762451\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5438178005814552\n",
            "Batch #200 Loss: 0.5467982348799706\n",
            "Batch #300 Loss: 0.5331408727169037\n",
            "\u001b[92mTrain accuracy: 39223/48000 =  81.71 % ||| loss 0.5136333107948303\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9851/12000 =  82.09 % ||| loss 0.5098299980163574\u001b[0m\n",
            "\u001b[92mTest accuracy: 8058/10000 =  80.58 % ||| loss 0.5385727882385254\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5309012448787689\n",
            "Batch #200 Loss: 0.5205040583014489\n",
            "Batch #300 Loss: 0.5284954124689102\n",
            "\u001b[92mTrain accuracy: 38934/48000 =  81.11 % ||| loss 0.5134879350662231\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9766/12000 =  81.38 % ||| loss 0.5088022351264954\u001b[0m\n",
            "\u001b[92mTest accuracy: 8013/10000 =  80.13 % ||| loss 0.5412189364433289\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.5039674669504166\n",
            "Batch #200 Loss: 0.5131576895713806\n",
            "Batch #300 Loss: 0.5374826213717461\n",
            "\u001b[92mTrain accuracy: 38541/48000 =  80.29 % ||| loss 0.5350557565689087\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9618/12000 =  80.15 % ||| loss 0.532182514667511\u001b[0m\n",
            "\u001b[92mTest accuracy: 7892/10000 =  78.92 % ||| loss 0.5608537197113037\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.5167865592241287\n",
            "Batch #200 Loss: 0.5179975381493569\n",
            "Batch #300 Loss: 0.504599966108799\n",
            "\u001b[92mTrain accuracy: 39584/48000 =  82.47 % ||| loss 0.49932584166526794\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9927/12000 =  82.73 % ||| loss 0.4980291426181793\u001b[0m\n",
            "\u001b[92mTest accuracy: 8120/10000 =  81.2 % ||| loss 0.5280677676200867\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.5017193725705147\n",
            "Batch #200 Loss: 0.5069480323791504\n",
            "Batch #300 Loss: 0.5018995708227157\n",
            "\u001b[92mTrain accuracy: 39281/48000 =  81.84 % ||| loss 0.4962051212787628\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9820/12000 =  81.83 % ||| loss 0.4977952241897583\u001b[0m\n",
            "\u001b[92mTest accuracy: 8083/10000 =  80.83 % ||| loss 0.5180538296699524\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.49675011545419695\n",
            "Batch #200 Loss: 0.49768872916698453\n",
            "Batch #300 Loss: 0.4970734065771103\n",
            "\u001b[92mTrain accuracy: 38863/48000 =  80.96 % ||| loss 0.5101189613342285\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9732/12000 =  81.1 % ||| loss 0.5093989372253418\u001b[0m\n",
            "\u001b[92mTest accuracy: 7982/10000 =  79.82 % ||| loss 0.538763165473938\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5025244995951652\n",
            "Batch #200 Loss: 0.48390906989574434\n",
            "Batch #300 Loss: 0.49369296431541443\n",
            "\u001b[92mTrain accuracy: 39643/48000 =  82.59 % ||| loss 0.4798031747341156\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9964/12000 =  83.03 % ||| loss 0.47822055220603943\u001b[0m\n",
            "\u001b[92mTest accuracy: 8164/10000 =  81.64 % ||| loss 0.5052786469459534\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.49700767278671265\n",
            "Batch #200 Loss: 0.48275382041931153\n",
            "Batch #300 Loss: 0.4938963836431503\n",
            "\u001b[92mTrain accuracy: 39699/48000 =  82.71 % ||| loss 0.48624756932258606\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9919/12000 =  82.66 % ||| loss 0.48746952414512634\u001b[0m\n",
            "\u001b[92mTest accuracy: 8157/10000 =  81.57 % ||| loss 0.5132549405097961\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_10</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_10' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_10</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_115836-Lenet5Decay_1726155046.322212_11</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_11' target=\"_blank\">Lenet5Decay_1726155046.322212_11</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_11' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.301312427520752\n",
            "Batch #200 Loss: 2.2945161962509157\n",
            "Batch #300 Loss: 2.285303750038147\n",
            "\u001b[92mTrain accuracy: 14925/48000 =  31.09 % ||| loss 2.2577385902404785\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3715/12000 =  30.96 % ||| loss 2.257702112197876\u001b[0m\n",
            "\u001b[92mTest accuracy: 3096/10000 =  30.96 % ||| loss 2.2579524517059326\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2232479214668275\n",
            "Batch #200 Loss: 1.892208105325699\n",
            "Batch #300 Loss: 1.1994588148593903\n",
            "\u001b[92mTrain accuracy: 30930/48000 =  64.44 % ||| loss 0.9682297110557556\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7850/12000 =  65.42 % ||| loss 0.957830011844635\u001b[0m\n",
            "\u001b[92mTest accuracy: 6383/10000 =  63.83 % ||| loss 0.9849864840507507\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.9490406066179276\n",
            "Batch #200 Loss: 0.9291017490625382\n",
            "Batch #300 Loss: 0.8893663901090622\n",
            "\u001b[92mTrain accuracy: 32769/48000 =  68.27 % ||| loss 0.8249101638793945\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8205/12000 =  68.38 % ||| loss 0.815809965133667\u001b[0m\n",
            "\u001b[92mTest accuracy: 6750/10000 =  67.5 % ||| loss 0.8434136509895325\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.8361827659606934\n",
            "Batch #200 Loss: 0.8142760699987411\n",
            "Batch #300 Loss: 0.8018477320671081\n",
            "\u001b[92mTrain accuracy: 33655/48000 =  70.11 % ||| loss 0.7667964696884155\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8441/12000 =  70.34 % ||| loss 0.7550593614578247\u001b[0m\n",
            "\u001b[92mTest accuracy: 6956/10000 =  69.56 % ||| loss 0.783013105392456\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.7839487439393997\n",
            "Batch #200 Loss: 0.7453162544965743\n",
            "Batch #300 Loss: 0.7396966302394867\n",
            "\u001b[92mTrain accuracy: 35321/48000 =  73.59 % ||| loss 0.7133997678756714\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8824/12000 =  73.53 % ||| loss 0.7065240740776062\u001b[0m\n",
            "\u001b[92mTest accuracy: 7255/10000 =  72.55 % ||| loss 0.7400680184364319\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.7048152381181717\n",
            "Batch #200 Loss: 0.7128955656290055\n",
            "Batch #300 Loss: 0.7061358338594437\n",
            "\u001b[92mTrain accuracy: 36339/48000 =  75.71 % ||| loss 0.6559029221534729\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9120/12000 =  76.0 % ||| loss 0.6445848345756531\u001b[0m\n",
            "\u001b[92mTest accuracy: 7494/10000 =  74.94 % ||| loss 0.6757833957672119\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.6993140983581543\n",
            "Batch #200 Loss: 0.6671379065513611\n",
            "Batch #300 Loss: 0.6542910858988762\n",
            "\u001b[92mTrain accuracy: 36606/48000 =  76.26 % ||| loss 0.6300057172775269\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9175/12000 =  76.46 % ||| loss 0.6226319074630737\u001b[0m\n",
            "\u001b[92mTest accuracy: 7552/10000 =  75.52 % ||| loss 0.6523798704147339\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.6462483584880829\n",
            "Batch #200 Loss: 0.6222162881493568\n",
            "Batch #300 Loss: 0.6279867231845856\n",
            "\u001b[92mTrain accuracy: 37262/48000 =  77.63 % ||| loss 0.6108106970787048\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9369/12000 =  78.08 % ||| loss 0.6017159819602966\u001b[0m\n",
            "\u001b[92mTest accuracy: 7698/10000 =  76.98 % ||| loss 0.6334252953529358\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.6119868832826615\n",
            "Batch #200 Loss: 0.6175942420959473\n",
            "Batch #300 Loss: 0.6070752122998238\n",
            "\u001b[92mTrain accuracy: 37368/48000 =  77.85 % ||| loss 0.5817469358444214\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9333/12000 =  77.78 % ||| loss 0.5727967619895935\u001b[0m\n",
            "\u001b[92mTest accuracy: 7712/10000 =  77.12 % ||| loss 0.6075277924537659\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.5795423525571823\n",
            "Batch #200 Loss: 0.5869774994254112\n",
            "Batch #300 Loss: 0.5719087678194046\n",
            "\u001b[92mTrain accuracy: 38454/48000 =  80.11 % ||| loss 0.5525991916656494\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9669/12000 =  80.58 % ||| loss 0.5466559529304504\u001b[0m\n",
            "\u001b[92mTest accuracy: 7904/10000 =  79.04 % ||| loss 0.5764684677124023\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.5611861070990563\n",
            "Batch #200 Loss: 0.5527164739370346\n",
            "Batch #300 Loss: 0.5484880840778351\n",
            "\u001b[92mTrain accuracy: 37962/48000 =  79.09 % ||| loss 0.5500466823577881\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9489/12000 =  79.07 % ||| loss 0.5443723797798157\u001b[0m\n",
            "\u001b[92mTest accuracy: 7838/10000 =  78.38 % ||| loss 0.5758281946182251\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5309427028894425\n",
            "Batch #200 Loss: 0.5456226393580437\n",
            "Batch #300 Loss: 0.5446369451284409\n",
            "\u001b[92mTrain accuracy: 38510/48000 =  80.23 % ||| loss 0.5306467413902283\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9668/12000 =  80.57 % ||| loss 0.522680401802063\u001b[0m\n",
            "\u001b[92mTest accuracy: 7974/10000 =  79.74 % ||| loss 0.5576192140579224\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5157851886749267\n",
            "Batch #200 Loss: 0.5283083871006966\n",
            "Batch #300 Loss: 0.5203253507614136\n",
            "\u001b[92mTrain accuracy: 39001/48000 =  81.25 % ||| loss 0.5131681561470032\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9785/12000 =  81.54 % ||| loss 0.507057785987854\u001b[0m\n",
            "\u001b[92mTest accuracy: 8013/10000 =  80.13 % ||| loss 0.5422980189323425\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.5082228884100914\n",
            "Batch #200 Loss: 0.520904783308506\n",
            "Batch #300 Loss: 0.5007721772789955\n",
            "\u001b[92mTrain accuracy: 38710/48000 =  80.65 % ||| loss 0.5184744596481323\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9707/12000 =  80.89 % ||| loss 0.5173373818397522\u001b[0m\n",
            "\u001b[92mTest accuracy: 7960/10000 =  79.6 % ||| loss 0.5412430763244629\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.5016433224081993\n",
            "Batch #200 Loss: 0.4877288246154785\n",
            "Batch #300 Loss: 0.4871921506524086\n",
            "\u001b[92mTrain accuracy: 39632/48000 =  82.57 % ||| loss 0.4818207621574402\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9916/12000 =  82.63 % ||| loss 0.48230093717575073\u001b[0m\n",
            "\u001b[92mTest accuracy: 8139/10000 =  81.39 % ||| loss 0.5092687606811523\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.47279723435640336\n",
            "Batch #200 Loss: 0.4845625668764114\n",
            "Batch #300 Loss: 0.48514035969972613\n",
            "\u001b[92mTrain accuracy: 38300/48000 =  79.79 % ||| loss 0.5314232707023621\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9578/12000 =  79.82 % ||| loss 0.5317121148109436\u001b[0m\n",
            "\u001b[92mTest accuracy: 7823/10000 =  78.23 % ||| loss 0.5686827301979065\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.4744646602869034\n",
            "Batch #200 Loss: 0.46698194324970244\n",
            "Batch #300 Loss: 0.46538234382867816\n",
            "\u001b[92mTrain accuracy: 40125/48000 =  83.59 % ||| loss 0.4544863998889923\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10040/12000 =  83.67 % ||| loss 0.4600563943386078\u001b[0m\n",
            "\u001b[92mTest accuracy: 8288/10000 =  82.88 % ||| loss 0.4827600419521332\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.45431906104087827\n",
            "Batch #200 Loss: 0.4565500667691231\n",
            "Batch #300 Loss: 0.47097125053405764\n",
            "\u001b[92mTrain accuracy: 40171/48000 =  83.69 % ||| loss 0.4525418281555176\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10034/12000 =  83.62 % ||| loss 0.4541229009628296\u001b[0m\n",
            "\u001b[92mTest accuracy: 8265/10000 =  82.65 % ||| loss 0.4912807047367096\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.462457457780838\n",
            "Batch #200 Loss: 0.4334115394949913\n",
            "Batch #300 Loss: 0.46683137357234955\n",
            "\u001b[92mTrain accuracy: 39797/48000 =  82.91 % ||| loss 0.45971059799194336\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9932/12000 =  82.77 % ||| loss 0.4646974802017212\u001b[0m\n",
            "\u001b[92mTest accuracy: 8205/10000 =  82.05 % ||| loss 0.49720531702041626\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.4429012995958328\n",
            "Batch #200 Loss: 0.4384408682584763\n",
            "Batch #300 Loss: 0.4339039519429207\n",
            "\u001b[92mTrain accuracy: 40443/48000 =  84.26 % ||| loss 0.4295460879802704\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10117/12000 =  84.31 % ||| loss 0.43541380763053894\u001b[0m\n",
            "\u001b[92mTest accuracy: 8336/10000 =  83.36 % ||| loss 0.4612683653831482\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.440554131269455\n",
            "Batch #200 Loss: 0.4397028416395187\n",
            "Batch #300 Loss: 0.4306004971265793\n",
            "\u001b[92mTrain accuracy: 40154/48000 =  83.65 % ||| loss 0.4420031011104584\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10038/12000 =  83.65 % ||| loss 0.4470751881599426\u001b[0m\n",
            "\u001b[92mTest accuracy: 8257/10000 =  82.57 % ||| loss 0.47728797793388367\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.4249571570754051\n",
            "Batch #200 Loss: 0.4393544805049896\n",
            "Batch #300 Loss: 0.4253289440274239\n",
            "\u001b[92mTrain accuracy: 40377/48000 =  84.12 % ||| loss 0.4400835335254669\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10072/12000 =  83.93 % ||| loss 0.4505622982978821\u001b[0m\n",
            "\u001b[92mTest accuracy: 8273/10000 =  82.73 % ||| loss 0.47762230038642883\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.4190121802687645\n",
            "Batch #200 Loss: 0.42465187728404996\n",
            "Batch #300 Loss: 0.41851275861263276\n",
            "\u001b[92mTrain accuracy: 41028/48000 =  85.47 % ||| loss 0.4012845456600189\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10248/12000 =  85.4 % ||| loss 0.411980003118515\u001b[0m\n",
            "\u001b[92mTest accuracy: 8448/10000 =  84.48 % ||| loss 0.43762341141700745\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.4209228178858757\n",
            "Batch #200 Loss: 0.4188863372802734\n",
            "Batch #300 Loss: 0.4037104219198227\n",
            "\u001b[92mTrain accuracy: 41026/48000 =  85.47 % ||| loss 0.39993199706077576\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10222/12000 =  85.18 % ||| loss 0.41206875443458557\u001b[0m\n",
            "\u001b[92mTest accuracy: 8438/10000 =  84.38 % ||| loss 0.4351382255554199\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.4031476867198944\n",
            "Batch #200 Loss: 0.4076767581701279\n",
            "Batch #300 Loss: 0.40303524807095525\n",
            "\u001b[92mTrain accuracy: 41312/48000 =  86.07 % ||| loss 0.38685160875320435\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10268/12000 =  85.57 % ||| loss 0.39813289046287537\u001b[0m\n",
            "\u001b[92mTest accuracy: 8496/10000 =  84.96 % ||| loss 0.4256899952888489\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_11</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_11' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_11</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_120107-Lenet5Decay_1726155046.322212_12</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_12' target=\"_blank\">Lenet5Decay_1726155046.322212_12</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_12' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_12</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3026477599143984\n",
            "Batch #200 Loss: 2.302730236053467\n",
            "Batch #300 Loss: 2.3027497839927675\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302720069885254\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.303110122680664\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027491569519043\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3027217054367064\n",
            "Batch #200 Loss: 2.302721984386444\n",
            "Batch #300 Loss: 2.302651846408844\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302612066268921\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3027470111846924\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30260968208313\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3026205015182497\n",
            "Batch #200 Loss: 2.3026968836784363\n",
            "Batch #300 Loss: 2.302506501674652\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025851249694824\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302734136581421\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026340007781982\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3026529479026796\n",
            "Batch #200 Loss: 2.30263090133667\n",
            "Batch #300 Loss: 2.3028591871261597\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3026366233825684\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302706241607666\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025975227355957\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3025888347625734\n",
            "Batch #200 Loss: 2.3027703309059144\n",
            "Batch #300 Loss: 2.302653889656067\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302631378173828\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026812076568604\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026680946350098\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3027084708213805\n",
            "Batch #200 Loss: 2.302766406536102\n",
            "Batch #300 Loss: 2.3027043652534487\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3025894165039062\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.302717447280884\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026556968688965\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.3026938438415527\n",
            "Batch #200 Loss: 2.3026947164535523\n",
            "Batch #300 Loss: 2.302634227275848\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.3026366233825684\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.3025758266448975\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302616834640503\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.302879774570465\n",
            "Batch #200 Loss: 2.3027852892875673\n",
            "Batch #300 Loss: 2.302525963783264\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302656412124634\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302696943283081\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302678108215332\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.30278856754303\n",
            "Batch #200 Loss: 2.3026216793060303\n",
            "Batch #300 Loss: 2.302755343914032\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.3026721477508545\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.302619218826294\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302705764770508\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.3027280449867247\n",
            "Batch #200 Loss: 2.3026606893539427\n",
            "Batch #300 Loss: 2.302641575336456\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.302672863006592\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.302820920944214\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302687168121338\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.302783579826355\n",
            "Batch #200 Loss: 2.302804498672485\n",
            "Batch #300 Loss: 2.3026406812667846\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.302640914916992\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.30273175239563\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026347160339355\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.3027061676979064\n",
            "Batch #200 Loss: 2.302670056819916\n",
            "Batch #300 Loss: 2.302752573490143\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.302600145339966\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.302650213241577\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302581548690796\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.302658352851868\n",
            "Batch #200 Loss: 2.3027878642082213\n",
            "Batch #300 Loss: 2.302761960029602\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.302640199661255\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.302718162536621\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026328086853027\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.302524242401123\n",
            "Batch #200 Loss: 2.3028525376319884\n",
            "Batch #300 Loss: 2.3027103900909425\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3026628494262695\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3028621673583984\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302698850631714\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.302923467159271\n",
            "Batch #200 Loss: 2.302774240970612\n",
            "Batch #300 Loss: 2.302669641971588\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302608013153076\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3027443885803223\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302617311477661\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.3027251100540163\n",
            "Batch #200 Loss: 2.302712299823761\n",
            "Batch #300 Loss: 2.302718234062195\n",
            "\u001b[92mTrain accuracy: 4739/48000 =  9.873 % ||| loss 2.3027212619781494\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1261/12000 =  10.51 % ||| loss 2.3026528358459473\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3027329444885254\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.30255336523056\n",
            "Batch #200 Loss: 2.302753984928131\n",
            "Batch #300 Loss: 2.3027509546279905\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3026177883148193\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.302625894546509\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302623987197876\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.30253627538681\n",
            "Batch #200 Loss: 2.3028165173530577\n",
            "Batch #300 Loss: 2.3027708864212038\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302638292312622\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3027215003967285\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302699565887451\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.3026880645751953\n",
            "Batch #200 Loss: 2.302704677581787\n",
            "Batch #300 Loss: 2.3026361417770387\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.302596092224121\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.3026676177978516\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302582263946533\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.302654082775116\n",
            "Batch #200 Loss: 2.302780659198761\n",
            "Batch #300 Loss: 2.3027256822586057\n",
            "\u001b[92mTrain accuracy: 4809/48000 =  10.02 % ||| loss 2.3025896549224854\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1191/12000 =  9.925 % ||| loss 2.3027377128601074\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025965690612793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.3025948452949523\n",
            "Batch #200 Loss: 2.3026417183876036\n",
            "Batch #300 Loss: 2.3028223156929015\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3026981353759766\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302773952484131\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302785873413086\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.302563579082489\n",
            "Batch #200 Loss: 2.3027654552459715\n",
            "Batch #300 Loss: 2.302745940685272\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302615165710449\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026504516601562\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026373386383057\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.302686769962311\n",
            "Batch #200 Loss: 2.302824008464813\n",
            "Batch #300 Loss: 2.3027387046813965\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3026459217071533\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3028130531311035\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026278018951416\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.302745714187622\n",
            "Batch #200 Loss: 2.3028459620475767\n",
            "Batch #300 Loss: 2.302756049633026\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302614212036133\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3027234077453613\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026068210601807\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3026566338539123\n",
            "Batch #200 Loss: 2.3028045105934143\n",
            "Batch #300 Loss: 2.302688672542572\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3026909828186035\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302678108215332\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302696466445923\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_12</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_12' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_12</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_120337-Lenet5Decay_1726155046.322212_13</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_13' target=\"_blank\">Lenet5Decay_1726155046.322212_13</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_13' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_13</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.2610666251182554\n",
            "Batch #200 Loss: 1.1210403370857238\n",
            "Batch #300 Loss: 0.8366144835948944\n",
            "\u001b[92mTrain accuracy: 34778/48000 =  72.45 % ||| loss 0.724271833896637\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8686/12000 =  72.38 % ||| loss 0.7174884080886841\u001b[0m\n",
            "\u001b[92mTest accuracy: 7150/10000 =  71.5 % ||| loss 0.7493770718574524\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6906332406401634\n",
            "Batch #200 Loss: 0.6646463778614998\n",
            "Batch #300 Loss: 0.6250656154751778\n",
            "\u001b[92mTrain accuracy: 37425/48000 =  77.97 % ||| loss 0.5937955975532532\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9425/12000 =  78.54 % ||| loss 0.5932868123054504\u001b[0m\n",
            "\u001b[92mTest accuracy: 7711/10000 =  77.11 % ||| loss 0.6074548959732056\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5812312814593316\n",
            "Batch #200 Loss: 0.5595566067099571\n",
            "Batch #300 Loss: 0.5384953612089157\n",
            "\u001b[92mTrain accuracy: 39108/48000 =  81.47 % ||| loss 0.5110223889350891\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9809/12000 =  81.74 % ||| loss 0.5102763772010803\u001b[0m\n",
            "\u001b[92mTest accuracy: 8016/10000 =  80.16 % ||| loss 0.5411295294761658\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.5067873042821884\n",
            "Batch #200 Loss: 0.49849882632493975\n",
            "Batch #300 Loss: 0.4953932213783264\n",
            "\u001b[92mTrain accuracy: 39743/48000 =  82.8 % ||| loss 0.4703608453273773\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9974/12000 =  83.12 % ||| loss 0.47461843490600586\u001b[0m\n",
            "\u001b[92mTest accuracy: 8193/10000 =  81.93 % ||| loss 0.49523496627807617\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4693258944153786\n",
            "Batch #200 Loss: 0.48504179835319516\n",
            "Batch #300 Loss: 0.4736393803358078\n",
            "\u001b[92mTrain accuracy: 39914/48000 =  83.15 % ||| loss 0.47199419140815735\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9974/12000 =  83.12 % ||| loss 0.4776317775249481\u001b[0m\n",
            "\u001b[92mTest accuracy: 8237/10000 =  82.37 % ||| loss 0.49356094002723694\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.44717820942401887\n",
            "Batch #200 Loss: 0.4772357603907585\n",
            "Batch #300 Loss: 0.4361273780465126\n",
            "\u001b[92mTrain accuracy: 39627/48000 =  82.56 % ||| loss 0.4772421419620514\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9827/12000 =  81.89 % ||| loss 0.4847857356071472\u001b[0m\n",
            "\u001b[92mTest accuracy: 8143/10000 =  81.43 % ||| loss 0.506671667098999\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.43516735374927523\n",
            "Batch #200 Loss: 0.42907959342002866\n",
            "Batch #300 Loss: 0.4369951844215393\n",
            "\u001b[92mTrain accuracy: 40352/48000 =  84.07 % ||| loss 0.4301479458808899\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10099/12000 =  84.16 % ||| loss 0.43283531069755554\u001b[0m\n",
            "\u001b[92mTest accuracy: 8305/10000 =  83.05 % ||| loss 0.4566343426704407\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.4352785375714302\n",
            "Batch #200 Loss: 0.4301478087902069\n",
            "Batch #300 Loss: 0.4217657646536827\n",
            "\u001b[92mTrain accuracy: 41188/48000 =  85.81 % ||| loss 0.39469489455223083\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10309/12000 =  85.91 % ||| loss 0.399608314037323\u001b[0m\n",
            "\u001b[92mTest accuracy: 8470/10000 =  84.7 % ||| loss 0.4236330986022949\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.40913334488868713\n",
            "Batch #200 Loss: 0.4121671780943871\n",
            "Batch #300 Loss: 0.40826051473617553\n",
            "\u001b[92mTrain accuracy: 40628/48000 =  84.64 % ||| loss 0.41880205273628235\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10157/12000 =  84.64 % ||| loss 0.42447325587272644\u001b[0m\n",
            "\u001b[92mTest accuracy: 8374/10000 =  83.74 % ||| loss 0.44571492075920105\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.41432603091001513\n",
            "Batch #200 Loss: 0.4076014685630798\n",
            "Batch #300 Loss: 0.40885575383901596\n",
            "\u001b[92mTrain accuracy: 41274/48000 =  85.99 % ||| loss 0.3869701027870178\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10337/12000 =  86.14 % ||| loss 0.38911789655685425\u001b[0m\n",
            "\u001b[92mTest accuracy: 8510/10000 =  85.1 % ||| loss 0.41901537775993347\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.39615464717149734\n",
            "Batch #200 Loss: 0.40904945075511934\n",
            "Batch #300 Loss: 0.39361788645386697\n",
            "\u001b[92mTrain accuracy: 41507/48000 =  86.47 % ||| loss 0.3856559693813324\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10324/12000 =  86.03 % ||| loss 0.39490213990211487\u001b[0m\n",
            "\u001b[92mTest accuracy: 8525/10000 =  85.25 % ||| loss 0.41264140605926514\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.39897093042731285\n",
            "Batch #200 Loss: 0.39446075111627577\n",
            "Batch #300 Loss: 0.4179478758573532\n",
            "\u001b[92mTrain accuracy: 41020/48000 =  85.46 % ||| loss 0.4083813428878784\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10260/12000 =  85.5 % ||| loss 0.4110666513442993\u001b[0m\n",
            "\u001b[92mTest accuracy: 8464/10000 =  84.64 % ||| loss 0.43420690298080444\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.39134748563170435\n",
            "Batch #200 Loss: 0.40381365835666655\n",
            "Batch #300 Loss: 0.39965800166130067\n",
            "\u001b[92mTrain accuracy: 41723/48000 =  86.92 % ||| loss 0.3835655450820923\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10381/12000 =  86.51 % ||| loss 0.39218568801879883\u001b[0m\n",
            "\u001b[92mTest accuracy: 8557/10000 =  85.57 % ||| loss 0.4147295355796814\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.3982986681163311\n",
            "Batch #200 Loss: 0.40160003244876863\n",
            "Batch #300 Loss: 0.3918644365668297\n",
            "\u001b[92mTrain accuracy: 41290/48000 =  86.02 % ||| loss 0.3926991820335388\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10277/12000 =  85.64 % ||| loss 0.3999156951904297\u001b[0m\n",
            "\u001b[92mTest accuracy: 8514/10000 =  85.14 % ||| loss 0.4156436622142792\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.39256638020277024\n",
            "Batch #200 Loss: 0.3869308790564537\n",
            "Batch #300 Loss: 0.3969568130373955\n",
            "\u001b[92mTrain accuracy: 40686/48000 =  84.76 % ||| loss 0.41955211758613586\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10147/12000 =  84.56 % ||| loss 0.4238061010837555\u001b[0m\n",
            "\u001b[92mTest accuracy: 8379/10000 =  83.79 % ||| loss 0.44621238112449646\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3903869934380054\n",
            "Batch #200 Loss: 0.3965801878273487\n",
            "Batch #300 Loss: 0.3944312670826912\n",
            "\u001b[92mTrain accuracy: 41775/48000 =  87.03 % ||| loss 0.3741448223590851\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10418/12000 =  86.82 % ||| loss 0.38306182622909546\u001b[0m\n",
            "\u001b[92mTest accuracy: 8589/10000 =  85.89 % ||| loss 0.4040394425392151\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.3825884287059307\n",
            "Batch #200 Loss: 0.40228226095438\n",
            "Batch #300 Loss: 0.3801543420553207\n",
            "\u001b[92mTrain accuracy: 41840/48000 =  87.17 % ||| loss 0.3672759234905243\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10451/12000 =  87.09 % ||| loss 0.37613463401794434\u001b[0m\n",
            "\u001b[92mTest accuracy: 8602/10000 =  86.02 % ||| loss 0.39590421319007874\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.37190521851181985\n",
            "Batch #200 Loss: 0.40672589153051375\n",
            "Batch #300 Loss: 0.3798605339229107\n",
            "\u001b[92mTrain accuracy: 41611/48000 =  86.69 % ||| loss 0.3725929260253906\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10376/12000 =  86.47 % ||| loss 0.3785158097743988\u001b[0m\n",
            "\u001b[92mTest accuracy: 8532/10000 =  85.32 % ||| loss 0.40143248438835144\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.3751106791198254\n",
            "Batch #200 Loss: 0.39218752324581146\n",
            "Batch #300 Loss: 0.38411202490329743\n",
            "\u001b[92mTrain accuracy: 41623/48000 =  86.71 % ||| loss 0.37411412596702576\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10396/12000 =  86.63 % ||| loss 0.38077470660209656\u001b[0m\n",
            "\u001b[92mTest accuracy: 8594/10000 =  85.94 % ||| loss 0.3983534872531891\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.3891919599473476\n",
            "Batch #200 Loss: 0.3859866064786911\n",
            "Batch #300 Loss: 0.39466718047857285\n",
            "\u001b[92mTrain accuracy: 41954/48000 =  87.4 % ||| loss 0.35791295766830444\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10449/12000 =  87.08 % ||| loss 0.3667210638523102\u001b[0m\n",
            "\u001b[92mTest accuracy: 8632/10000 =  86.32 % ||| loss 0.38777971267700195\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.3799970023334026\n",
            "Batch #200 Loss: 0.3602092781662941\n",
            "Batch #300 Loss: 0.3846777054667473\n",
            "\u001b[92mTrain accuracy: 41495/48000 =  86.45 % ||| loss 0.3833661675453186\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10339/12000 =  86.16 % ||| loss 0.39190587401390076\u001b[0m\n",
            "\u001b[92mTest accuracy: 8545/10000 =  85.45 % ||| loss 0.4122695028781891\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.375768763422966\n",
            "Batch #200 Loss: 0.37706543028354644\n",
            "Batch #300 Loss: 0.37370453715324403\n",
            "\u001b[92mTrain accuracy: 41302/48000 =  86.05 % ||| loss 0.3807299733161926\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10318/12000 =  85.98 % ||| loss 0.3865324854850769\u001b[0m\n",
            "\u001b[92mTest accuracy: 8476/10000 =  84.76 % ||| loss 0.40874287486076355\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.3722373555600643\n",
            "Batch #200 Loss: 0.3856530436873436\n",
            "Batch #300 Loss: 0.3820882523059845\n",
            "\u001b[92mTrain accuracy: 41405/48000 =  86.26 % ||| loss 0.3892473578453064\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10296/12000 =  85.8 % ||| loss 0.39997798204421997\u001b[0m\n",
            "\u001b[92mTest accuracy: 8508/10000 =  85.08 % ||| loss 0.4235870838165283\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.36879572317004206\n",
            "Batch #200 Loss: 0.39925831735134126\n",
            "Batch #300 Loss: 0.38860925257205964\n",
            "\u001b[92mTrain accuracy: 41743/48000 =  86.96 % ||| loss 0.36774152517318726\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10390/12000 =  86.58 % ||| loss 0.3745906949043274\u001b[0m\n",
            "\u001b[92mTest accuracy: 8573/10000 =  85.73 % ||| loss 0.4008026421070099\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.37544019997119904\n",
            "Batch #200 Loss: 0.3858327040076256\n",
            "Batch #300 Loss: 0.37262992054224015\n",
            "\u001b[92mTrain accuracy: 42138/48000 =  87.79 % ||| loss 0.34948909282684326\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10503/12000 =  87.52 % ||| loss 0.35599055886268616\u001b[0m\n",
            "\u001b[92mTest accuracy: 8662/10000 =  86.62 % ||| loss 0.3790782392024994\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_13</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_13' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_13</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_120608-Lenet5Decay_1726155046.322212_14</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_14' target=\"_blank\">Lenet5Decay_1726155046.322212_14</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_14' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_14</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.9, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.1345347499847414\n",
            "Batch #200 Loss: 0.9800327140092849\n",
            "Batch #300 Loss: 0.7881089395284653\n",
            "\u001b[92mTrain accuracy: 35189/48000 =  73.31 % ||| loss 0.6805986762046814\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8838/12000 =  73.65 % ||| loss 0.6706403493881226\u001b[0m\n",
            "\u001b[92mTest accuracy: 7230/10000 =  72.3 % ||| loss 0.7094528675079346\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 0.6739737138152122\n",
            "Batch #200 Loss: 0.6094662967324257\n",
            "Batch #300 Loss: 0.5665519466996193\n",
            "\u001b[92mTrain accuracy: 37434/48000 =  77.99 % ||| loss 0.5728896856307983\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9441/12000 =  78.67 % ||| loss 0.5631381273269653\u001b[0m\n",
            "\u001b[92mTest accuracy: 7713/10000 =  77.13 % ||| loss 0.6024238467216492\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.5299477806687355\n",
            "Batch #200 Loss: 0.4942220738530159\n",
            "Batch #300 Loss: 0.484535326063633\n",
            "\u001b[92mTrain accuracy: 39947/48000 =  83.22 % ||| loss 0.45916748046875\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10020/12000 =  83.5 % ||| loss 0.4612230360507965\u001b[0m\n",
            "\u001b[92mTest accuracy: 8222/10000 =  82.22 % ||| loss 0.4910886883735657\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.4576549986004829\n",
            "Batch #200 Loss: 0.4380525329709053\n",
            "Batch #300 Loss: 0.4278789788484573\n",
            "\u001b[92mTrain accuracy: 40913/48000 =  85.24 % ||| loss 0.40170031785964966\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10210/12000 =  85.08 % ||| loss 0.4124619960784912\u001b[0m\n",
            "\u001b[92mTest accuracy: 8433/10000 =  84.33 % ||| loss 0.4346455931663513\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.4025336714088917\n",
            "Batch #200 Loss: 0.40320406764745714\n",
            "Batch #300 Loss: 0.39479669168591497\n",
            "\u001b[92mTrain accuracy: 41212/48000 =  85.86 % ||| loss 0.3838266134262085\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10237/12000 =  85.31 % ||| loss 0.3979043662548065\u001b[0m\n",
            "\u001b[92mTest accuracy: 8476/10000 =  84.76 % ||| loss 0.4227434992790222\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.3723349048197269\n",
            "Batch #200 Loss: 0.37316827222704885\n",
            "Batch #300 Loss: 0.3752317215502262\n",
            "\u001b[92mTrain accuracy: 41783/48000 =  87.05 % ||| loss 0.352407306432724\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10376/12000 =  86.47 % ||| loss 0.36653685569763184\u001b[0m\n",
            "\u001b[92mTest accuracy: 8580/10000 =  85.8 % ||| loss 0.39133790135383606\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.3580376416444778\n",
            "Batch #200 Loss: 0.3490601667761803\n",
            "Batch #300 Loss: 0.357898308634758\n",
            "\u001b[92mTrain accuracy: 42020/48000 =  87.54 % ||| loss 0.34130120277404785\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10421/12000 =  86.84 % ||| loss 0.36153626441955566\u001b[0m\n",
            "\u001b[92mTest accuracy: 8611/10000 =  86.11 % ||| loss 0.3801004886627197\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.3375983674824238\n",
            "Batch #200 Loss: 0.33953020244836807\n",
            "Batch #300 Loss: 0.3440812058746815\n",
            "\u001b[92mTrain accuracy: 42273/48000 =  88.07 % ||| loss 0.3199772238731384\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10496/12000 =  87.47 % ||| loss 0.3405429720878601\u001b[0m\n",
            "\u001b[92mTest accuracy: 8662/10000 =  86.62 % ||| loss 0.36351314187049866\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.3307592360675335\n",
            "Batch #200 Loss: 0.3205228804051876\n",
            "Batch #300 Loss: 0.316892132461071\n",
            "\u001b[92mTrain accuracy: 42313/48000 =  88.15 % ||| loss 0.3234279155731201\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10449/12000 =  87.08 % ||| loss 0.3444000780582428\u001b[0m\n",
            "\u001b[92mTest accuracy: 8680/10000 =  86.8 % ||| loss 0.36621350049972534\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3138742819428444\n",
            "Batch #200 Loss: 0.3084482389688492\n",
            "Batch #300 Loss: 0.32180977299809455\n",
            "\u001b[92mTrain accuracy: 42313/48000 =  88.15 % ||| loss 0.317343145608902\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10461/12000 =  87.17 % ||| loss 0.3405986428260803\u001b[0m\n",
            "\u001b[92mTest accuracy: 8687/10000 =  86.87 % ||| loss 0.35768792033195496\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.30307886824011804\n",
            "Batch #200 Loss: 0.3053769125044346\n",
            "Batch #300 Loss: 0.31318871200084686\n",
            "\u001b[92mTrain accuracy: 42748/48000 =  89.06 % ||| loss 0.29349279403686523\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10554/12000 =  87.95 % ||| loss 0.3223212659358978\u001b[0m\n",
            "\u001b[92mTest accuracy: 8770/10000 =  87.7 % ||| loss 0.34653550386428833\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.29274208381772043\n",
            "Batch #200 Loss: 0.3038648983836174\n",
            "Batch #300 Loss: 0.2986681243777275\n",
            "\u001b[92mTrain accuracy: 42951/48000 =  89.48 % ||| loss 0.2835041582584381\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10620/12000 =  88.5 % ||| loss 0.31413203477859497\u001b[0m\n",
            "\u001b[92mTest accuracy: 8783/10000 =  87.83 % ||| loss 0.3316110074520111\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.2884827680885792\n",
            "Batch #200 Loss: 0.2967797528207302\n",
            "Batch #300 Loss: 0.2893012861162424\n",
            "\u001b[92mTrain accuracy: 42542/48000 =  88.63 % ||| loss 0.2992003858089447\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10548/12000 =  87.9 % ||| loss 0.3264288306236267\u001b[0m\n",
            "\u001b[92mTest accuracy: 8696/10000 =  86.96 % ||| loss 0.3539513647556305\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.27714367642998694\n",
            "Batch #200 Loss: 0.2832241773605347\n",
            "Batch #300 Loss: 0.27992508068680766\n",
            "\u001b[92mTrain accuracy: 43283/48000 =  90.17 % ||| loss 0.2685619592666626\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10692/12000 =  89.1 % ||| loss 0.30075037479400635\u001b[0m\n",
            "\u001b[92mTest accuracy: 8826/10000 =  88.26 % ||| loss 0.32205015420913696\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.2781700474023819\n",
            "Batch #200 Loss: 0.2846736365556717\n",
            "Batch #300 Loss: 0.27093983069062233\n",
            "\u001b[92mTrain accuracy: 43259/48000 =  90.12 % ||| loss 0.26722002029418945\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10662/12000 =  88.85 % ||| loss 0.29986539483070374\u001b[0m\n",
            "\u001b[92mTest accuracy: 8857/10000 =  88.57 % ||| loss 0.3238884210586548\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.26720552682876586\n",
            "Batch #200 Loss: 0.27109045028686524\n",
            "Batch #300 Loss: 0.27269804999232294\n",
            "\u001b[92mTrain accuracy: 43488/48000 =  90.6 % ||| loss 0.2556818127632141\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10723/12000 =  89.36 % ||| loss 0.29187360405921936\u001b[0m\n",
            "\u001b[92mTest accuracy: 8836/10000 =  88.36 % ||| loss 0.3172862231731415\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.2686167466640472\n",
            "Batch #200 Loss: 0.26823195315897463\n",
            "Batch #300 Loss: 0.2678620952367783\n",
            "\u001b[92mTrain accuracy: 43697/48000 =  91.04 % ||| loss 0.2465166449546814\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10740/12000 =  89.5 % ||| loss 0.28594595193862915\u001b[0m\n",
            "\u001b[92mTest accuracy: 8896/10000 =  88.96 % ||| loss 0.30599454045295715\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.25759275257587433\n",
            "Batch #200 Loss: 0.2588679327070713\n",
            "Batch #300 Loss: 0.26883081771433354\n",
            "\u001b[92mTrain accuracy: 43496/48000 =  90.62 % ||| loss 0.25516578555107117\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10690/12000 =  89.08 % ||| loss 0.2929331660270691\u001b[0m\n",
            "\u001b[92mTest accuracy: 8850/10000 =  88.5 % ||| loss 0.31424573063850403\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.2676682536303997\n",
            "Batch #200 Loss: 0.2609121975302696\n",
            "Batch #300 Loss: 0.2495003293454647\n",
            "\u001b[92mTrain accuracy: 43565/48000 =  90.76 % ||| loss 0.25304877758026123\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10736/12000 =  89.47 % ||| loss 0.2895490229129791\u001b[0m\n",
            "\u001b[92mTest accuracy: 8856/10000 =  88.56 % ||| loss 0.3150860667228699\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.25007636204361916\n",
            "Batch #200 Loss: 0.25048895567655566\n",
            "Batch #300 Loss: 0.25221619859337807\n",
            "\u001b[92mTrain accuracy: 43354/48000 =  90.32 % ||| loss 0.2541787624359131\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10636/12000 =  88.63 % ||| loss 0.29764324426651\u001b[0m\n",
            "\u001b[92mTest accuracy: 8809/10000 =  88.09 % ||| loss 0.3207816779613495\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.236966799646616\n",
            "Batch #200 Loss: 0.25924658015370367\n",
            "Batch #300 Loss: 0.2501945662498474\n",
            "\u001b[92mTrain accuracy: 43549/48000 =  90.73 % ||| loss 0.2527516782283783\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10714/12000 =  89.28 % ||| loss 0.29967835545539856\u001b[0m\n",
            "\u001b[92mTest accuracy: 8823/10000 =  88.23 % ||| loss 0.32681092619895935\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.2401350512355566\n",
            "Batch #200 Loss: 0.25272606998682023\n",
            "Batch #300 Loss: 0.2515006556361914\n",
            "\u001b[92mTrain accuracy: 43535/48000 =  90.7 % ||| loss 0.25133275985717773\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10645/12000 =  88.71 % ||| loss 0.3035305440425873\u001b[0m\n",
            "\u001b[92mTest accuracy: 8814/10000 =  88.14 % ||| loss 0.32467716932296753\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.23998221278190612\n",
            "Batch #200 Loss: 0.24041277185082435\n",
            "Batch #300 Loss: 0.2388178950548172\n",
            "\u001b[92mTrain accuracy: 43951/48000 =  91.56 % ||| loss 0.2292703241109848\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10778/12000 =  89.82 % ||| loss 0.2804904580116272\u001b[0m\n",
            "\u001b[92mTest accuracy: 8933/10000 =  89.33 % ||| loss 0.3032380938529968\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.23581960201263427\n",
            "Batch #200 Loss: 0.24365930765867233\n",
            "Batch #300 Loss: 0.2382628381252289\n",
            "\u001b[92mTrain accuracy: 44331/48000 =  92.36 % ||| loss 0.21440592408180237\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10829/12000 =  90.24 % ||| loss 0.26660576462745667\u001b[0m\n",
            "\u001b[92mTest accuracy: 8969/10000 =  89.69 % ||| loss 0.28562793135643005\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.24306170612573624\n",
            "Batch #200 Loss: 0.2402021098136902\n",
            "Batch #300 Loss: 0.23808910250663756\n",
            "\u001b[92mTrain accuracy: 43991/48000 =  91.65 % ||| loss 0.22648125886917114\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10786/12000 =  89.88 % ||| loss 0.2801387310028076\u001b[0m\n",
            "\u001b[92mTest accuracy: 8877/10000 =  88.77 % ||| loss 0.30666714906692505\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_14</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_14' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_14</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_120842-Lenet5Decay_1726155046.322212_15</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_15' target=\"_blank\">Lenet5Decay_1726155046.322212_15</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_15' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_15</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.301479561328888\n",
            "Batch #200 Loss: 2.3019167733192445\n",
            "Batch #300 Loss: 2.302438519001007\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.302553653717041\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302684783935547\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025949001312256\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.302561662197113\n",
            "Batch #200 Loss: 2.3026901268959046\n",
            "Batch #300 Loss: 2.302640974521637\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302581787109375\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.30261492729187\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302586078643799\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.302561559677124\n",
            "Batch #200 Loss: 2.3026859879493715\n",
            "Batch #300 Loss: 2.3026698994636536\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.302584648132324\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.302614688873291\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026018142700195\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3026500439643858\n",
            "Batch #200 Loss: 2.302687573432922\n",
            "Batch #300 Loss: 2.3026570677757263\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3025777339935303\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.302656650543213\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026037216186523\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.3026101899147036\n",
            "Batch #200 Loss: 2.3026492047309874\n",
            "Batch #300 Loss: 2.302645401954651\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.302577257156372\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.30264949798584\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302595853805542\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3026294255256654\n",
            "Batch #200 Loss: 2.3025984072685244\n",
            "Batch #300 Loss: 2.302638885974884\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.302604913711548\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.302568197250366\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302607297897339\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.302556450366974\n",
            "Batch #200 Loss: 2.302656967639923\n",
            "Batch #300 Loss: 2.302650396823883\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302600860595703\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026609420776367\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026123046875\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.3026736974716187\n",
            "Batch #200 Loss: 2.3026843309402465\n",
            "Batch #300 Loss: 2.302640640735626\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025801181793213\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3027193546295166\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302626132965088\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.3026365399360658\n",
            "Batch #200 Loss: 2.302606384754181\n",
            "Batch #300 Loss: 2.302619264125824\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.302586317062378\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3026235103607178\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302586078643799\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.302640697956085\n",
            "Batch #200 Loss: 2.3026908922195433\n",
            "Batch #300 Loss: 2.3025465631484985\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302586317062378\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3025991916656494\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025805950164795\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.3026244163513185\n",
            "Batch #200 Loss: 2.302613704204559\n",
            "Batch #300 Loss: 2.302674765586853\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.302598714828491\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.302593469619751\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026199340820312\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.3026381158828735\n",
            "Batch #200 Loss: 2.30263072013855\n",
            "Batch #300 Loss: 2.3026571440696717\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3025882244110107\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3026232719421387\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025877475738525\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.3026902747154234\n",
            "Batch #200 Loss: 2.302662584781647\n",
            "Batch #300 Loss: 2.302675588130951\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.30258846282959\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302729368209839\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026325702667236\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.3024927616119384\n",
            "Batch #200 Loss: 2.30261834859848\n",
            "Batch #300 Loss: 2.3027172541618346\n",
            "\u001b[92mTrain accuracy: 4786/48000 =  9.971 % ||| loss 2.302593469619751\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1214/12000 =  10.12 % ||| loss 2.3026607036590576\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026251792907715\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.3026682567596435\n",
            "Batch #200 Loss: 2.3026365852355957\n",
            "Batch #300 Loss: 2.3026420521736144\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025853633880615\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.302682638168335\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30257248878479\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.302638154029846\n",
            "Batch #200 Loss: 2.3026142954826354\n",
            "Batch #300 Loss: 2.302642138004303\n",
            "\u001b[92mTrain accuracy: 4788/48000 =  9.975 % ||| loss 2.3025858402252197\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1212/12000 =  10.1 % ||| loss 2.302616596221924\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302572727203369\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.302593550682068\n",
            "Batch #200 Loss: 2.30257817029953\n",
            "Batch #300 Loss: 2.302738564014435\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.302595615386963\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.302565574645996\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025903701782227\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.3026538133621215\n",
            "Batch #200 Loss: 2.3026272368431093\n",
            "Batch #300 Loss: 2.3026589918136597\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302584648132324\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.3026416301727295\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302600383758545\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.3026339292526243\n",
            "Batch #200 Loss: 2.3025548386573793\n",
            "Batch #300 Loss: 2.302684535980225\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.3025829792022705\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.3026161193847656\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026058673858643\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.3026916360855103\n",
            "Batch #200 Loss: 2.3026637196540833\n",
            "Batch #300 Loss: 2.302683515548706\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302578926086426\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3027050495147705\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302584409713745\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.302554955482483\n",
            "Batch #200 Loss: 2.3026381874084474\n",
            "Batch #300 Loss: 2.3027125573158265\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302586793899536\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026676177978516\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302598714828491\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.3026974773406983\n",
            "Batch #200 Loss: 2.3025796890258787\n",
            "Batch #300 Loss: 2.302661418914795\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025810718536377\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026390075683594\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025946617126465\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.302624294757843\n",
            "Batch #200 Loss: 2.3026054430007936\n",
            "Batch #300 Loss: 2.3027246308326723\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.30258846282959\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.30265736579895\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025832176208496\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.3026663088798522\n",
            "Batch #200 Loss: 2.302633059024811\n",
            "Batch #300 Loss: 2.3026365661621093\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.3025858402252197\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.30269193649292\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302633762359619\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3026591777801513\n",
            "Batch #200 Loss: 2.302653350830078\n",
            "Batch #300 Loss: 2.3026426649093628\n",
            "\u001b[92mTrain accuracy: 4815/48000 =  10.03 % ||| loss 2.302584648132324\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1185/12000 =  9.875 % ||| loss 2.30269455909729\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025901317596436\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_15</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_15' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_15</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_121117-Lenet5Decay_1726155046.322212_16</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_16' target=\"_blank\">Lenet5Decay_1726155046.322212_16</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_16' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_16</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3016242456436156\n",
            "Batch #200 Loss: 2.2967000579833985\n",
            "Batch #300 Loss: 2.2823516511917115\n",
            "\u001b[92mTrain accuracy: 22242/48000 =  46.34 % ||| loss 1.5941435098648071\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5555/12000 =  46.29 % ||| loss 1.590908408164978\u001b[0m\n",
            "\u001b[92mTest accuracy: 4643/10000 =  46.43 % ||| loss 1.5970886945724487\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 1.1351032823324203\n",
            "Batch #200 Loss: 0.9229026758670806\n",
            "Batch #300 Loss: 0.8608343029022216\n",
            "\u001b[92mTrain accuracy: 33301/48000 =  69.38 % ||| loss 0.7794541120529175\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8419/12000 =  70.16 % ||| loss 0.7651576399803162\u001b[0m\n",
            "\u001b[92mTest accuracy: 6914/10000 =  69.14 % ||| loss 0.7973002195358276\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.7748227989673615\n",
            "Batch #200 Loss: 0.7374849569797516\n",
            "Batch #300 Loss: 0.717287821173668\n",
            "\u001b[92mTrain accuracy: 35533/48000 =  74.03 % ||| loss 0.6702678799629211\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8910/12000 =  74.25 % ||| loss 0.6608439683914185\u001b[0m\n",
            "\u001b[92mTest accuracy: 7339/10000 =  73.39 % ||| loss 0.6988856792449951\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.6582121896743774\n",
            "Batch #200 Loss: 0.6772409784793854\n",
            "Batch #300 Loss: 0.6445477533340455\n",
            "\u001b[92mTrain accuracy: 36379/48000 =  75.79 % ||| loss 0.6261644959449768\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9145/12000 =  76.21 % ||| loss 0.6142880916595459\u001b[0m\n",
            "\u001b[92mTest accuracy: 7455/10000 =  74.55 % ||| loss 0.6475257277488708\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5996378934383393\n",
            "Batch #200 Loss: 0.5984747168421746\n",
            "Batch #300 Loss: 0.6015636491775512\n",
            "\u001b[92mTrain accuracy: 37956/48000 =  79.07 % ||| loss 0.5590546727180481\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9532/12000 =  79.43 % ||| loss 0.5543911457061768\u001b[0m\n",
            "\u001b[92mTest accuracy: 7823/10000 =  78.23 % ||| loss 0.5753912925720215\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.5794957455992699\n",
            "Batch #200 Loss: 0.5654223039746284\n",
            "Batch #300 Loss: 0.5679900300502777\n",
            "\u001b[92mTrain accuracy: 38092/48000 =  79.36 % ||| loss 0.5521755218505859\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9565/12000 =  79.71 % ||| loss 0.548372745513916\u001b[0m\n",
            "\u001b[92mTest accuracy: 7863/10000 =  78.63 % ||| loss 0.574324369430542\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.5385442227125168\n",
            "Batch #200 Loss: 0.5429313677549362\n",
            "Batch #300 Loss: 0.5284336465597153\n",
            "\u001b[92mTrain accuracy: 38671/48000 =  80.56 % ||| loss 0.5246397852897644\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9677/12000 =  80.64 % ||| loss 0.5239294171333313\u001b[0m\n",
            "\u001b[92mTest accuracy: 7973/10000 =  79.73 % ||| loss 0.544133722782135\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.5268017971515655\n",
            "Batch #200 Loss: 0.5108533388376236\n",
            "Batch #300 Loss: 0.5040998446941376\n",
            "\u001b[92mTrain accuracy: 39239/48000 =  81.75 % ||| loss 0.49593472480773926\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9835/12000 =  81.96 % ||| loss 0.49758124351501465\u001b[0m\n",
            "\u001b[92mTest accuracy: 8096/10000 =  80.96 % ||| loss 0.5163546204566956\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.5007646423578263\n",
            "Batch #200 Loss: 0.4769393652677536\n",
            "Batch #300 Loss: 0.4936525520682335\n",
            "\u001b[92mTrain accuracy: 39511/48000 =  82.31 % ||| loss 0.4850972890853882\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9892/12000 =  82.43 % ||| loss 0.4829266667366028\u001b[0m\n",
            "\u001b[92mTest accuracy: 8132/10000 =  81.32 % ||| loss 0.5079426765441895\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.48316156446933745\n",
            "Batch #200 Loss: 0.48534038454294204\n",
            "Batch #300 Loss: 0.47253059804439546\n",
            "\u001b[92mTrain accuracy: 38831/48000 =  80.9 % ||| loss 0.5080066919326782\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9747/12000 =  81.23 % ||| loss 0.504902184009552\u001b[0m\n",
            "\u001b[92mTest accuracy: 7994/10000 =  79.94 % ||| loss 0.5337204337120056\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.468000080883503\n",
            "Batch #200 Loss: 0.4584712424874306\n",
            "Batch #300 Loss: 0.4616143217682838\n",
            "\u001b[92mTrain accuracy: 40732/48000 =  84.86 % ||| loss 0.44024553894996643\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10207/12000 =  85.06 % ||| loss 0.4412287175655365\u001b[0m\n",
            "\u001b[92mTest accuracy: 8388/10000 =  83.88 % ||| loss 0.4628353714942932\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.4430666324496269\n",
            "Batch #200 Loss: 0.44732130974531176\n",
            "Batch #300 Loss: 0.44887629956007\n",
            "\u001b[92mTrain accuracy: 40195/48000 =  83.74 % ||| loss 0.4493870437145233\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10033/12000 =  83.61 % ||| loss 0.4535733163356781\u001b[0m\n",
            "\u001b[92mTest accuracy: 8257/10000 =  82.57 % ||| loss 0.4752120077610016\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.4553133523464203\n",
            "Batch #200 Loss: 0.4327720245718956\n",
            "Batch #300 Loss: 0.43267427533864977\n",
            "\u001b[92mTrain accuracy: 40781/48000 =  84.96 % ||| loss 0.42578551173210144\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10200/12000 =  85.0 % ||| loss 0.43022966384887695\u001b[0m\n",
            "\u001b[92mTest accuracy: 8405/10000 =  84.05 % ||| loss 0.4500991702079773\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.4283005487918854\n",
            "Batch #200 Loss: 0.4316813471913338\n",
            "Batch #300 Loss: 0.43551558032631876\n",
            "\u001b[92mTrain accuracy: 40662/48000 =  84.71 % ||| loss 0.424888551235199\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10172/12000 =  84.77 % ||| loss 0.42384594678878784\u001b[0m\n",
            "\u001b[92mTest accuracy: 8377/10000 =  83.77 % ||| loss 0.4454728066921234\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.4144673484563828\n",
            "Batch #200 Loss: 0.42742143034935\n",
            "Batch #300 Loss: 0.4353473633527756\n",
            "\u001b[92mTrain accuracy: 41054/48000 =  85.53 % ||| loss 0.4071420431137085\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10238/12000 =  85.32 % ||| loss 0.4110642969608307\u001b[0m\n",
            "\u001b[92mTest accuracy: 8440/10000 =  84.4 % ||| loss 0.43488749861717224\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.4308640533685684\n",
            "Batch #200 Loss: 0.4157408663630486\n",
            "Batch #300 Loss: 0.4092093470692635\n",
            "\u001b[92mTrain accuracy: 40757/48000 =  84.91 % ||| loss 0.41913723945617676\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10157/12000 =  84.64 % ||| loss 0.4222816228866577\u001b[0m\n",
            "\u001b[92mTest accuracy: 8425/10000 =  84.25 % ||| loss 0.44378015398979187\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.41583999127149585\n",
            "Batch #200 Loss: 0.4155723676085472\n",
            "Batch #300 Loss: 0.41420189052820205\n",
            "\u001b[92mTrain accuracy: 41226/48000 =  85.89 % ||| loss 0.3987977206707001\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10259/12000 =  85.49 % ||| loss 0.40619346499443054\u001b[0m\n",
            "\u001b[92mTest accuracy: 8481/10000 =  84.81 % ||| loss 0.42258065938949585\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.41132120192050936\n",
            "Batch #200 Loss: 0.3975235366821289\n",
            "Batch #300 Loss: 0.4109885546565056\n",
            "\u001b[92mTrain accuracy: 41189/48000 =  85.81 % ||| loss 0.4026934802532196\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10253/12000 =  85.44 % ||| loss 0.4081971049308777\u001b[0m\n",
            "\u001b[92mTest accuracy: 8471/10000 =  84.71 % ||| loss 0.43468672037124634\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.40837818682193755\n",
            "Batch #200 Loss: 0.4027784910798073\n",
            "Batch #300 Loss: 0.40629908233880996\n",
            "\u001b[92mTrain accuracy: 41409/48000 =  86.27 % ||| loss 0.38475850224494934\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10320/12000 =  86.0 % ||| loss 0.3899255692958832\u001b[0m\n",
            "\u001b[92mTest accuracy: 8529/10000 =  85.29 % ||| loss 0.4132176637649536\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.39323429808020594\n",
            "Batch #200 Loss: 0.40206110358238223\n",
            "Batch #300 Loss: 0.39925547301769254\n",
            "\u001b[92mTrain accuracy: 40902/48000 =  85.21 % ||| loss 0.4178128242492676\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10226/12000 =  85.22 % ||| loss 0.42082008719444275\u001b[0m\n",
            "\u001b[92mTest accuracy: 8455/10000 =  84.55 % ||| loss 0.4489448666572571\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.3935643316805363\n",
            "Batch #200 Loss: 0.39825639992952344\n",
            "Batch #300 Loss: 0.3882999692857265\n",
            "\u001b[92mTrain accuracy: 41451/48000 =  86.36 % ||| loss 0.38281896710395813\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10299/12000 =  85.82 % ||| loss 0.39208313822746277\u001b[0m\n",
            "\u001b[92mTest accuracy: 8564/10000 =  85.64 % ||| loss 0.40991342067718506\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.4005227291584015\n",
            "Batch #200 Loss: 0.3947138160467148\n",
            "Batch #300 Loss: 0.3848635819554329\n",
            "\u001b[92mTrain accuracy: 41475/48000 =  86.41 % ||| loss 0.38385453820228577\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10319/12000 =  85.99 % ||| loss 0.3941563069820404\u001b[0m\n",
            "\u001b[92mTest accuracy: 8566/10000 =  85.66 % ||| loss 0.41004517674446106\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.3960536181926727\n",
            "Batch #200 Loss: 0.3997970229387283\n",
            "Batch #300 Loss: 0.38399561002850535\n",
            "\u001b[92mTrain accuracy: 41265/48000 =  85.97 % ||| loss 0.38984236121177673\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10266/12000 =  85.55 % ||| loss 0.39407697319984436\u001b[0m\n",
            "\u001b[92mTest accuracy: 8472/10000 =  84.72 % ||| loss 0.42084088921546936\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.3937472632527351\n",
            "Batch #200 Loss: 0.3830969542264938\n",
            "Batch #300 Loss: 0.38329302817583083\n",
            "\u001b[92mTrain accuracy: 41314/48000 =  86.07 % ||| loss 0.38736751675605774\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10309/12000 =  85.91 % ||| loss 0.3947579860687256\u001b[0m\n",
            "\u001b[92mTest accuracy: 8475/10000 =  84.75 % ||| loss 0.41881653666496277\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.38360109135508536\n",
            "Batch #200 Loss: 0.38759676218032835\n",
            "Batch #300 Loss: 0.38224590584635737\n",
            "\u001b[92mTrain accuracy: 41738/48000 =  86.95 % ||| loss 0.3729307949542999\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10416/12000 =  86.8 % ||| loss 0.3804799020290375\u001b[0m\n",
            "\u001b[92mTest accuracy: 8583/10000 =  85.83 % ||| loss 0.40392082929611206\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_16</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_16' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_16</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_121352-Lenet5Decay_1726155046.322212_17</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_17' target=\"_blank\">Lenet5Decay_1726155046.322212_17</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_17' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_17</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.01, 'momentum': 0.7, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302862706184387\n",
            "Batch #200 Loss: 2.3006328773498534\n",
            "Batch #300 Loss: 2.296996352672577\n",
            "\u001b[92mTrain accuracy: 16416/48000 =  34.2 % ||| loss 2.287032127380371\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4095/12000 =  34.12 % ||| loss 2.2869656085968018\u001b[0m\n",
            "\u001b[92mTest accuracy: 3394/10000 =  33.94 % ||| loss 2.2871475219726562\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.2749633955955506\n",
            "Batch #200 Loss: 2.08711568236351\n",
            "Batch #300 Loss: 1.0371975207328796\n",
            "\u001b[92mTrain accuracy: 34629/48000 =  72.14 % ||| loss 0.7555156946182251\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8687/12000 =  72.39 % ||| loss 0.7421075701713562\u001b[0m\n",
            "\u001b[92mTest accuracy: 7173/10000 =  71.73 % ||| loss 0.7673439979553223\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 0.7259320539236068\n",
            "Batch #200 Loss: 0.6704123359918595\n",
            "Batch #300 Loss: 0.6455094355344773\n",
            "\u001b[92mTrain accuracy: 36476/48000 =  75.99 % ||| loss 0.6134776473045349\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9138/12000 =  76.15 % ||| loss 0.6084661483764648\u001b[0m\n",
            "\u001b[92mTest accuracy: 7543/10000 =  75.43 % ||| loss 0.6297435760498047\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.596303995847702\n",
            "Batch #200 Loss: 0.5716791141033173\n",
            "Batch #300 Loss: 0.5606239849328994\n",
            "\u001b[92mTrain accuracy: 38270/48000 =  79.73 % ||| loss 0.5453545451164246\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9601/12000 =  80.01 % ||| loss 0.5376343727111816\u001b[0m\n",
            "\u001b[92mTest accuracy: 7846/10000 =  78.46 % ||| loss 0.5658830404281616\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.5368229460716247\n",
            "Batch #200 Loss: 0.5123393768072129\n",
            "Batch #300 Loss: 0.49284663021564484\n",
            "\u001b[92mTrain accuracy: 39850/48000 =  83.02 % ||| loss 0.47306498885154724\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9994/12000 =  83.28 % ||| loss 0.46857091784477234\u001b[0m\n",
            "\u001b[92mTest accuracy: 8224/10000 =  82.24 % ||| loss 0.4955993890762329\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.4910647851228714\n",
            "Batch #200 Loss: 0.4815806493163109\n",
            "Batch #300 Loss: 0.4554146763682365\n",
            "\u001b[92mTrain accuracy: 40306/48000 =  83.97 % ||| loss 0.4435369074344635\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10093/12000 =  84.11 % ||| loss 0.44338515400886536\u001b[0m\n",
            "\u001b[92mTest accuracy: 8337/10000 =  83.37 % ||| loss 0.4636939465999603\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.4336537156999111\n",
            "Batch #200 Loss: 0.44591678470373153\n",
            "Batch #300 Loss: 0.44174477726221084\n",
            "\u001b[92mTrain accuracy: 40596/48000 =  84.58 % ||| loss 0.4239881634712219\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10098/12000 =  84.15 % ||| loss 0.4283566474914551\u001b[0m\n",
            "\u001b[92mTest accuracy: 8389/10000 =  83.89 % ||| loss 0.44747403264045715\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.4154886332154274\n",
            "Batch #200 Loss: 0.43108019590377805\n",
            "Batch #300 Loss: 0.41398888647556303\n",
            "\u001b[92mTrain accuracy: 40754/48000 =  84.9 % ||| loss 0.41604113578796387\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10173/12000 =  84.78 % ||| loss 0.41893476247787476\u001b[0m\n",
            "\u001b[92mTest accuracy: 8371/10000 =  83.71 % ||| loss 0.44471731781959534\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.39423723340034483\n",
            "Batch #200 Loss: 0.39753551229834555\n",
            "Batch #300 Loss: 0.4011923146247864\n",
            "\u001b[92mTrain accuracy: 40969/48000 =  85.35 % ||| loss 0.39806631207466125\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10228/12000 =  85.23 % ||| loss 0.4008704125881195\u001b[0m\n",
            "\u001b[92mTest accuracy: 8422/10000 =  84.22 % ||| loss 0.43180176615715027\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.3890335899591446\n",
            "Batch #200 Loss: 0.39299766570329664\n",
            "Batch #300 Loss: 0.382271194010973\n",
            "\u001b[92mTrain accuracy: 41857/48000 =  87.2 % ||| loss 0.358650803565979\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10423/12000 =  86.86 % ||| loss 0.3655654788017273\u001b[0m\n",
            "\u001b[92mTest accuracy: 8624/10000 =  86.24 % ||| loss 0.3918607234954834\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.37112123414874076\n",
            "Batch #200 Loss: 0.3745367467403412\n",
            "Batch #300 Loss: 0.3632961294054985\n",
            "\u001b[92mTrain accuracy: 41731/48000 =  86.94 % ||| loss 0.358376145362854\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10411/12000 =  86.76 % ||| loss 0.36890915036201477\u001b[0m\n",
            "\u001b[92mTest accuracy: 8595/10000 =  85.95 % ||| loss 0.38556841015815735\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.36463354632258416\n",
            "Batch #200 Loss: 0.3592648482322693\n",
            "Batch #300 Loss: 0.3628127075731754\n",
            "\u001b[92mTrain accuracy: 41862/48000 =  87.21 % ||| loss 0.3485656678676605\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10426/12000 =  86.88 % ||| loss 0.35751914978027344\u001b[0m\n",
            "\u001b[92mTest accuracy: 8633/10000 =  86.33 % ||| loss 0.38047704100608826\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.35816037997603417\n",
            "Batch #200 Loss: 0.34173722222447395\n",
            "Batch #300 Loss: 0.3425876985490322\n",
            "\u001b[92mTrain accuracy: 42187/48000 =  87.89 % ||| loss 0.3341410160064697\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10458/12000 =  87.15 % ||| loss 0.3517681658267975\u001b[0m\n",
            "\u001b[92mTest accuracy: 8651/10000 =  86.51 % ||| loss 0.3691254258155823\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.34960039854049685\n",
            "Batch #200 Loss: 0.33305386632680894\n",
            "Batch #300 Loss: 0.3404037745296955\n",
            "\u001b[92mTrain accuracy: 42151/48000 =  87.81 % ||| loss 0.33084940910339355\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10486/12000 =  87.38 % ||| loss 0.3442426323890686\u001b[0m\n",
            "\u001b[92mTest accuracy: 8630/10000 =  86.3 % ||| loss 0.36695989966392517\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.33724233642220497\n",
            "Batch #200 Loss: 0.33157519832253457\n",
            "Batch #300 Loss: 0.32039385437965395\n",
            "\u001b[92mTrain accuracy: 42415/48000 =  88.36 % ||| loss 0.31938689947128296\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10547/12000 =  87.89 % ||| loss 0.33859342336654663\u001b[0m\n",
            "\u001b[92mTest accuracy: 8716/10000 =  87.16 % ||| loss 0.3568941354751587\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.3248878374695778\n",
            "Batch #200 Loss: 0.321819027364254\n",
            "Batch #300 Loss: 0.32779181078076364\n",
            "\u001b[92mTrain accuracy: 42797/48000 =  89.16 % ||| loss 0.30025890469551086\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10594/12000 =  88.28 % ||| loss 0.3189258575439453\u001b[0m\n",
            "\u001b[92mTest accuracy: 8760/10000 =  87.6 % ||| loss 0.3424636125564575\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.31861337155103686\n",
            "Batch #200 Loss: 0.3244390732049942\n",
            "Batch #300 Loss: 0.311461276113987\n",
            "\u001b[92mTrain accuracy: 42544/48000 =  88.63 % ||| loss 0.3097976744174957\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10562/12000 =  88.02 % ||| loss 0.32925769686698914\u001b[0m\n",
            "\u001b[92mTest accuracy: 8727/10000 =  87.27 % ||| loss 0.35344117879867554\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.31623221158981324\n",
            "Batch #200 Loss: 0.3068437546491623\n",
            "Batch #300 Loss: 0.30895097136497496\n",
            "\u001b[92mTrain accuracy: 42135/48000 =  87.78 % ||| loss 0.32835090160369873\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10480/12000 =  87.33 % ||| loss 0.34731441736221313\u001b[0m\n",
            "\u001b[92mTest accuracy: 8629/10000 =  86.29 % ||| loss 0.37295278906822205\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.3091834047436714\n",
            "Batch #200 Loss: 0.3187111678719521\n",
            "Batch #300 Loss: 0.29326013177633287\n",
            "\u001b[92mTrain accuracy: 42527/48000 =  88.6 % ||| loss 0.3225004971027374\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10526/12000 =  87.72 % ||| loss 0.3507941663265228\u001b[0m\n",
            "\u001b[92mTest accuracy: 8704/10000 =  87.04 % ||| loss 0.370192289352417\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.3013223585486412\n",
            "Batch #200 Loss: 0.29152049899101257\n",
            "Batch #300 Loss: 0.3012770514190197\n",
            "\u001b[92mTrain accuracy: 42963/48000 =  89.51 % ||| loss 0.28769969940185547\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10610/12000 =  88.42 % ||| loss 0.313385546207428\u001b[0m\n",
            "\u001b[92mTest accuracy: 8785/10000 =  87.85 % ||| loss 0.3320200443267822\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.2957755398750305\n",
            "Batch #200 Loss: 0.2957133640348911\n",
            "Batch #300 Loss: 0.2950668902695179\n",
            "\u001b[92mTrain accuracy: 43022/48000 =  89.63 % ||| loss 0.2868863046169281\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10612/12000 =  88.43 % ||| loss 0.3150026798248291\u001b[0m\n",
            "\u001b[92mTest accuracy: 8811/10000 =  88.11 % ||| loss 0.3369351327419281\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.28713349625468254\n",
            "Batch #200 Loss: 0.28714570730924605\n",
            "Batch #300 Loss: 0.2872887195646763\n",
            "\u001b[92mTrain accuracy: 43234/48000 =  90.07 % ||| loss 0.27827784419059753\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10639/12000 =  88.66 % ||| loss 0.3096746504306793\u001b[0m\n",
            "\u001b[92mTest accuracy: 8821/10000 =  88.21 % ||| loss 0.32463526725769043\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.287620245963335\n",
            "Batch #200 Loss: 0.28370159402489664\n",
            "Batch #300 Loss: 0.28853284671902657\n",
            "\u001b[92mTrain accuracy: 43325/48000 =  90.26 % ||| loss 0.27195248007774353\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10708/12000 =  89.23 % ||| loss 0.3035133481025696\u001b[0m\n",
            "\u001b[92mTest accuracy: 8867/10000 =  88.67 % ||| loss 0.3229430019855499\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.283665159791708\n",
            "Batch #200 Loss: 0.27580206498503684\n",
            "Batch #300 Loss: 0.28514059498906136\n",
            "\u001b[92mTrain accuracy: 43458/48000 =  90.54 % ||| loss 0.26262760162353516\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10708/12000 =  89.23 % ||| loss 0.2955480217933655\u001b[0m\n",
            "\u001b[92mTest accuracy: 8861/10000 =  88.61 % ||| loss 0.31617918610572815\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.27600636556744573\n",
            "Batch #200 Loss: 0.2721376837790012\n",
            "Batch #300 Loss: 0.27299227863550185\n",
            "\u001b[92mTrain accuracy: 43210/48000 =  90.02 % ||| loss 0.2731494605541229\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10660/12000 =  88.83 % ||| loss 0.30758407711982727\u001b[0m\n",
            "\u001b[92mTest accuracy: 8834/10000 =  88.34 % ||| loss 0.33061161637306213\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_17</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_17' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_17</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_121634-Lenet5Decay_1726155046.322212_18</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_18' target=\"_blank\">Lenet5Decay_1726155046.322212_18</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_18' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_18</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3024652910232546\n",
            "Batch #200 Loss: 2.303188045024872\n",
            "Batch #300 Loss: 2.302926504611969\n",
            "\u001b[92mTrain accuracy: 4656/48000 =  9.7 % ||| loss 2.3026204109191895\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1182/12000 =  9.85 % ||| loss 2.302520990371704\u001b[0m\n",
            "\u001b[92mTest accuracy: 963/10000 =  9.63 % ||| loss 2.302523374557495\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.303417778015137\n",
            "Batch #200 Loss: 2.302093288898468\n",
            "Batch #300 Loss: 2.302358787059784\n",
            "\u001b[92mTrain accuracy: 4747/48000 =  9.89 % ||| loss 2.302266836166382\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1201/12000 =  10.01 % ||| loss 2.302185297012329\u001b[0m\n",
            "\u001b[92mTest accuracy: 992/10000 =  9.92 % ||| loss 2.3022639751434326\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3025795340538027\n",
            "Batch #200 Loss: 2.302347366809845\n",
            "Batch #300 Loss: 2.301887969970703\n",
            "\u001b[92mTrain accuracy: 4803/48000 =  10.01 % ||| loss 2.3020431995391846\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1221/12000 =  10.17 % ||| loss 2.301952838897705\u001b[0m\n",
            "\u001b[92mTest accuracy: 1003/10000 =  10.03 % ||| loss 2.302253484725952\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3020104289054872\n",
            "Batch #200 Loss: 2.302014801502228\n",
            "Batch #300 Loss: 2.301589694023132\n",
            "\u001b[92mTrain accuracy: 5327/48000 =  11.1 % ||| loss 2.3019070625305176\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1322/12000 =  11.02 % ||| loss 2.3018345832824707\u001b[0m\n",
            "\u001b[92mTest accuracy: 1087/10000 =  10.87 % ||| loss 2.3020403385162354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.302092478275299\n",
            "Batch #200 Loss: 2.3018314123153685\n",
            "Batch #300 Loss: 2.3015450501441954\n",
            "\u001b[92mTrain accuracy: 7633/48000 =  15.9 % ||| loss 2.3018369674682617\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1853/12000 =  15.44 % ||| loss 2.301788330078125\u001b[0m\n",
            "\u001b[92mTest accuracy: 1582/10000 =  15.82 % ||| loss 2.301743507385254\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3019909143447874\n",
            "Batch #200 Loss: 2.3018772768974305\n",
            "Batch #300 Loss: 2.302152991294861\n",
            "\u001b[92mTrain accuracy: 9061/48000 =  18.88 % ||| loss 2.30183482170105\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2256/12000 =  18.8 % ||| loss 2.3017866611480713\u001b[0m\n",
            "\u001b[92mTest accuracy: 1886/10000 =  18.86 % ||| loss 2.301780939102173\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.3020211839675904\n",
            "Batch #200 Loss: 2.3019333720207213\n",
            "Batch #300 Loss: 2.3016129207611082\n",
            "\u001b[92mTrain accuracy: 8365/48000 =  17.43 % ||| loss 2.3018646240234375\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2072/12000 =  17.27 % ||| loss 2.3018276691436768\u001b[0m\n",
            "\u001b[92mTest accuracy: 1720/10000 =  17.2 % ||| loss 2.301940441131592\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.3020101976394653\n",
            "Batch #200 Loss: 2.3020638608932495\n",
            "Batch #300 Loss: 2.3017717480659483\n",
            "\u001b[92mTrain accuracy: 6455/48000 =  13.45 % ||| loss 2.3019118309020996\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1586/12000 =  13.22 % ||| loss 2.3018832206726074\u001b[0m\n",
            "\u001b[92mTest accuracy: 1301/10000 =  13.01 % ||| loss 2.3019073009490967\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.301776719093323\n",
            "Batch #200 Loss: 2.301971740722656\n",
            "Batch #300 Loss: 2.3018867659568785\n",
            "\u001b[92mTrain accuracy: 5142/48000 =  10.71 % ||| loss 2.3019678592681885\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1253/12000 =  10.44 % ||| loss 2.3019440174102783\u001b[0m\n",
            "\u001b[92mTest accuracy: 1074/10000 =  10.74 % ||| loss 2.301970958709717\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.3021120738983156\n",
            "Batch #200 Loss: 2.3014810991287233\n",
            "Batch #300 Loss: 2.3022224283218384\n",
            "\u001b[92mTrain accuracy: 4833/48000 =  10.07 % ||| loss 2.3020284175872803\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1176/12000 =  9.8 % ||| loss 2.3020241260528564\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.301988124847412\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.301914908885956\n",
            "Batch #200 Loss: 2.301848793029785\n",
            "Batch #300 Loss: 2.302238895893097\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302091121673584\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302086591720581\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302004337310791\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.3019940495491027\n",
            "Batch #200 Loss: 2.302219114303589\n",
            "Batch #300 Loss: 2.3021227884292603\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3021504878997803\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302150011062622\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3021535873413086\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.3021746134757994\n",
            "Batch #200 Loss: 2.302389311790466\n",
            "Batch #300 Loss: 2.3020581507682802\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.30220627784729\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302215337753296\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302245616912842\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.302138156890869\n",
            "Batch #200 Loss: 2.3021570873260497\n",
            "Batch #300 Loss: 2.3023562169075014\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302257537841797\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302272319793701\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3022730350494385\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.3021637558937074\n",
            "Batch #200 Loss: 2.302601172924042\n",
            "Batch #300 Loss: 2.302192602157593\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302305221557617\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302328109741211\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3022878170013428\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.3021196579933165\n",
            "Batch #200 Loss: 2.302327892780304\n",
            "Batch #300 Loss: 2.302327826023102\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3023488521575928\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3023674488067627\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302431583404541\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.302497503757477\n",
            "Batch #200 Loss: 2.3025689625740053\n",
            "Batch #300 Loss: 2.302133674621582\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302385091781616\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3024137020111084\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3023977279663086\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.3024384498596193\n",
            "Batch #200 Loss: 2.3024195623397827\n",
            "Batch #300 Loss: 2.302434120178223\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302417516708374\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.30245041847229\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3023571968078613\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.3025500178337097\n",
            "Batch #200 Loss: 2.3024747371673584\n",
            "Batch #300 Loss: 2.3022699046134947\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302445650100708\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302471876144409\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3024117946624756\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.3024576902389526\n",
            "Batch #200 Loss: 2.302537696361542\n",
            "Batch #300 Loss: 2.3023636531829834\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302469491958618\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3024978637695312\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302455186843872\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.302577404975891\n",
            "Batch #200 Loss: 2.30237863779068\n",
            "Batch #300 Loss: 2.3024432277679443\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3024895191192627\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3025219440460205\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3024959564208984\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.302533826828003\n",
            "Batch #200 Loss: 2.302440512180328\n",
            "Batch #300 Loss: 2.302574620246887\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025059700012207\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302546739578247\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302513837814331\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.3026047372817993\n",
            "Batch #200 Loss: 2.3025617742538453\n",
            "Batch #300 Loss: 2.302507996559143\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302521228790283\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302560567855835\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302504301071167\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.3024540448188784\n",
            "Batch #200 Loss: 2.3024897623062133\n",
            "Batch #300 Loss: 2.3026103568077088\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025319576263428\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3025763034820557\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026044368743896\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.3024264574050903\n",
            "Batch #200 Loss: 2.302610716819763\n",
            "Batch #300 Loss: 2.3025707483291624\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025405406951904\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3025856018066406\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302520275115967\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_18</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_18' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_18</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_121903-Lenet5Decay_1726155046.322212_19</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_19' target=\"_blank\">Lenet5Decay_1726155046.322212_19</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_19' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_19</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3029961824417113\n",
            "Batch #200 Loss: 2.3025041794776917\n",
            "Batch #300 Loss: 2.303466477394104\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3026297092437744\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3028359413146973\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302422523498535\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3028251361846923\n",
            "Batch #200 Loss: 2.3020421719551085\n",
            "Batch #300 Loss: 2.3023418831825255\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3017544746398926\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.301941156387329\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.301588535308838\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3022508931159975\n",
            "Batch #200 Loss: 2.3007256150245667\n",
            "Batch #300 Loss: 2.3013473653793337\n",
            "\u001b[92mTrain accuracy: 4827/48000 =  10.06 % ||| loss 2.3008604049682617\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1177/12000 =  9.808 % ||| loss 2.301034927368164\u001b[0m\n",
            "\u001b[92mTest accuracy: 1001/10000 =  10.01 % ||| loss 2.30067777633667\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.300635280609131\n",
            "Batch #200 Loss: 2.2999411106109617\n",
            "Batch #300 Loss: 2.300595564842224\n",
            "\u001b[92mTrain accuracy: 4853/48000 =  10.11 % ||| loss 2.299964189529419\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1183/12000 =  9.858 % ||| loss 2.3001246452331543\u001b[0m\n",
            "\u001b[92mTest accuracy: 1004/10000 =  10.04 % ||| loss 2.2998814582824707\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.2995823359489442\n",
            "Batch #200 Loss: 2.300249891281128\n",
            "Batch #300 Loss: 2.298693561553955\n",
            "\u001b[92mTrain accuracy: 4904/48000 =  10.22 % ||| loss 2.2990381717681885\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1201/12000 =  10.01 % ||| loss 2.2992048263549805\u001b[0m\n",
            "\u001b[92mTest accuracy: 1021/10000 =  10.21 % ||| loss 2.299021005630493\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.29927375793457\n",
            "Batch #200 Loss: 2.2981460094451904\n",
            "Batch #300 Loss: 2.2987687039375304\n",
            "\u001b[92mTrain accuracy: 5098/48000 =  10.62 % ||| loss 2.298067569732666\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1253/12000 =  10.44 % ||| loss 2.2982280254364014\u001b[0m\n",
            "\u001b[92mTest accuracy: 1070/10000 =  10.7 % ||| loss 2.2982399463653564\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.297904064655304\n",
            "Batch #200 Loss: 2.2978253030776976\n",
            "Batch #300 Loss: 2.297644157409668\n",
            "\u001b[92mTrain accuracy: 5737/48000 =  11.95 % ||| loss 2.297045946121216\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1414/12000 =  11.78 % ||| loss 2.297194242477417\u001b[0m\n",
            "\u001b[92mTest accuracy: 1203/10000 =  12.03 % ||| loss 2.297168731689453\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.297453451156616\n",
            "Batch #200 Loss: 2.296229124069214\n",
            "Batch #300 Loss: 2.2962616872787476\n",
            "\u001b[92mTrain accuracy: 6884/48000 =  14.34 % ||| loss 2.2959303855895996\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1706/12000 =  14.22 % ||| loss 2.296083927154541\u001b[0m\n",
            "\u001b[92mTest accuracy: 1434/10000 =  14.34 % ||| loss 2.295919179916382\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.295383768081665\n",
            "Batch #200 Loss: 2.295495402812958\n",
            "Batch #300 Loss: 2.295067071914673\n",
            "\u001b[92mTrain accuracy: 7897/48000 =  16.45 % ||| loss 2.2946574687957764\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1932/12000 =  16.1 % ||| loss 2.294795274734497\u001b[0m\n",
            "\u001b[92mTest accuracy: 1639/10000 =  16.39 % ||| loss 2.294677734375\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.2947624635696413\n",
            "Batch #200 Loss: 2.2937318468093872\n",
            "Batch #300 Loss: 2.2939163279533386\n",
            "\u001b[92mTrain accuracy: 8477/48000 =  17.66 % ||| loss 2.2931361198425293\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2079/12000 =  17.32 % ||| loss 2.293299674987793\u001b[0m\n",
            "\u001b[92mTest accuracy: 1774/10000 =  17.74 % ||| loss 2.293379545211792\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.2926744413375855\n",
            "Batch #200 Loss: 2.2929536485671997\n",
            "Batch #300 Loss: 2.29193701505661\n",
            "\u001b[92mTrain accuracy: 8752/48000 =  18.23 % ||| loss 2.291226387023926\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2168/12000 =  18.07 % ||| loss 2.2913410663604736\u001b[0m\n",
            "\u001b[92mTest accuracy: 1815/10000 =  18.15 % ||| loss 2.291253089904785\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.290600640773773\n",
            "Batch #200 Loss: 2.2905709242820738\n",
            "Batch #300 Loss: 2.289925289154053\n",
            "\u001b[92mTrain accuracy: 8711/48000 =  18.15 % ||| loss 2.2888083457946777\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2174/12000 =  18.12 % ||| loss 2.2889482975006104\u001b[0m\n",
            "\u001b[92mTest accuracy: 1809/10000 =  18.09 % ||| loss 2.288818597793579\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.2888517999649047\n",
            "Batch #200 Loss: 2.287561841011047\n",
            "Batch #300 Loss: 2.2867495226860046\n",
            "\u001b[92mTrain accuracy: 8535/48000 =  17.78 % ||| loss 2.2857959270477295\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2138/12000 =  17.82 % ||| loss 2.285905361175537\u001b[0m\n",
            "\u001b[92mTest accuracy: 1781/10000 =  17.81 % ||| loss 2.285876989364624\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.285348196029663\n",
            "Batch #200 Loss: 2.284444465637207\n",
            "Batch #300 Loss: 2.283615131378174\n",
            "\u001b[92mTrain accuracy: 8334/48000 =  17.36 % ||| loss 2.282033681869507\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2100/12000 =  17.5 % ||| loss 2.28214168548584\u001b[0m\n",
            "\u001b[92mTest accuracy: 1726/10000 =  17.26 % ||| loss 2.2820169925689697\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.2815636134147645\n",
            "Batch #200 Loss: 2.2802985954284667\n",
            "Batch #300 Loss: 2.2789646649360655\n",
            "\u001b[92mTrain accuracy: 8427/48000 =  17.56 % ||| loss 2.2771248817443848\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2109/12000 =  17.57 % ||| loss 2.2772068977355957\u001b[0m\n",
            "\u001b[92mTest accuracy: 1741/10000 =  17.41 % ||| loss 2.2773330211639404\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.276429998874664\n",
            "Batch #200 Loss: 2.274738166332245\n",
            "Batch #300 Loss: 2.2729268980026247\n",
            "\u001b[92mTrain accuracy: 11865/48000 =  24.72 % ||| loss 2.27040433883667\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2952/12000 =  24.6 % ||| loss 2.270465135574341\u001b[0m\n",
            "\u001b[92mTest accuracy: 2460/10000 =  24.6 % ||| loss 2.2703728675842285\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.2692719316482544\n",
            "Batch #200 Loss: 2.267146689891815\n",
            "Batch #300 Loss: 2.2639519357681275\n",
            "\u001b[92mTrain accuracy: 15437/48000 =  32.16 % ||| loss 2.260781764984131\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3831/12000 =  31.92 % ||| loss 2.2607946395874023\u001b[0m\n",
            "\u001b[92mTest accuracy: 3193/10000 =  31.93 % ||| loss 2.260833501815796\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.2592332983016967\n",
            "Batch #200 Loss: 2.255601978302002\n",
            "Batch #300 Loss: 2.2519452786445617\n",
            "\u001b[92mTrain accuracy: 15798/48000 =  32.91 % ||| loss 2.2460949420928955\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3916/12000 =  32.63 % ||| loss 2.246091365814209\u001b[0m\n",
            "\u001b[92mTest accuracy: 3283/10000 =  32.83 % ||| loss 2.2461907863616943\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.2432187962532044\n",
            "Batch #200 Loss: 2.238581664562225\n",
            "Batch #300 Loss: 2.231255419254303\n",
            "\u001b[92mTrain accuracy: 14917/48000 =  31.08 % ||| loss 2.2208707332611084\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3701/12000 =  30.84 % ||| loss 2.220761775970459\u001b[0m\n",
            "\u001b[92mTest accuracy: 3070/10000 =  30.7 % ||| loss 2.221561908721924\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.215283899307251\n",
            "Batch #200 Loss: 2.2064978742599486\n",
            "Batch #300 Loss: 2.1935739374160765\n",
            "\u001b[92mTrain accuracy: 13861/48000 =  28.88 % ||| loss 2.1731045246124268\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3439/12000 =  28.66 % ||| loss 2.1728293895721436\u001b[0m\n",
            "\u001b[92mTest accuracy: 2867/10000 =  28.67 % ||| loss 2.1735875606536865\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.1652790474891663\n",
            "Batch #200 Loss: 2.1399200224876402\n",
            "Batch #300 Loss: 2.114281816482544\n",
            "\u001b[92mTrain accuracy: 13780/48000 =  28.71 % ||| loss 2.0705697536468506\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3443/12000 =  28.69 % ||| loss 2.0697431564331055\u001b[0m\n",
            "\u001b[92mTest accuracy: 2842/10000 =  28.42 % ||| loss 2.070870876312256\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.0478560483455657\n",
            "Batch #200 Loss: 1.997859239578247\n",
            "Batch #300 Loss: 1.9458095026016236\n",
            "\u001b[92mTrain accuracy: 18773/48000 =  39.11 % ||| loss 1.8495676517486572\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4735/12000 =  39.46 % ||| loss 1.8474359512329102\u001b[0m\n",
            "\u001b[92mTest accuracy: 3892/10000 =  38.92 % ||| loss 1.8508219718933105\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 1.808475865125656\n",
            "Batch #200 Loss: 1.7271402084827423\n",
            "Batch #300 Loss: 1.6269494414329528\n",
            "\u001b[92mTrain accuracy: 26075/48000 =  54.32 % ||| loss 1.5188050270080566\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6591/12000 =  54.93 % ||| loss 1.515193223953247\u001b[0m\n",
            "\u001b[92mTest accuracy: 5432/10000 =  54.32 % ||| loss 1.519263505935669\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 1.4791237318515777\n",
            "Batch #200 Loss: 1.4044935047626494\n",
            "Batch #300 Loss: 1.331253091096878\n",
            "\u001b[92mTrain accuracy: 27987/48000 =  58.31 % ||| loss 1.263875126838684\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7059/12000 =  58.83 % ||| loss 1.25826096534729\u001b[0m\n",
            "\u001b[92mTest accuracy: 5815/10000 =  58.15 % ||| loss 1.2668377161026\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 1.236069566011429\n",
            "Batch #200 Loss: 1.2023408043384551\n",
            "Batch #300 Loss: 1.1578727817535401\n",
            "\u001b[92mTrain accuracy: 28420/48000 =  59.21 % ||| loss 1.1194931268692017\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7084/12000 =  59.03 % ||| loss 1.1128040552139282\u001b[0m\n",
            "\u001b[92mTest accuracy: 5891/10000 =  58.91 % ||| loss 1.1278213262557983\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_19</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_19' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_19</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_122130-Lenet5Decay_1726155046.322212_20</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_20' target=\"_blank\">Lenet5Decay_1726155046.322212_20</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_20' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_20</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.302865755558014\n",
            "Batch #200 Loss: 2.3034583210945128\n",
            "Batch #300 Loss: 2.3018719077110292\n",
            "\u001b[92mTrain accuracy: 4806/48000 =  10.01 % ||| loss 2.3017349243164062\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1176/12000 =  9.8 % ||| loss 2.302190065383911\u001b[0m\n",
            "\u001b[92mTest accuracy: 991/10000 =  9.91 % ||| loss 2.301625967025757\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.301905460357666\n",
            "Batch #200 Loss: 2.301233506202698\n",
            "Batch #300 Loss: 2.300721266269684\n",
            "\u001b[92mTrain accuracy: 4894/48000 =  10.2 % ||| loss 2.300609588623047\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1195/12000 =  9.958 % ||| loss 2.3010189533233643\u001b[0m\n",
            "\u001b[92mTest accuracy: 1009/10000 =  10.09 % ||| loss 2.300565242767334\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3006251096725463\n",
            "Batch #200 Loss: 2.3004568552970888\n",
            "Batch #300 Loss: 2.299539108276367\n",
            "\u001b[92mTrain accuracy: 5944/48000 =  12.38 % ||| loss 2.2994093894958496\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1455/12000 =  12.12 % ||| loss 2.299773931503296\u001b[0m\n",
            "\u001b[92mTest accuracy: 1223/10000 =  12.23 % ||| loss 2.2995002269744873\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.299185087680817\n",
            "Batch #200 Loss: 2.2985384011268617\n",
            "Batch #300 Loss: 2.299135172367096\n",
            "\u001b[92mTrain accuracy: 7173/48000 =  14.94 % ||| loss 2.2980711460113525\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1772/12000 =  14.77 % ||| loss 2.2983899116516113\u001b[0m\n",
            "\u001b[92mTest accuracy: 1459/10000 =  14.59 % ||| loss 2.2979743480682373\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.297894036769867\n",
            "Batch #200 Loss: 2.2972988414764406\n",
            "Batch #300 Loss: 2.297580542564392\n",
            "\u001b[92mTrain accuracy: 7725/48000 =  16.09 % ||| loss 2.296553134918213\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1911/12000 =  15.93 % ||| loss 2.2968711853027344\u001b[0m\n",
            "\u001b[92mTest accuracy: 1584/10000 =  15.84 % ||| loss 2.296678066253662\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.2966163563728332\n",
            "Batch #200 Loss: 2.2960677123069764\n",
            "Batch #300 Loss: 2.2951178050041197\n",
            "\u001b[92mTrain accuracy: 8003/48000 =  16.67 % ||| loss 2.2947278022766113\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1967/12000 =  16.39 % ||| loss 2.2950217723846436\u001b[0m\n",
            "\u001b[92mTest accuracy: 1635/10000 =  16.35 % ||| loss 2.294628858566284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.2944260811805726\n",
            "Batch #200 Loss: 2.294362986087799\n",
            "Batch #300 Loss: 2.2932953071594238\n",
            "\u001b[92mTrain accuracy: 8075/48000 =  16.82 % ||| loss 2.2925772666931152\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1992/12000 =  16.6 % ||| loss 2.292855978012085\u001b[0m\n",
            "\u001b[92mTest accuracy: 1654/10000 =  16.54 % ||| loss 2.2928104400634766\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.2926875734329224\n",
            "Batch #200 Loss: 2.290915701389313\n",
            "Batch #300 Loss: 2.291034071445465\n",
            "\u001b[92mTrain accuracy: 8078/48000 =  16.83 % ||| loss 2.2900068759918213\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1993/12000 =  16.61 % ||| loss 2.2902565002441406\u001b[0m\n",
            "\u001b[92mTest accuracy: 1675/10000 =  16.75 % ||| loss 2.290205478668213\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.2897071266174316\n",
            "Batch #200 Loss: 2.288964853286743\n",
            "Batch #300 Loss: 2.287705240249634\n",
            "\u001b[92mTrain accuracy: 8068/48000 =  16.81 % ||| loss 2.286806583404541\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1987/12000 =  16.56 % ||| loss 2.287064790725708\u001b[0m\n",
            "\u001b[92mTest accuracy: 1672/10000 =  16.72 % ||| loss 2.287060499191284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.286228280067444\n",
            "Batch #200 Loss: 2.2854471898078916\n",
            "Batch #300 Loss: 2.284076380729675\n",
            "\u001b[92mTrain accuracy: 8027/48000 =  16.72 % ||| loss 2.282646656036377\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1985/12000 =  16.54 % ||| loss 2.2828855514526367\u001b[0m\n",
            "\u001b[92mTest accuracy: 1662/10000 =  16.62 % ||| loss 2.282609462738037\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.28187762260437\n",
            "Batch #200 Loss: 2.280519700050354\n",
            "Batch #300 Loss: 2.279208562374115\n",
            "\u001b[92mTrain accuracy: 7976/48000 =  16.62 % ||| loss 2.2768654823303223\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1964/12000 =  16.37 % ||| loss 2.277092933654785\u001b[0m\n",
            "\u001b[92mTest accuracy: 1651/10000 =  16.51 % ||| loss 2.2768898010253906\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.2753745436668398\n",
            "Batch #200 Loss: 2.2737049579620363\n",
            "Batch #300 Loss: 2.271575083732605\n",
            "\u001b[92mTrain accuracy: 11305/48000 =  23.55 % ||| loss 2.2678706645965576\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2790/12000 =  23.25 % ||| loss 2.2680397033691406\u001b[0m\n",
            "\u001b[92mTest accuracy: 2338/10000 =  23.38 % ||| loss 2.2679741382598877\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.266488261222839\n",
            "Batch #200 Loss: 2.262495856285095\n",
            "Batch #300 Loss: 2.2594783091545105\n",
            "\u001b[92mTrain accuracy: 13410/48000 =  27.94 % ||| loss 2.2535126209259033\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3357/12000 =  27.98 % ||| loss 2.2536463737487793\u001b[0m\n",
            "\u001b[92mTest accuracy: 2787/10000 =  27.87 % ||| loss 2.2536966800689697\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.251266047954559\n",
            "Batch #200 Loss: 2.2445611119270326\n",
            "Batch #300 Loss: 2.2388932418823244\n",
            "\u001b[92mTrain accuracy: 16062/48000 =  33.46 % ||| loss 2.2294273376464844\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3955/12000 =  32.96 % ||| loss 2.229560613632202\u001b[0m\n",
            "\u001b[92mTest accuracy: 3398/10000 =  33.98 % ||| loss 2.229623556137085\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.2244885110855104\n",
            "Batch #200 Loss: 2.2147682428359987\n",
            "Batch #300 Loss: 2.2027844071388243\n",
            "\u001b[92mTrain accuracy: 16126/48000 =  33.6 % ||| loss 2.1832010746002197\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4019/12000 =  33.49 % ||| loss 2.183297872543335\u001b[0m\n",
            "\u001b[92mTest accuracy: 3370/10000 =  33.7 % ||| loss 2.1832926273345947\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.173744885921478\n",
            "Batch #200 Loss: 2.153050754070282\n",
            "Batch #300 Loss: 2.1257308554649352\n",
            "\u001b[92mTrain accuracy: 17049/48000 =  35.52 % ||| loss 2.080653429031372\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4257/12000 =  35.48 % ||| loss 2.0807011127471924\u001b[0m\n",
            "\u001b[92mTest accuracy: 3522/10000 =  35.22 % ||| loss 2.080322027206421\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.0587687706947326\n",
            "Batch #200 Loss: 2.0055270266532896\n",
            "Batch #300 Loss: 1.940089659690857\n",
            "\u001b[92mTrain accuracy: 23621/48000 =  49.21 % ||| loss 1.837733268737793\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5880/12000 =  49.0 % ||| loss 1.8376513719558716\u001b[0m\n",
            "\u001b[92mTest accuracy: 4880/10000 =  48.8 % ||| loss 1.839396357536316\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 1.791949051618576\n",
            "Batch #200 Loss: 1.6974656581878662\n",
            "Batch #300 Loss: 1.592752742767334\n",
            "\u001b[92mTrain accuracy: 28442/48000 =  59.25 % ||| loss 1.4734373092651367\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7046/12000 =  58.72 % ||| loss 1.472590684890747\u001b[0m\n",
            "\u001b[92mTest accuracy: 5894/10000 =  58.94 % ||| loss 1.4782254695892334\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 1.4295677065849304\n",
            "Batch #200 Loss: 1.337799974679947\n",
            "Batch #300 Loss: 1.2717799901962281\n",
            "\u001b[92mTrain accuracy: 29266/48000 =  60.97 % ||| loss 1.1909757852554321\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7273/12000 =  60.61 % ||| loss 1.1880179643630981\u001b[0m\n",
            "\u001b[92mTest accuracy: 6063/10000 =  60.63 % ||| loss 1.194445013999939\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 1.1652711474895476\n",
            "Batch #200 Loss: 1.1183294290304184\n",
            "Batch #300 Loss: 1.0748983299732209\n",
            "\u001b[92mTrain accuracy: 30355/48000 =  63.24 % ||| loss 1.0410404205322266\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7548/12000 =  62.9 % ||| loss 1.0349664688110352\u001b[0m\n",
            "\u001b[92mTest accuracy: 6243/10000 =  62.43 % ||| loss 1.0449867248535156\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 1.030857053399086\n",
            "Batch #200 Loss: 0.995983999967575\n",
            "Batch #300 Loss: 0.9900346249341965\n",
            "\u001b[92mTrain accuracy: 30832/48000 =  64.23 % ||| loss 0.9653249382972717\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7685/12000 =  64.04 % ||| loss 0.9573270082473755\u001b[0m\n",
            "\u001b[92mTest accuracy: 6324/10000 =  63.24 % ||| loss 0.9730933308601379\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.962783454656601\n",
            "Batch #200 Loss: 0.935456039905548\n",
            "Batch #300 Loss: 0.9501442843675614\n",
            "\u001b[92mTrain accuracy: 31470/48000 =  65.56 % ||| loss 0.9214016199111938\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7851/12000 =  65.42 % ||| loss 0.9127807021141052\u001b[0m\n",
            "\u001b[92mTest accuracy: 6438/10000 =  64.38 % ||| loss 0.9332784414291382\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.912533443570137\n",
            "Batch #200 Loss: 0.9054868096113204\n",
            "Batch #300 Loss: 0.915080401301384\n",
            "\u001b[92mTrain accuracy: 32054/48000 =  66.78 % ||| loss 0.893259584903717\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8017/12000 =  66.81 % ||| loss 0.883495569229126\u001b[0m\n",
            "\u001b[92mTest accuracy: 6607/10000 =  66.07 % ||| loss 0.9012933969497681\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.879653480052948\n",
            "Batch #200 Loss: 0.8893081510066986\n",
            "Batch #300 Loss: 0.8766010236740113\n",
            "\u001b[92mTrain accuracy: 32281/48000 =  67.25 % ||| loss 0.8708462715148926\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8099/12000 =  67.49 % ||| loss 0.8618824481964111\u001b[0m\n",
            "\u001b[92mTest accuracy: 6639/10000 =  66.39 % ||| loss 0.8898555636405945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.8723061507940293\n",
            "Batch #200 Loss: 0.8635071921348572\n",
            "Batch #300 Loss: 0.865807574391365\n",
            "\u001b[92mTrain accuracy: 32730/48000 =  68.19 % ||| loss 0.8563718795776367\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8163/12000 =  68.03 % ||| loss 0.8471535444259644\u001b[0m\n",
            "\u001b[92mTest accuracy: 6715/10000 =  67.15 % ||| loss 0.8727535009384155\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_20</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_20' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_20</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_122357-Lenet5Decay_1726155046.322212_21</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_21' target=\"_blank\">Lenet5Decay_1726155046.322212_21</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_21' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_21</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.305482385158539\n",
            "Batch #200 Loss: 2.3028934955596925\n",
            "Batch #300 Loss: 2.301846749782562\n",
            "\u001b[92mTrain accuracy: 5146/48000 =  10.72 % ||| loss 2.3022937774658203\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1296/12000 =  10.8 % ||| loss 2.301780939102173\u001b[0m\n",
            "\u001b[92mTest accuracy: 1073/10000 =  10.73 % ||| loss 2.302320957183838\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.302375113964081\n",
            "Batch #200 Loss: 2.302244563102722\n",
            "Batch #300 Loss: 2.3025030946731566\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.302476167678833\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3023040294647217\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3024609088897705\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3024091935157776\n",
            "Batch #200 Loss: 2.3026074242591856\n",
            "Batch #300 Loss: 2.302638363838196\n",
            "\u001b[92mTrain accuracy: 4800/48000 =  10.0 % ||| loss 2.302570104598999\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1200/12000 =  10.0 % ||| loss 2.3025293350219727\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025760650634766\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3025993728637695\n",
            "Batch #200 Loss: 2.30260085105896\n",
            "Batch #300 Loss: 2.3026203513145447\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025808334350586\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.30258846282959\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302583932876587\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.302613368034363\n",
            "Batch #200 Loss: 2.302619800567627\n",
            "Batch #300 Loss: 2.3025853729248045\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025782108306885\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026211261749268\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302579402923584\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3025912833213806\n",
            "Batch #200 Loss: 2.3026150965690615\n",
            "Batch #300 Loss: 2.3025995302200317\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302643299102783\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025786876678467\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.302591714859009\n",
            "Batch #200 Loss: 2.3026195192337036\n",
            "Batch #300 Loss: 2.302591495513916\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025763034820557\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026363849639893\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302593469619751\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.302583520412445\n",
            "Batch #200 Loss: 2.302618727684021\n",
            "Batch #300 Loss: 2.30261709690094\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025765419006348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302643060684204\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302583694458008\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.302646017074585\n",
            "Batch #200 Loss: 2.302605564594269\n",
            "Batch #300 Loss: 2.3026070857048033\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302659511566162\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302579879760742\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.30254674911499\n",
            "Batch #200 Loss: 2.30263667345047\n",
            "Batch #300 Loss: 2.3025946521759035\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302647113800049\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302579402923584\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.302580940723419\n",
            "Batch #200 Loss: 2.302596890926361\n",
            "Batch #300 Loss: 2.302631320953369\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302574396133423\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302644968032837\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025808334350586\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.3026091384887697\n",
            "Batch #200 Loss: 2.302602529525757\n",
            "Batch #300 Loss: 2.3025946497917174\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025755882263184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026440143585205\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025858402252197\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.302574830055237\n",
            "Batch #200 Loss: 2.302604389190674\n",
            "Batch #300 Loss: 2.3026109623908995\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302635908126831\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025906085968018\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.3025700139999388\n",
            "Batch #200 Loss: 2.302638635635376\n",
            "Batch #300 Loss: 2.3026206994056704\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025755882263184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302640676498413\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302586317062378\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.302603406906128\n",
            "Batch #200 Loss: 2.302557089328766\n",
            "Batch #300 Loss: 2.302607569694519\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025760650634766\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302640676498413\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025927543640137\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.302547676563263\n",
            "Batch #200 Loss: 2.302627625465393\n",
            "Batch #300 Loss: 2.302640664577484\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025765419006348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026340007781982\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025875091552734\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.3026168012619017\n",
            "Batch #200 Loss: 2.3025691080093384\n",
            "Batch #300 Loss: 2.302622449398041\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025763034820557\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.30263614654541\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302581787109375\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.302595043182373\n",
            "Batch #200 Loss: 2.302560248374939\n",
            "Batch #300 Loss: 2.30260826587677\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025753498077393\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302640199661255\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302583694458008\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.302557134628296\n",
            "Batch #200 Loss: 2.3025976276397704\n",
            "Batch #300 Loss: 2.3026295042037965\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025765419006348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302635908126831\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025853633880615\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.3026055240631105\n",
            "Batch #200 Loss: 2.302603552341461\n",
            "Batch #300 Loss: 2.3025952315330507\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025765419006348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026411533355713\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302598237991333\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.302582156658173\n",
            "Batch #200 Loss: 2.3025588822364806\n",
            "Batch #300 Loss: 2.3026552200317383\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025758266448975\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302645683288574\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026018142700195\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.302598476409912\n",
            "Batch #200 Loss: 2.3025907111167907\n",
            "Batch #300 Loss: 2.302579939365387\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025739192962646\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.302645206451416\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025906085968018\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.3026115250587464\n",
            "Batch #200 Loss: 2.302560319900513\n",
            "Batch #300 Loss: 2.302601108551025\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026411533355713\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025965690612793\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.302577657699585\n",
            "Batch #200 Loss: 2.3025720572471617\n",
            "Batch #300 Loss: 2.3026274847984314\n",
            "\u001b[92mTrain accuracy: 4820/48000 =  10.04 % ||| loss 2.3025765419006348\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1180/12000 =  9.833 % ||| loss 2.3026363849639893\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302584409713745\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.302593650817871\n",
            "Batch #200 Loss: 2.302599720954895\n",
            "Batch #300 Loss: 2.3025844526290893\n",
            "\u001b[92mTrain accuracy: 4825/48000 =  10.05 % ||| loss 2.3025763034820557\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1175/12000 =  9.792 % ||| loss 2.3026347160339355\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302583932876587\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_21</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_21' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_21</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_122631-Lenet5Decay_1726155046.322212_22</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_22' target=\"_blank\">Lenet5Decay_1726155046.322212_22</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_22' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_22</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.306221170425415\n",
            "Batch #200 Loss: 2.3040104150772094\n",
            "Batch #300 Loss: 2.303421051502228\n",
            "\u001b[92mTrain accuracy: 4894/48000 =  10.2 % ||| loss 2.3026745319366455\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1196/12000 =  9.967 % ||| loss 2.302560567855835\u001b[0m\n",
            "\u001b[92mTest accuracy: 1013/10000 =  10.13 % ||| loss 2.3025641441345215\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3026993656158448\n",
            "Batch #200 Loss: 2.3019285798072815\n",
            "Batch #300 Loss: 2.3009299516677855\n",
            "\u001b[92mTrain accuracy: 5443/48000 =  11.34 % ||| loss 2.300642490386963\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1318/12000 =  10.98 % ||| loss 2.300652265548706\u001b[0m\n",
            "\u001b[92mTest accuracy: 1147/10000 =  11.47 % ||| loss 2.300685167312622\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.3005305886268617\n",
            "Batch #200 Loss: 2.299786479473114\n",
            "Batch #300 Loss: 2.299233899116516\n",
            "\u001b[92mTrain accuracy: 9644/48000 =  20.09 % ||| loss 2.298464059829712\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2402/12000 =  20.02 % ||| loss 2.2984960079193115\u001b[0m\n",
            "\u001b[92mTest accuracy: 2007/10000 =  20.07 % ||| loss 2.29843807220459\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.298214282989502\n",
            "Batch #200 Loss: 2.2972562599182127\n",
            "Batch #300 Loss: 2.2960411715507507\n",
            "\u001b[92mTrain accuracy: 15295/48000 =  31.86 % ||| loss 2.294191360473633\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3802/12000 =  31.68 % ||| loss 2.294236898422241\u001b[0m\n",
            "\u001b[92mTest accuracy: 3134/10000 =  31.34 % ||| loss 2.29425311088562\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.2934585571289063\n",
            "Batch #200 Loss: 2.290681002140045\n",
            "Batch #300 Loss: 2.2870243978500366\n",
            "\u001b[92mTrain accuracy: 17809/48000 =  37.1 % ||| loss 2.2784690856933594\u001b[0m\n",
            "\u001b[92mValidation accuracy: 4468/12000 =  37.23 % ||| loss 2.2784085273742676\u001b[0m\n",
            "\u001b[92mTest accuracy: 3674/10000 =  36.74 % ||| loss 2.27864408493042\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.272437424659729\n",
            "Batch #200 Loss: 2.2506044030189516\n",
            "Batch #300 Loss: 2.180935275554657\n",
            "\u001b[92mTrain accuracy: 23699/48000 =  49.37 % ||| loss 1.563961148262024\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5915/12000 =  49.29 % ||| loss 1.5600388050079346\u001b[0m\n",
            "\u001b[92mTest accuracy: 4930/10000 =  49.3 % ||| loss 1.5655386447906494\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 1.2526131182909013\n",
            "Batch #200 Loss: 1.0081688779592515\n",
            "Batch #300 Loss: 0.9372280776500702\n",
            "\u001b[92mTrain accuracy: 32106/48000 =  66.89 % ||| loss 0.8660380244255066\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8061/12000 =  67.17 % ||| loss 0.8572865724563599\u001b[0m\n",
            "\u001b[92mTest accuracy: 6608/10000 =  66.08 % ||| loss 0.8845897912979126\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.8482510060071945\n",
            "Batch #200 Loss: 0.8349603688716889\n",
            "Batch #300 Loss: 0.8046292340755463\n",
            "\u001b[92mTrain accuracy: 33551/48000 =  69.9 % ||| loss 0.7791365385055542\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8422/12000 =  70.18 % ||| loss 0.7702018022537231\u001b[0m\n",
            "\u001b[92mTest accuracy: 6916/10000 =  69.16 % ||| loss 0.7977509498596191\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.7724716013669968\n",
            "Batch #200 Loss: 0.7612330585718154\n",
            "Batch #300 Loss: 0.7428934556245804\n",
            "\u001b[92mTrain accuracy: 34889/48000 =  72.69 % ||| loss 0.7197573184967041\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8819/12000 =  73.49 % ||| loss 0.7098013758659363\u001b[0m\n",
            "\u001b[92mTest accuracy: 7249/10000 =  72.49 % ||| loss 0.7437922358512878\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.7177977126836776\n",
            "Batch #200 Loss: 0.7316564053297043\n",
            "Batch #300 Loss: 0.732239099740982\n",
            "\u001b[92mTrain accuracy: 35885/48000 =  74.76 % ||| loss 0.6808896064758301\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9032/12000 =  75.27 % ||| loss 0.6716020703315735\u001b[0m\n",
            "\u001b[92mTest accuracy: 7466/10000 =  74.66 % ||| loss 0.7042054533958435\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.6952038812637329\n",
            "Batch #200 Loss: 0.6928922539949417\n",
            "Batch #300 Loss: 0.6834347635507584\n",
            "\u001b[92mTrain accuracy: 36026/48000 =  75.05 % ||| loss 0.6608489751815796\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9034/12000 =  75.28 % ||| loss 0.6542431712150574\u001b[0m\n",
            "\u001b[92mTest accuracy: 7425/10000 =  74.25 % ||| loss 0.6820774078369141\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.669717269539833\n",
            "Batch #200 Loss: 0.662381854057312\n",
            "Batch #300 Loss: 0.6647660827636719\n",
            "\u001b[92mTrain accuracy: 36557/48000 =  76.16 % ||| loss 0.6339983940124512\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9132/12000 =  76.1 % ||| loss 0.6284427642822266\u001b[0m\n",
            "\u001b[92mTest accuracy: 7537/10000 =  75.37 % ||| loss 0.6608931422233582\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.6573797246813774\n",
            "Batch #200 Loss: 0.6390088120102883\n",
            "Batch #300 Loss: 0.6366501998901367\n",
            "\u001b[92mTrain accuracy: 36856/48000 =  76.78 % ||| loss 0.6207188963890076\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9233/12000 =  76.94 % ||| loss 0.6161373853683472\u001b[0m\n",
            "\u001b[92mTest accuracy: 7581/10000 =  75.81 % ||| loss 0.647754430770874\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.6413255962729454\n",
            "Batch #200 Loss: 0.6189195656776428\n",
            "Batch #300 Loss: 0.6198764118552208\n",
            "\u001b[92mTrain accuracy: 37150/48000 =  77.4 % ||| loss 0.6100317239761353\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9310/12000 =  77.58 % ||| loss 0.6066802144050598\u001b[0m\n",
            "\u001b[92mTest accuracy: 7611/10000 =  76.11 % ||| loss 0.634754478931427\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.6148791980743408\n",
            "Batch #200 Loss: 0.6147057276964187\n",
            "Batch #300 Loss: 0.5999935138225555\n",
            "\u001b[92mTrain accuracy: 36802/48000 =  76.67 % ||| loss 0.6285412311553955\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9227/12000 =  76.89 % ||| loss 0.6251345276832581\u001b[0m\n",
            "\u001b[92mTest accuracy: 7590/10000 =  75.9 % ||| loss 0.6541242003440857\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.5963067165017129\n",
            "Batch #200 Loss: 0.5988021031022072\n",
            "Batch #300 Loss: 0.5902697998285293\n",
            "\u001b[92mTrain accuracy: 37583/48000 =  78.3 % ||| loss 0.589279055595398\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9439/12000 =  78.66 % ||| loss 0.5820372104644775\u001b[0m\n",
            "\u001b[92mTest accuracy: 7705/10000 =  77.05 % ||| loss 0.6114925146102905\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.5873524349927902\n",
            "Batch #200 Loss: 0.5899961900711059\n",
            "Batch #300 Loss: 0.5692123591899871\n",
            "\u001b[92mTrain accuracy: 38043/48000 =  79.26 % ||| loss 0.5692957043647766\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9556/12000 =  79.63 % ||| loss 0.5651142597198486\u001b[0m\n",
            "\u001b[92mTest accuracy: 7821/10000 =  78.21 % ||| loss 0.5922471284866333\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.5722766548395157\n",
            "Batch #200 Loss: 0.5618785670399666\n",
            "Batch #300 Loss: 0.5771796897053718\n",
            "\u001b[92mTrain accuracy: 37807/48000 =  78.76 % ||| loss 0.5689929723739624\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9409/12000 =  78.41 % ||| loss 0.5648715496063232\u001b[0m\n",
            "\u001b[92mTest accuracy: 7775/10000 =  77.75 % ||| loss 0.5940311551094055\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.5625666043162346\n",
            "Batch #200 Loss: 0.5736837270855903\n",
            "Batch #300 Loss: 0.5344565552473068\n",
            "\u001b[92mTrain accuracy: 37287/48000 =  77.68 % ||| loss 0.5696258544921875\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9310/12000 =  77.58 % ||| loss 0.5669004917144775\u001b[0m\n",
            "\u001b[92mTest accuracy: 7679/10000 =  76.79 % ||| loss 0.594986617565155\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.5502144989371299\n",
            "Batch #200 Loss: 0.5469029608368874\n",
            "Batch #300 Loss: 0.5463984978199005\n",
            "\u001b[92mTrain accuracy: 38775/48000 =  80.78 % ||| loss 0.5312002301216125\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9749/12000 =  81.24 % ||| loss 0.525834858417511\u001b[0m\n",
            "\u001b[92mTest accuracy: 7969/10000 =  79.69 % ||| loss 0.5621476769447327\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.55118645966053\n",
            "Batch #200 Loss: 0.532233159840107\n",
            "Batch #300 Loss: 0.5332108667492866\n",
            "\u001b[92mTrain accuracy: 38894/48000 =  81.03 % ||| loss 0.5318528413772583\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9773/12000 =  81.44 % ||| loss 0.5307603478431702\u001b[0m\n",
            "\u001b[92mTest accuracy: 7994/10000 =  79.94 % ||| loss 0.5552552342414856\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.5263514912128449\n",
            "Batch #200 Loss: 0.537410882115364\n",
            "Batch #300 Loss: 0.5376819071173667\n",
            "\u001b[92mTrain accuracy: 38771/48000 =  80.77 % ||| loss 0.5267223119735718\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9710/12000 =  80.92 % ||| loss 0.521716296672821\u001b[0m\n",
            "\u001b[92mTest accuracy: 7963/10000 =  79.63 % ||| loss 0.553563117980957\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5158244815468788\n",
            "Batch #200 Loss: 0.5193013948202133\n",
            "Batch #300 Loss: 0.5266737458109856\n",
            "\u001b[92mTrain accuracy: 38939/48000 =  81.12 % ||| loss 0.5212766528129578\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9773/12000 =  81.44 % ||| loss 0.5171797275543213\u001b[0m\n",
            "\u001b[92mTest accuracy: 8016/10000 =  80.16 % ||| loss 0.5474746823310852\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.5030240181088448\n",
            "Batch #200 Loss: 0.5224376034736633\n",
            "Batch #300 Loss: 0.516906076669693\n",
            "\u001b[92mTrain accuracy: 39360/48000 =  82.0 % ||| loss 0.5061905384063721\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9887/12000 =  82.39 % ||| loss 0.499714195728302\u001b[0m\n",
            "\u001b[92mTest accuracy: 8087/10000 =  80.87 % ||| loss 0.5422390103340149\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.5032268676161766\n",
            "Batch #200 Loss: 0.5109628728032112\n",
            "Batch #300 Loss: 0.5090473541617393\n",
            "\u001b[92mTrain accuracy: 39578/48000 =  82.45 % ||| loss 0.49257156252861023\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9948/12000 =  82.9 % ||| loss 0.49198824167251587\u001b[0m\n",
            "\u001b[92mTest accuracy: 8153/10000 =  81.53 % ||| loss 0.51613450050354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_22</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_22' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_22</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_122904-Lenet5Decay_1726155046.322212_23</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_23' target=\"_blank\">Lenet5Decay_1726155046.322212_23</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_23' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_23</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.9, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3035977363586424\n",
            "Batch #200 Loss: 2.3018246150016783\n",
            "Batch #300 Loss: 2.2993291997909546\n",
            "\u001b[92mTrain accuracy: 8063/48000 =  16.8 % ||| loss 2.2953274250030518\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1962/12000 =  16.35 % ||| loss 2.295207977294922\u001b[0m\n",
            "\u001b[92mTest accuracy: 1670/10000 =  16.7 % ||| loss 2.2953457832336426\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.293723087310791\n",
            "Batch #200 Loss: 2.287320840358734\n",
            "Batch #300 Loss: 2.2755513215065\n",
            "\u001b[92mTrain accuracy: 9545/48000 =  19.89 % ||| loss 2.2353646755218506\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2387/12000 =  19.89 % ||| loss 2.2349741458892822\u001b[0m\n",
            "\u001b[92mTest accuracy: 1972/10000 =  19.72 % ||| loss 2.2357800006866455\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.1868382906913757\n",
            "Batch #200 Loss: 1.8445884525775909\n",
            "Batch #300 Loss: 1.164568340778351\n",
            "\u001b[92mTrain accuracy: 30426/48000 =  63.39 % ||| loss 0.9536919593811035\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7637/12000 =  63.64 % ||| loss 0.9474753737449646\u001b[0m\n",
            "\u001b[92mTest accuracy: 6252/10000 =  62.52 % ||| loss 0.977480947971344\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 0.9413479572534561\n",
            "Batch #200 Loss: 0.8706470400094986\n",
            "Batch #300 Loss: 0.8410071855783463\n",
            "\u001b[92mTrain accuracy: 33413/48000 =  69.61 % ||| loss 0.7922479510307312\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8379/12000 =  69.83 % ||| loss 0.7834845781326294\u001b[0m\n",
            "\u001b[92mTest accuracy: 6829/10000 =  68.29 % ||| loss 0.813827633857727\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 0.7792257994413376\n",
            "Batch #200 Loss: 0.7533124196529388\n",
            "Batch #300 Loss: 0.7473183530569076\n",
            "\u001b[92mTrain accuracy: 35363/48000 =  73.67 % ||| loss 0.727705717086792\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8857/12000 =  73.81 % ||| loss 0.7229658365249634\u001b[0m\n",
            "\u001b[92mTest accuracy: 7336/10000 =  73.36 % ||| loss 0.7498617172241211\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.7095851981639862\n",
            "Batch #200 Loss: 0.6796864497661591\n",
            "Batch #300 Loss: 0.6807771641016006\n",
            "\u001b[92mTrain accuracy: 35789/48000 =  74.56 % ||| loss 0.6652758121490479\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8987/12000 =  74.89 % ||| loss 0.6577333211898804\u001b[0m\n",
            "\u001b[92mTest accuracy: 7354/10000 =  73.54 % ||| loss 0.6890723705291748\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.6714233148097992\n",
            "Batch #200 Loss: 0.6380964821577072\n",
            "Batch #300 Loss: 0.6288616874814034\n",
            "\u001b[92mTrain accuracy: 37337/48000 =  77.79 % ||| loss 0.6112912893295288\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9385/12000 =  78.21 % ||| loss 0.6038975715637207\u001b[0m\n",
            "\u001b[92mTest accuracy: 7667/10000 =  76.67 % ||| loss 0.6351152658462524\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.6303137049078942\n",
            "Batch #200 Loss: 0.6366560363769531\n",
            "Batch #300 Loss: 0.6116239556670189\n",
            "\u001b[92mTrain accuracy: 37242/48000 =  77.59 % ||| loss 0.5985587239265442\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9368/12000 =  78.07 % ||| loss 0.5913282632827759\u001b[0m\n",
            "\u001b[92mTest accuracy: 7639/10000 =  76.39 % ||| loss 0.626194179058075\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.6118936666846275\n",
            "Batch #200 Loss: 0.595518539249897\n",
            "Batch #300 Loss: 0.5639926794171334\n",
            "\u001b[92mTrain accuracy: 37901/48000 =  78.96 % ||| loss 0.569591224193573\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9484/12000 =  79.03 % ||| loss 0.5614893436431885\u001b[0m\n",
            "\u001b[92mTest accuracy: 7802/10000 =  78.02 % ||| loss 0.5899344682693481\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.5755291241407394\n",
            "Batch #200 Loss: 0.5606994277238846\n",
            "Batch #300 Loss: 0.5601457446813584\n",
            "\u001b[92mTrain accuracy: 37950/48000 =  79.06 % ||| loss 0.5596016049385071\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9568/12000 =  79.73 % ||| loss 0.5486773252487183\u001b[0m\n",
            "\u001b[92mTest accuracy: 7789/10000 =  77.89 % ||| loss 0.5791866183280945\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.5501910042762757\n",
            "Batch #200 Loss: 0.5495852044224739\n",
            "Batch #300 Loss: 0.5375953024625778\n",
            "\u001b[92mTrain accuracy: 38970/48000 =  81.19 % ||| loss 0.522749662399292\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9777/12000 =  81.47 % ||| loss 0.5133094191551208\u001b[0m\n",
            "\u001b[92mTest accuracy: 8004/10000 =  80.04 % ||| loss 0.5470086336135864\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.5387031975388527\n",
            "Batch #200 Loss: 0.5175499162077903\n",
            "Batch #300 Loss: 0.5226024731993675\n",
            "\u001b[92mTrain accuracy: 39280/48000 =  81.83 % ||| loss 0.497704416513443\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9837/12000 =  81.97 % ||| loss 0.4916309714317322\u001b[0m\n",
            "\u001b[92mTest accuracy: 8087/10000 =  80.87 % ||| loss 0.5236764550209045\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.5129821890592575\n",
            "Batch #200 Loss: 0.5071623834967613\n",
            "Batch #300 Loss: 0.5007615223526954\n",
            "\u001b[92mTrain accuracy: 39770/48000 =  82.85 % ||| loss 0.48157328367233276\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10005/12000 =  83.38 % ||| loss 0.47759583592414856\u001b[0m\n",
            "\u001b[92mTest accuracy: 8169/10000 =  81.69 % ||| loss 0.5058579444885254\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.5016277679800987\n",
            "Batch #200 Loss: 0.4910080650448799\n",
            "Batch #300 Loss: 0.47393550246953964\n",
            "\u001b[92mTrain accuracy: 39551/48000 =  82.4 % ||| loss 0.487498939037323\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9895/12000 =  82.46 % ||| loss 0.4849836528301239\u001b[0m\n",
            "\u001b[92mTest accuracy: 8147/10000 =  81.47 % ||| loss 0.5114889740943909\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.47910186618566514\n",
            "Batch #200 Loss: 0.47635254949331285\n",
            "Batch #300 Loss: 0.46398515403270724\n",
            "\u001b[92mTrain accuracy: 40062/48000 =  83.46 % ||| loss 0.4616379737854004\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9992/12000 =  83.27 % ||| loss 0.45812341570854187\u001b[0m\n",
            "\u001b[92mTest accuracy: 8246/10000 =  82.46 % ||| loss 0.4825390577316284\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.4572898417711258\n",
            "Batch #200 Loss: 0.4697889831662178\n",
            "Batch #300 Loss: 0.4592254596948624\n",
            "\u001b[92mTrain accuracy: 40458/48000 =  84.29 % ||| loss 0.44155094027519226\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10137/12000 =  84.47 % ||| loss 0.44085174798965454\u001b[0m\n",
            "\u001b[92mTest accuracy: 8335/10000 =  83.35 % ||| loss 0.4682875871658325\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.45720473855733873\n",
            "Batch #200 Loss: 0.44987964391708374\n",
            "Batch #300 Loss: 0.441004179418087\n",
            "\u001b[92mTrain accuracy: 39245/48000 =  81.76 % ||| loss 0.49327778816223145\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9809/12000 =  81.74 % ||| loss 0.4956508278846741\u001b[0m\n",
            "\u001b[92mTest accuracy: 8093/10000 =  80.93 % ||| loss 0.5175904631614685\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.4432530257105827\n",
            "Batch #200 Loss: 0.4372763502597809\n",
            "Batch #300 Loss: 0.4349270489811897\n",
            "\u001b[92mTrain accuracy: 40711/48000 =  84.81 % ||| loss 0.42706674337387085\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10181/12000 =  84.84 % ||| loss 0.42718663811683655\u001b[0m\n",
            "\u001b[92mTest accuracy: 8377/10000 =  83.77 % ||| loss 0.4612502455711365\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.431655592918396\n",
            "Batch #200 Loss: 0.4418057045340538\n",
            "Batch #300 Loss: 0.41556759119033815\n",
            "\u001b[92mTrain accuracy: 40806/48000 =  85.01 % ||| loss 0.42027297616004944\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10203/12000 =  85.02 % ||| loss 0.42300766706466675\u001b[0m\n",
            "\u001b[92mTest accuracy: 8439/10000 =  84.39 % ||| loss 0.4467945694923401\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.42054262906312945\n",
            "Batch #200 Loss: 0.41138936936855314\n",
            "Batch #300 Loss: 0.4253574311733246\n",
            "\u001b[92mTrain accuracy: 41089/48000 =  85.6 % ||| loss 0.40885090827941895\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10231/12000 =  85.26 % ||| loss 0.41012516617774963\u001b[0m\n",
            "\u001b[92mTest accuracy: 8467/10000 =  84.67 % ||| loss 0.4321402907371521\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.4160630328953266\n",
            "Batch #200 Loss: 0.41548011630773546\n",
            "Batch #300 Loss: 0.40542334988713263\n",
            "\u001b[92mTrain accuracy: 40930/48000 =  85.27 % ||| loss 0.41454410552978516\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10224/12000 =  85.2 % ||| loss 0.41743695735931396\u001b[0m\n",
            "\u001b[92mTest accuracy: 8428/10000 =  84.28 % ||| loss 0.44421425461769104\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.41137720733880995\n",
            "Batch #200 Loss: 0.39841571837663653\n",
            "Batch #300 Loss: 0.39852127015590666\n",
            "\u001b[92mTrain accuracy: 40461/48000 =  84.29 % ||| loss 0.42847681045532227\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10097/12000 =  84.14 % ||| loss 0.43408897519111633\u001b[0m\n",
            "\u001b[92mTest accuracy: 8351/10000 =  83.51 % ||| loss 0.4525143504142761\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.40076142191886904\n",
            "Batch #200 Loss: 0.4051811085641384\n",
            "Batch #300 Loss: 0.39154465615749356\n",
            "\u001b[92mTrain accuracy: 41390/48000 =  86.23 % ||| loss 0.38594624400138855\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10336/12000 =  86.13 % ||| loss 0.3907870352268219\u001b[0m\n",
            "\u001b[92mTest accuracy: 8528/10000 =  85.28 % ||| loss 0.41632118821144104\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.3889739941060543\n",
            "Batch #200 Loss: 0.3900627653300762\n",
            "Batch #300 Loss: 0.39287854298949243\n",
            "\u001b[92mTrain accuracy: 41420/48000 =  86.29 % ||| loss 0.38136279582977295\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10291/12000 =  85.76 % ||| loss 0.38898515701293945\u001b[0m\n",
            "\u001b[92mTest accuracy: 8512/10000 =  85.12 % ||| loss 0.41233333945274353\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.3959137123823166\n",
            "Batch #200 Loss: 0.3793526890873909\n",
            "Batch #300 Loss: 0.37627303779125215\n",
            "\u001b[92mTrain accuracy: 41599/48000 =  86.66 % ||| loss 0.37431493401527405\u001b[0m\n",
            "\u001b[92mValidation accuracy: 10373/12000 =  86.44 % ||| loss 0.3835461437702179\u001b[0m\n",
            "\u001b[92mTest accuracy: 8536/10000 =  85.36 % ||| loss 0.4028250575065613\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_23</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_23' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_23</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_123132-Lenet5Decay_1726155046.322212_24</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_24' target=\"_blank\">Lenet5Decay_1726155046.322212_24</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_24' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_24</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7, 'weight_decay': 0.1}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.3040493750572204\n",
            "Batch #200 Loss: 2.303790192604065\n",
            "Batch #300 Loss: 2.302453725337982\n",
            "\u001b[92mTrain accuracy: 4802/48000 =  10.0 % ||| loss 2.3022279739379883\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1170/12000 =  9.75 % ||| loss 2.302293300628662\u001b[0m\n",
            "\u001b[92mTest accuracy: 995/10000 =  9.95 % ||| loss 2.302304267883301\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3021425366401673\n",
            "Batch #200 Loss: 2.301898844242096\n",
            "Batch #300 Loss: 2.3017125082015992\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.301762580871582\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.301849603652954\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.301847219467163\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.301866953372955\n",
            "Batch #200 Loss: 2.301884799003601\n",
            "Batch #300 Loss: 2.301716742515564\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.301823139190674\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.301912546157837\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30185866355896\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.3017218923568725\n",
            "Batch #200 Loss: 2.3018198585510254\n",
            "Batch #300 Loss: 2.301963217258453\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302018165588379\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3021228313446045\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30202579498291\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.302201769351959\n",
            "Batch #200 Loss: 2.30190642118454\n",
            "Batch #300 Loss: 2.302128324508667\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302213430404663\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3023104667663574\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3022301197052\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.3022580671310426\n",
            "Batch #200 Loss: 2.3021149706840514\n",
            "Batch #300 Loss: 2.302462360858917\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3023593425750732\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3024585247039795\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3023765087127686\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 2.302344923019409\n",
            "Batch #200 Loss: 2.30241117477417\n",
            "Batch #300 Loss: 2.302551341056824\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3024516105651855\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302543878555298\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30249285697937\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 2.302598404884338\n",
            "Batch #200 Loss: 2.3023374462127686\n",
            "Batch #300 Loss: 2.3025514674186707\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302508592605591\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302600860595703\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025002479553223\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 2.3025522804260254\n",
            "Batch #200 Loss: 2.3024700808525087\n",
            "Batch #300 Loss: 2.302573068141937\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025388717651367\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026304244995117\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025660514831543\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 2.302619023323059\n",
            "Batch #200 Loss: 2.302573139667511\n",
            "Batch #300 Loss: 2.3025234031677244\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025574684143066\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026492595672607\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025691509246826\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 2.302604246139526\n",
            "Batch #200 Loss: 2.302588691711426\n",
            "Batch #300 Loss: 2.3025380182266235\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302565336227417\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026537895202637\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025898933410645\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 2.30261914730072\n",
            "Batch #200 Loss: 2.302577278614044\n",
            "Batch #300 Loss: 2.3025852727890013\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025712966918945\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026559352874756\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025903701782227\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 2.302600862979889\n",
            "Batch #200 Loss: 2.302585475444794\n",
            "Batch #300 Loss: 2.3025747776031493\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302572727203369\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302656412124634\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025927543640137\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 2.302554638385773\n",
            "Batch #200 Loss: 2.3026021218299864\n",
            "Batch #300 Loss: 2.3025857162475587\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025729656219482\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026552200317383\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302612543106079\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 2.3025720596313475\n",
            "Batch #200 Loss: 2.3026159691810606\n",
            "Batch #300 Loss: 2.302568678855896\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026530742645264\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302593469619751\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 2.3025924611091613\n",
            "Batch #200 Loss: 2.302600426673889\n",
            "Batch #300 Loss: 2.3025383305549623\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025729656219482\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302652359008789\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025968074798584\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 2.3025573539733886\n",
            "Batch #200 Loss: 2.3025533890724184\n",
            "Batch #300 Loss: 2.3026139998435973\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302647113800049\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.30258846282959\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 2.302607696056366\n",
            "Batch #200 Loss: 2.302602515220642\n",
            "Batch #300 Loss: 2.3025715565681457\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025741577148438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302649736404419\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025972843170166\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 2.302577564716339\n",
            "Batch #200 Loss: 2.302600610256195\n",
            "Batch #300 Loss: 2.302596085071564\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302645683288574\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3025918006896973\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 2.3025749564170837\n",
            "Batch #200 Loss: 2.3025837922096253\n",
            "Batch #300 Loss: 2.3025901079177857\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025741577148438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302647590637207\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302581548690796\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 2.3025785207748415\n",
            "Batch #200 Loss: 2.3025988960266113\n",
            "Batch #300 Loss: 2.3025716280937196\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.30257511138916\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302645206451416\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302579402923584\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 2.302576410770416\n",
            "Batch #200 Loss: 2.302568030357361\n",
            "Batch #300 Loss: 2.302611744403839\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025755882263184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302644968032837\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302598476409912\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 2.3026037931442263\n",
            "Batch #200 Loss: 2.3025435471534728\n",
            "Batch #300 Loss: 2.3025843024253847\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.302574634552002\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.3026444911956787\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302584409713745\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 2.302581503391266\n",
            "Batch #200 Loss: 2.302578113079071\n",
            "Batch #300 Loss: 2.302567286491394\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025755882263184\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302642822265625\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.302598476409912\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 2.302581310272217\n",
            "Batch #200 Loss: 2.302568690776825\n",
            "Batch #300 Loss: 2.3026071095466616\n",
            "\u001b[92mTrain accuracy: 4826/48000 =  10.05 % ||| loss 2.3025741577148438\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1174/12000 =  9.783 % ||| loss 2.302643060684204\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.3026018142700195\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_24</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_24' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_24</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_123403-Lenet5Decay_1726155046.322212_25</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_25' target=\"_blank\">Lenet5Decay_1726155046.322212_25</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_25' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_25</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7, 'weight_decay': 0.01}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.303686125278473\n",
            "Batch #200 Loss: 2.3005492901802063\n",
            "Batch #300 Loss: 2.2979230380058286\n",
            "\u001b[92mTrain accuracy: 7269/48000 =  15.14 % ||| loss 2.2946581840515137\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1808/12000 =  15.07 % ||| loss 2.294670820236206\u001b[0m\n",
            "\u001b[92mTest accuracy: 1536/10000 =  15.36 % ||| loss 2.294881582260132\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.293001618385315\n",
            "Batch #200 Loss: 2.2897812867164613\n",
            "Batch #300 Loss: 2.285141098499298\n",
            "\u001b[92mTrain accuracy: 15374/48000 =  32.03 % ||| loss 2.2783846855163574\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3850/12000 =  32.08 % ||| loss 2.278428792953491\u001b[0m\n",
            "\u001b[92mTest accuracy: 3186/10000 =  31.86 % ||| loss 2.2783827781677246\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.274685966968536\n",
            "Batch #200 Loss: 2.2650090742111204\n",
            "Batch #300 Loss: 2.249443378448486\n",
            "\u001b[92mTrain accuracy: 12369/48000 =  25.77 % ||| loss 2.218036413192749\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3057/12000 =  25.47 % ||| loss 2.218061685562134\u001b[0m\n",
            "\u001b[92mTest accuracy: 2593/10000 =  25.93 % ||| loss 2.2185332775115967\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.1969409441947936\n",
            "Batch #200 Loss: 2.1316953921318054\n",
            "Batch #300 Loss: 2.001014406681061\n",
            "\u001b[92mTrain accuracy: 25850/48000 =  53.85 % ||| loss 1.663684606552124\u001b[0m\n",
            "\u001b[92mValidation accuracy: 6479/12000 =  53.99 % ||| loss 1.66305673122406\u001b[0m\n",
            "\u001b[92mTest accuracy: 5347/10000 =  53.47 % ||| loss 1.664503812789917\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 1.513215423822403\n",
            "Batch #200 Loss: 1.2608013725280762\n",
            "Batch #300 Loss: 1.0943879163265229\n",
            "\u001b[92mTrain accuracy: 30125/48000 =  62.76 % ||| loss 1.0044363737106323\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7522/12000 =  62.68 % ||| loss 0.9982086420059204\u001b[0m\n",
            "\u001b[92mTest accuracy: 6169/10000 =  61.69 % ||| loss 1.0135769844055176\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 0.9865452015399933\n",
            "Batch #200 Loss: 0.9498824447393417\n",
            "Batch #300 Loss: 0.9275488662719726\n",
            "\u001b[92mTrain accuracy: 31336/48000 =  65.28 % ||| loss 0.9068188667297363\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7857/12000 =  65.48 % ||| loss 0.8979281783103943\u001b[0m\n",
            "\u001b[92mTest accuracy: 6432/10000 =  64.32 % ||| loss 0.9217358231544495\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 0.8987365126609802\n",
            "Batch #200 Loss: 0.883308247923851\n",
            "Batch #300 Loss: 0.8573288422822952\n",
            "\u001b[92mTrain accuracy: 32711/48000 =  68.15 % ||| loss 0.8446506261825562\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8223/12000 =  68.53 % ||| loss 0.8344647288322449\u001b[0m\n",
            "\u001b[92mTest accuracy: 6734/10000 =  67.34 % ||| loss 0.8637968897819519\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 0.8309177923202514\n",
            "Batch #200 Loss: 0.8374214935302734\n",
            "Batch #300 Loss: 0.8259203869104386\n",
            "\u001b[92mTrain accuracy: 33667/48000 =  70.14 % ||| loss 0.8040671944618225\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8465/12000 =  70.54 % ||| loss 0.7953203916549683\u001b[0m\n",
            "\u001b[92mTest accuracy: 6971/10000 =  69.71 % ||| loss 0.82253497838974\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.795250461101532\n",
            "Batch #200 Loss: 0.7970401847362518\n",
            "Batch #300 Loss: 0.7956326925754547\n",
            "\u001b[92mTrain accuracy: 33259/48000 =  69.29 % ||| loss 0.7838029861450195\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8338/12000 =  69.48 % ||| loss 0.7729905843734741\u001b[0m\n",
            "\u001b[92mTest accuracy: 6862/10000 =  68.62 % ||| loss 0.8048833608627319\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.7763925355672836\n",
            "Batch #200 Loss: 0.773011047244072\n",
            "Batch #300 Loss: 0.7696602648496628\n",
            "\u001b[92mTrain accuracy: 33927/48000 =  70.68 % ||| loss 0.7627017498016357\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8552/12000 =  71.27 % ||| loss 0.7551077008247375\u001b[0m\n",
            "\u001b[92mTest accuracy: 7017/10000 =  70.17 % ||| loss 0.780235767364502\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.765444700717926\n",
            "Batch #200 Loss: 0.7438452684879303\n",
            "Batch #300 Loss: 0.7504217541217804\n",
            "\u001b[92mTrain accuracy: 34490/48000 =  71.85 % ||| loss 0.7589380145072937\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8594/12000 =  71.62 % ||| loss 0.753709614276886\u001b[0m\n",
            "\u001b[92mTest accuracy: 7086/10000 =  70.86 % ||| loss 0.7833378911018372\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.7430017673969269\n",
            "Batch #200 Loss: 0.750124037861824\n",
            "Batch #300 Loss: 0.7322961956262588\n",
            "\u001b[92mTrain accuracy: 34694/48000 =  72.28 % ||| loss 0.7260274887084961\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8691/12000 =  72.42 % ||| loss 0.7181885838508606\u001b[0m\n",
            "\u001b[92mTest accuracy: 7121/10000 =  71.21 % ||| loss 0.7503345608711243\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.7384321540594101\n",
            "Batch #200 Loss: 0.7221711337566376\n",
            "Batch #300 Loss: 0.7248039215803146\n",
            "\u001b[92mTrain accuracy: 35374/48000 =  73.7 % ||| loss 0.7075117230415344\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8883/12000 =  74.02 % ||| loss 0.6985633373260498\u001b[0m\n",
            "\u001b[92mTest accuracy: 7336/10000 =  73.36 % ||| loss 0.7276597023010254\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.7140992987155914\n",
            "Batch #200 Loss: 0.7209736752510071\n",
            "Batch #300 Loss: 0.6996119970083237\n",
            "\u001b[92mTrain accuracy: 35492/48000 =  73.94 % ||| loss 0.7047550678253174\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8890/12000 =  74.08 % ||| loss 0.6952212452888489\u001b[0m\n",
            "\u001b[92mTest accuracy: 7305/10000 =  73.05 % ||| loss 0.7251262664794922\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.7179440253973007\n",
            "Batch #200 Loss: 0.6879410421848298\n",
            "Batch #300 Loss: 0.6945983183383941\n",
            "\u001b[92mTrain accuracy: 35966/48000 =  74.93 % ||| loss 0.6803321242332458\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8989/12000 =  74.91 % ||| loss 0.6720841526985168\u001b[0m\n",
            "\u001b[92mTest accuracy: 7408/10000 =  74.08 % ||| loss 0.6991598606109619\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.6906916576623917\n",
            "Batch #200 Loss: 0.6762009817361831\n",
            "Batch #300 Loss: 0.6961340278387069\n",
            "\u001b[92mTrain accuracy: 35814/48000 =  74.61 % ||| loss 0.6788768172264099\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8991/12000 =  74.92 % ||| loss 0.6696767210960388\u001b[0m\n",
            "\u001b[92mTest accuracy: 7343/10000 =  73.43 % ||| loss 0.6964504718780518\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.6798289477825165\n",
            "Batch #200 Loss: 0.659749589562416\n",
            "Batch #300 Loss: 0.6750278633832931\n",
            "\u001b[92mTrain accuracy: 35149/48000 =  73.23 % ||| loss 0.690966010093689\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8799/12000 =  73.32 % ||| loss 0.6844909191131592\u001b[0m\n",
            "\u001b[92mTest accuracy: 7299/10000 =  72.99 % ||| loss 0.7160542607307434\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.6754046303033828\n",
            "Batch #200 Loss: 0.66382208943367\n",
            "Batch #300 Loss: 0.6660254266858101\n",
            "\u001b[92mTrain accuracy: 36368/48000 =  75.77 % ||| loss 0.6515600681304932\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9101/12000 =  75.84 % ||| loss 0.6464393138885498\u001b[0m\n",
            "\u001b[92mTest accuracy: 7485/10000 =  74.85 % ||| loss 0.6786810755729675\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.6668446895480156\n",
            "Batch #200 Loss: 0.646756010055542\n",
            "Batch #300 Loss: 0.6573773470520973\n",
            "\u001b[92mTrain accuracy: 36361/48000 =  75.75 % ||| loss 0.6469101309776306\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9116/12000 =  75.97 % ||| loss 0.6383126974105835\u001b[0m\n",
            "\u001b[92mTest accuracy: 7490/10000 =  74.9 % ||| loss 0.6690484881401062\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.6601911959052086\n",
            "Batch #200 Loss: 0.6399595308303833\n",
            "Batch #300 Loss: 0.6381023621559143\n",
            "\u001b[92mTrain accuracy: 36465/48000 =  75.97 % ||| loss 0.6394581198692322\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9167/12000 =  76.39 % ||| loss 0.6334301829338074\u001b[0m\n",
            "\u001b[92mTest accuracy: 7516/10000 =  75.16 % ||| loss 0.6611638069152832\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.6308411797881126\n",
            "Batch #200 Loss: 0.637598232626915\n",
            "Batch #300 Loss: 0.6396155843138694\n",
            "\u001b[92mTrain accuracy: 36905/48000 =  76.89 % ||| loss 0.6242974400520325\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9241/12000 =  77.01 % ||| loss 0.6168476939201355\u001b[0m\n",
            "\u001b[92mTest accuracy: 7569/10000 =  75.69 % ||| loss 0.64683598279953\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.6351645457744598\n",
            "Batch #200 Loss: 0.6235248962044716\n",
            "Batch #300 Loss: 0.6321012136340142\n",
            "\u001b[92mTrain accuracy: 37067/48000 =  77.22 % ||| loss 0.6212143301963806\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9296/12000 =  77.47 % ||| loss 0.6124340891838074\u001b[0m\n",
            "\u001b[92mTest accuracy: 7639/10000 =  76.39 % ||| loss 0.6419044733047485\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.6261451143026352\n",
            "Batch #200 Loss: 0.6339755886793137\n",
            "Batch #300 Loss: 0.6225266000628471\n",
            "\u001b[92mTrain accuracy: 37223/48000 =  77.55 % ||| loss 0.61553555727005\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9314/12000 =  77.62 % ||| loss 0.6092462539672852\u001b[0m\n",
            "\u001b[92mTest accuracy: 7684/10000 =  76.84 % ||| loss 0.6386847496032715\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.6146493867039681\n",
            "Batch #200 Loss: 0.6268239790201187\n",
            "Batch #300 Loss: 0.6127986580133438\n",
            "\u001b[92mTrain accuracy: 37181/48000 =  77.46 % ||| loss 0.617056667804718\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9344/12000 =  77.87 % ||| loss 0.6127116084098816\u001b[0m\n",
            "\u001b[92mTest accuracy: 7626/10000 =  76.26 % ||| loss 0.6370787024497986\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.6167602986097336\n",
            "Batch #200 Loss: 0.6171197509765625\n",
            "Batch #300 Loss: 0.6104639130830765\n",
            "\u001b[92mTrain accuracy: 37712/48000 =  78.57 % ||| loss 0.6004095077514648\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9407/12000 =  78.39 % ||| loss 0.5965359210968018\u001b[0m\n",
            "\u001b[92mTest accuracy: 7745/10000 =  77.45 % ||| loss 0.6253148913383484\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_25</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_25' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_25</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-1/wandb/run-20240912_123636-Lenet5Decay_1726155046.322212_26</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_26' target=\"_blank\">Lenet5Decay_1726155046.322212_26</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_26' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_26</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XXXXXXXX Tuning Network Lenet5Decay XXXXXXXXX\n",
            "Hyperparameter Config: {'learning_rate': 0.001, 'momentum': 0.7, 'weight_decay': 0.001}\n",
            "----------- Epoch #1 ------------\n",
            "Batch #100 Loss: 2.304697024822235\n",
            "Batch #200 Loss: 2.3035178804397582\n",
            "Batch #300 Loss: 2.303333616256714\n",
            "\u001b[92mTrain accuracy: 4792/48000 =  9.983 % ||| loss 2.301349401473999\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1208/12000 =  10.07 % ||| loss 2.301069974899292\u001b[0m\n",
            "\u001b[92mTest accuracy: 1000/10000 =  10.0 % ||| loss 2.301522731781006\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2 ------------\n",
            "Batch #100 Loss: 2.3020508766174315\n",
            "Batch #200 Loss: 2.298968029022217\n",
            "Batch #300 Loss: 2.2981632924079896\n",
            "\u001b[92mTrain accuracy: 7228/48000 =  15.06 % ||| loss 2.29653000831604\u001b[0m\n",
            "\u001b[92mValidation accuracy: 1842/12000 =  15.35 % ||| loss 2.2962634563446045\u001b[0m\n",
            "\u001b[92mTest accuracy: 1507/10000 =  15.07 % ||| loss 2.296448230743408\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3 ------------\n",
            "Batch #100 Loss: 2.2951358795166015\n",
            "Batch #200 Loss: 2.2942628479003906\n",
            "Batch #300 Loss: 2.2918601775169374\n",
            "\u001b[92mTrain accuracy: 8992/48000 =  18.73 % ||| loss 2.2883756160736084\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2272/12000 =  18.93 % ||| loss 2.288137674331665\u001b[0m\n",
            "\u001b[92mTest accuracy: 1864/10000 =  18.64 % ||| loss 2.288440465927124\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4 ------------\n",
            "Batch #100 Loss: 2.2869634795188905\n",
            "Batch #200 Loss: 2.2825344252586364\n",
            "Batch #300 Loss: 2.278487038612366\n",
            "\u001b[92mTrain accuracy: 12206/48000 =  25.43 % ||| loss 2.2703545093536377\u001b[0m\n",
            "\u001b[92mValidation accuracy: 3033/12000 =  25.27 % ||| loss 2.270179033279419\u001b[0m\n",
            "\u001b[92mTest accuracy: 2501/10000 =  25.01 % ||| loss 2.270388603210449\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5 ------------\n",
            "Batch #100 Loss: 2.26654531955719\n",
            "Batch #200 Loss: 2.255097200870514\n",
            "Batch #300 Loss: 2.2385094213485717\n",
            "\u001b[92mTrain accuracy: 11662/48000 =  24.3 % ||| loss 2.206817626953125\u001b[0m\n",
            "\u001b[92mValidation accuracy: 2928/12000 =  24.4 % ||| loss 2.2066457271575928\u001b[0m\n",
            "\u001b[92mTest accuracy: 2422/10000 =  24.22 % ||| loss 2.207152843475342\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6 ------------\n",
            "Batch #100 Loss: 2.1856684827804567\n",
            "Batch #200 Loss: 2.1307059502601624\n",
            "Batch #300 Loss: 2.0321784484386445\n",
            "\u001b[92mTrain accuracy: 21308/48000 =  44.39 % ||| loss 1.7782078981399536\u001b[0m\n",
            "\u001b[92mValidation accuracy: 5328/12000 =  44.4 % ||| loss 1.7775933742523193\u001b[0m\n",
            "\u001b[92mTest accuracy: 4398/10000 =  43.98 % ||| loss 1.7784134149551392\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7 ------------\n",
            "Batch #100 Loss: 1.6209747540950774\n",
            "Batch #200 Loss: 1.3496893656253814\n",
            "Batch #300 Loss: 1.173371880054474\n",
            "\u001b[92mTrain accuracy: 28227/48000 =  58.81 % ||| loss 1.0542218685150146\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7073/12000 =  58.94 % ||| loss 1.049183964729309\u001b[0m\n",
            "\u001b[92mTest accuracy: 5840/10000 =  58.4 % ||| loss 1.0590214729309082\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8 ------------\n",
            "Batch #100 Loss: 1.0258659839630127\n",
            "Batch #200 Loss: 0.9751688873767853\n",
            "Batch #300 Loss: 0.9585920685529709\n",
            "\u001b[92mTrain accuracy: 31431/48000 =  65.48 % ||| loss 0.910013735294342\u001b[0m\n",
            "\u001b[92mValidation accuracy: 7865/12000 =  65.54 % ||| loss 0.9044955372810364\u001b[0m\n",
            "\u001b[92mTest accuracy: 6472/10000 =  64.72 % ||| loss 0.9218491911888123\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9 ------------\n",
            "Batch #100 Loss: 0.9094917261600495\n",
            "Batch #200 Loss: 0.8664316135644913\n",
            "Batch #300 Loss: 0.8477443492412567\n",
            "\u001b[92mTrain accuracy: 33246/48000 =  69.26 % ||| loss 0.8294657468795776\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8341/12000 =  69.51 % ||| loss 0.8230328559875488\u001b[0m\n",
            "\u001b[92mTest accuracy: 6862/10000 =  68.62 % ||| loss 0.845432698726654\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10 ------------\n",
            "Batch #100 Loss: 0.8137418985366821\n",
            "Batch #200 Loss: 0.7956842750310897\n",
            "Batch #300 Loss: 0.8022154819965363\n",
            "\u001b[92mTrain accuracy: 34285/48000 =  71.43 % ||| loss 0.7752992510795593\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8642/12000 =  72.02 % ||| loss 0.7665541768074036\u001b[0m\n",
            "\u001b[92mTest accuracy: 7107/10000 =  71.07 % ||| loss 0.7847090363502502\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11 ------------\n",
            "Batch #100 Loss: 0.7697786563634872\n",
            "Batch #200 Loss: 0.7616334652900696\n",
            "Batch #300 Loss: 0.7518798351287842\n",
            "\u001b[92mTrain accuracy: 33997/48000 =  70.83 % ||| loss 0.7495549917221069\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8548/12000 =  71.23 % ||| loss 0.7407546639442444\u001b[0m\n",
            "\u001b[92mTest accuracy: 7001/10000 =  70.01 % ||| loss 0.7705842852592468\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12 ------------\n",
            "Batch #100 Loss: 0.7540996372699738\n",
            "Batch #200 Loss: 0.7158639621734619\n",
            "Batch #300 Loss: 0.7276611393690109\n",
            "\u001b[92mTrain accuracy: 35250/48000 =  73.44 % ||| loss 0.7170465588569641\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8849/12000 =  73.74 % ||| loss 0.7071807384490967\u001b[0m\n",
            "\u001b[92mTest accuracy: 7288/10000 =  72.88 % ||| loss 0.7350451946258545\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13 ------------\n",
            "Batch #100 Loss: 0.7118697869777679\n",
            "Batch #200 Loss: 0.6999428874254227\n",
            "Batch #300 Loss: 0.7218980830907822\n",
            "\u001b[92mTrain accuracy: 35083/48000 =  73.09 % ||| loss 0.704789400100708\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8817/12000 =  73.47 % ||| loss 0.6943743228912354\u001b[0m\n",
            "\u001b[92mTest accuracy: 7237/10000 =  72.37 % ||| loss 0.7191806435585022\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14 ------------\n",
            "Batch #100 Loss: 0.6985511595010757\n",
            "Batch #200 Loss: 0.7004996323585511\n",
            "Batch #300 Loss: 0.6946376994252205\n",
            "\u001b[92mTrain accuracy: 35425/48000 =  73.8 % ||| loss 0.6846541166305542\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8907/12000 =  74.22 % ||| loss 0.6744961738586426\u001b[0m\n",
            "\u001b[92mTest accuracy: 7316/10000 =  73.16 % ||| loss 0.7130517959594727\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15 ------------\n",
            "Batch #100 Loss: 0.6794956529140472\n",
            "Batch #200 Loss: 0.6860568770766258\n",
            "Batch #300 Loss: 0.6880871564149856\n",
            "\u001b[92mTrain accuracy: 35795/48000 =  74.57 % ||| loss 0.6786594986915588\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9002/12000 =  75.02 % ||| loss 0.669540286064148\u001b[0m\n",
            "\u001b[92mTest accuracy: 7408/10000 =  74.08 % ||| loss 0.6947128772735596\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16 ------------\n",
            "Batch #100 Loss: 0.678775560259819\n",
            "Batch #200 Loss: 0.6697640687227249\n",
            "Batch #300 Loss: 0.6691075211763382\n",
            "\u001b[92mTrain accuracy: 35510/48000 =  73.98 % ||| loss 0.6675829887390137\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8948/12000 =  74.57 % ||| loss 0.658912718296051\u001b[0m\n",
            "\u001b[92mTest accuracy: 7288/10000 =  72.88 % ||| loss 0.6881189942359924\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17 ------------\n",
            "Batch #100 Loss: 0.6622916144132615\n",
            "Batch #200 Loss: 0.6521961230039597\n",
            "Batch #300 Loss: 0.6592512574791908\n",
            "\u001b[92mTrain accuracy: 34844/48000 =  72.59 % ||| loss 0.6852655410766602\u001b[0m\n",
            "\u001b[92mValidation accuracy: 8796/12000 =  73.3 % ||| loss 0.6762001514434814\u001b[0m\n",
            "\u001b[92mTest accuracy: 7181/10000 =  71.81 % ||| loss 0.7132859826087952\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18 ------------\n",
            "Batch #100 Loss: 0.6458363631367683\n",
            "Batch #200 Loss: 0.6527761080861092\n",
            "Batch #300 Loss: 0.6541876995563507\n",
            "\u001b[92mTrain accuracy: 36654/48000 =  76.36 % ||| loss 0.6406329870223999\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9170/12000 =  76.42 % ||| loss 0.6315706968307495\u001b[0m\n",
            "\u001b[92mTest accuracy: 7549/10000 =  75.49 % ||| loss 0.6640547513961792\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19 ------------\n",
            "Batch #100 Loss: 0.6469581109285355\n",
            "Batch #200 Loss: 0.6537931641936302\n",
            "Batch #300 Loss: 0.636932113468647\n",
            "\u001b[92mTrain accuracy: 36084/48000 =  75.17 % ||| loss 0.6376067996025085\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9060/12000 =  75.5 % ||| loss 0.6277264952659607\u001b[0m\n",
            "\u001b[92mTest accuracy: 7421/10000 =  74.21 % ||| loss 0.6603634357452393\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20 ------------\n",
            "Batch #100 Loss: 0.6459114268422127\n",
            "Batch #200 Loss: 0.638199134171009\n",
            "Batch #300 Loss: 0.6279430666565895\n",
            "\u001b[92mTrain accuracy: 36718/48000 =  76.5 % ||| loss 0.6239839792251587\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9211/12000 =  76.76 % ||| loss 0.6162489056587219\u001b[0m\n",
            "\u001b[92mTest accuracy: 7565/10000 =  75.65 % ||| loss 0.6469730138778687\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21 ------------\n",
            "Batch #100 Loss: 0.6222925162315369\n",
            "Batch #200 Loss: 0.62139081209898\n",
            "Batch #300 Loss: 0.6458444815874099\n",
            "\u001b[92mTrain accuracy: 36977/48000 =  77.04 % ||| loss 0.6152709722518921\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9279/12000 =  77.33 % ||| loss 0.6050633192062378\u001b[0m\n",
            "\u001b[92mTest accuracy: 7628/10000 =  76.28 % ||| loss 0.6374941468238831\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22 ------------\n",
            "Batch #100 Loss: 0.6256706035137176\n",
            "Batch #200 Loss: 0.6163925632834435\n",
            "Batch #300 Loss: 0.6082031980156899\n",
            "\u001b[92mTrain accuracy: 36973/48000 =  77.03 % ||| loss 0.6044736504554749\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9268/12000 =  77.23 % ||| loss 0.5970692038536072\u001b[0m\n",
            "\u001b[92mTest accuracy: 7618/10000 =  76.18 % ||| loss 0.6250264048576355\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23 ------------\n",
            "Batch #100 Loss: 0.5982450994849206\n",
            "Batch #200 Loss: 0.6198495149612426\n",
            "Batch #300 Loss: 0.6085398209095001\n",
            "\u001b[92mTrain accuracy: 37379/48000 =  77.87 % ||| loss 0.5979480147361755\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9354/12000 =  77.95 % ||| loss 0.5890765190124512\u001b[0m\n",
            "\u001b[92mTest accuracy: 7693/10000 =  76.93 % ||| loss 0.6214237213134766\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24 ------------\n",
            "Batch #100 Loss: 0.6141890740394592\n",
            "Batch #200 Loss: 0.6023343303799629\n",
            "Batch #300 Loss: 0.5929850745201111\n",
            "\u001b[92mTrain accuracy: 37459/48000 =  78.04 % ||| loss 0.589801013469696\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9379/12000 =  78.16 % ||| loss 0.5799996852874756\u001b[0m\n",
            "\u001b[92mTest accuracy: 7710/10000 =  77.1 % ||| loss 0.6158012747764587\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25 ------------\n",
            "Batch #100 Loss: 0.580413778424263\n",
            "Batch #200 Loss: 0.6048606646060943\n",
            "Batch #300 Loss: 0.5936778035759925\n",
            "\u001b[92mTrain accuracy: 37433/48000 =  77.99 % ||| loss 0.5860491394996643\u001b[0m\n",
            "\u001b[92mValidation accuracy: 9408/12000 =  78.4 % ||| loss 0.5754554271697998\u001b[0m\n",
            "\u001b[92mTest accuracy: 7703/10000 =  77.03 % ||| loss 0.6100056171417236\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Lenet5Decay_1726155046.322212_26</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_26' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1/runs/Lenet5Decay_1726155046.322212_26</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[93m!!!!!!! Hyper Param Tuning Finished!!!!!!!!!!!\u001b[0m\n",
            "Best Model: Lenet5Decay(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "wandb name: Lenet5Decay_1726155046.322212_14\n",
            "\n",
            "HyperParams: {'learning_rate': 0.001, 'momentum': 0.7, 'weight_decay': 0.001}\n",
            "\n",
            "Accuracies: {'train': 0.9164791666666666, 'val': 0.8988333333333334, 'test': 0.8877}\n"
          ]
        }
      ],
      "source": [
        "class Lenet5Decay(Lenet5):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.decay = kwargs['weight_decay']\n",
        "    \n",
        "param_grid = {\n",
        "  'learning_rate':[0.1, 0.01,0.001],\n",
        "  'momentum':[0, 0.9, 0.7],\n",
        "  'weight_decay':[0.1, 0.01, 0.001]\n",
        "}\n",
        "\n",
        "best_weightdecay = hyperparameter_tuning(Lenet5Decay, dataloaders, device, 25, **param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChEh4AqnjmkK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOeqxEEwILJczy2HVXTs3LD",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1lAy_YT2kZxUVJvk7umNcXW3_Ded87R2i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
